[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v1",
                "updated": "2025-01-21T03:13:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v1",
                "updated": "2025-01-20T21:07:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v1",
                "updated": "2025-01-19T17:33:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming often relies on\ncomplex successive interference cancellation (SIC) structures to decode a\nsuperposition of multiple streams received by each user. Traditional\nsignal-level schemes require the regeneration of interfering signals from the\ncache, adding significant computational complexity. To address this, we propose\na bit-level multicast scheduling scheme enabling linear, SIC-free decoding of\nparallel streams by repeatedly transmitting data terms with linearly\nindependent coefficients. Two reference strategies for constructing the\ncoefficients matrix are considered: a random strategy, which lacks control over\nmatrix construction, and an equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the proposed sparse strategy\nminimizes the number of multicast streams transmitted in parallel during each\ninterval, simplifying the system while optimizing resource usage. To further\nenhance the symmetric rate, a successive projection algorithm is applied to\nexploit channel properties and optimize user ordering. With the coefficients\nmatrix and optimized user ordering in place, multicast beamformers are refined\nto aggregate desired data from relevant multicast streams. Numerical\nsimulations validate the effectiveness of the sparse strategy, demonstrating\nsignificant gains in symmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v1",
                "updated": "2025-01-18T19:10:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or \"phantom\" antenna users, bridging the performance gains of the min-G\nand Grouping schemes. These strategies jointly optimize the number of users,\n$\\Omega$, and the parallel streams decoded by each user, $\\beta_k$, ensuring\nlinear decodability for all target users. Analytical and numerical results\nconfirm that the proposed schemes achieve significant DoF improvements across\nvarious system configurations, demonstrating the potential of content-aware\nMIMO-CC strategies for enhancing wireless network performance."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v3",
                "updated": "2025-01-14T20:04:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    20,
                    4,
                    15,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v2",
                "updated": "2025-01-14T02:02:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    2,
                    2,
                    1,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_doi": "10.3847/1538-4365/ad9b8d",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad9b8d",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "arxiv_journal_ref": "Astrophys. j., suppl. ser. 276 (2025) 40",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12387v1",
                "updated": "2025-01-21T18:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    59,
                    23,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:59:23Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    59,
                    23,
                    1,
                    21,
                    0
                ],
                "title": "Continuous 3D Perception Model with Persistent State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous 3D Perception Model with Persistent State"
                },
                "summary": "We present a unified framework capable of solving a broad range of 3D tasks.\nOur approach features a stateful recurrent model that continuously updates its\nstate representation with each new observation. Given a stream of images, this\nevolving state can be used to generate metric-scale pointmaps (per-pixel 3D\npoints) for each new input in an online fashion. These pointmaps reside within\na common coordinate system, and can be accumulated into a coherent, dense scene\nreconstruction that updates as new images arrive. Our model, called CUT3R\n(Continuous Updating Transformer for 3D Reconstruction), captures rich priors\nof real-world scenes: not only can it predict accurate pointmaps from image\nobservations, but it can also infer unseen regions of the scene by probing at\nvirtual, unobserved views. Our method is simple yet highly flexible, naturally\naccepting varying lengths of images that may be either video streams or\nunordered photo collections, containing both static and dynamic content. We\nevaluate our method on various 3D/4D tasks and demonstrate competitive or\nstate-of-the-art performance in each. Project Page: https://cut3r.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a unified framework capable of solving a broad range of 3D tasks.\nOur approach features a stateful recurrent model that continuously updates its\nstate representation with each new observation. Given a stream of images, this\nevolving state can be used to generate metric-scale pointmaps (per-pixel 3D\npoints) for each new input in an online fashion. These pointmaps reside within\na common coordinate system, and can be accumulated into a coherent, dense scene\nreconstruction that updates as new images arrive. Our model, called CUT3R\n(Continuous Updating Transformer for 3D Reconstruction), captures rich priors\nof real-world scenes: not only can it predict accurate pointmaps from image\nobservations, but it can also infer unseen regions of the scene by probing at\nvirtual, unobserved views. Our method is simple yet highly flexible, naturally\naccepting varying lengths of images that may be either video streams or\nunordered photo collections, containing both static and dynamic content. We\nevaluate our method on various 3D/4D tasks and demonstrate competitive or\nstate-of-the-art performance in each. Project Page: https://cut3r.github.io/"
                },
                "authors": [
                    {
                        "name": "Qianqian Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Aleksander Holynski"
                    },
                    {
                        "name": "Alexei A. Efros"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ],
                "author_detail": {
                    "name": "Angjoo Kanazawa"
                },
                "author": "Angjoo Kanazawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12375v1",
                "updated": "2025-01-21T18:53:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    53,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:53:30Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    53,
                    30,
                    1,
                    21,
                    0
                ],
                "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos"
                },
                "summary": "Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth Anything has achieved remarkable success in monocular depth estimation\nwith strong generalization ability. However, it suffers from temporal\ninconsistency in videos, hindering its practical applications. Various methods\nhave been proposed to alleviate this issue by leveraging video generation\nmodels or introducing priors from optical flow and camera poses. Nonetheless,\nthese methods are only applicable to short videos (< 10 seconds) and require a\ntrade-off between quality and computational efficiency. We propose Video Depth\nAnything for high-quality, consistent depth estimation in super-long videos\n(over several minutes) without sacrificing efficiency. We base our model on\nDepth Anything V2 and replace its head with an efficient spatial-temporal head.\nWe design a straightforward yet effective temporal consistency loss by\nconstraining the temporal depth gradient, eliminating the need for additional\ngeometric priors. The model is trained on a joint dataset of video depth and\nunlabeled images, similar to Depth Anything V2. Moreover, a novel\nkey-frame-based strategy is developed for long video inference. Experiments\nshow that our model can be applied to arbitrarily long videos without\ncompromising quality, consistency, or generalization ability. Comprehensive\nevaluations on multiple video benchmarks demonstrate that our approach sets a\nnew state-of-the-art in zero-shot video depth estimation. We offer models of\ndifferent scales to support a range of scenarios, with our smallest model\ncapable of real-time performance at 30 FPS."
                },
                "authors": [
                    {
                        "name": "Sili Chen"
                    },
                    {
                        "name": "Hengkai Guo"
                    },
                    {
                        "name": "Shengnan Zhu"
                    },
                    {
                        "name": "Feihu Zhang"
                    },
                    {
                        "name": "Zilong Huang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Bingyi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Bingyi Kang"
                },
                "author": "Bingyi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v1",
                "updated": "2025-01-21T18:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12354v1",
                "updated": "2025-01-21T18:33:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    33,
                    8,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:33:08Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    33,
                    8,
                    1,
                    21,
                    0
                ],
                "title": "Diffusion-aware Censored Gaussian Processes for Demand Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-aware Censored Gaussian Processes for Demand Modelling"
                },
                "summary": "Inferring the true demand for a product or a service from aggregate data is\noften challenging due to the limited available supply, thus resulting in\nobservations that are censored and correspond to the realized demand, thereby\nnot accounting for the unsatisfied demand. Censored regression models are able\nto account for the effect of censoring due to the limited supply, but they\ndon't consider the effect of substitutions, which may cause the demand for\nsimilar alternative products or services to increase. This paper proposes\nDiffusion-aware Censored Demand Models, which combine a Tobit likelihood with a\ngraph diffusion process in order to model the latent process of transfer of\nunsatisfied demand between similar products or services. We instantiate this\nnew class of models under the framework of GPs and, based on both simulated and\nreal-world data for modeling sales, bike-sharing demand, and EV charging\ndemand, demonstrate its ability to better recover the true demand and produce\nmore accurate out-of-sample predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the true demand for a product or a service from aggregate data is\noften challenging due to the limited available supply, thus resulting in\nobservations that are censored and correspond to the realized demand, thereby\nnot accounting for the unsatisfied demand. Censored regression models are able\nto account for the effect of censoring due to the limited supply, but they\ndon't consider the effect of substitutions, which may cause the demand for\nsimilar alternative products or services to increase. This paper proposes\nDiffusion-aware Censored Demand Models, which combine a Tobit likelihood with a\ngraph diffusion process in order to model the latent process of transfer of\nunsatisfied demand between similar products or services. We instantiate this\nnew class of models under the framework of GPs and, based on both simulated and\nreal-world data for modeling sales, bike-sharing demand, and EV charging\ndemand, demonstrate its ability to better recover the true demand and produce\nmore accurate out-of-sample predictions."
                },
                "authors": [
                    {
                        "name": "Filipe Rodrigues"
                    }
                ],
                "author_detail": {
                    "name": "Filipe Rodrigues"
                },
                "author": "Filipe Rodrigues",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14655v2",
                "updated": "2025-01-21T18:14:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    14,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-18T17:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens"
                },
                "summary": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Rasool Fakoor"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Fakoor"
                },
                "author": "Rasool Fakoor",
                "arxiv_comment": "Published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12339v1",
                "updated": "2025-01-21T18:13:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:13:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Treefix: Enabling Execution with a Tree of Prefixes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treefix: Enabling Execution with a Tree of Prefixes"
                },
                "summary": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets."
                },
                "authors": [
                    {
                        "name": "Beatriz Souza"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted in research track of the EEE/ACM International Conference on\n  Software Engineering (ICSE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12332v1",
                "updated": "2025-01-21T18:06:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    6,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:06:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    6,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "Automatic Labelling with Open-source LLMs using Dynamic Label Schema\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Labelling with Open-source LLMs using Dynamic Label Schema\n  Integration"
                },
                "summary": "Acquiring labelled training data remains a costly task in real world machine\nlearning projects to meet quantity and quality requirements. Recently Large\nLanguage Models (LLMs), notably GPT-4, have shown great promises in labelling\ndata with high accuracy. However, privacy and cost concerns prevent the\nubiquitous use of GPT-4. In this work, we explore effectively leveraging\nopen-source models for automatic labelling. We identify integrating label\nschema as a promising technology but found that naively using the label\ndescription for classification leads to poor performance on high cardinality\ntasks. To address this, we propose Retrieval Augmented Classification (RAC) for\nwhich LLM performs inferences for one label at a time using corresponding label\nschema; we start with the most related label and iterates until a label is\nchosen by the LLM. We show that our method, which dynamically integrates label\ndescription, leads to performance improvements in labelling tasks. We further\nshow that by focusing only on the most promising labels, RAC can trade off\nbetween label quality and coverage - a property we leverage to automatically\nlabel our internal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiring labelled training data remains a costly task in real world machine\nlearning projects to meet quantity and quality requirements. Recently Large\nLanguage Models (LLMs), notably GPT-4, have shown great promises in labelling\ndata with high accuracy. However, privacy and cost concerns prevent the\nubiquitous use of GPT-4. In this work, we explore effectively leveraging\nopen-source models for automatic labelling. We identify integrating label\nschema as a promising technology but found that naively using the label\ndescription for classification leads to poor performance on high cardinality\ntasks. To address this, we propose Retrieval Augmented Classification (RAC) for\nwhich LLM performs inferences for one label at a time using corresponding label\nschema; we start with the most related label and iterates until a label is\nchosen by the LLM. We show that our method, which dynamically integrates label\ndescription, leads to performance improvements in labelling tasks. We further\nshow that by focusing only on the most promising labels, RAC can trade off\nbetween label quality and coverage - a property we leverage to automatically\nlabel our internal datasets."
                },
                "authors": [
                    {
                        "name": "Thomas Walshe"
                    },
                    {
                        "name": "Sae Young Moon"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Yawwani Gunawardana"
                    },
                    {
                        "name": "Fran Silavong"
                    }
                ],
                "author_detail": {
                    "name": "Fran Silavong"
                },
                "author": "Fran Silavong",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12320v1",
                "updated": "2025-01-21T17:40:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    40,
                    44,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T17:40:44Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    40,
                    44,
                    1,
                    21,
                    0
                ],
                "title": "Fully quantum inflation: quantum marginal problem constraints in the\n  service of causal inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully quantum inflation: quantum marginal problem constraints in the\n  service of causal inference"
                },
                "summary": "Consider the problem of deciding, for a particular multipartite quantum\nstate, whether or not it is realizable in a quantum network with a particular\ncausal structure. This is a fully quantum version of what causal inference\nresearchers refer to as the problem of causal discovery. In this work, we\nintroduce a fully quantum version of the inflation technique for causal\ninference, which leverages the quantum marginal problem. We illustrate the\nutility of this method using a simple example: testing compatibility of\ntripartite quantum states with the quantum network known as the triangle\nscenario. We show, in particular, how the method yields a complete\nclassification of pure three-qubit states into those that are and those that\nare not compatible with the triangle scenario. We also provide some\nillustrative examples involving mixed states and some where one or more of the\nsystems is higher-dimensional. Finally, we examine the question of when the\nincompatibility of a multipartite quantum state with a causal structure can be\ninferred from the incompatibility of a joint probability distribution induced\nby implementing measurements on each subsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider the problem of deciding, for a particular multipartite quantum\nstate, whether or not it is realizable in a quantum network with a particular\ncausal structure. This is a fully quantum version of what causal inference\nresearchers refer to as the problem of causal discovery. In this work, we\nintroduce a fully quantum version of the inflation technique for causal\ninference, which leverages the quantum marginal problem. We illustrate the\nutility of this method using a simple example: testing compatibility of\ntripartite quantum states with the quantum network known as the triangle\nscenario. We show, in particular, how the method yields a complete\nclassification of pure three-qubit states into those that are and those that\nare not compatible with the triangle scenario. We also provide some\nillustrative examples involving mixed states and some where one or more of the\nsystems is higher-dimensional. Finally, we examine the question of when the\nincompatibility of a multipartite quantum state with a causal structure can be\ninferred from the incompatibility of a joint probability distribution induced\nby implementing measurements on each subsystem."
                },
                "authors": [
                    {
                        "name": "Isaac D. Smith"
                    },
                    {
                        "name": "Elie Wolfe"
                    },
                    {
                        "name": "Robert W. Spekkens"
                    }
                ],
                "author_detail": {
                    "name": "Robert W. Spekkens"
                },
                "author": "Robert W. Spekkens",
                "arxiv_comment": "20+6 pages; 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12314v1",
                "updated": "2025-01-21T17:28:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    28,
                    52,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T17:28:52Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    28,
                    52,
                    1,
                    21,
                    0
                ],
                "title": "Uncertainty Quantification With Noise Injection in Neural Networks: A\n  Bayesian Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification With Noise Injection in Neural Networks: A\n  Bayesian Perspective"
                },
                "summary": "Model uncertainty quantification involves measuring and evaluating the\nuncertainty linked to a model's predictions, helping assess their reliability\nand confidence. Noise injection is a technique used to enhance the robustness\nof neural networks by introducing randomness. In this paper, we establish a\nconnection between noise injection and uncertainty quantification from a\nBayesian standpoint. We theoretically demonstrate that injecting noise into the\nweights of a neural network is equivalent to Bayesian inference on a deep\nGaussian process. Consequently, we introduce a Monte Carlo Noise Injection\n(MCNI) method, which involves injecting noise into the parameters during\ntraining and performing multiple forward propagations during inference to\nestimate the uncertainty of the prediction. Through simulation and experiments\non regression and classification tasks, our method demonstrates superior\nperformance compared to the baseline model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model uncertainty quantification involves measuring and evaluating the\nuncertainty linked to a model's predictions, helping assess their reliability\nand confidence. Noise injection is a technique used to enhance the robustness\nof neural networks by introducing randomness. In this paper, we establish a\nconnection between noise injection and uncertainty quantification from a\nBayesian standpoint. We theoretically demonstrate that injecting noise into the\nweights of a neural network is equivalent to Bayesian inference on a deep\nGaussian process. Consequently, we introduce a Monte Carlo Noise Injection\n(MCNI) method, which involves injecting noise into the parameters during\ntraining and performing multiple forward propagations during inference to\nestimate the uncertainty of the prediction. Through simulation and experiments\non regression and classification tasks, our method demonstrates superior\nperformance compared to the baseline model."
                },
                "authors": [
                    {
                        "name": "Xueqiong Yuan"
                    },
                    {
                        "name": "Jipeng Li"
                    },
                    {
                        "name": "Ercan Engin Kuruoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ercan Engin Kuruoglu"
                },
                "author": "Ercan Engin Kuruoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10622v3",
                "updated": "2025-01-21T17:20:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    20,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-14T00:05:42Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    0,
                    5,
                    42,
                    5,
                    349,
                    0
                ],
                "title": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options"
                },
                "summary": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple-choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet -- with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\nability, the correct answer options in the questions were replaced with \"None\nof the above.\" Then, the explain-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning ability. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answer options with 'None of the above', all models\nexhibited a considerable decline in performance, suggesting room for\nimprovement. The explain-first and step-by-step instruction prompts helped\nenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and\nClaude 3.5 Sonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics education and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple-choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet -- with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\nability, the correct answer options in the questions were replaced with \"None\nof the above.\" Then, the explain-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning ability. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answer options with 'None of the above', all models\nexhibited a considerable decline in performance, suggesting room for\nimprovement. The explain-first and step-by-step instruction prompts helped\nenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and\nClaude 3.5 Sonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics education and training."
                },
                "authors": [
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Dequan Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Jiajian Shen"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12300v1",
                "updated": "2025-01-21T17:13:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    13,
                    13,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T17:13:13Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    13,
                    13,
                    1,
                    21,
                    0
                ],
                "title": "LLM-Assisted Knowledge Graph Completion for Curriculum and Domain\n  Modelling in Personalized Higher Education Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Knowledge Graph Completion for Curriculum and Domain\n  Modelling in Personalized Higher Education Recommendations"
                },
                "summary": "While learning personalization offers great potential for learners, modern\npractices in higher education require a deeper consideration of domain models\nand learning contexts, to develop effective personalization algorithms. This\npaper introduces an innovative approach to higher education curriculum\nmodelling that utilizes large language models (LLMs) for knowledge graph (KG)\ncompletion, with the goal of creating personalized learning-path\nrecommendations. Our research focuses on modelling university subjects and\nlinking their topics to corresponding domain models, enabling the integration\nof learning modules from different faculties and institutions in the student's\nlearning path. Central to our approach is a collaborative process, where LLMs\nassist human experts in extracting high-quality, fine-grained topics from\nlecture materials. We develop a domain, curriculum, and user models for\nuniversity modules and stakeholders. We implement this model to create the KG\nfrom two study modules: Embedded Systems and Development of Embedded Systems\nUsing FPGA. The resulting KG structures the curriculum and links it to the\ndomain models. We evaluate our approach through qualitative expert feedback and\nquantitative graph quality metrics. Domain experts validated the relevance and\naccuracy of the model, while the graph quality metrics measured the structural\nproperties of our KG. Our results show that the LLM-assisted graph completion\napproach enhances the ability to connect related courses across disciplines to\npersonalize the learning experience. Expert feedback also showed high\nacceptance of the proposed collaborative approach for concept extraction and\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While learning personalization offers great potential for learners, modern\npractices in higher education require a deeper consideration of domain models\nand learning contexts, to develop effective personalization algorithms. This\npaper introduces an innovative approach to higher education curriculum\nmodelling that utilizes large language models (LLMs) for knowledge graph (KG)\ncompletion, with the goal of creating personalized learning-path\nrecommendations. Our research focuses on modelling university subjects and\nlinking their topics to corresponding domain models, enabling the integration\nof learning modules from different faculties and institutions in the student's\nlearning path. Central to our approach is a collaborative process, where LLMs\nassist human experts in extracting high-quality, fine-grained topics from\nlecture materials. We develop a domain, curriculum, and user models for\nuniversity modules and stakeholders. We implement this model to create the KG\nfrom two study modules: Embedded Systems and Development of Embedded Systems\nUsing FPGA. The resulting KG structures the curriculum and links it to the\ndomain models. We evaluate our approach through qualitative expert feedback and\nquantitative graph quality metrics. Domain experts validated the relevance and\naccuracy of the model, while the graph quality metrics measured the structural\nproperties of our KG. Our results show that the LLM-assisted graph completion\napproach enhances the ability to connect related courses across disciplines to\npersonalize the learning experience. Expert feedback also showed high\nacceptance of the proposed collaborative approach for concept extraction and\nclassification."
                },
                "authors": [
                    {
                        "name": "Hasan Abu-Rasheed"
                    },
                    {
                        "name": "Constance Jumbo"
                    },
                    {
                        "name": "Rashed Al Amin"
                    },
                    {
                        "name": "Christian Weber"
                    },
                    {
                        "name": "Veit Wiese"
                    },
                    {
                        "name": "Roman Obermaisser"
                    },
                    {
                        "name": "Madjid Fathi"
                    }
                ],
                "author_detail": {
                    "name": "Madjid Fathi"
                },
                "author": "Madjid Fathi",
                "arxiv_comment": "Accepted in the IEEE Global Engineering Education Conference\n  (EDUCON2025), London, UK, 22-25 April, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12298v1",
                "updated": "2025-01-21T17:07:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    7,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T17:07:22Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    7,
                    22,
                    1,
                    21,
                    0
                ],
                "title": "Towards spectral descriptions of cyclic functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards spectral descriptions of cyclic functions"
                },
                "summary": "We build on a characterization of inner functions $f$ due to Le, in terms of\nthe spectral properties of the operator $V=M_f^*M_f$ and study to what extent\nthe cyclicity on weighted Hardy spaces $H^2_\\omega$ of the function $z \\mapsto\na-z$ can be inferred from the spectral properties of analogous operators $V_a$.\nWe describe several properties of the spectra that hold in a large class of\nspaces and then, we focus on the particular case of Bergman-type spaces, for\nwhich we describe completely the spectrum of such operators and find all\neigenfunctions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We build on a characterization of inner functions $f$ due to Le, in terms of\nthe spectral properties of the operator $V=M_f^*M_f$ and study to what extent\nthe cyclicity on weighted Hardy spaces $H^2_\\omega$ of the function $z \\mapsto\na-z$ can be inferred from the spectral properties of analogous operators $V_a$.\nWe describe several properties of the spectra that hold in a large class of\nspaces and then, we focus on the particular case of Bergman-type spaces, for\nwhich we describe completely the spectrum of such operators and find all\neigenfunctions."
                },
                "authors": [
                    {
                        "name": "Miguel Monsalve"
                    },
                    {
                        "name": "Daniel Seco"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Seco"
                },
                "author": "Daniel Seco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.FA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary: 47B32, Secondary: 30J05, 47A10, 47A16",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12273v1",
                "updated": "2025-01-21T16:44:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    44,
                    12,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T16:44:12Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    44,
                    12,
                    1,
                    21,
                    0
                ],
                "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement"
                },
                "summary": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Maosong Cao"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Chuyu Zhang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Tech Report. Github: https://github.com/InternLM/Condor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09138v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09138v2",
                "updated": "2025-01-21T16:40:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    40,
                    55,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-13T14:07:52Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    14,
                    7,
                    52,
                    3,
                    165,
                    0
                ],
                "title": "Leveraging Explicit Reasoning for Inference Integration in\n  Commonsense-Augmented Dialogue Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Explicit Reasoning for Inference Integration in\n  Commonsense-Augmented Dialogue Models"
                },
                "summary": "Open-domain dialogue systems need to grasp social commonsense to understand\nand respond effectively to human users. Commonsense-augmented dialogue models\nhave been proposed that aim to infer commonsense knowledge from dialogue\ncontexts in order to improve response quality. However, existing approaches to\ncommonsense-augmented dialogue rely on implicit reasoning to integrate\ncommonsense inferences during response generation. In this study, we explore\nthe impact of explicit reasoning against implicit reasoning over commonsense\nfor dialogue response generation. Our findings demonstrate that separating\ncommonsense reasoning into explicit steps for generating, selecting, and\nintegrating commonsense into responses leads to better dialogue interactions,\nimproving naturalness, engagement, specificity, and overall quality. Subsequent\nanalyses of these findings unveil insights into the effectiveness of various\ntypes of commonsense in generating responses and the particular response traits\nenhanced through explicit reasoning for commonsense integration. Our work\nadvances research in open-domain dialogue by achieving a new state-of-the-art\nin commonsense-augmented response generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain dialogue systems need to grasp social commonsense to understand\nand respond effectively to human users. Commonsense-augmented dialogue models\nhave been proposed that aim to infer commonsense knowledge from dialogue\ncontexts in order to improve response quality. However, existing approaches to\ncommonsense-augmented dialogue rely on implicit reasoning to integrate\ncommonsense inferences during response generation. In this study, we explore\nthe impact of explicit reasoning against implicit reasoning over commonsense\nfor dialogue response generation. Our findings demonstrate that separating\ncommonsense reasoning into explicit steps for generating, selecting, and\nintegrating commonsense into responses leads to better dialogue interactions,\nimproving naturalness, engagement, specificity, and overall quality. Subsequent\nanalyses of these findings unveil insights into the effectiveness of various\ntypes of commonsense in generating responses and the particular response traits\nenhanced through explicit reasoning for commonsense integration. Our work\nadvances research in open-domain dialogue by achieving a new state-of-the-art\nin commonsense-augmented response generation."
                },
                "authors": [
                    {
                        "name": "Sarah E. Finch"
                    },
                    {
                        "name": "Jinho D. Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jinho D. Choi"
                },
                "author": "Jinho D. Choi",
                "arxiv_comment": "Accepted to COLING 2025\n  (https://aclanthology.org/2025.coling-main.152/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09138v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09138v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06687v3",
                "updated": "2025-01-21T16:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    15,
                    37,
                    1,
                    21,
                    0
                ],
                "published": "2024-05-06T18:09:32Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    18,
                    9,
                    32,
                    0,
                    127,
                    0
                ],
                "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes"
                },
                "summary": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes."
                },
                "authors": [
                    {
                        "name": "Damin Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Geetanjali Bihani"
                    },
                    {
                        "name": "Julia Rayz"
                    }
                ],
                "author_detail": {
                    "name": "Julia Rayz"
                },
                "author": "Julia Rayz",
                "arxiv_comment": "COLING 2025",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11414v2",
                "updated": "2025-01-21T16:05:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    5,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-15T09:02:09Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    2,
                    9,
                    1,
                    289,
                    0
                ],
                "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability"
                },
                "summary": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads."
                },
                "authors": [
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "23pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12243v1",
                "updated": "2025-01-21T16:03:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    3,
                    42,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T16:03:42Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    3,
                    42,
                    1,
                    21,
                    0
                ],
                "title": "FOCUS: First Order Concentrated Updating Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: First Order Concentrated Updating Scheme"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions."
                },
                "authors": [
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Jeff Gore"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Gore"
                },
                "author": "Jeff Gore",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08962v2",
                "updated": "2025-01-21T16:03:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    3,
                    33,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-16T18:20:46Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    18,
                    20,
                    46,
                    4,
                    229,
                    0
                ],
                "title": "A Dusty Dawn: Galactic Dust Buildup at $z\\gtrsim5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dusty Dawn: Galactic Dust Buildup at $z\\gtrsim5$"
                },
                "summary": "Over the last decade, the Atacama Large Millimeter Array has revealed\nmassive, dusty star-forming galaxies at $z\\gtrsim5$, and the James Webb Space\nTelescope is primed to uncover even more information about them. These\nobservations need dust evolution theory to provide context and are excellent\nbenchmarks to test this theory. Here, we investigate the evolution of galactic\ndust budget at cosmic dawn using a suite of cosmological zoom-in simulations of\nmoderately massive, high-redshift ($M_{\\rm star}\\gtrsim10^9 M_{\\odot}$;\n$z\\gtrsim5$) galaxies from the FIRE project, the highest resolution ($m_{\\rm b}\n\\approx 7100\\, M_{\\odot}$) of such simulations to date. Our simulations\nincorporate a dust evolution model that accounts for the dominant sources of\ndust production, growth, and destruction and follows the evolution of specific\ndust species, allowing it to replicate a wide range of present-day\nobservations. We find, similar to other theoretical works, that dust growth via\ngas-dust accretion is the dominant producer of dust mass for these massive,\n$z\\gtrsim 5$ galaxies. However, our fiducial model produces $M_{\\rm dust}$ that\nfall ${\\gtrsim}1$ dex below observations at any given $M_{\\rm star}$ (typical\nuncertainties are ${\\sim}1$ dex), which we attribute to reduced accretion\nefficiencies caused by a combination of low galactic metallicities and\nextremely bursty star formation. Modest enhancements (i.e., within\nobservational/theoretical uncertainties) to accretion and SNe II dust creation\nraise $M_{\\rm dust}$ by ${\\lesssim}1$ dex, but this still falls below\nobservations which assume $T_{\\rm dust}\\sim25$ K. One possibility is that\ninferred dust masses for $z\\gtrsim4$ galaxies are overestimated, and recent\nobservational/analytical works that find $T_{\\rm dust}\\sim50$ K along with\nmetallicity constraints tentatively support this.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the last decade, the Atacama Large Millimeter Array has revealed\nmassive, dusty star-forming galaxies at $z\\gtrsim5$, and the James Webb Space\nTelescope is primed to uncover even more information about them. These\nobservations need dust evolution theory to provide context and are excellent\nbenchmarks to test this theory. Here, we investigate the evolution of galactic\ndust budget at cosmic dawn using a suite of cosmological zoom-in simulations of\nmoderately massive, high-redshift ($M_{\\rm star}\\gtrsim10^9 M_{\\odot}$;\n$z\\gtrsim5$) galaxies from the FIRE project, the highest resolution ($m_{\\rm b}\n\\approx 7100\\, M_{\\odot}$) of such simulations to date. Our simulations\nincorporate a dust evolution model that accounts for the dominant sources of\ndust production, growth, and destruction and follows the evolution of specific\ndust species, allowing it to replicate a wide range of present-day\nobservations. We find, similar to other theoretical works, that dust growth via\ngas-dust accretion is the dominant producer of dust mass for these massive,\n$z\\gtrsim 5$ galaxies. However, our fiducial model produces $M_{\\rm dust}$ that\nfall ${\\gtrsim}1$ dex below observations at any given $M_{\\rm star}$ (typical\nuncertainties are ${\\sim}1$ dex), which we attribute to reduced accretion\nefficiencies caused by a combination of low galactic metallicities and\nextremely bursty star formation. Modest enhancements (i.e., within\nobservational/theoretical uncertainties) to accretion and SNe II dust creation\nraise $M_{\\rm dust}$ by ${\\lesssim}1$ dex, but this still falls below\nobservations which assume $T_{\\rm dust}\\sim25$ K. One possibility is that\ninferred dust masses for $z\\gtrsim4$ galaxies are overestimated, and recent\nobservational/analytical works that find $T_{\\rm dust}\\sim50$ K along with\nmetallicity constraints tentatively support this."
                },
                "authors": [
                    {
                        "name": "Caleb R. Choban"
                    },
                    {
                        "name": "Samir Salim"
                    },
                    {
                        "name": "Dušan Kereš"
                    },
                    {
                        "name": "Christopher C. Hayward"
                    },
                    {
                        "name": "Karin M. Sandstrom"
                    }
                ],
                "author_detail": {
                    "name": "Karin M. Sandstrom"
                },
                "author": "Karin M. Sandstrom",
                "arxiv_doi": "10.1093/mnras/staf118",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf118",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.08962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 13 figures, accepted for publication in MNRAS. New\n  mock-JWST visualizations and numerous minor revisions and additional\n  citations",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12231v1",
                "updated": "2025-01-21T15:55:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    55,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:55:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    55,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InsTALL: Context-aware Instructional Task Assistance with Multi-modal\n  Large Language Models"
                },
                "summary": "The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The improved competence of generative models can help building multi-modal\nvirtual assistants that leverage modalities beyond language. By observing\nhumans performing multi-step tasks, one can build assistants that have\nsituational awareness of actions and tasks being performed, enabling them to\ncater assistance based on this understanding. In this paper, we develop a\nContext-aware Instructional Task Assistant with Multi-modal Large Language\nModels (InsTALL) that leverages an online visual stream (e.g. a user's screen\nshare or video recording) and responds in real-time to user queries related to\nthe task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal\nmodel on task videos and paired textual data, and 2) automatically extracts\ntask graph from video data and leverages it at training and inference time. We\nshow InsTALL achieves state-of-the-art performance across proposed sub-tasks\nconsidered for multimodal activity understanding -- task recognition (TR),\naction recognition (AR), next action prediction (AP), and plan prediction (PP)\n-- and outperforms existing baselines on two novel sub-tasks related to\nautomatic error identification."
                },
                "authors": [
                    {
                        "name": "Pha Nguyen"
                    },
                    {
                        "name": "Sailik Sengupta"
                    },
                    {
                        "name": "Girik Malik"
                    },
                    {
                        "name": "Arshit Gupta"
                    },
                    {
                        "name": "Bonan Min"
                    }
                ],
                "author_detail": {
                    "name": "Bonan Min"
                },
                "author": "Bonan Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12226v1",
                "updated": "2025-01-21T15:51:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    51,
                    7,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:51:07Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    51,
                    7,
                    1,
                    21,
                    0
                ],
                "title": "CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning"
                },
                "summary": "Large Language Models (LLMs) have recently achieved impressive results in\ncomplex reasoning tasks through Chain of Thought (CoT) prompting. However, most\nexisting CoT methods rely on using the same prompts, whether manually designed\nor automatically generated, to handle the entire dataset. This\none-size-fits-all approach may fail to meet the specific needs arising from the\ndiversities within a single dataset. To solve this problem, we propose the\nClustered Distance-Weighted Chain of Thought (CDW-CoT) method, which\ndynamically constructs prompts tailored to the characteristics of each data\ninstance by integrating clustering and prompt optimization techniques. Our\nmethod employs clustering algorithms to categorize the dataset into distinct\ngroups, from which a candidate pool of prompts is selected to reflect the\ninherent diversity within the dataset. For each cluster, CDW-CoT trains the\noptimal prompt probability distribution tailored to their specific\ncharacteristics. Finally, it dynamically constructs a unique prompt probability\ndistribution for each test instance, based on its proximity to cluster centers,\nfrom which prompts are selected for reasoning. CDW-CoT consistently outperforms\ntraditional CoT methods across six datasets, including commonsense, symbolic,\nand mathematical reasoning tasks. Specifically, when compared to manual CoT,\nCDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and\n15.72% on LLaMA3 (8B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently achieved impressive results in\ncomplex reasoning tasks through Chain of Thought (CoT) prompting. However, most\nexisting CoT methods rely on using the same prompts, whether manually designed\nor automatically generated, to handle the entire dataset. This\none-size-fits-all approach may fail to meet the specific needs arising from the\ndiversities within a single dataset. To solve this problem, we propose the\nClustered Distance-Weighted Chain of Thought (CDW-CoT) method, which\ndynamically constructs prompts tailored to the characteristics of each data\ninstance by integrating clustering and prompt optimization techniques. Our\nmethod employs clustering algorithms to categorize the dataset into distinct\ngroups, from which a candidate pool of prompts is selected to reflect the\ninherent diversity within the dataset. For each cluster, CDW-CoT trains the\noptimal prompt probability distribution tailored to their specific\ncharacteristics. Finally, it dynamically constructs a unique prompt probability\ndistribution for each test instance, based on its proximity to cluster centers,\nfrom which prompts are selected for reasoning. CDW-CoT consistently outperforms\ntraditional CoT methods across six datasets, including commonsense, symbolic,\nand mathematical reasoning tasks. Specifically, when compared to manual CoT,\nCDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and\n15.72% on LLaMA3 (8B)."
                },
                "authors": [
                    {
                        "name": "Yuanheng Fang"
                    },
                    {
                        "name": "Guoqing Chao"
                    },
                    {
                        "name": "Wenqiang Lei"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Dianhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Dianhui Chu"
                },
                "author": "Dianhui Chu",
                "arxiv_comment": "aaai25(poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12221v1",
                "updated": "2025-01-21T15:47:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    47,
                    32,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:47:32Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    47,
                    32,
                    1,
                    21,
                    0
                ],
                "title": "Leveraging Large Language Models for Realizing Truly Intelligent User\n  Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Realizing Truly Intelligent User\n  Interfaces"
                },
                "summary": "The number of published scholarly articles is growing at a significant rate,\nmaking scholarly knowledge organization increasingly important. Various\napproaches have been proposed to organize scholarly information, including\ndescribing scholarly knowledge semantically leveraging knowledge graphs.\nTransforming unstructured knowledge, presented within articles, to structured\nand semantically represented knowledge generally requires human intelligence\nand labor since natural language processing methods alone typically do not\nrender sufficient precision and recall for many applications. With the recent\ndevelopments of Large Language Models (LLMs), it becomes increasingly possible\nto provide truly intelligent user interfaces guiding humans in the\ntransformation process. We present an approach to integrate non-intrusive LLMs\nguidance into existing user interfaces. More specifically, we integrate\nLLM-supported user interface components into an existing scholarly knowledge\ninfrastructure. Additionally, we provide our experiences with LLM integration,\ndetailing best practices and obstacles. Finally, we evaluate the approach using\na small-scale user evaluation with domain experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of published scholarly articles is growing at a significant rate,\nmaking scholarly knowledge organization increasingly important. Various\napproaches have been proposed to organize scholarly information, including\ndescribing scholarly knowledge semantically leveraging knowledge graphs.\nTransforming unstructured knowledge, presented within articles, to structured\nand semantically represented knowledge generally requires human intelligence\nand labor since natural language processing methods alone typically do not\nrender sufficient precision and recall for many applications. With the recent\ndevelopments of Large Language Models (LLMs), it becomes increasingly possible\nto provide truly intelligent user interfaces guiding humans in the\ntransformation process. We present an approach to integrate non-intrusive LLMs\nguidance into existing user interfaces. More specifically, we integrate\nLLM-supported user interface components into an existing scholarly knowledge\ninfrastructure. Additionally, we provide our experiences with LLM integration,\ndetailing best practices and obstacles. Finally, we evaluate the approach using\na small-scale user evaluation with domain experts."
                },
                "authors": [
                    {
                        "name": "Allard Oelen"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "arxiv_doi": "10.1145/3613905.3650949",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3613905.3650949",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16789v2",
                "updated": "2025-01-21T15:40:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2024-05-27T03:24:01Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    3,
                    24,
                    1,
                    0,
                    148,
                    0
                ],
                "title": "NoteLLM-2: Multimodal Large Representation Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoteLLM-2: Multimodal Large Representation Models for Recommendation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ntext understanding and embedding tasks. However, their potential in multimodal\nrepresentation, particularly for item-to-item (I2I) recommendations, remains\nunderexplored. While leveraging existing Multimodal Large Language Models\n(MLLMs) for such tasks is promising, challenges arise due to their delayed\nrelease compared to corresponding LLMs and the inefficiency in representation\ntasks. To address these issues, we propose an end-to-end fine-tuning method\nthat customizes the integration of any existing LLMs and vision encoders for\nefficient multimodal representation. Preliminary experiments revealed that\nfine-tuned LLMs often neglect image content. To counteract this, we propose\nNoteLLM-2, a novel framework that enhances visual information. Specifically, we\npropose two approaches: first, a prompt-based method that segregates visual and\ntextual content, employing a multimodal In-Context Learning strategy to balance\nfocus across modalities; second, a late fusion technique that directly\nintegrates visual information into the final representations. Extensive\nexperiments, both online and offline, demonstrate the effectiveness of our\napproach. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/NoteLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ntext understanding and embedding tasks. However, their potential in multimodal\nrepresentation, particularly for item-to-item (I2I) recommendations, remains\nunderexplored. While leveraging existing Multimodal Large Language Models\n(MLLMs) for such tasks is promising, challenges arise due to their delayed\nrelease compared to corresponding LLMs and the inefficiency in representation\ntasks. To address these issues, we propose an end-to-end fine-tuning method\nthat customizes the integration of any existing LLMs and vision encoders for\nefficient multimodal representation. Preliminary experiments revealed that\nfine-tuned LLMs often neglect image content. To counteract this, we propose\nNoteLLM-2, a novel framework that enhances visual information. Specifically, we\npropose two approaches: first, a prompt-based method that segregates visual and\ntextual content, employing a multimodal In-Context Learning strategy to balance\nfocus across modalities; second, a late fusion technique that directly\nintegrates visual information into the final representations. Extensive\nexperiments, both online and offline, demonstrate the effectiveness of our\napproach. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/NoteLLM."
                },
                "authors": [
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by KDD'25 ADS track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01957v3",
                "updated": "2025-01-21T15:36:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    36,
                    41,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-03T18:59:52Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "https://github.com/VITA-MLLM/VITA (2K+ Stars by now)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12216v1",
                "updated": "2025-01-21T15:36:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    36,
                    8,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:36:08Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    36,
                    8,
                    1,
                    21,
                    0
                ],
                "title": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression"
                },
                "summary": "Video encoders optimize compression for human perception by minimizing\nreconstruction error under bit-rate constraints. In many modern applications\nsuch as autonomous driving, an overwhelming majority of videos serve as input\nfor AI systems performing tasks like object recognition or segmentation, rather\nthan being watched by humans. It is therefore useful to optimize the encoder\nfor a downstream task instead of for perceptual image quality. However, a major\nchallenge is how to combine such downstream optimization with existing standard\nvideo encoders, which are highly efficient and popular. Here, we address this\nchallenge by controlling the Quantization Parameters (QPs) at the macro-block\nlevel to optimize the downstream task. This granular control allows us to\nprioritize encoding for task-relevant regions within each frame. We formulate\nthis optimization problem as a Reinforcement Learning (RL) task, where the\nagent learns to balance long-term implications of choosing QPs on both task\nperformance and bit-rate constraints. Notably, our policy does not require the\ndownstream task as an input during inference, making it suitable for streaming\napplications and edge devices such as vehicles. We demonstrate significant\nimprovements in two tasks, car detection, and ROI (saliency) encoding. Our\napproach improves task performance for a given bit rate compared to traditional\ntask agnostic encoding methods, paving the way for more efficient task-aware\nvideo compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video encoders optimize compression for human perception by minimizing\nreconstruction error under bit-rate constraints. In many modern applications\nsuch as autonomous driving, an overwhelming majority of videos serve as input\nfor AI systems performing tasks like object recognition or segmentation, rather\nthan being watched by humans. It is therefore useful to optimize the encoder\nfor a downstream task instead of for perceptual image quality. However, a major\nchallenge is how to combine such downstream optimization with existing standard\nvideo encoders, which are highly efficient and popular. Here, we address this\nchallenge by controlling the Quantization Parameters (QPs) at the macro-block\nlevel to optimize the downstream task. This granular control allows us to\nprioritize encoding for task-relevant regions within each frame. We formulate\nthis optimization problem as a Reinforcement Learning (RL) task, where the\nagent learns to balance long-term implications of choosing QPs on both task\nperformance and bit-rate constraints. Notably, our policy does not require the\ndownstream task as an input during inference, making it suitable for streaming\napplications and edge devices such as vehicles. We demonstrate significant\nimprovements in two tasks, car detection, and ROI (saliency) encoding. Our\napproach improves task performance for a given bit rate compared to traditional\ntask agnostic encoding methods, paving the way for more efficient task-aware\nvideo compression."
                },
                "authors": [
                    {
                        "name": "Uri Gadot"
                    },
                    {
                        "name": "Assaf Shocher"
                    },
                    {
                        "name": "Shie Mannor"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Assaf Hallak"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Hallak"
                },
                "author": "Assaf Hallak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00275v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00275v3",
                "updated": "2025-01-21T15:33:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    33,
                    35,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-01T04:29:34Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    4,
                    29,
                    34,
                    3,
                    214,
                    0
                ],
                "title": "A Search-to-Control Reinforcement Learning Based Framework for Quadrotor\n  Local Planning in Dense Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Search-to-Control Reinforcement Learning Based Framework for Quadrotor\n  Local Planning in Dense Environments"
                },
                "summary": "Agile flight in complex environments poses significant challenges to current\nmotion planning methods, as they often fail to fully leverage the quadrotor's\ndynamic potential, leading to performance failures and reduced efficiency\nduring aggressive maneuvers. Existing approaches frequently decouple trajectory\noptimization from control generation and neglect the dynamics, further limiting\ntheir ability to generate aggressive and feasible motions. To address these\nchallenges, we introduce an enhanced Search-to-Control planning framework that\nintegrates visibility path searching with reinforcement learning (RL) control\ngeneration, directly accounting for dynamics and bridging the gap between\nplanning and control. Our method first extracts control points from\ncollision-free paths using a proposed heuristic search, which are then refined\nby an RL policy to generate low-level control commands for the quadrotor's\ncontroller, utilizing reduced-dimensional obstacle observations for efficient\ninference with lightweight neural networks. We validate the framework through\nsimulations and real-world experiments, demonstrating improved time efficiency\nand dynamic maneuverability compared to existing methods, while confirming its\nrobustness and applicability. To support further research, We will release our\nimplementation as an open-source package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile flight in complex environments poses significant challenges to current\nmotion planning methods, as they often fail to fully leverage the quadrotor's\ndynamic potential, leading to performance failures and reduced efficiency\nduring aggressive maneuvers. Existing approaches frequently decouple trajectory\noptimization from control generation and neglect the dynamics, further limiting\ntheir ability to generate aggressive and feasible motions. To address these\nchallenges, we introduce an enhanced Search-to-Control planning framework that\nintegrates visibility path searching with reinforcement learning (RL) control\ngeneration, directly accounting for dynamics and bridging the gap between\nplanning and control. Our method first extracts control points from\ncollision-free paths using a proposed heuristic search, which are then refined\nby an RL policy to generate low-level control commands for the quadrotor's\ncontroller, utilizing reduced-dimensional obstacle observations for efficient\ninference with lightweight neural networks. We validate the framework through\nsimulations and real-world experiments, demonstrating improved time efficiency\nand dynamic maneuverability compared to existing methods, while confirming its\nrobustness and applicability. To support further research, We will release our\nimplementation as an open-source package."
                },
                "authors": [
                    {
                        "name": "Zhaohong Liu"
                    },
                    {
                        "name": "Wenxuan Gao"
                    },
                    {
                        "name": "Yinshuai Sun"
                    },
                    {
                        "name": "Peng Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peng Dong"
                },
                "author": "Peng Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00275v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00275v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12210v1",
                "updated": "2025-01-21T15:24:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    24,
                    29,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:24:29Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    24,
                    29,
                    1,
                    21,
                    0
                ],
                "title": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense"
                },
                "summary": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs."
                },
                "authors": [
                    {
                        "name": "Wuyuao Mai"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Baojun Liu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Haixin Duan"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10178v2",
                "updated": "2025-01-21T15:11:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    11,
                    41,
                    1,
                    21,
                    0
                ],
                "published": "2024-02-15T18:27:37Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    18,
                    27,
                    37,
                    3,
                    46,
                    0
                ],
                "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and\n  Agent Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and\n  Agent Generation"
                },
                "summary": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the\ndevelopment of LLM-based agents capable of addressing complex, real-world\ntasks. However, these agents often struggle during task execution due to\nmethodological constraints, such as error propagation and limited adaptability.\nTo address this issue, we propose a multi-agent framework based on dynamic Task\nDecomposition and Agent Generation (TDAG). This framework dynamically\ndecomposes complex tasks into smaller subtasks and assigns each to a\nspecifically generated subagent, thereby enhancing adaptability in diverse and\nunpredictable real-world tasks. Simultaneously, existing benchmarks often lack\nthe granularity needed to evaluate incremental progress in complex, multi-step\ntasks. In response, we introduce ItineraryBench in the context of travel\nplanning, featuring interconnected, progressively complex tasks with a\nfine-grained evaluation system. ItineraryBench is designed to assess agents'\nabilities in memory, planning, and tool usage across tasks of varying\ncomplexity. Our experimental results reveal that TDAG significantly outperforms\nestablished baselines, showcasing its superior adaptability and context\nawareness in complex task scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the\ndevelopment of LLM-based agents capable of addressing complex, real-world\ntasks. However, these agents often struggle during task execution due to\nmethodological constraints, such as error propagation and limited adaptability.\nTo address this issue, we propose a multi-agent framework based on dynamic Task\nDecomposition and Agent Generation (TDAG). This framework dynamically\ndecomposes complex tasks into smaller subtasks and assigns each to a\nspecifically generated subagent, thereby enhancing adaptability in diverse and\nunpredictable real-world tasks. Simultaneously, existing benchmarks often lack\nthe granularity needed to evaluate incremental progress in complex, multi-step\ntasks. In response, we introduce ItineraryBench in the context of travel\nplanning, featuring interconnected, progressively complex tasks with a\nfine-grained evaluation system. ItineraryBench is designed to assess agents'\nabilities in memory, planning, and tool usage across tasks of varying\ncomplexity. Our experimental results reveal that TDAG significantly outperforms\nestablished baselines, showcasing its superior adaptability and context\nawareness in complex task scenarios."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Junfeng Yao"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted by Neural Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11900v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11900v4",
                "updated": "2025-01-21T14:57:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    57,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-14T19:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    39,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Faithful Logic-Aided Reasoning and Exploration"
                },
                "summary": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pat Verga"
                    },
                    {
                        "name": "Patrick Lewis"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11900v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11900v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12190v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12190v1",
                "updated": "2025-01-21T14:54:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    54,
                    28,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:54:28Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    54,
                    28,
                    1,
                    21,
                    0
                ],
                "title": "Modelling polarized X-ray pulses from accreting millisecond pulsars with\n  X-PSI, using different surface patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling polarized X-ray pulses from accreting millisecond pulsars with\n  X-PSI, using different surface patterns"
                },
                "summary": "We present an analysis of polarized X-ray pulses based on simulated data for\naccreting millisecond pulsars (AMPs). We used the open-source X-ray Pulse\nSimulation and Inference code (previously applied to NICER observations), that\nwe upgraded to allow polarization analysis. We predicted neutron star parameter\nconstraints for the Imaging X-ray Polarimetry Explorer (IXPE) and found that\nstrong limits on the hot region geometries can be hard to obtain if the\nemitting hot region is large and the number of polarized photons relatively\nsmall. However, if the star is bright enough and the hot regions are small and\nlocated so that polarization degree is higher, the observer inclination and\nhotspot colatitude can be constrained to a precision of within a few degrees.\nWe also found that the shape of the hot region, whether a circle or a ring,\ncannot be distinguished in our most optimistic scenario. Nevertheless, future\nX-ray polarization missions are expected to improve the constraints, and\nalready the recent AMP polarization detections by IXPE should help to infer the\nneutron star mass and radius when combined with modelling of X-ray pulse data\nsets that do not contain polarization information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of polarized X-ray pulses based on simulated data for\naccreting millisecond pulsars (AMPs). We used the open-source X-ray Pulse\nSimulation and Inference code (previously applied to NICER observations), that\nwe upgraded to allow polarization analysis. We predicted neutron star parameter\nconstraints for the Imaging X-ray Polarimetry Explorer (IXPE) and found that\nstrong limits on the hot region geometries can be hard to obtain if the\nemitting hot region is large and the number of polarized photons relatively\nsmall. However, if the star is bright enough and the hot regions are small and\nlocated so that polarization degree is higher, the observer inclination and\nhotspot colatitude can be constrained to a precision of within a few degrees.\nWe also found that the shape of the hot region, whether a circle or a ring,\ncannot be distinguished in our most optimistic scenario. Nevertheless, future\nX-ray polarization missions are expected to improve the constraints, and\nalready the recent AMP polarization detections by IXPE should help to infer the\nneutron star mass and radius when combined with modelling of X-ray pulse data\nsets that do not contain polarization information."
                },
                "authors": [
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Bas Dorsman"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Anna Bobrikova"
                    },
                    {
                        "name": "Alessandro Di Marco"
                    },
                    {
                        "name": "Vladislav Loktev"
                    },
                    {
                        "name": "Alessandro Papitto"
                    },
                    {
                        "name": "Maura Pilia"
                    },
                    {
                        "name": "Juri Poutanen"
                    },
                    {
                        "name": "John Rankin"
                    }
                ],
                "author_detail": {
                    "name": "John Rankin"
                },
                "author": "John Rankin",
                "arxiv_comment": "8 pages, 3 figures, 1 table, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12190v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12190v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07455v2",
                "updated": "2025-01-21T14:53:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    53,
                    36,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-11T17:01:41Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    1,
                    41,
                    1,
                    163,
                    0
                ],
                "title": "Reinforcement Learning from Human Feedback without Reward Inference:\n  Model-Free Algorithm and Instance-Dependent Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback without Reward Inference:\n  Model-Free Algorithm and Instance-Dependent Analysis"
                },
                "summary": "In this paper, we study reinforcement learning from human feedback (RLHF)\nunder an episodic Markov decision process with a general trajectory-wise reward\nmodel. We developed a model-free RLHF best policy identification algorithm,\ncalled $\\mathsf{BSAD}$, without explicit reward model inference, which is a\ncritical intermediate step in the contemporary RLHF paradigms for training\nlarge language models (LLM). The algorithm identifies the optimal policy\ndirectly from human preference information in a backward manner, employing a\ndueling bandit sub-routine that constantly duels actions to identify the\nsuperior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and\nbest-arm-identification-like adaptive stopping criteria to equalize the\nvisitation among all states in the same decision step while moving to the\nprevious step as soon as the optimal action is identifiable, leading to a\nprovable, instance-dependent sample complexity\n$\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which\nresembles the result in classic RL, where $c_{\\mathcal{M}}$ is the\ninstance-dependent constant and $M$ is the batch size. Moreover,\n$\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with\nlogarithmic regret and generalized to discounted MDPs using a frame-based\napproach. Our results show: (i) sample-complexity-wise, RLHF is not\nsignificantly harder than classic RL and (ii) end-to-end RLHF may deliver\nimproved performance by avoiding pitfalls in reward inferring such as overfit\nand distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study reinforcement learning from human feedback (RLHF)\nunder an episodic Markov decision process with a general trajectory-wise reward\nmodel. We developed a model-free RLHF best policy identification algorithm,\ncalled $\\mathsf{BSAD}$, without explicit reward model inference, which is a\ncritical intermediate step in the contemporary RLHF paradigms for training\nlarge language models (LLM). The algorithm identifies the optimal policy\ndirectly from human preference information in a backward manner, employing a\ndueling bandit sub-routine that constantly duels actions to identify the\nsuperior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and\nbest-arm-identification-like adaptive stopping criteria to equalize the\nvisitation among all states in the same decision step while moving to the\nprevious step as soon as the optimal action is identifiable, leading to a\nprovable, instance-dependent sample complexity\n$\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which\nresembles the result in classic RL, where $c_{\\mathcal{M}}$ is the\ninstance-dependent constant and $M$ is the batch size. Moreover,\n$\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with\nlogarithmic regret and generalized to discounted MDPs using a frame-based\napproach. Our results show: (i) sample-complexity-wise, RLHF is not\nsignificantly harder than classic RL and (ii) end-to-end RLHF may deliver\nimproved performance by avoiding pitfalls in reward inferring such as overfit\nand distribution shift."
                },
                "authors": [
                    {
                        "name": "Qining Zhang"
                    },
                    {
                        "name": "Honghao Wei"
                    },
                    {
                        "name": "Lei Ying"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ying"
                },
                "author": "Lei Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v2",
                "updated": "2025-01-21T14:38:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    38,
                    35,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Hans Christian Farsethås"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "17 pages, 5 figures, 8 tables. Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12179v1",
                "updated": "2025-01-21T14:36:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    36,
                    41,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:36:41Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    36,
                    41,
                    1,
                    21,
                    0
                ],
                "title": "Block Adaptive Progressive Type-II Censored Sampling for the Inverted\n  Exponentiated Pareto Distribution: Parameter Inference and Reliability\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Adaptive Progressive Type-II Censored Sampling for the Inverted\n  Exponentiated Pareto Distribution: Parameter Inference and Reliability\n  Assessment"
                },
                "summary": "This article explores the estimation of unknown parameters and reliability\ncharacteristics under the assumption that the lifetimes of the testing units\nfollow an Inverted Exponentiated Pareto (IEP) distribution. Here, both point\nand interval estimates are calculated by employing the classical maximum\nlikelihood and a pivotal estimation methods. Also, existence and uniqueness of\nthe maximum likelihood estimates are verified. Further, asymptotic confidence\nintervals are derived by using the asymptotic normality property of the maximum\nlikelihood estimator. Moreover, generalized confidence intervals are obtained\nby utilizing the pivotal quantities. Additionally, some mathematical\ndevelopments of the IEP distribution are discussed based on the concept of\norder statistics. Furthermore, all the estimations are performed on the basis\nof the block censoring procedure, where an adaptive progressive Type-II\ncensoring is employed to every block. In this regard, the performances of two\nestimation methods, namely maximum likelihood estimation and pivotal\nestimation, is evaluated and compared through a simulation study. Finally, a\nreal data is illustrated to demonstrate the flexibility of the proposed IEP\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article explores the estimation of unknown parameters and reliability\ncharacteristics under the assumption that the lifetimes of the testing units\nfollow an Inverted Exponentiated Pareto (IEP) distribution. Here, both point\nand interval estimates are calculated by employing the classical maximum\nlikelihood and a pivotal estimation methods. Also, existence and uniqueness of\nthe maximum likelihood estimates are verified. Further, asymptotic confidence\nintervals are derived by using the asymptotic normality property of the maximum\nlikelihood estimator. Moreover, generalized confidence intervals are obtained\nby utilizing the pivotal quantities. Additionally, some mathematical\ndevelopments of the IEP distribution are discussed based on the concept of\norder statistics. Furthermore, all the estimations are performed on the basis\nof the block censoring procedure, where an adaptive progressive Type-II\ncensoring is employed to every block. In this regard, the performances of two\nestimation methods, namely maximum likelihood estimation and pivotal\nestimation, is evaluated and compared through a simulation study. Finally, a\nreal data is illustrated to demonstrate the flexibility of the proposed IEP\nmodel."
                },
                "authors": [
                    {
                        "name": "Rajendranath Mondal"
                    },
                    {
                        "name": "Aditi Kar Gangopadhyay"
                    },
                    {
                        "name": "Raju Bhakta"
                    },
                    {
                        "name": "Kousik Maiti"
                    }
                ],
                "author_detail": {
                    "name": "Kousik Maiti"
                },
                "author": "Kousik Maiti",
                "arxiv_comment": "26 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06589v2",
                "updated": "2025-01-21T14:33:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    33,
                    38,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-11T17:06:30Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    17,
                    6,
                    30,
                    5,
                    11,
                    0
                ],
                "title": "Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ladder-residual: parallelism-aware architecture for accelerating large\n  model inference with communication overlapping"
                },
                "summary": "Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 30% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model inference is both memory-intensive and time-consuming,\noften requiring distributed algorithms to efficiently scale. Various model\nparallelism strategies are used in multi-gpu training and inference to\npartition computation across multiple devices, reducing memory load and\ncomputation time. However, using model parallelism necessitates communication\nof information between GPUs, which has been a major bottleneck and limits the\ngains obtained by scaling up the number of devices. We introduce Ladder\nResidual, a simple architectural modification applicable to all residual-based\nmodels that enables straightforward overlapping that effectively hides the\nlatency of communication. Our insight is that in addition to systems\noptimization, one can also redesign the model architecture to decouple\ncommunication from computation. While Ladder Residual can allow\ncommunication-computation decoupling in conventional parallelism patterns, we\nfocus on Tensor Parallelism in this paper, which is particularly bottlenecked\nby its heavy communication. For a Transformer model with 70B parameters,\napplying Ladder Residual to all its layers can achieve 30% end-to-end wall\nclock speed up at inference time with TP sharding over 8 devices. We refer the\nresulting Transformer model as the Ladder Transformer. We train a 1B and 3B\nLadder Transformer from scratch and observe comparable performance to a\nstandard dense transformer baseline. We also show that it is possible to\nconvert parts of the Llama-3.1 8B model to our Ladder Residual architecture\nwith minimal accuracy degradation by only retraining for 3B tokens."
                },
                "authors": [
                    {
                        "name": "Muru Zhang"
                    },
                    {
                        "name": "Mayank Mishra"
                    },
                    {
                        "name": "Zhongzhu Zhou"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Shuaiwen Leon Song"
                    },
                    {
                        "name": "Ben Athiwaratkun"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12174v1",
                "updated": "2025-01-21T14:32:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    32,
                    50,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    32,
                    50,
                    1,
                    21,
                    0
                ],
                "title": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks"
                },
                "summary": "The rapid proliferation of Large Language Models (LLMs) has raised concerns\nabout misuse and the challenges of distinguishing AI-generated text from\nhuman-written content. Existing watermarking techniques, such as \\kgw, still\nface limitations under low watermark strength, stringent false-positive\nrequirements, and low-entropy scenarios. Our analysis reveals that current\ndetection methods rely on coarse estimates of non-watermarked text, which\nconstrains watermark detectability. We propose the Bipolar Watermark\n(BiMarker), a novel approach that divides generated text into positive and\nnegative poles, leveraging the difference in green token counts for detection.\nThis differential mechanism significantly enhances the detectability of\nwatermarked text. Theoretical analysis and experimental results demonstrate\nBiMarker's effectiveness and compatibility with existing optimization\ntechniques, offering a new optimization dimension for watermarking in\nLLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has raised concerns\nabout misuse and the challenges of distinguishing AI-generated text from\nhuman-written content. Existing watermarking techniques, such as \\kgw, still\nface limitations under low watermark strength, stringent false-positive\nrequirements, and low-entropy scenarios. Our analysis reveals that current\ndetection methods rely on coarse estimates of non-watermarked text, which\nconstrains watermark detectability. We propose the Bipolar Watermark\n(BiMarker), a novel approach that divides generated text into positive and\nnegative poles, leveraging the difference in green token counts for detection.\nThis differential mechanism significantly enhances the detectability of\nwatermarked text. Theoretical analysis and experimental results demonstrate\nBiMarker's effectiveness and compatibility with existing optimization\ntechniques, offering a new optimization dimension for watermarking in\nLLM-generated content."
                },
                "authors": [
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12162v1",
                "updated": "2025-01-21T14:15:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    15,
                    1,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:15:01Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    15,
                    1,
                    1,
                    21,
                    0
                ],
                "title": "AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative\n  Decoding"
                },
                "summary": "This paper introduces AdaServe, the first LLM serving system to support SLO\ncustomization through fine-grained speculative decoding. AdaServe leverages the\nlogits of a draft model to predict the speculative accuracy of tokens and\nemploys a theoretically optimal algorithm to construct token trees for\nverification. To accommodate diverse SLO requirements without compromising\nthroughput, AdaServe employs a speculation-and-selection scheme that first\nconstructs candidate token trees for each request and then dynamically selects\ntokens to meet individual SLO constraints while optimizing throughput.\nComprehensive evaluations demonstrate that AdaServe achieves up to 73% higher\nSLO attainment and 74% higher goodput compared to state-of-the-art systems.\nThese results underscore AdaServe's potential to enhance the efficiency and\nadaptability of LLM deployments across varied application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AdaServe, the first LLM serving system to support SLO\ncustomization through fine-grained speculative decoding. AdaServe leverages the\nlogits of a draft model to predict the speculative accuracy of tokens and\nemploys a theoretically optimal algorithm to construct token trees for\nverification. To accommodate diverse SLO requirements without compromising\nthroughput, AdaServe employs a speculation-and-selection scheme that first\nconstructs candidate token trees for each request and then dynamically selects\ntokens to meet individual SLO constraints while optimizing throughput.\nComprehensive evaluations demonstrate that AdaServe achieves up to 73% higher\nSLO attainment and 74% higher goodput compared to state-of-the-art systems.\nThese results underscore AdaServe's potential to enhance the efficiency and\nadaptability of LLM deployments across varied application scenarios."
                },
                "authors": [
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Remi Delacourt"
                    },
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Qinghan Chen"
                    },
                    {
                        "name": "Shuhuai Lin"
                    },
                    {
                        "name": "April Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Sean Lai"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12152v1",
                "updated": "2025-01-21T14:02:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:02:39Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "title": "Contextualizing Recommendation Explanations with LLMs: A User Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing Recommendation Explanations with LLMs: A User Study"
                },
                "summary": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems."
                },
                "authors": [
                    {
                        "name": "Yuanjun Feng"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Yash Raj Shrestha"
                    }
                ],
                "author_detail": {
                    "name": "Yash Raj Shrestha"
                },
                "author": "Yash Raj Shrestha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12147v1",
                "updated": "2025-01-21T14:00:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    0,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:00:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    0,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities"
                },
                "summary": "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities."
                },
                "authors": [
                    {
                        "name": "Qirun Dai"
                    },
                    {
                        "name": "Dylan Zhang"
                    },
                    {
                        "name": "Jiaqi W. Ma"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12134v1",
                "updated": "2025-01-21T13:47:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    47,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T13:47:22Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    47,
                    22,
                    1,
                    21,
                    0
                ],
                "title": "Do LLMs Provide Links to Code Similar to what they Generate? A Study\n  with Gemini and Bing CoPilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Provide Links to Code Similar to what they Generate? A Study\n  with Gemini and Bing CoPilot"
                },
                "summary": "Large Language Models (LLMs) are currently used for various software\ndevelopment tasks, including generating code snippets to solve specific\nproblems. Unlike reuse from the Web, LLMs are limited in providing provenance\ninformation about the generated code, which may have important trustworthiness\nand legal consequences. While LLM-based assistants may provide external links\nthat are \"related\" to the generated code, we do not know how relevant such\nlinks are. This paper presents the findings of an empirical study assessing the\nextent to which 243 and 194 code snippets, across six programming languages,\ngenerated by Bing CoPilot and Google Gemini, likely originate from the links\nprovided by these two LLM-based assistants. The study leverages automated code\nsimilarity assessments with thorough manual analysis. The study's findings\nindicate that the LLM-based assistants provide a mix of relevant and irrelevant\nlinks having a different nature. Specifically, although 66% of the links from\nBing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants\nstill suffer from serious \"provenance debt\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are currently used for various software\ndevelopment tasks, including generating code snippets to solve specific\nproblems. Unlike reuse from the Web, LLMs are limited in providing provenance\ninformation about the generated code, which may have important trustworthiness\nand legal consequences. While LLM-based assistants may provide external links\nthat are \"related\" to the generated code, we do not know how relevant such\nlinks are. This paper presents the findings of an empirical study assessing the\nextent to which 243 and 194 code snippets, across six programming languages,\ngenerated by Bing CoPilot and Google Gemini, likely originate from the links\nprovided by these two LLM-based assistants. The study leverages automated code\nsimilarity assessments with thorough manual analysis. The study's findings\nindicate that the LLM-based assistants provide a mix of relevant and irrelevant\nlinks having a different nature. Specifically, although 66% of the links from\nBing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants\nstill suffer from serious \"provenance debt\"."
                },
                "authors": [
                    {
                        "name": "Daniele Bifolco"
                    },
                    {
                        "name": "Pietro Cassieri"
                    },
                    {
                        "name": "Giuseppe Scanniello"
                    },
                    {
                        "name": "Massimiliano Di Penta"
                    },
                    {
                        "name": "Fiorella Zampetti"
                    }
                ],
                "author_detail": {
                    "name": "Fiorella Zampetti"
                },
                "author": "Fiorella Zampetti",
                "arxiv_journal_ref": "Proceedings of the 22nd ACM/IEEE International Conference on\n  Mining Software Repositories (MSR 2025), April 28-29 2025, Ottawa, ON, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12128v1",
                "updated": "2025-01-21T13:42:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    42,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T13:42:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    42,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced\n  Human-Robot Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced\n  Human-Robot Interactions"
                },
                "summary": "To achieve natural and intuitive interaction with people, HRI frameworks\ncombine a wide array of methods for human perception, intention communication,\nhuman-aware navigation and collaborative action. In practice, when encountering\nunpredictable behavior of people or unexpected states of the environment, these\nframeworks may lack the ability to dynamically recognize such states, adapt and\nrecover to resume the interaction. Large Language Models (LLMs), owing to their\nadvanced reasoning capabilities and context retention, present a promising\nsolution for enhancing robot adaptability. This potential, however, may not\ndirectly translate to improved interaction metrics. This paper considers a\nrepresentative interaction with an industrial robot involving approach,\ninstruction, and object manipulation, implemented in two conditions: (1) fully\nscripted and (2) including LLM-enhanced responses. We use gaze tracking and\nquestionnaires to measure the participants' task efficiency, engagement, and\nrobot perception. The results indicate higher subjective ratings for the LLM\ncondition, but objective metrics show that the scripted condition performs\ncomparably, particularly in efficiency and focus during simple tasks. We also\nnote that the scripted condition may have an edge over LLM-enhanced responses\nin terms of response latency and energy consumption, especially for trivial and\nrepetitive interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve natural and intuitive interaction with people, HRI frameworks\ncombine a wide array of methods for human perception, intention communication,\nhuman-aware navigation and collaborative action. In practice, when encountering\nunpredictable behavior of people or unexpected states of the environment, these\nframeworks may lack the ability to dynamically recognize such states, adapt and\nrecover to resume the interaction. Large Language Models (LLMs), owing to their\nadvanced reasoning capabilities and context retention, present a promising\nsolution for enhancing robot adaptability. This potential, however, may not\ndirectly translate to improved interaction metrics. This paper considers a\nrepresentative interaction with an industrial robot involving approach,\ninstruction, and object manipulation, implemented in two conditions: (1) fully\nscripted and (2) including LLM-enhanced responses. We use gaze tracking and\nquestionnaires to measure the participants' task efficiency, engagement, and\nrobot perception. The results indicate higher subjective ratings for the LLM\ncondition, but objective metrics show that the scripted condition performs\ncomparably, particularly in efficiency and focus during simple tasks. We also\nnote that the scripted condition may have an edge over LLM-enhanced responses\nin terms of response latency and energy consumption, especially for trivial and\nrepetitive interactions."
                },
                "authors": [
                    {
                        "name": "Tim Schreiter"
                    },
                    {
                        "name": "Jens V. Rüppel"
                    },
                    {
                        "name": "Rishi Hazra"
                    },
                    {
                        "name": "Andrey Rudenko"
                    },
                    {
                        "name": "Martin Magnusson"
                    },
                    {
                        "name": "Achim J. Lilienthal"
                    }
                ],
                "author_detail": {
                    "name": "Achim J. Lilienthal"
                },
                "author": "Achim J. Lilienthal",
                "arxiv_comment": "Accepted as a Late-Breaking Report to the 2025, 20th ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v1",
                "updated": "2025-01-21T12:56:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "48 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12093v1",
                "updated": "2025-01-21T12:38:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    38,
                    4,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:38:04Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    38,
                    4,
                    1,
                    21,
                    0
                ],
                "title": "Checkification: A Practical Approach for Testing Static Analysis Truths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checkification: A Practical Approach for Testing Static Analysis Truths"
                },
                "summary": "Static analysis is an essential component of many modern software development\ntools. Unfortunately, the ever-increasing complexity of static analyzers makes\ntheir coding error-prone. Even analysis tools based on rigorous mathematical\ntechniques, such as abstract interpretation, are not immune to bugs. Ensuring\nthe correctness and reliability of software analyzers is critical if they are\nto be inserted in production compilers and development environments. While\ncompiler validation has seen notable success, formal validation of static\nanalysis tools remains relatively unexplored. In this paper, we propose a\nmethod for testing abstract interpretation-based static analyzers. Broadly, it\nconsists in checking, over a suite of benchmarks, that the properties inferred\nstatically are satisfied dynamically. The main advantage of our approach lies\nin its simplicity, which stems directly from framing it within the Ciao\nassertion-based validation framework, and its blended static/dynamic assertion\nchecking approach. We demonstrate that in this setting, the analysis can be\ntested with little effort by combining the following components already present\nin the framework: 1) the static analyzer, which outputs its results as the\noriginal program source with assertions interspersed; 2) the assertion run-time\nchecking mechanism, which instruments a program to ensure that no assertion is\nviolated at run time; 3) the random test case generator, which generates random\ntest cases satisfying the properties present in assertion preconditions; and 4)\nthe unit-test framework, which executes those test cases. We have applied our\napproach to the CiaoPP static analyzer, resulting in the identification of many\nbugs with reasonable overhead. Most of these bugs have been either fixed or\nconfirmed, helping us detect a range of errors not only related to analysis\nsoundness but also within other aspects of the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static analysis is an essential component of many modern software development\ntools. Unfortunately, the ever-increasing complexity of static analyzers makes\ntheir coding error-prone. Even analysis tools based on rigorous mathematical\ntechniques, such as abstract interpretation, are not immune to bugs. Ensuring\nthe correctness and reliability of software analyzers is critical if they are\nto be inserted in production compilers and development environments. While\ncompiler validation has seen notable success, formal validation of static\nanalysis tools remains relatively unexplored. In this paper, we propose a\nmethod for testing abstract interpretation-based static analyzers. Broadly, it\nconsists in checking, over a suite of benchmarks, that the properties inferred\nstatically are satisfied dynamically. The main advantage of our approach lies\nin its simplicity, which stems directly from framing it within the Ciao\nassertion-based validation framework, and its blended static/dynamic assertion\nchecking approach. We demonstrate that in this setting, the analysis can be\ntested with little effort by combining the following components already present\nin the framework: 1) the static analyzer, which outputs its results as the\noriginal program source with assertions interspersed; 2) the assertion run-time\nchecking mechanism, which instruments a program to ensure that no assertion is\nviolated at run time; 3) the random test case generator, which generates random\ntest cases satisfying the properties present in assertion preconditions; and 4)\nthe unit-test framework, which executes those test cases. We have applied our\napproach to the CiaoPP static analyzer, resulting in the identification of many\nbugs with reasonable overhead. Most of these bugs have been either fixed or\nconfirmed, helping us detect a range of errors not only related to analysis\nsoundness but also within other aspects of the framework."
                },
                "authors": [
                    {
                        "name": "Daniela Ferreiro"
                    },
                    {
                        "name": "Ignacio Casso"
                    },
                    {
                        "name": "Jose F. Morales"
                    },
                    {
                        "name": "Pedro López-García"
                    },
                    {
                        "name": "Manuel V. Hermenegildo"
                    }
                ],
                "author_detail": {
                    "name": "Manuel V. Hermenegildo"
                },
                "author": "Manuel V. Hermenegildo",
                "arxiv_comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP). Extended, revised version of our work published in LOPSTR (Casso et\n  al. 2021)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12087v1",
                "updated": "2025-01-21T12:29:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    29,
                    45,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:29:45Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    29,
                    45,
                    1,
                    21,
                    0
                ],
                "title": "UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer\n  Model"
                },
                "summary": "Disaster recovery and management present significant challenges, particularly\nin unstable environments and hard-to-reach terrains. These difficulties can be\novercome by employing unmanned aerial vehicles (UAVs) equipped with onboard\nembedded platforms and camera sensors. In this work, we address the critical\nneed for accurate and timely disaster detection by enabling onboard aerial\nimagery processing and avoiding connectivity, privacy, and latency issues\ndespite the challenges posed by limited onboard hardware resources. We propose\na UAV-assisted edge framework for real-time disaster management, leveraging our\nproposed model optimized for real-time aerial image classification. The\noptimization of the model employs post-training quantization techniques. For\nreal-world disaster scenarios, we introduce a novel dataset, DisasterEye,\nfeaturing UAV-captured disaster scenes as well as ground-level images taken by\nindividuals on-site. Experimental results demonstrate the effectiveness of our\nmodel, achieving high accuracy with reduced inference latency and memory usage\non resource-constrained devices. The framework's scalability and adaptability\nmake it a robust solution for real-time disaster detection on resource-limited\nUAV platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaster recovery and management present significant challenges, particularly\nin unstable environments and hard-to-reach terrains. These difficulties can be\novercome by employing unmanned aerial vehicles (UAVs) equipped with onboard\nembedded platforms and camera sensors. In this work, we address the critical\nneed for accurate and timely disaster detection by enabling onboard aerial\nimagery processing and avoiding connectivity, privacy, and latency issues\ndespite the challenges posed by limited onboard hardware resources. We propose\na UAV-assisted edge framework for real-time disaster management, leveraging our\nproposed model optimized for real-time aerial image classification. The\noptimization of the model employs post-training quantization techniques. For\nreal-world disaster scenarios, we introduce a novel dataset, DisasterEye,\nfeaturing UAV-captured disaster scenes as well as ground-level images taken by\nindividuals on-site. Experimental results demonstrate the effectiveness of our\nmodel, achieving high accuracy with reduced inference latency and memory usage\non resource-constrained devices. The framework's scalability and adaptability\nmake it a robust solution for real-time disaster detection on resource-limited\nUAV platforms."
                },
                "authors": [
                    {
                        "name": "Branislava Jankovic"
                    },
                    {
                        "name": "Sabina Jangirova"
                    },
                    {
                        "name": "Waseem Ullah"
                    },
                    {
                        "name": "Latif U. Khan"
                    },
                    {
                        "name": "Mohsen Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Guizani"
                },
                "author": "Mohsen Guizani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01475v2",
                "updated": "2025-01-21T12:20:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    20,
                    18,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-02T12:27:55Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    27,
                    55,
                    2,
                    276,
                    0
                ],
                "title": "Exploring Learning Rate Selection in Generalised Bayesian Inference\n  using Posterior Predictive Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Learning Rate Selection in Generalised Bayesian Inference\n  using Posterior Predictive Checks"
                },
                "summary": "Generalised Bayesian Inference (GBI) attempts to address model\nmisspecification in a standard Bayesian setup by tempering the likelihood. The\nlikelihood is raised to a fractional power, called the learning rate, which\nreduces its importance in the posterior and has been established as a method to\naddress certain kinds of model misspecification. Posterior Predictive Checks\n(PPC) attempt to detect model misspecification by locating a diagnostic,\ncomputed on the observed data, within the posterior predictive distribution of\nthe diagnostic. This can be used to construct a hypothesis test where a small\n$p$-value indicates potential misfit. The recent Embedded Diachronic Sense\nChange (EDiSC) model suffers from misspecification and benefits from likelihood\ntempering. Using EDiSC as a case study, this exploratory work examines whether\nPPC could be used in a novel way to set the learning rate in a GBI setup.\nSpecifically, the learning rate selected is the lowest value for which a\nhypothesis test using the log likelihood diagnostic is not rejected at the 10%\nlevel. The experimental results are promising, though not definitive, and\nindicate the need for further research along the lines suggested here.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalised Bayesian Inference (GBI) attempts to address model\nmisspecification in a standard Bayesian setup by tempering the likelihood. The\nlikelihood is raised to a fractional power, called the learning rate, which\nreduces its importance in the posterior and has been established as a method to\naddress certain kinds of model misspecification. Posterior Predictive Checks\n(PPC) attempt to detect model misspecification by locating a diagnostic,\ncomputed on the observed data, within the posterior predictive distribution of\nthe diagnostic. This can be used to construct a hypothesis test where a small\n$p$-value indicates potential misfit. The recent Embedded Diachronic Sense\nChange (EDiSC) model suffers from misspecification and benefits from likelihood\ntempering. Using EDiSC as a case study, this exploratory work examines whether\nPPC could be used in a novel way to set the learning rate in a GBI setup.\nSpecifically, the learning rate selected is the lowest value for which a\nhypothesis test using the log likelihood diagnostic is not rejected at the 10%\nlevel. The experimental results are promising, though not definitive, and\nindicate the need for further research along the lines suggested here."
                },
                "authors": [
                    {
                        "name": "Schyan Zafar"
                    },
                    {
                        "name": "Geoff K. Nicholls"
                    }
                ],
                "author_detail": {
                    "name": "Geoff K. Nicholls"
                },
                "author": "Geoff K. Nicholls",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08706v2",
                "updated": "2025-01-21T12:08:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    8,
                    11,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-11T10:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    48,
                    51,
                    4,
                    285,
                    0
                ],
                "title": "Goal-Oriented Status Updating for Real-time Remote Inference over\n  Networks with Two-Way~Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-Oriented Status Updating for Real-time Remote Inference over\n  Networks with Two-Way~Delay"
                },
                "summary": "We study a setting where an intelligent model (e.g., a pre-trained neural\nnetwork) predicts the real-time value of a target signal using data samples\ntransmitted from a remote source according to a scheduling policy. The\nscheduler decides on i) the age of the samples to be sent, ii) when to send\nthem, and iii) the length of each packet (i.e., the number of samples contained\nin each packet). The dependence of inference quality on the Age of Information\n(AoI) for a given packet length is modeled by a general relationship. Previous\nwork assumed i.i.d. transmission delays with immediate feedback or were\nrestricted to the case where inference performance degrades as the input data\nages. Our formulation, in addition to capturing non-monotone age dependence,\nalso covers Markovian delay on both forward and feedback links. We model this\nas an infinite-horizon average-cost Semi-Markov Decision Process. We obtain a\nclosed-form solution that decides on (i) and (ii) for any constant packet\nlength. The solution for when to send is an index-based threshold policy, where\nthe index function is expressed in terms of the delay state and AoI at the\nreceiver. The age of the packet selected is a function of the delay state. We\nseparately optimize the value of the constant length. We also develop an\nindex-based threshold policy for the variable length case, which allows a\ncomplexity reduction. In simulation results, we observe that our goal-oriented\nscheduler drops inference error down to one sixth with respect to age-based\nscheduling of unit-length packets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a setting where an intelligent model (e.g., a pre-trained neural\nnetwork) predicts the real-time value of a target signal using data samples\ntransmitted from a remote source according to a scheduling policy. The\nscheduler decides on i) the age of the samples to be sent, ii) when to send\nthem, and iii) the length of each packet (i.e., the number of samples contained\nin each packet). The dependence of inference quality on the Age of Information\n(AoI) for a given packet length is modeled by a general relationship. Previous\nwork assumed i.i.d. transmission delays with immediate feedback or were\nrestricted to the case where inference performance degrades as the input data\nages. Our formulation, in addition to capturing non-monotone age dependence,\nalso covers Markovian delay on both forward and feedback links. We model this\nas an infinite-horizon average-cost Semi-Markov Decision Process. We obtain a\nclosed-form solution that decides on (i) and (ii) for any constant packet\nlength. The solution for when to send is an index-based threshold policy, where\nthe index function is expressed in terms of the delay state and AoI at the\nreceiver. The age of the packet selected is a function of the delay state. We\nseparately optimize the value of the constant length. We also develop an\nindex-based threshold policy for the variable length case, which allows a\ncomplexity reduction. In simulation results, we observe that our goal-oriented\nscheduler drops inference error down to one sixth with respect to age-based\nscheduling of unit-length packets."
                },
                "authors": [
                    {
                        "name": "Cagri Ari"
                    },
                    {
                        "name": "Md Kamran Chowdhury Shisher"
                    },
                    {
                        "name": "Yin Sun"
                    },
                    {
                        "name": "Elif Uysal"
                    }
                ],
                "author_detail": {
                    "name": "Elif Uysal"
                },
                "author": "Elif Uysal",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12067v1",
                "updated": "2025-01-21T11:42:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    42,
                    9,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:42:09Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    42,
                    9,
                    1,
                    21,
                    0
                ],
                "title": "EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular\n  Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular\n  Value Decomposition"
                },
                "summary": "Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of\ntrainable parameters. However, they often suffer from scalability issues and\ndifferences between their learning pattern and full fine-tuning. To overcome\nthese limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation\n(EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude\nand directional components. By freezing low-rank matrices, initializing them by\nsingular value decomposition, and introducing a small trainable matrix between\nthem, EDoRA achieves substantial reduction in trainable parameters while\nmaintaining learning capacity. Experimental results on the GLUE benchmark\ndemonstrate that EDoRA achieves competitive or superior performance compared to\nstate-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable\nparameters. This makes EDoRA a highly efficient solution for adapting LLMs to\ndiverse tasks under memory-constrained settings. Code is available at\nhttps://github.com/Hamid-Nasiri/EDoRA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of\ntrainable parameters. However, they often suffer from scalability issues and\ndifferences between their learning pattern and full fine-tuning. To overcome\nthese limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation\n(EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude\nand directional components. By freezing low-rank matrices, initializing them by\nsingular value decomposition, and introducing a small trainable matrix between\nthem, EDoRA achieves substantial reduction in trainable parameters while\nmaintaining learning capacity. Experimental results on the GLUE benchmark\ndemonstrate that EDoRA achieves competitive or superior performance compared to\nstate-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable\nparameters. This makes EDoRA a highly efficient solution for adapting LLMs to\ndiverse tasks under memory-constrained settings. Code is available at\nhttps://github.com/Hamid-Nasiri/EDoRA ."
                },
                "authors": [
                    {
                        "name": "Hamid Nasiri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v7",
                "updated": "2025-01-21T11:36:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    36,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12053v1",
                "updated": "2025-01-21T11:26:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    26,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:26:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    26,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "PINNsAgent: Automated PDE Surrogation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINNsAgent: Automated PDE Surrogation with Large Language Models"
                },
                "summary": "Solving partial differential equations (PDEs) using neural methods has been a\nlong-standing scientific and engineering research pursuit. Physics-Informed\nNeural Networks (PINNs) have emerged as a promising alternative to traditional\nnumerical methods for solving PDEs. However, the gap between domain-specific\nknowledge and deep learning expertise often limits the practical application of\nPINNs. Previous works typically involve manually conducting extensive PINNs\nexperiments and summarizing heuristic rules for hyperparameter tuning. In this\nwork, we introduce PINNsAgent, a novel surrogation framework that leverages\nlarge language models (LLMs) and utilizes PINNs as a foundation to bridge the\ngap between domain-specific knowledge and deep learning. Specifically,\nPINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes\nthe essential characteristics of PDEs and their associated best-performing\nPINNs configurations into a structured format, enabling efficient knowledge\ntransfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a\nstrategy that effectively explores the search space for optimal PINNs\narchitectures. By leveraging LLMs and exploration strategies, PINNsAgent\nenhances the automation and efficiency of PINNs-based solutions. We evaluate\nPINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating\nthe surrogation process and significantly improving the accuracy of PINNs-based\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving partial differential equations (PDEs) using neural methods has been a\nlong-standing scientific and engineering research pursuit. Physics-Informed\nNeural Networks (PINNs) have emerged as a promising alternative to traditional\nnumerical methods for solving PDEs. However, the gap between domain-specific\nknowledge and deep learning expertise often limits the practical application of\nPINNs. Previous works typically involve manually conducting extensive PINNs\nexperiments and summarizing heuristic rules for hyperparameter tuning. In this\nwork, we introduce PINNsAgent, a novel surrogation framework that leverages\nlarge language models (LLMs) and utilizes PINNs as a foundation to bridge the\ngap between domain-specific knowledge and deep learning. Specifically,\nPINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes\nthe essential characteristics of PDEs and their associated best-performing\nPINNs configurations into a structured format, enabling efficient knowledge\ntransfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a\nstrategy that effectively explores the search space for optimal PINNs\narchitectures. By leveraging LLMs and exploration strategies, PINNsAgent\nenhances the automation and efficiency of PINNs-based solutions. We evaluate\nPINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating\nthe surrogation process and significantly improving the accuracy of PINNs-based\nsolutions."
                },
                "authors": [
                    {
                        "name": "Qingpo Wuwu"
                    },
                    {
                        "name": "Chonghan Gao"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Yihang Huang"
                    },
                    {
                        "name": "Yuekai Zhang"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Jianxin Li"
                    },
                    {
                        "name": "Haoyi Zhou"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "9 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12051v1",
                "updated": "2025-01-21T11:24:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    24,
                    55,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:24:55Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    24,
                    55,
                    1,
                    21,
                    0
                ],
                "title": "MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow\n  Thinking"
                },
                "summary": "Medical language models (MLMs) have become pivotal in advancing medical\nnatural language processing. However, prior models that rely on pre-training or\nsupervised fine-tuning often exhibit low data efficiency and limited\npracticality in real-world clinical applications. While OpenAIs O1 highlights\ntest-time scaling in mathematics, attempts to replicate this approach in\nmedicine typically distill responses from GPT-series models to open-source\nmodels, focusing primarily on multiple-choice tasks. This strategy, though\nstraightforward, neglects critical concerns like data privacy and realistic\ndeployment in clinical settings. In this work, we present a deployable,\nsmall-scale medical language model, \\mone, designed for long-chain reasoning in\nclinical tasks using a self-evolution paradigm. Starting with a seed dataset of\naround 8,000 instances spanning five domains and 16 datasets, we prompt a base\npolicy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable\nreasoning chains. Each reasoning step is assigned an evolution rollout value,\nallowing verified trajectories to train the policy model and the reward model.\nDuring inference, the policy model generates multiple responses, and the reward\nmodel selects the one with the highest reward score. Experiments on eleven\nevaluation datasets demonstrate that \\mone outperforms prior open-source models\nby 2 points, with the addition of the reward model further boosting performance\n($\\sim$13 points), surpassing GPT-4o-mini. Code and data are available at\n\\url{https://github.com/pixas/MedSSS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical language models (MLMs) have become pivotal in advancing medical\nnatural language processing. However, prior models that rely on pre-training or\nsupervised fine-tuning often exhibit low data efficiency and limited\npracticality in real-world clinical applications. While OpenAIs O1 highlights\ntest-time scaling in mathematics, attempts to replicate this approach in\nmedicine typically distill responses from GPT-series models to open-source\nmodels, focusing primarily on multiple-choice tasks. This strategy, though\nstraightforward, neglects critical concerns like data privacy and realistic\ndeployment in clinical settings. In this work, we present a deployable,\nsmall-scale medical language model, \\mone, designed for long-chain reasoning in\nclinical tasks using a self-evolution paradigm. Starting with a seed dataset of\naround 8,000 instances spanning five domains and 16 datasets, we prompt a base\npolicy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable\nreasoning chains. Each reasoning step is assigned an evolution rollout value,\nallowing verified trajectories to train the policy model and the reward model.\nDuring inference, the policy model generates multiple responses, and the reward\nmodel selects the one with the highest reward score. Experiments on eleven\nevaluation datasets demonstrate that \\mone outperforms prior open-source models\nby 2 points, with the addition of the reward model further boosting performance\n($\\sim$13 points), surpassing GPT-4o-mini. Code and data are available at\n\\url{https://github.com/pixas/MedSSS}."
                },
                "authors": [
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "19 pages; technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09720v2",
                "updated": "2025-01-21T11:03:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    3,
                    55,
                    1,
                    21,
                    0
                ],
                "published": "2024-05-15T22:30:51Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    22,
                    30,
                    51,
                    2,
                    136,
                    0
                ],
                "title": "Inferring the Ionizing Photon Contributions of High-Redshift Galaxies to\n  Reionization with JWST NIRCam Photometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the Ionizing Photon Contributions of High-Redshift Galaxies to\n  Reionization with JWST NIRCam Photometry"
                },
                "summary": "JWST is providing constraints on the history of reionization owing to its\nability to detect faint galaxies at $z\\gg6$. Modeling this history requires\nunderstanding both the ionizing photon production rate ($\\xi_{\\rm ion}$) and\nthe fraction of those photons that escape into the intergalactic medium\n($f_{\\rm esc}$). Observational estimates of these quantities generally rely on\nspectroscopy for which large samples with well-defined selection functions are\nlimited. To overcome this challenge, we present and release an implicit\nlikelihood inference pipeline, PHOTONIOn, trained on mock photometry to predict\nthe escaped ionizing luminosity of individual galaxies ($\\dot{N}_{\\rm ion}$)\nbased on photometric magnitudes and redshifts. We show that PHOTONIOn is able\nto reliably infer $\\dot{N}_{\\rm ion}$ from photometry. This is in contrast to\ntraditional SED-fitting approaches which rely on $f_{\\rm esc}$ prescriptions\nthat often over-predict $\\dot{N}_{\\rm ion}$ for LyC-dim galaxies, even when\ngiven access to spectroscopic data. We have deployed PHOTONIOn on a sample of\n4,559 high-redshift galaxies from the JADES Deep survey, finding gentle\nredshift evolutions of $\\log_{10}(\\dot{N}_{\\rm ion}) = (0.08\\pm0.01)z +\n(51.60\\pm0.06)$ and $\\log_{10}(f_{\\rm esc}\\xi_{\\rm ion}) = (0.07\\pm0.01)z +\n(24.12\\pm0.07)$. Late-time values for the ionizing photon production rate\ndensity are consistent with theoretical models and observations. We measure the\nevolution of the IGM ionized fraction to find that observed populations of\nstar-forming galaxies are capable of driving reionization in GOODS-S to\ncompletion by $z\\sim 5.3$ without the need for AGN or other exotic sources,\nconsistent with other studies of the same field. The $20\\%$ of UV-brightest\ngalaxies ($M_{\\rm UV}<-18.5$) reionize $35\\%$ of the survey volume,\ndemonstrating that UV faint LyC emitters are crucial for reionization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST is providing constraints on the history of reionization owing to its\nability to detect faint galaxies at $z\\gg6$. Modeling this history requires\nunderstanding both the ionizing photon production rate ($\\xi_{\\rm ion}$) and\nthe fraction of those photons that escape into the intergalactic medium\n($f_{\\rm esc}$). Observational estimates of these quantities generally rely on\nspectroscopy for which large samples with well-defined selection functions are\nlimited. To overcome this challenge, we present and release an implicit\nlikelihood inference pipeline, PHOTONIOn, trained on mock photometry to predict\nthe escaped ionizing luminosity of individual galaxies ($\\dot{N}_{\\rm ion}$)\nbased on photometric magnitudes and redshifts. We show that PHOTONIOn is able\nto reliably infer $\\dot{N}_{\\rm ion}$ from photometry. This is in contrast to\ntraditional SED-fitting approaches which rely on $f_{\\rm esc}$ prescriptions\nthat often over-predict $\\dot{N}_{\\rm ion}$ for LyC-dim galaxies, even when\ngiven access to spectroscopic data. We have deployed PHOTONIOn on a sample of\n4,559 high-redshift galaxies from the JADES Deep survey, finding gentle\nredshift evolutions of $\\log_{10}(\\dot{N}_{\\rm ion}) = (0.08\\pm0.01)z +\n(51.60\\pm0.06)$ and $\\log_{10}(f_{\\rm esc}\\xi_{\\rm ion}) = (0.07\\pm0.01)z +\n(24.12\\pm0.07)$. Late-time values for the ionizing photon production rate\ndensity are consistent with theoretical models and observations. We measure the\nevolution of the IGM ionized fraction to find that observed populations of\nstar-forming galaxies are capable of driving reionization in GOODS-S to\ncompletion by $z\\sim 5.3$ without the need for AGN or other exotic sources,\nconsistent with other studies of the same field. The $20\\%$ of UV-brightest\ngalaxies ($M_{\\rm UV}<-18.5$) reionize $35\\%$ of the survey volume,\ndemonstrating that UV faint LyC emitters are crucial for reionization."
                },
                "authors": [
                    {
                        "name": "Nicholas Choustikov"
                    },
                    {
                        "name": "Richard Stiskalek"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Harley Katz"
                    },
                    {
                        "name": "Julien Devriendt"
                    },
                    {
                        "name": "Adrianne Slyz"
                    }
                ],
                "author_detail": {
                    "name": "Adrianne Slyz"
                },
                "author": "Adrianne Slyz",
                "arxiv_comment": "18 pages, 11 figures, 1 table, accepted for publication in MNRAS. New\n  version includes a deeper comparison with a traditional SED-fitting code\n  coupled to various escape fraction models and a new discussion of caveats",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13783v2",
                "updated": "2025-01-21T11:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    2,
                    11,
                    1,
                    21,
                    0
                ],
                "published": "2024-02-21T13:07:43Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    13,
                    7,
                    43,
                    2,
                    52,
                    0
                ],
                "title": "A comparison of shrinkage estimators of the cosmological precision\n  matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comparison of shrinkage estimators of the cosmological precision\n  matrix"
                },
                "summary": "The determination of the covariance matrix and its inverse, the precision\nmatrix, is critical in the statistical analysis of cosmological measurements.\nThe covariance matrix is typically estimated with a limited number of\nsimulations at great computational cost before inversion into the precision\nmatrix; therefore, it can be ill-conditioned and overly noisy when the sample\nsize $n$ used for estimation is not much larger than the data vector dimension.\nIn this work, we consider a class of methods known as shrinkage estimation for\nthe precision matrix, which combines an empirical estimate with a target that\nis either analytical or stochastic. These methods include linear and non-linear\nshrinkage applied to the covariance matrix (the latter represented by the\nso-called NERCOME estimator), and the direct linear shrinkage estimation of the\nprecision matrix which we introduce in a cosmological setting. Using Bayesian\nparameter inference as well as metrics like matrix loss functions and the\neigenvalue spectrum, we compare their performance against the standard sample\nestimator with varying sample size $n$. We have found the shrinkage estimators\nto significantly improve the posterior distribution at low $n$, especially for\nthe linear shrinkage estimators either inverted from the covariance matrix or\napplied directly to the precision matrix, with an empirical target constructed\nfrom the sample estimate. Our results should be particularly relevant to the\nanalyses of Stage-IV spectroscopic galaxy surveys such as the Dark Energy\nSpectroscopic Instrument (DESI) and Euclid, whose statistical power can be\nlimited by the computational cost of obtaining an accurate precision matrix\nestimate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The determination of the covariance matrix and its inverse, the precision\nmatrix, is critical in the statistical analysis of cosmological measurements.\nThe covariance matrix is typically estimated with a limited number of\nsimulations at great computational cost before inversion into the precision\nmatrix; therefore, it can be ill-conditioned and overly noisy when the sample\nsize $n$ used for estimation is not much larger than the data vector dimension.\nIn this work, we consider a class of methods known as shrinkage estimation for\nthe precision matrix, which combines an empirical estimate with a target that\nis either analytical or stochastic. These methods include linear and non-linear\nshrinkage applied to the covariance matrix (the latter represented by the\nso-called NERCOME estimator), and the direct linear shrinkage estimation of the\nprecision matrix which we introduce in a cosmological setting. Using Bayesian\nparameter inference as well as metrics like matrix loss functions and the\neigenvalue spectrum, we compare their performance against the standard sample\nestimator with varying sample size $n$. We have found the shrinkage estimators\nto significantly improve the posterior distribution at low $n$, especially for\nthe linear shrinkage estimators either inverted from the covariance matrix or\napplied directly to the precision matrix, with an empirical target constructed\nfrom the sample estimate. Our results should be particularly relevant to the\nanalyses of Stage-IV spectroscopic galaxy surveys such as the Dark Energy\nSpectroscopic Instrument (DESI) and Euclid, whose statistical power can be\nlimited by the computational cost of obtaining an accurate precision matrix\nestimate."
                },
                "authors": [
                    {
                        "name": "Marnix J. Looijmans"
                    },
                    {
                        "name": "Mike Shengbo Wang"
                    },
                    {
                        "name": "Florian Beutler"
                    }
                ],
                "author_detail": {
                    "name": "Florian Beutler"
                },
                "author": "Florian Beutler",
                "arxiv_doi": "10.1093/mnras/stae2786",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2786",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.13783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 11 figures, 4 tables. Accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03538v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03538v3",
                "updated": "2025-01-21T11:01:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    1,
                    24,
                    1,
                    21,
                    0
                ],
                "published": "2023-12-06T15:01:47Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    15,
                    1,
                    47,
                    2,
                    340,
                    0
                ],
                "title": "Bayesian variable selection in sample selection models using\n  spike-and-slab priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian variable selection in sample selection models using\n  spike-and-slab priors"
                },
                "summary": "Sample selection models represent a common methodology for correcting bias\ninduced by data missing not at random. These models are not empirically\nidentifiable without exclusion restrictions. In other words, some variables\npredictive of missingness do not affect the outcome model of interest. The\ndrive to establish this requirement often leads to the inclusion of irrelevant\nvariables in the model. A recent proposal uses adaptive LASSO to circumvent\nthis problem, but its performance depends on the so-called covariance\nassumption, which can be violated in small to moderate samples. Additionally,\nthere are no tools yet for post-selection inference for this model. To address\nthese challenges, we propose two families of spike-and-slab priors to conduct\nBayesian variable selection in sample selection models. These prior structures\nallow for constructing a Gibbs sampler with tractable conditionals, which is\nscalable to the dimensions of practical interest. We illustrate the performance\nof the proposed methodology through a simulation study and present a comparison\nagainst adaptive LASSO and stepwise selection. We also provide two applications\nusing publicly available real data. An implementation and code to reproduce the\nresults in this paper can be found at\nhttps://github.com/adam-iqbal/selection-spike-slab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample selection models represent a common methodology for correcting bias\ninduced by data missing not at random. These models are not empirically\nidentifiable without exclusion restrictions. In other words, some variables\npredictive of missingness do not affect the outcome model of interest. The\ndrive to establish this requirement often leads to the inclusion of irrelevant\nvariables in the model. A recent proposal uses adaptive LASSO to circumvent\nthis problem, but its performance depends on the so-called covariance\nassumption, which can be violated in small to moderate samples. Additionally,\nthere are no tools yet for post-selection inference for this model. To address\nthese challenges, we propose two families of spike-and-slab priors to conduct\nBayesian variable selection in sample selection models. These prior structures\nallow for constructing a Gibbs sampler with tractable conditionals, which is\nscalable to the dimensions of practical interest. We illustrate the performance\nof the proposed methodology through a simulation study and present a comparison\nagainst adaptive LASSO and stepwise selection. We also provide two applications\nusing publicly available real data. An implementation and code to reproduce the\nresults in this paper can be found at\nhttps://github.com/adam-iqbal/selection-spike-slab."
                },
                "authors": [
                    {
                        "name": "Adam Iqbal"
                    },
                    {
                        "name": "Emmanuel O. Ogundimu"
                    },
                    {
                        "name": "F. Javier Rubio"
                    }
                ],
                "author_detail": {
                    "name": "F. Javier Rubio"
                },
                "author": "F. Javier Rubio",
                "arxiv_comment": "An implementation and code used to reproduce simulation studies and\n  the real data applications can be found at\n  https://github.com/adam-iqbal/selection-spike-slab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03538v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03538v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12031v1",
                "updated": "2025-01-21T10:48:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    48,
                    55,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T10:48:55Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    48,
                    55,
                    1,
                    21,
                    0
                ],
                "title": "Probing Dark Star Parameters Through $f$-Mode Gravitational Wave Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Dark Star Parameters Through $f$-Mode Gravitational Wave Signals"
                },
                "summary": "Theoretical models of self-interacting dark matter offer a promising solution\nto several unresolved issues within the collisionless cold dark matter\nframework. For asymmetric dark matter, these self-interactions may encourage\ngravitational collapse, potentially leading to the creation of compact objects\nprimarily composed of dark matter. By considering both fermionic and bosonic\nequations of state, we analyze the equilibrium structure of non-rotating dark\nstars, examining their bulk properties and comparing them with baryonic neutron\nstars. We show that the frequency and damping rate of $f$-mode oscillations of\ndark compact stars can be expressed in terms of universal functions of stellar\nmass, radius and moment of inertia. Finally, by employing the universality in\nthe $f$-mode, we propose a scheme to infer accurate values of the physical\nparameters of dark compact stars from their $f$-mode gravitational wave signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical models of self-interacting dark matter offer a promising solution\nto several unresolved issues within the collisionless cold dark matter\nframework. For asymmetric dark matter, these self-interactions may encourage\ngravitational collapse, potentially leading to the creation of compact objects\nprimarily composed of dark matter. By considering both fermionic and bosonic\nequations of state, we analyze the equilibrium structure of non-rotating dark\nstars, examining their bulk properties and comparing them with baryonic neutron\nstars. We show that the frequency and damping rate of $f$-mode oscillations of\ndark compact stars can be expressed in terms of universal functions of stellar\nmass, radius and moment of inertia. Finally, by employing the universality in\nthe $f$-mode, we propose a scheme to infer accurate values of the physical\nparameters of dark compact stars from their $f$-mode gravitational wave signal."
                },
                "authors": [
                    {
                        "name": "Mariachiara Celato"
                    },
                    {
                        "name": "Christian J. Krüger"
                    },
                    {
                        "name": "Kostas D. Kokkotas"
                    }
                ],
                "author_detail": {
                    "name": "Kostas D. Kokkotas"
                },
                "author": "Kostas D. Kokkotas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06066v2",
                "updated": "2025-01-21T10:48:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    48,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-10T15:57:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Distilling Calibration via Conformalized Credal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Calibration via Conformalized Credal Inference"
                },
                "summary": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12012v1",
                "updated": "2025-01-21T10:06:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    6,
                    19,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T10:06:19Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    6,
                    19,
                    1,
                    21,
                    0
                ],
                "title": "TabularARGN: A Flexible and Efficient Auto-Regressive Framework for\n  Generating High-Fidelity Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabularARGN: A Flexible and Efficient Auto-Regressive Framework for\n  Generating High-Fidelity Synthetic Data"
                },
                "summary": "Synthetic data generation for tabular datasets must balance fidelity,\nefficiency, and versatility to meet the demands of real-world applications. We\nintroduce the Tabular Auto-Regressive Generative Network (TabularARGN), a\nflexible framework designed to handle mixed-type, multivariate, and sequential\ndatasets. By training on all possible conditional probabilities, TabularARGN\nsupports advanced features such as fairness-aware generation, imputation, and\nconditional generation on any subset of columns. The framework achieves\nstate-of-the-art synthetic data quality while significantly reducing training\nand inference times, making it ideal for large-scale datasets with diverse\nstructures. Evaluated across established benchmarks, including realistic\ndatasets with complex relationships, TabularARGN demonstrates its capability to\nsynthesize high-quality data efficiently. By unifying flexibility and\nperformance, this framework paves the way for practical synthetic data\ngeneration across industries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic data generation for tabular datasets must balance fidelity,\nefficiency, and versatility to meet the demands of real-world applications. We\nintroduce the Tabular Auto-Regressive Generative Network (TabularARGN), a\nflexible framework designed to handle mixed-type, multivariate, and sequential\ndatasets. By training on all possible conditional probabilities, TabularARGN\nsupports advanced features such as fairness-aware generation, imputation, and\nconditional generation on any subset of columns. The framework achieves\nstate-of-the-art synthetic data quality while significantly reducing training\nand inference times, making it ideal for large-scale datasets with diverse\nstructures. Evaluated across established benchmarks, including realistic\ndatasets with complex relationships, TabularARGN demonstrates its capability to\nsynthesize high-quality data efficiently. By unifying flexibility and\nperformance, this framework paves the way for practical synthetic data\ngeneration across industries."
                },
                "authors": [
                    {
                        "name": "Paul Tiwald"
                    },
                    {
                        "name": "Ivona Krchova"
                    },
                    {
                        "name": "Andrey Sidorenko"
                    },
                    {
                        "name": "Mariana Vargas-Vieyra"
                    },
                    {
                        "name": "Mario Scriminaci"
                    },
                    {
                        "name": "Michael Platzer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Platzer"
                },
                "author": "Michael Platzer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11997v1",
                "updated": "2025-01-21T09:45:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    9,
                    45,
                    24,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T09:45:24Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    9,
                    45,
                    24,
                    1,
                    21,
                    0
                ],
                "title": "Deep Photometric and Astrometric Investigation of the Non-relaxed Star\n  Cluster Stock 3 using Gaia DR3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Photometric and Astrometric Investigation of the Non-relaxed Star\n  Cluster Stock 3 using Gaia DR3"
                },
                "summary": "The study presents both photometric and kinematic analyses of the non-relaxed\nopen cluster Stock 3 with Gaia DR3 which found to be positioned at 2.945 $\\pm$\n0.700 kpc and having an age of 16.00 $\\pm$ 4.00 Myr. We analyse the data to\ninfer the membership and thus determine the total mass, IMF and the dynamical\nand kinematical status.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study presents both photometric and kinematic analyses of the non-relaxed\nopen cluster Stock 3 with Gaia DR3 which found to be positioned at 2.945 $\\pm$\n0.700 kpc and having an age of 16.00 $\\pm$ 4.00 Myr. We analyse the data to\ninfer the membership and thus determine the total mass, IMF and the dynamical\nand kinematical status."
                },
                "authors": [
                    {
                        "name": "A. Ahmed"
                    },
                    {
                        "name": "Amira R. Youssef"
                    },
                    {
                        "name": "M. S. El-Nawawy"
                    },
                    {
                        "name": "W. H. Elsanhoury"
                    }
                ],
                "author_detail": {
                    "name": "W. H. Elsanhoury"
                },
                "author": "W. H. Elsanhoury",
                "arxiv_comment": "14 pages, 13 figures, 4 Tables, Accepted for publication in the\n  Astrophysical Bulletin journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v5",
                "updated": "2025-01-21T09:25:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    9,
                    25,
                    25,
                    1,
                    21,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with various strategies of instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. At its core, the greedy selection procedure iteratively picks\nclient centers that maximize the diversity and coverage of the instruction\nspace while avoiding redundancy with previously selected centers. This ensures\nbroad yet efficient coverage of the domain distribution across clients. For\nclient-side computational efficiency and system scalability, FedDCA$^*$, the\nvariant of FedDCA, utilizes heterogeneous encoders with server-side feature\nalignment. Extensive experiments across code, medical, financial, and\nmathematical domains substantiate the effectiveness of both methods, as well as\nplug-and-play capability. We further analyze privacy preservation against\nmemory extraction attacks, showing that while privacy leakage risk is\nindependent of augmented public data ratio, it decreases or converges as\ntraining progresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with various strategies of instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. At its core, the greedy selection procedure iteratively picks\nclient centers that maximize the diversity and coverage of the instruction\nspace while avoiding redundancy with previously selected centers. This ensures\nbroad yet efficient coverage of the domain distribution across clients. For\nclient-side computational efficiency and system scalability, FedDCA$^*$, the\nvariant of FedDCA, utilizes heterogeneous encoders with server-side feature\nalignment. Extensive experiments across code, medical, financial, and\nmathematical domains substantiate the effectiveness of both methods, as well as\nplug-and-play capability. We further analyze privacy preservation against\nmemory extraction attacks, showing that while privacy leakage risk is\nindependent of augmented public data ratio, it decreases or converges as\ntraining progresses."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yugang Jiang"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11986v1",
                "updated": "2025-01-21T09:05:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    9,
                    5,
                    46,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T09:05:46Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    9,
                    5,
                    46,
                    1,
                    21,
                    0
                ],
                "title": "Diffeomorphic ICP Registration for Single and Multiple Point Sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffeomorphic ICP Registration for Single and Multiple Point Sets"
                },
                "summary": "We propose a generalization of the iterative closest point (ICP) algorithm\nfor point set registration, in which the registration functions are non-rigid\nand follow the large deformation diffeomorphic metric mapping (LDDMM)\nframework. The algorithm is formulated as a well-posed probabilistic inference,\nand requires to solve a novel variation of LDDMM landmark registration with an\nadditional term involving the Jacobian of the mapping. The algorithm can easily\nbe generalized to construct a diffeomorphic, statistical atlas of multiple\npoint sets. The method is successfully validated on a first set of synthetic\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a generalization of the iterative closest point (ICP) algorithm\nfor point set registration, in which the registration functions are non-rigid\nand follow the large deformation diffeomorphic metric mapping (LDDMM)\nframework. The algorithm is formulated as a well-posed probabilistic inference,\nand requires to solve a novel variation of LDDMM landmark registration with an\nadditional term involving the Jacobian of the mapping. The algorithm can easily\nbe generalized to construct a diffeomorphic, statistical atlas of multiple\npoint sets. The method is successfully validated on a first set of synthetic\ndata."
                },
                "authors": [
                    {
                        "name": "Adrien Wohrer"
                    }
                ],
                "author_detail": {
                    "name": "Adrien Wohrer"
                },
                "arxiv_affiliation": "IP",
                "author": "Adrien Wohrer",
                "arxiv_doi": "10.1007/978-3-031-38299-4_58",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-38299-4_58",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.11986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "GSI 2023 - International Conference on Geometric Science of\n  Information, Aug 2023, Saint-Malo, France. pp.563-573",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11979v1",
                "updated": "2025-01-21T08:52:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    52,
                    47,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:52:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    52,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Linear Feedback Control Systems for Iterative Prompt Optimization in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Feedback Control Systems for Iterative Prompt Optimization in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various applications by\ngenerating outputs based on given prompts. However, achieving the desired\noutput requires iterative prompt refinement. This paper presents a novel\napproach that draws parallels between the iterative prompt optimization process\nin LLMs and feedback control systems. We iteratively refine the prompt by\ntreating the deviation between the LLM output and the desired result as an\nerror term until the output criteria are met. This process is akin to a\nfeedback control system, where the LLM, despite being non-linear and\nnon-deterministic, is managed using principles from linear feedback control\nsystems. We explore the application of different types of controllers within\nthis framework, providing a mathematical foundation for integrating linear\nfeedback control mechanisms with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various applications by\ngenerating outputs based on given prompts. However, achieving the desired\noutput requires iterative prompt refinement. This paper presents a novel\napproach that draws parallels between the iterative prompt optimization process\nin LLMs and feedback control systems. We iteratively refine the prompt by\ntreating the deviation between the LLM output and the desired result as an\nerror term until the output criteria are met. This process is akin to a\nfeedback control system, where the LLM, despite being non-linear and\nnon-deterministic, is managed using principles from linear feedback control\nsystems. We explore the application of different types of controllers within\nthis framework, providing a mathematical foundation for integrating linear\nfeedback control mechanisms with LLMs."
                },
                "authors": [
                    {
                        "name": "Rupesh Raj Karn"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Raj Karn"
                },
                "author": "Rupesh Raj Karn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11977v1",
                "updated": "2025-01-21T08:51:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    51,
                    12,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:51:12Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    51,
                    12,
                    1,
                    21,
                    0
                ],
                "title": "Leveraging Graph Structures and Large Language Models for End-to-End\n  Synthetic Task-Oriented Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Graph Structures and Large Language Models for End-to-End\n  Synthetic Task-Oriented Dialogues"
                },
                "summary": "Training task-oriented dialogue systems is both costly and time-consuming,\ndue to the need for high-quality datasets encompassing diverse intents.\nTraditional methods depend on extensive human annotation, while recent\nadvancements leverage large language models (LLMs) to generate synthetic data.\nHowever, these approaches often require custom prompts or code, limiting\naccessibility for non-technical users. We introduce GraphTOD, an end-to-end\nframework that simplifies the generation of task-oriented dialogues. Users can\ncreate dialogues by specifying transition graphs in JSON format. Our evaluation\ndemonstrates that GraphTOD generates high-quality dialogues across various\ndomains, significantly lowering the cost and complexity of dataset creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training task-oriented dialogue systems is both costly and time-consuming,\ndue to the need for high-quality datasets encompassing diverse intents.\nTraditional methods depend on extensive human annotation, while recent\nadvancements leverage large language models (LLMs) to generate synthetic data.\nHowever, these approaches often require custom prompts or code, limiting\naccessibility for non-technical users. We introduce GraphTOD, an end-to-end\nframework that simplifies the generation of task-oriented dialogues. Users can\ncreate dialogues by specifying transition graphs in JSON format. Our evaluation\ndemonstrates that GraphTOD generates high-quality dialogues across various\ndomains, significantly lowering the cost and complexity of dataset creation."
                },
                "authors": [
                    {
                        "name": "Maya Medjad"
                    },
                    {
                        "name": "Hugo Imbert"
                    },
                    {
                        "name": "Bruno Yun"
                    },
                    {
                        "name": "Raphaël Szymocha"
                    },
                    {
                        "name": "Frédéric Armetta"
                    }
                ],
                "author_detail": {
                    "name": "Frédéric Armetta"
                },
                "author": "Frédéric Armetta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20954v2",
                "updated": "2025-01-21T08:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    38,
                    1,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-30T13:50:20Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "title": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents"
                },
                "summary": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort."
                },
                "authors": [
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Tianyun Ma"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Guanglin Xu"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xiaqing Li"
                    },
                    {
                        "name": "Yuanbo Wen"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06714v2",
                "updated": "2025-01-21T08:33:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    33,
                    26,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-12T04:44:44Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    44,
                    44,
                    6,
                    12,
                    0
                ],
                "title": "F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with\n  Cycle-Consistent Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with\n  Cycle-Consistent Gaussian Splatting"
                },
                "summary": "This paper tackles the problem of generalizable 3D-aware generation from\nmonocular datasets, e.g., ImageNet. The key challenge of this task is learning\na robust 3D-aware representation without multi-view or dynamic data, while\nensuring consistent texture and geometry across different viewpoints. Although\nsome baseline methods are capable of 3D-aware generation, the quality of the\ngenerated images still lags behind state-of-the-art 2D generation approaches,\nwhich excel in producing high-quality, detailed images. To address this severe\nlimitation, we propose a novel feed-forward pipeline based on pixel-aligned\nGaussian Splatting, coined as F3D-Gaus, which can produce more realistic and\nreliable 3D renderings from monocular inputs. In addition, we introduce a\nself-supervised cycle-consistent constraint to enforce cross-view consistency\nin the learned 3D representation. This training strategy naturally allows\naggregation of multiple aligned Gaussian primitives and significantly\nalleviates the interpolation limitations inherent in single-view pixel-aligned\nGaussian Splatting. Furthermore, we incorporate video model priors to perform\ngeometry-aware refinement, enhancing the generation of fine details in\nwide-viewpoint scenarios and improving the model's capability to capture\nintricate 3D textures. Extensive experiments demonstrate that our approach not\nonly achieves high-quality, multi-view consistent 3D-aware generation from\nmonocular datasets, but also significantly improves training and inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the problem of generalizable 3D-aware generation from\nmonocular datasets, e.g., ImageNet. The key challenge of this task is learning\na robust 3D-aware representation without multi-view or dynamic data, while\nensuring consistent texture and geometry across different viewpoints. Although\nsome baseline methods are capable of 3D-aware generation, the quality of the\ngenerated images still lags behind state-of-the-art 2D generation approaches,\nwhich excel in producing high-quality, detailed images. To address this severe\nlimitation, we propose a novel feed-forward pipeline based on pixel-aligned\nGaussian Splatting, coined as F3D-Gaus, which can produce more realistic and\nreliable 3D renderings from monocular inputs. In addition, we introduce a\nself-supervised cycle-consistent constraint to enforce cross-view consistency\nin the learned 3D representation. This training strategy naturally allows\naggregation of multiple aligned Gaussian primitives and significantly\nalleviates the interpolation limitations inherent in single-view pixel-aligned\nGaussian Splatting. Furthermore, we incorporate video model priors to perform\ngeometry-aware refinement, enhancing the generation of fine details in\nwide-viewpoint scenarios and improving the model's capability to capture\nintricate 3D textures. Extensive experiments demonstrate that our approach not\nonly achieves high-quality, multi-view consistent 3D-aware generation from\nmonocular datasets, but also significantly improves training and inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Qianyi Wu"
                    },
                    {
                        "name": "Dan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Xu"
                },
                "author": "Dan Xu",
                "arxiv_comment": "Project Page: https://w-ted.github.io/publications/F3D-Gaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11967v1",
                "updated": "2025-01-21T08:26:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    26,
                    20,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:26:20Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    26,
                    20,
                    1,
                    21,
                    0
                ],
                "title": "A Hybrid Attention Framework for Fake News Detection with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Attention Framework for Fake News Detection with Large Language\n  Models"
                },
                "summary": "With the rapid growth of online information, the spread of fake news has\nbecome a serious social challenge. In this study, we propose a novel detection\nframework based on Large Language Models (LLMs) to identify and classify fake\nnews by integrating textual statistical features and deep semantic features.\nOur approach utilizes the contextual understanding capability of the large\nlanguage model for text analysis and introduces a hybrid attention mechanism to\nfocus on feature combinations that are particularly important for fake news\nidentification. Extensive experiments on the WELFake news dataset show that our\nmodel significantly outperforms existing methods, with a 1.5\\% improvement in\nF1 score. In addition, we assess the interpretability of the model through\nattention heat maps and SHAP values, providing actionable insights for content\nreview strategies. Our framework provides a scalable and efficient solution to\ndeal with the spread of fake news and helps build a more reliable online\ninformation ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of online information, the spread of fake news has\nbecome a serious social challenge. In this study, we propose a novel detection\nframework based on Large Language Models (LLMs) to identify and classify fake\nnews by integrating textual statistical features and deep semantic features.\nOur approach utilizes the contextual understanding capability of the large\nlanguage model for text analysis and introduces a hybrid attention mechanism to\nfocus on feature combinations that are particularly important for fake news\nidentification. Extensive experiments on the WELFake news dataset show that our\nmodel significantly outperforms existing methods, with a 1.5\\% improvement in\nF1 score. In addition, we assess the interpretability of the model through\nattention heat maps and SHAP values, providing actionable insights for content\nreview strategies. Our framework provides a scalable and efficient solution to\ndeal with the spread of fake news and helps build a more reliable online\ninformation ecosystem."
                },
                "authors": [
                    {
                        "name": "Xiaochuan Xu"
                    },
                    {
                        "name": "Peiyang Yu"
                    },
                    {
                        "name": "Zeqiu Xu"
                    },
                    {
                        "name": "Jiani Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiani Wang"
                },
                "author": "Jiani Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02044v2",
                "updated": "2025-01-21T08:17:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    17,
                    27,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-04T07:27:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    27,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "QROA: A Black-Box Query-Response Optimization Attack on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QROA: A Black-Box Query-Response Optimization Attack on LLMs"
                },
                "summary": "Large Language Models (LLMs) have surged in popularity in recent months, yet\nthey possess concerning capabilities for generating harmful content when\nmanipulated. This study introduces the Query-Response Optimization Attack\n(QROA), an optimization-based strategy designed to exploit LLMs through a\nblack-box, query-only interaction. QROA adds an optimized trigger to a\nmalicious instruction to compel the LLM to generate harmful content. Unlike\nprevious approaches, QROA does not require access to the model's logit\ninformation or any other internal data and operates solely through the standard\nquery-response interface of LLMs. Inspired by deep Q-learning and Greedy\ncoordinate descent, the method iteratively updates tokens to maximize a\ndesigned reward function. We tested our method on various LLMs such as Vicuna,\nFalcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%. We also\ntested the model against Llama2-chat, the fine-tuned version of Llama2 designed\nto resist Jailbreak attacks, achieving good ASR with a suboptimal initial\ntrigger seed. This study demonstrates the feasibility of generating jailbreak\nattacks against deployed LLMs in the public domain using black-box optimization\nmethods, enabling more comprehensive safety testing of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have surged in popularity in recent months, yet\nthey possess concerning capabilities for generating harmful content when\nmanipulated. This study introduces the Query-Response Optimization Attack\n(QROA), an optimization-based strategy designed to exploit LLMs through a\nblack-box, query-only interaction. QROA adds an optimized trigger to a\nmalicious instruction to compel the LLM to generate harmful content. Unlike\nprevious approaches, QROA does not require access to the model's logit\ninformation or any other internal data and operates solely through the standard\nquery-response interface of LLMs. Inspired by deep Q-learning and Greedy\ncoordinate descent, the method iteratively updates tokens to maximize a\ndesigned reward function. We tested our method on various LLMs such as Vicuna,\nFalcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%. We also\ntested the model against Llama2-chat, the fine-tuned version of Llama2 designed\nto resist Jailbreak attacks, achieving good ASR with a suboptimal initial\ntrigger seed. This study demonstrates the feasibility of generating jailbreak\nattacks against deployed LLMs in the public domain using black-box optimization\nmethods, enabling more comprehensive safety testing of LLMs."
                },
                "authors": [
                    {
                        "name": "Hussein Jawad"
                    },
                    {
                        "name": "Nicolas J. -B. BRUNEL"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas J. -B. BRUNEL"
                },
                "arxiv_affiliation": "LaMME",
                "author": "Nicolas J. -B. BRUNEL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03659v4",
                "updated": "2025-01-21T08:09:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    9,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-07T09:47:46Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    47,
                    46,
                    1,
                    7,
                    0
                ],
                "title": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting"
                },
                "summary": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current novel view synthesis tasks primarily rely on high-quality and clear\nimages. However, in foggy scenes, scattering and attenuation can significantly\ndegrade the reconstruction and rendering quality. Although NeRF-based dehazing\nreconstruction algorithms have been developed, their use of deep fully\nconnected neural networks and per-ray sampling strategies leads to high\ncomputational costs. Moreover, NeRF's implicit representation struggles to\nrecover fine details from hazy scenes. In contrast, recent advancements in 3D\nGaussian Splatting achieve high-quality 3D scene reconstruction by explicitly\nmodeling point clouds into 3D Gaussians. In this paper, we propose leveraging\nthe explicit Gaussian representation to explain the foggy image formation\nprocess through a physically accurate forward rendering process. We introduce\nDehazeGS, a method capable of decomposing and rendering a fog-free background\nfrom participating media using only muti-view foggy images as input. We model\nthe transmission within each Gaussian distribution to simulate the formation of\nfog. During this process, we jointly learn the atmospheric light and scattering\ncoefficient while optimizing the Gaussian representation of the hazy scene. In\nthe inference stage, we eliminate the effects of scattering and attenuation on\nthe Gaussians and directly project them onto a 2D plane to obtain a clear view.\nExperiments on both synthetic and real-world foggy datasets demonstrate that\nDehazeGS achieves state-of-the-art performance in terms of both rendering\nquality and computational efficiency. visualizations are available at\nhttps://dehazegs.github.io/"
                },
                "authors": [
                    {
                        "name": "Jinze Yu"
                    },
                    {
                        "name": "Yiqun Wang"
                    },
                    {
                        "name": "Zhengda Lu"
                    },
                    {
                        "name": "Jianwei Guo"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Hongxing Qin"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "9 pages,4 figures. visualizations are available at\n  https://dehazegs.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11956v1",
                "updated": "2025-01-21T08:07:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    7,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:07:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    7,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Modified gravitational wave propagations in linearized gravity with\n  Lorentz and diffeomorphism violations and their gravitational wave\n  constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modified gravitational wave propagations in linearized gravity with\n  Lorentz and diffeomorphism violations and their gravitational wave\n  constraints"
                },
                "summary": "The standard model extension (SME) is an effective field theory framework\nthat can be used to study the possible violations of Lorentz symmetry and\ndiffeomorphism invariance in the gravitational interaction. In this paper, we\nexplore both the Lorentz- and diffeomorphism-violating effects on the\npropagations of gravitational waves in the SME's linearized gravity. It is\nshown that the violations of Lorentz symmetry and diffeomorphism invariance\nmodify the conventional linear dispersion relation of gravitational waves,\nleading to anisotropy, birefringence, and dispersion effects in the propagation\nof gravitational waves. With these modified dispersion relations, we then\ncalculate the dephasing effects due to the Lorentz and diffeomorphism\nviolations in the waveforms of gravitational waves produced by the coalescence\nof compact binaries. With the distorted waveforms, we perform full Bayesian\ninference with the help of the open source software \\texttt{BILBY} on the\ngravitational wave events of the compact binary mergers in the LIGO-Virgo-KAGRA\ncatalogs GWTC-3. We consider the effects from the operators with the lowest\nmass dimension $d=2$ and $d=3$ due to the Lorentz and diffeomorphism violations\nin the linearized gravity. No signature of Lorentz and diffeomorphism\nviolations arsing from the SME's linearized gravity are found for most GW\nevents, which allows us to give a $90\\%$ confidence interval for each Lorentz-\nand diffeomorphism-violating coefficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard model extension (SME) is an effective field theory framework\nthat can be used to study the possible violations of Lorentz symmetry and\ndiffeomorphism invariance in the gravitational interaction. In this paper, we\nexplore both the Lorentz- and diffeomorphism-violating effects on the\npropagations of gravitational waves in the SME's linearized gravity. It is\nshown that the violations of Lorentz symmetry and diffeomorphism invariance\nmodify the conventional linear dispersion relation of gravitational waves,\nleading to anisotropy, birefringence, and dispersion effects in the propagation\nof gravitational waves. With these modified dispersion relations, we then\ncalculate the dephasing effects due to the Lorentz and diffeomorphism\nviolations in the waveforms of gravitational waves produced by the coalescence\nof compact binaries. With the distorted waveforms, we perform full Bayesian\ninference with the help of the open source software \\texttt{BILBY} on the\ngravitational wave events of the compact binary mergers in the LIGO-Virgo-KAGRA\ncatalogs GWTC-3. We consider the effects from the operators with the lowest\nmass dimension $d=2$ and $d=3$ due to the Lorentz and diffeomorphism violations\nin the linearized gravity. No signature of Lorentz and diffeomorphism\nviolations arsing from the SME's linearized gravity are found for most GW\nevents, which allows us to give a $90\\%$ confidence interval for each Lorentz-\nand diffeomorphism-violating coefficient."
                },
                "authors": [
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Jian-Ming Yan"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Wen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhao"
                },
                "author": "Wen Zhao",
                "arxiv_comment": "15 pages, 3 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08261v2",
                "updated": "2025-01-21T08:04:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    4,
                    19,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-14T17:15:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    15,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "GPU-accelerated LISA parameter estimation with full time domain response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated LISA parameter estimation with full time domain response"
                },
                "summary": "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy."
                },
                "authors": [
                    {
                        "name": "Cecilio García-Quirós"
                    },
                    {
                        "name": "Shubhanshu Tiwari"
                    },
                    {
                        "name": "Stanislav Babak"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Babak"
                },
                "author": "Stanislav Babak",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11953v1",
                "updated": "2025-01-21T07:54:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    54,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:54:22Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    54,
                    22,
                    1,
                    21,
                    0
                ],
                "title": "Proverbs Run in Pairs: Evaluating Proverb Translation Capability of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proverbs Run in Pairs: Evaluating Proverb Translation Capability of\n  Large Language Model"
                },
                "summary": "Despite achieving remarkable performance, machine translation (MT) research\nremains underexplored in terms of translating cultural elements in languages,\nsuch as idioms, proverbs, and colloquial expressions. This paper investigates\nthe capability of state-of-the-art neural machine translation (NMT) and large\nlanguage models (LLMs) in translating proverbs, which are deeply rooted in\ncultural contexts. We construct a translation dataset of standalone proverbs\nand proverbs in conversation for four language pairs. Our experiments show that\nthe studied models can achieve good translation between languages with similar\ncultural backgrounds, and LLMs generally outperform NMT models in proverb\ntranslation. Furthermore, we find that current automatic evaluation metrics\nsuch as BLEU, CHRF++ and COMET are inadequate for reliably assessing the\nquality of proverb translation, highlighting the need for more culturally aware\nevaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite achieving remarkable performance, machine translation (MT) research\nremains underexplored in terms of translating cultural elements in languages,\nsuch as idioms, proverbs, and colloquial expressions. This paper investigates\nthe capability of state-of-the-art neural machine translation (NMT) and large\nlanguage models (LLMs) in translating proverbs, which are deeply rooted in\ncultural contexts. We construct a translation dataset of standalone proverbs\nand proverbs in conversation for four language pairs. Our experiments show that\nthe studied models can achieve good translation between languages with similar\ncultural backgrounds, and LLMs generally outperform NMT models in proverb\ntranslation. Furthermore, we find that current automatic evaluation metrics\nsuch as BLEU, CHRF++ and COMET are inadequate for reliably assessing the\nquality of proverb translation, highlighting the need for more culturally aware\nevaluation metrics."
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Viet-Thanh Pham"
                    },
                    {
                        "name": "Farhad Moghimifar"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    }
                ],
                "author_detail": {
                    "name": "Thuy-Trang Vu"
                },
                "author": "Thuy-Trang Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11949v1",
                "updated": "2025-01-21T07:47:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    47,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:47:03Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    47,
                    3,
                    1,
                    21,
                    0
                ],
                "title": "GLAM: Global-Local Variation Awareness in Mamba-based World Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLAM: Global-Local Variation Awareness in Mamba-based World Model"
                },
                "summary": "Mimicking the real interaction trajectory in the inference of the world model\nhas been shown to improve the sample efficiency of model-based reinforcement\nlearning (MBRL) algorithms. Many methods directly use known state sequences for\nreasoning. However, this approach fails to enhance the quality of reasoning by\ncapturing the subtle variation between states. Much like how humans infer\ntrends in event development from this variation, in this work, we introduce\nGlobal-Local variation Awareness Mamba-based world model (GLAM) that improves\nreasoning quality by perceiving and predicting variation between states. GLAM\ncomprises two Mambabased parallel reasoning modules, GMamba and LMamba, which\nfocus on perceiving variation from global and local perspectives, respectively,\nduring the reasoning process. GMamba focuses on identifying patterns of\nvariation between states in the input sequence and leverages these patterns to\nenhance the prediction of future state variation. LMamba emphasizes reasoning\nabout unknown information, such as rewards, termination signals, and visual\nrepresentations, by perceiving variation in adjacent states. By integrating the\nstrengths of the two modules, GLAM accounts for highervalue variation in\nenvironmental changes, providing the agent with more efficient\nimagination-based training. We demonstrate that our method outperforms existing\nmethods in normalized human scores on the Atari 100k benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mimicking the real interaction trajectory in the inference of the world model\nhas been shown to improve the sample efficiency of model-based reinforcement\nlearning (MBRL) algorithms. Many methods directly use known state sequences for\nreasoning. However, this approach fails to enhance the quality of reasoning by\ncapturing the subtle variation between states. Much like how humans infer\ntrends in event development from this variation, in this work, we introduce\nGlobal-Local variation Awareness Mamba-based world model (GLAM) that improves\nreasoning quality by perceiving and predicting variation between states. GLAM\ncomprises two Mambabased parallel reasoning modules, GMamba and LMamba, which\nfocus on perceiving variation from global and local perspectives, respectively,\nduring the reasoning process. GMamba focuses on identifying patterns of\nvariation between states in the input sequence and leverages these patterns to\nenhance the prediction of future state variation. LMamba emphasizes reasoning\nabout unknown information, such as rewards, termination signals, and visual\nrepresentations, by perceiving variation in adjacent states. By integrating the\nstrengths of the two modules, GLAM accounts for highervalue variation in\nenvironmental changes, providing the agent with more efficient\nimagination-based training. We demonstrate that our method outperforms existing\nmethods in normalized human scores on the Atari 100k benchmark."
                },
                "authors": [
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Wenqi Liang"
                    },
                    {
                        "name": "Chunhui Hao"
                    },
                    {
                        "name": "Gan Sun"
                    },
                    {
                        "name": "Jiandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Jiandong Tian"
                },
                "author": "Jiandong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04194v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04194v2",
                "updated": "2025-01-21T07:41:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2024-07-05T00:40:03Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    0,
                    40,
                    3,
                    4,
                    187,
                    0
                ],
                "title": "Regularization Using Synthetic Data in High-Dimensional Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularization Using Synthetic Data in High-Dimensional Models"
                },
                "summary": "To overcome challenges in fitting complex models with small samples,\ncatalytic priors were recently proposed to stabilize the inference by\nsupplementing observed data with synthetic data generated from simpler models.\nThe resulting Maximum A Posteriori (MAP) estimator is a regularized method that\nmaximizes the weighted likelihood of the combined data. While this estimator is\ncomputationally straightforward and empirically promising, its theoretical\nproperties are unexplored. This paper provides a theoretical analysis of this\nMAP estimator in generalized linear models, focusing on logistic regression. We\nfirst establish the existence and stability, even in high dimensions. We then\nprove the consistency when the dimension of covariates diverges. Furthermore,\nwe use the convex Gaussian min-max theorem to characterize the asymptotic\nbehavior of the MAP estimator when the dimension grows linearly with the sample\nsize. Our theory clarifies the role of the tuning parameters and provides\npractical guidance, particularly for high-dimensional inference tasks such as\nconstructing confidence intervals and performing variable selection. We\ndemonstrate the effectiveness of our methods on simulations and real-world\ndata. Our work provides a theoretically justified framework for enhancing\nstatistical inference using synthetic data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To overcome challenges in fitting complex models with small samples,\ncatalytic priors were recently proposed to stabilize the inference by\nsupplementing observed data with synthetic data generated from simpler models.\nThe resulting Maximum A Posteriori (MAP) estimator is a regularized method that\nmaximizes the weighted likelihood of the combined data. While this estimator is\ncomputationally straightforward and empirically promising, its theoretical\nproperties are unexplored. This paper provides a theoretical analysis of this\nMAP estimator in generalized linear models, focusing on logistic regression. We\nfirst establish the existence and stability, even in high dimensions. We then\nprove the consistency when the dimension of covariates diverges. Furthermore,\nwe use the convex Gaussian min-max theorem to characterize the asymptotic\nbehavior of the MAP estimator when the dimension grows linearly with the sample\nsize. Our theory clarifies the role of the tuning parameters and provides\npractical guidance, particularly for high-dimensional inference tasks such as\nconstructing confidence intervals and performing variable selection. We\ndemonstrate the effectiveness of our methods on simulations and real-world\ndata. Our work provides a theoretically justified framework for enhancing\nstatistical inference using synthetic data."
                },
                "authors": [
                    {
                        "name": "Weihao Li"
                    },
                    {
                        "name": "Dongming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Dongming Huang"
                },
                "author": "Dongming Huang",
                "arxiv_comment": "98 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04194v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04194v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v3",
                "updated": "2025-01-21T07:34:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    34,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference"
                },
                "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11935v1",
                "updated": "2025-01-21T07:16:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    16,
                    18,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    16,
                    18,
                    1,
                    21,
                    0
                ],
                "title": "Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students"
                },
                "summary": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information."
                },
                "authors": [
                    {
                        "name": "Aayush Kumar"
                    },
                    {
                        "name": "Daniel Prol"
                    },
                    {
                        "name": "Amin Alipour"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    }
                ],
                "author_detail": {
                    "name": "Sruti Srinivasa Ragavan"
                },
                "author": "Sruti Srinivasa Ragavan",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11673v2",
                "updated": "2025-01-21T07:09:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    9,
                    35,
                    1,
                    21,
                    0
                ],
                "published": "2024-09-18T03:20:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    20,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model"
                },
                "summary": "Unified information extraction (UIE) aims to extract diverse structured\ninformation from unstructured text. While large language models (LLMs) have\nshown promise for UIE, they require significant computational resources and\noften struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based\nUnified Information Extraction), a framework that leverages in-context learning\nfor efficient task generalization. RUIE introduces a novel demonstration\nselection mechanism combining LLM preferences with a keyword-enhanced reward\nmodel, and employs a bi-encoder retriever trained through contrastive learning\nand knowledge distillation. As the first trainable retrieval framework for UIE,\nRUIE serves as a universal plugin for various LLMs. Experimental results on\neight held-out datasets demonstrate RUIE's effectiveness, with average F1-score\nimprovements of 19.22 and 3.22 compared to instruction-tuning methods and other\nretrievers, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified information extraction (UIE) aims to extract diverse structured\ninformation from unstructured text. While large language models (LLMs) have\nshown promise for UIE, they require significant computational resources and\noften struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based\nUnified Information Extraction), a framework that leverages in-context learning\nfor efficient task generalization. RUIE introduces a novel demonstration\nselection mechanism combining LLM preferences with a keyword-enhanced reward\nmodel, and employs a bi-encoder retriever trained through contrastive learning\nand knowledge distillation. As the first trainable retrieval framework for UIE,\nRUIE serves as a universal plugin for various LLMs. Experimental results on\neight held-out datasets demonstrate RUIE's effectiveness, with average F1-score\nimprovements of 19.22 and 3.22 compared to instruction-tuning methods and other\nretrievers, respectively."
                },
                "authors": [
                    {
                        "name": "Xincheng Liao"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yixi Huang"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "arxiv_comment": "To appear in COLING 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11929v1",
                "updated": "2025-01-21T07:07:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    7,
                    58,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:07:58Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    7,
                    58,
                    1,
                    21,
                    0
                ],
                "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems have been shown to improve the\naccuracy of Large Language Model (LLM) outputs. However, these models can often\nachieve low accuracy when applied to new data domains.\n  We introduce the Automatic Local Fine Tuning of Retrieval Augmented\nGeneration models (ALoFTRAG) framework, designed to improve the accuracy of RAG\nsystems on a given domain by training LLMs without manually labeled data or\nusing larger teacher models.\n  By generating and filtering synthetic training data and performing LoRA\nfine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets\nin 26 languages by, on average, 8.3% and 3.0% respectively.\n  Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and\ndata-secure solution for improving RAG accuracy, making it particularly\napplicable to sensitive domains such as healthcare and finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems have been shown to improve the\naccuracy of Large Language Model (LLM) outputs. However, these models can often\nachieve low accuracy when applied to new data domains.\n  We introduce the Automatic Local Fine Tuning of Retrieval Augmented\nGeneration models (ALoFTRAG) framework, designed to improve the accuracy of RAG\nsystems on a given domain by training LLMs without manually labeled data or\nusing larger teacher models.\n  By generating and filtering synthetic training data and performing LoRA\nfine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets\nin 26 languages by, on average, 8.3% and 3.0% respectively.\n  Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and\ndata-secure solution for improving RAG accuracy, making it particularly\napplicable to sensitive domains such as healthcare and finance."
                },
                "authors": [
                    {
                        "name": "Peter Devine"
                    }
                ],
                "author_detail": {
                    "name": "Peter Devine"
                },
                "author": "Peter Devine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20151v2",
                "updated": "2025-01-21T06:26:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    26,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2023-10-31T03:37:11Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    3,
                    37,
                    11,
                    1,
                    304,
                    0
                ],
                "title": "Multi-Agent Consensus Seeking via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Consensus Seeking via Large Language Models"
                },
                "summary": "Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: windylab.github.io/ConsensusLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: windylab.github.io/ConsensusLLM/."
                },
                "authors": [
                    {
                        "name": "Huaben Chen"
                    },
                    {
                        "name": "Wenkang Ji"
                    },
                    {
                        "name": "Lufeng Xu"
                    },
                    {
                        "name": "Shiyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Zhao"
                },
                "author": "Shiyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11911v1",
                "updated": "2025-01-21T06:12:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    12,
                    49,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T06:12:49Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    12,
                    49,
                    1,
                    21,
                    0
                ],
                "title": "Integrate Temporal Graph Learning into LLM-based Temporal Knowledge\n  Graph Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrate Temporal Graph Learning into LLM-based Temporal Knowledge\n  Graph Model"
                },
                "summary": "Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events\nbased on the observed events in history. Recently, Large Language Models (LLMs)\nhave exhibited remarkable capabilities, generating significant research\ninterest in their application for reasoning over temporal knowledge graphs\n(TKGs). Existing LLM-based methods have integrated retrieved historical facts\nor static graph representations into LLMs. Despite the notable performance of\nLLM-based methods, they are limited by the insufficient modeling of temporal\npatterns and ineffective cross-modal alignment between graph and language,\nhindering the ability of LLMs to fully grasp the temporal and structural\ninformation in TKGs. To tackle these issues, we propose a novel framework\nTGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge\ngraph model. Specifically, we introduce temporal graph learning to capture the\ntemporal and relational patterns and obtain the historical graph embedding.\nFurthermore, we design a hybrid graph tokenization to sufficiently model the\ntemporal patterns within LLMs. To achieve better alignment between graph and\nlanguage, we employ a two-stage training paradigm to finetune LLMs on\nhigh-quality and diverse data, thereby resulting in better performance.\nExtensive experiments on three real-world datasets show that our approach\noutperforms a range of state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events\nbased on the observed events in history. Recently, Large Language Models (LLMs)\nhave exhibited remarkable capabilities, generating significant research\ninterest in their application for reasoning over temporal knowledge graphs\n(TKGs). Existing LLM-based methods have integrated retrieved historical facts\nor static graph representations into LLMs. Despite the notable performance of\nLLM-based methods, they are limited by the insufficient modeling of temporal\npatterns and ineffective cross-modal alignment between graph and language,\nhindering the ability of LLMs to fully grasp the temporal and structural\ninformation in TKGs. To tackle these issues, we propose a novel framework\nTGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge\ngraph model. Specifically, we introduce temporal graph learning to capture the\ntemporal and relational patterns and obtain the historical graph embedding.\nFurthermore, we design a hybrid graph tokenization to sufficiently model the\ntemporal patterns within LLMs. To achieve better alignment between graph and\nlanguage, we employ a two-stage training paradigm to finetune LLMs on\nhigh-quality and diverse data, thereby resulting in better performance.\nExtensive experiments on three real-world datasets show that our approach\noutperforms a range of state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "He Chang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Zhulin Tao"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Xianglin Huang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08561v2",
                "updated": "2025-01-21T06:12:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    12,
                    26,
                    1,
                    21,
                    0
                ],
                "published": "2024-11-13T12:18:00Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    12,
                    18,
                    0,
                    2,
                    318,
                    0
                ],
                "title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogLLM: Log-based Anomaly Detection Using Large Language Models"
                },
                "summary": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately."
                },
                "authors": [
                    {
                        "name": "Wei Guan"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Shiyou Qian"
                    },
                    {
                        "name": "Jianqi Gao"
                    },
                    {
                        "name": "Chun Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Ouyang"
                },
                "author": "Chun Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02253v2",
                "updated": "2025-01-21T06:03:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    3,
                    7,
                    1,
                    21,
                    0
                ],
                "published": "2023-12-04T18:35:27Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    35,
                    27,
                    0,
                    338,
                    0
                ],
                "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with\n  Synthetic Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with\n  Synthetic Images"
                },
                "summary": "Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization."
                },
                "authors": [
                    {
                        "name": "Zhuoran Yu"
                    },
                    {
                        "name": "Chenchen Zhu"
                    },
                    {
                        "name": "Sean Culatana"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Fanyi Xiao"
                    },
                    {
                        "name": "Yong Jae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jae Lee"
                },
                "author": "Yong Jae Lee",
                "arxiv_comment": "Accepted by Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13265v2",
                "updated": "2025-01-21T05:50:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    50,
                    19,
                    1,
                    21,
                    0
                ],
                "published": "2024-09-20T06:54:00Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    6,
                    54,
                    0,
                    4,
                    264,
                    0
                ],
                "title": "Towards LifeSpan Cognitive Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LifeSpan Cognitive Systems"
                },
                "summary": "Building a human-like system that continuously interacts with complex\nenvironments -- whether simulated digital worlds or human society -- presents\nseveral key challenges. Central to this is enabling continuous, high-frequency\ninteractions, where the interactions are termed experiences. We refer to this\nenvisioned system as the LifeSpan Cognitive System (LSCS). A critical feature\nof LSCS is its ability to engage in incremental and rapid updates while\nretaining and accurately recalling past experiences. In this paper we focus on\nthe domain of Large Language Models (LLMs), where we identify two major\nchallenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention\nwith Accurate Recall. These properties are essential for storing new\nexperiences, organizing past experiences, and responding to the environment in\nways that leverage relevant historical data. Unlike language models with\ncontinual learning, which typically rely on large corpora for fine-tuning and\nfocus on improving performance within specific domains or tasks, LSCS must\nrapidly and incrementally update with new information from its environment at a\nhigh frequency. Existing technologies with the potential of solving the above\ntwo major challenges can be classified into four classes based on a conceptual\nmetric called Storage Complexity, which measures the relative space required to\nstore past experiences. Each of these four classes of technologies has its own\nstrengths and limitations while we argue none of them alone can achieve LSCS\nalone. To this end, we propose a potential instantiation for LSCS that can\nintegrate all four classes of technologies. The new instantiation, serving as a\nconjecture, operates through two core processes: Absorbing Experiences and\nGenerating Responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a human-like system that continuously interacts with complex\nenvironments -- whether simulated digital worlds or human society -- presents\nseveral key challenges. Central to this is enabling continuous, high-frequency\ninteractions, where the interactions are termed experiences. We refer to this\nenvisioned system as the LifeSpan Cognitive System (LSCS). A critical feature\nof LSCS is its ability to engage in incremental and rapid updates while\nretaining and accurately recalling past experiences. In this paper we focus on\nthe domain of Large Language Models (LLMs), where we identify two major\nchallenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention\nwith Accurate Recall. These properties are essential for storing new\nexperiences, organizing past experiences, and responding to the environment in\nways that leverage relevant historical data. Unlike language models with\ncontinual learning, which typically rely on large corpora for fine-tuning and\nfocus on improving performance within specific domains or tasks, LSCS must\nrapidly and incrementally update with new information from its environment at a\nhigh frequency. Existing technologies with the potential of solving the above\ntwo major challenges can be classified into four classes based on a conceptual\nmetric called Storage Complexity, which measures the relative space required to\nstore past experiences. Each of these four classes of technologies has its own\nstrengths and limitations while we argue none of them alone can achieve LSCS\nalone. To this end, we propose a potential instantiation for LSCS that can\nintegrate all four classes of technologies. The new instantiation, serving as a\nconjecture, operates through two core processes: Absorbing Experiences and\nGenerating Responses."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Xiaoxin He"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Nafis Sadeq"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Zexue He"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11900v1",
                "updated": "2025-01-21T05:30:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    30,
                    20,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T05:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    30,
                    20,
                    1,
                    21,
                    0
                ],
                "title": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation"
                },
                "summary": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines."
                },
                "authors": [
                    {
                        "name": "Junhong Lian"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "arxiv_comment": "Accepted to The ACM Web Conference 2025 (WWW'25, short paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00261v2",
                "updated": "2025-01-21T05:20:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    20,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2024-05-01T00:36:39Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    0,
                    36,
                    39,
                    2,
                    122,
                    0
                ],
                "title": "Informed total-error-minimizing priors: Interpretable cosmological\n  parameter constraints despite complex nuisance effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Informed total-error-minimizing priors: Interpretable cosmological\n  parameter constraints despite complex nuisance effects"
                },
                "summary": "While Bayesian inference techniques are standard in cosmological analyses, it\nis common to interpret resulting parameter constraints with a frequentist\nintuition. This intuition can fail, for example, when marginalizing\nhigh-dimensional parameter spaces onto subsets of parameters, because of what\nhas come to be known as projection effects or prior volume effects. We present\nthe method of informed total-error-minimizing (ITEM) priors to address this\nproblem. An ITEM prior is a prior distribution on a set of nuisance parameters,\nsuch as those describing astrophysical or calibration systematics, intended to\nenforce the validity of a frequentist interpretation of the posterior\nconstraints derived for a set of target parameters (e.g., cosmological\nparameters). Our method works as follows. For a set of plausible nuisance\nrealizations, we generate target parameter posteriors using several different\ncandidate priors for the nuisance parameters. We reject candidate priors that\ndo not accomplish the minimum requirements of bias (of point estimates) and\ncoverage (of confidence regions among a set of noisy realizations of the data)\nfor the target parameters on one or more of the plausible nuisance\nrealizations. Of the priors that survive this cut, we select the ITEM prior as\nthe one that minimizes the total error of the marginalized posteriors of the\ntarget parameters. As a proof of concept, we applied our method to the density\nsplit statistics measured in Dark Energy Survey Year 1 data. We demonstrate\nthat the ITEM priors substantially reduce prior volume effects that otherwise\narise and that they allow for sharpened yet robust constraints on the\nparameters of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Bayesian inference techniques are standard in cosmological analyses, it\nis common to interpret resulting parameter constraints with a frequentist\nintuition. This intuition can fail, for example, when marginalizing\nhigh-dimensional parameter spaces onto subsets of parameters, because of what\nhas come to be known as projection effects or prior volume effects. We present\nthe method of informed total-error-minimizing (ITEM) priors to address this\nproblem. An ITEM prior is a prior distribution on a set of nuisance parameters,\nsuch as those describing astrophysical or calibration systematics, intended to\nenforce the validity of a frequentist interpretation of the posterior\nconstraints derived for a set of target parameters (e.g., cosmological\nparameters). Our method works as follows. For a set of plausible nuisance\nrealizations, we generate target parameter posteriors using several different\ncandidate priors for the nuisance parameters. We reject candidate priors that\ndo not accomplish the minimum requirements of bias (of point estimates) and\ncoverage (of confidence regions among a set of noisy realizations of the data)\nfor the target parameters on one or more of the plausible nuisance\nrealizations. Of the priors that survive this cut, we select the ITEM prior as\nthe one that minimizes the total error of the marginalized posteriors of the\ntarget parameters. As a proof of concept, we applied our method to the density\nsplit statistics measured in Dark Energy Survey Year 1 data. We demonstrate\nthat the ITEM priors substantially reduce prior volume effects that otherwise\narise and that they allow for sharpened yet robust constraints on the\nparameters of interest."
                },
                "authors": [
                    {
                        "name": "Bernardita Ried Guachalla"
                    },
                    {
                        "name": "Dylan Britt"
                    },
                    {
                        "name": "Daniel Gruen"
                    },
                    {
                        "name": "Oliver Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Friedrich"
                },
                "author": "Oliver Friedrich",
                "arxiv_doi": "10.1051/0004-6361/202450575",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450575",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages + appendix. Matching the accepted version with the published\n  one in A&A",
                "arxiv_journal_ref": "A&A 693, A178 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08501v2",
                "updated": "2025-01-21T04:51:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    51,
                    11,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-15T00:38:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    0,
                    38,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks"
                },
                "summary": "Uncertainty quantification (UQ) plays a pivotal role in scientific machine\nlearning, especially when surrogate models are used to approximate complex\nsystems. Although multilayer perceptions (MLPs) are commonly employed as\nsurrogates, they often suffer from overfitting due to their large number of\nparameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution\nwith fewer parameters. However, gradient-based inference methods, such as\nHamiltonian Monte Carlo (HMC), may result in computational inefficiency when\napplied to KANs, especially for large-scale datasets, due to the high cost of\nback-propagation. To address these challenges, we propose a novel approach,\ncombining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev\nKANs. This gradient-free method effectively mitigates overfitting and enhances\nnumerical stability. Additionally, we incorporate the active subspace method to\nreduce the parameter-space dimensionality, allowing us to improve the accuracy\nof predictions and obtain more reliable uncertainty estimates. Extensive\nexperiments demonstrate the efficacy of our approach in various test cases,\nincluding scenarios with large datasets and high noise levels. Our results show\nthat the new method achieves comparable or better accuracy, much higher\nefficiency as well as stability compared to HMC, in addition to scalability.\nMoreover, by leveraging the low-dimensional parameter subspace, our method\npreserves prediction accuracy while substantially reducing further the\ncomputational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification (UQ) plays a pivotal role in scientific machine\nlearning, especially when surrogate models are used to approximate complex\nsystems. Although multilayer perceptions (MLPs) are commonly employed as\nsurrogates, they often suffer from overfitting due to their large number of\nparameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution\nwith fewer parameters. However, gradient-based inference methods, such as\nHamiltonian Monte Carlo (HMC), may result in computational inefficiency when\napplied to KANs, especially for large-scale datasets, due to the high cost of\nback-propagation. To address these challenges, we propose a novel approach,\ncombining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev\nKANs. This gradient-free method effectively mitigates overfitting and enhances\nnumerical stability. Additionally, we incorporate the active subspace method to\nreduce the parameter-space dimensionality, allowing us to improve the accuracy\nof predictions and obtain more reliable uncertainty estimates. Extensive\nexperiments demonstrate the efficacy of our approach in various test cases,\nincluding scenarios with large datasets and high noise levels. Our results show\nthat the new method achieves comparable or better accuracy, much higher\nefficiency as well as stability compared to HMC, in addition to scalability.\nMoreover, by leveraging the low-dimensional parameter subspace, our method\npreserves prediction accuracy while substantially reducing further the\ncomputational cost."
                },
                "authors": [
                    {
                        "name": "Zhiwei Gao"
                    },
                    {
                        "name": "George Em Karniadakis"
                    }
                ],
                "author_detail": {
                    "name": "George Em Karniadakis"
                },
                "author": "George Em Karniadakis",
                "arxiv_comment": "28 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11885v1",
                "updated": "2025-01-21T04:40:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T04:40:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00976v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00976v4",
                "updated": "2025-01-21T04:20:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    20,
                    26,
                    1,
                    21,
                    0
                ],
                "published": "2024-02-01T19:47:31Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    19,
                    47,
                    31,
                    3,
                    32,
                    0
                ],
                "title": "Investigating Recurrent Transformers with Dynamic Halt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Recurrent Transformers with Dynamic Halt"
                },
                "summary": "In this paper, we comprehensively study the inductive biases of two major\napproaches to augmenting Transformers with a recurrent mechanism: (1) the\napproach of incorporating a depth-wise recurrence similar to Universal\nTransformers; and (2) the approach of incorporating a chunk-wise temporal\nrecurrence like Temporal Latent Bottleneck. Furthermore, we propose and\ninvestigate novel ways to extend and combine the above methods - for example,\nwe propose a global mean-based dynamic halting mechanism for Universal\nTransformers and an augmentation of Temporal Latent Bottleneck with elements\nfrom Universal Transformer. We compare the models and probe their inductive\nbiases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop\nlanguage modeling, ListOps, and Logical Inference. The code is released in:\nhttps://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we comprehensively study the inductive biases of two major\napproaches to augmenting Transformers with a recurrent mechanism: (1) the\napproach of incorporating a depth-wise recurrence similar to Universal\nTransformers; and (2) the approach of incorporating a chunk-wise temporal\nrecurrence like Temporal Latent Bottleneck. Furthermore, we propose and\ninvestigate novel ways to extend and combine the above methods - for example,\nwe propose a global mean-based dynamic halting mechanism for Universal\nTransformers and an augmentation of Temporal Latent Bottleneck with elements\nfrom Universal Transformer. We compare the models and probe their inductive\nbiases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop\nlanguage modeling, ListOps, and Logical Inference. The code is released in:\nhttps://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main"
                },
                "authors": [
                    {
                        "name": "Jishnu Ray Chowdhury"
                    },
                    {
                        "name": "Cornelia Caragea"
                    }
                ],
                "author_detail": {
                    "name": "Cornelia Caragea"
                },
                "author": "Cornelia Caragea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00976v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00976v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11877v1",
                "updated": "2025-01-21T04:11:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    11,
                    59,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T04:11:59Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    11,
                    59,
                    1,
                    21,
                    0
                ],
                "title": "From Drafts to Answers: Unlocking LLM Potential via Aggregation\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Drafts to Answers: Unlocking LLM Potential via Aggregation\n  Fine-Tuning"
                },
                "summary": "Scaling data and model size has been proven effective for boosting the\nperformance of large language models. In addition to training-time scaling,\nrecent studies have revealed that increasing test-time computational resources\ncan further improve performance. In this work, we introduce Aggregation\nFine-Tuning (AFT), a supervised finetuning paradigm where the model learns to\nsynthesize multiple draft responses, referred to as proposals, into a single,\nrefined answer, termed aggregation. At inference time, a propose-and-aggregate\nstrategy further boosts performance by iteratively generating proposals and\naggregating them. Empirical evaluations on benchmark datasets show that\nAFT-trained models substantially outperform standard SFT. Notably, an AFT\nmodel, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC\nwin rate on AlpacaEval 2, surpassing significantly larger LLMs such as\nLlama3.1-405B-Instruct and GPT4. By combining sequential refinement and\nparallel sampling, the propose-and-aggregate framework scales inference-time\ncomputation in a flexible manner. Overall, These findings position AFT as a\npromising approach to unlocking additional capabilities of LLMs without\nresorting to increasing data volume or model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling data and model size has been proven effective for boosting the\nperformance of large language models. In addition to training-time scaling,\nrecent studies have revealed that increasing test-time computational resources\ncan further improve performance. In this work, we introduce Aggregation\nFine-Tuning (AFT), a supervised finetuning paradigm where the model learns to\nsynthesize multiple draft responses, referred to as proposals, into a single,\nrefined answer, termed aggregation. At inference time, a propose-and-aggregate\nstrategy further boosts performance by iteratively generating proposals and\naggregating them. Empirical evaluations on benchmark datasets show that\nAFT-trained models substantially outperform standard SFT. Notably, an AFT\nmodel, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC\nwin rate on AlpacaEval 2, surpassing significantly larger LLMs such as\nLlama3.1-405B-Instruct and GPT4. By combining sequential refinement and\nparallel sampling, the propose-and-aggregate framework scales inference-time\ncomputation in a flexible manner. Overall, These findings position AFT as a\npromising approach to unlocking additional capabilities of LLMs without\nresorting to increasing data volume or model size."
                },
                "authors": [
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Tingchen Fu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "20 pages; work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12624v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12624v5",
                "updated": "2025-01-21T04:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    10,
                    13,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-18T13:49:54Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    49,
                    54,
                    1,
                    170,
                    0
                ],
                "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges"
                },
                "summary": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores."
                },
                "authors": [
                    {
                        "name": "Aman Singh Thakur"
                    },
                    {
                        "name": "Kartik Choudhary"
                    },
                    {
                        "name": "Venkat Srinik Ramayapally"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12624v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12624v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11051v2",
                "updated": "2025-01-21T04:06:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    6,
                    9,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-20T17:57:46Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    46,
                    1,
                    233,
                    0
                ],
                "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for route summarization, and end-to-end\ntraining on VLN datasets. The augmented datasets are synthesized automatically.\nExperimental results demonstrate FLAME's superiority over existing methods,\nsurpassing state-of-the-art methods by a 7.3% increase in task completion on\nTouchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)\nin complex navigation tasks, representing an advancement towards applications\nof MLLMs in the field of embodied intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for route summarization, and end-to-end\ntraining on VLN datasets. The augmented datasets are synthesized automatically.\nExperimental results demonstrate FLAME's superiority over existing methods,\nsurpassing state-of-the-art methods by a 7.3% increase in task completion on\nTouchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)\nin complex navigation tasks, representing an advancement towards applications\nof MLLMs in the field of embodied intelligence."
                },
                "authors": [
                    {
                        "name": "Yunzhe Xu"
                    },
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "arxiv_comment": "Accepted to AAAI 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11873v1",
                "updated": "2025-01-21T04:04:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    4,
                    39,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T04:04:39Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    4,
                    39,
                    1,
                    21,
                    0
                ],
                "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training\n  Specialized Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demons in the Detail: On Implementing Load Balancing Loss for Training\n  Specialized Mixture-of-Expert Models"
                },
                "summary": "This paper revisits the implementation of\n$\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E\n\\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$\nrepresents the frequency of expert $i$ being selected, and $p_i$ denotes the\naverage gating score of the expert $i$. Existing MoE training frameworks\nusually employ the parallel training strategy so that $f_i$ and the LBL are\ncalculated within a $\\textbf{micro-batch}$ and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a $\\textbf{global-batch}$ to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize $f_i$ across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n$\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the implementation of\n$\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E\n\\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$\nrepresents the frequency of expert $i$ being selected, and $p_i$ denotes the\naverage gating score of the expert $i$. Existing MoE training frameworks\nusually employ the parallel training strategy so that $f_i$ and the LBL are\ncalculated within a $\\textbf{micro-batch}$ and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a $\\textbf{global-batch}$ to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize $f_i$ across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n$\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts."
                },
                "authors": [
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Ivan Titov"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10718v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10718v4",
                "updated": "2025-01-21T04:00:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    0,
                    36,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-14T07:22:03Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    7,
                    22,
                    3,
                    5,
                    349,
                    0
                ],
                "title": "Grid: Omni Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grid: Omni Visual Generation"
                },
                "summary": "Visual generation has witnessed remarkable progress in single-image tasks,\nyet extending these capabilities to temporal sequences remains challenging.\nCurrent approaches either build specialized video models from scratch with\nenormous computational costs or add separate motion modules to image\ngenerators, both requiring learning temporal dynamics anew. We observe that\nmodern image generation models possess underutilized potential in handling\nstructured layouts with implicit temporal understanding. Building on this\ninsight, we introduce GRID, which reformulates temporal sequences as grid\nlayouts, enabling holistic processing of visual sequences while leveraging\nexisting model capabilities. Through a parallel flow-matching training strategy\nwith coarse-to-fine scheduling, our approach achieves up to 67 faster inference\nspeeds while using <1/1000 of the computational resources compared to\nspecialized models. Extensive experiments demonstrate that GRID not only excels\nin temporal tasks from Text-to-Video to 3D Editing but also preserves strong\nperformance in image generation, establishing itself as an efficient and\nversatile omni-solution for visual generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation has witnessed remarkable progress in single-image tasks,\nyet extending these capabilities to temporal sequences remains challenging.\nCurrent approaches either build specialized video models from scratch with\nenormous computational costs or add separate motion modules to image\ngenerators, both requiring learning temporal dynamics anew. We observe that\nmodern image generation models possess underutilized potential in handling\nstructured layouts with implicit temporal understanding. Building on this\ninsight, we introduce GRID, which reformulates temporal sequences as grid\nlayouts, enabling holistic processing of visual sequences while leveraging\nexisting model capabilities. Through a parallel flow-matching training strategy\nwith coarse-to-fine scheduling, our approach achieves up to 67 faster inference\nspeeds while using <1/1000 of the computational resources compared to\nspecialized models. Extensive experiments demonstrate that GRID not only excels\nin temporal tasks from Text-to-Video to 3D Editing but also preserves strong\nperformance in image generation, establishing itself as an efficient and\nversatile omni-solution for visual generation."
                },
                "authors": [
                    {
                        "name": "Cong Wan"
                    },
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Zijian Cai"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Yunlong Zhao"
                    },
                    {
                        "name": "Yifan Bai"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "arxiv_comment": "Codes: https://github.com/Should-AI-Lab/GRID",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10718v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10718v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11868v1",
                "updated": "2025-01-21T03:50:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    50,
                    51,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:50:51Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    50,
                    51,
                    1,
                    21,
                    0
                ],
                "title": "Automatic Debiased Machine Learning for Smooth Functionals of\n  Nonparametric M-Estimands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Debiased Machine Learning for Smooth Functionals of\n  Nonparametric M-Estimands"
                },
                "summary": "We propose a unified framework for automatic debiased machine learning\n(autoDML) to perform inference on smooth functionals of infinite-dimensional\nM-estimands, defined as population risk minimizers over Hilbert spaces. By\nautomating debiased estimation and inference procedures in causal inference and\nsemiparametric statistics, our framework enables practitioners to construct\nvalid estimators for complex parameters without requiring specialized\nexpertise. The framework supports Neyman-orthogonal loss functions with unknown\nnuisance parameters requiring data-driven estimation, as well as vector-valued\nM-estimands involving simultaneous loss minimization across multiple Hilbert\nspace models. We formalize the class of parameters efficiently estimable by\nautoDML as a novel class of nonparametric projection parameters, defined via\northogonal minimum loss objectives. We introduce three autoDML estimators based\non one-step estimation, targeted minimum loss-based estimation, and the method\nof sieves. For data-driven model selection, we derive a novel decomposition of\nmodel approximation error for smooth functionals of M-estimands and propose\nadaptive debiased machine learning estimators that are superefficient and\nadaptive to the functional form of the M-estimand. Finally, we illustrate the\nflexibility of our framework by constructing autoDML estimators for the\nlong-term survival under a beta-geometric model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unified framework for automatic debiased machine learning\n(autoDML) to perform inference on smooth functionals of infinite-dimensional\nM-estimands, defined as population risk minimizers over Hilbert spaces. By\nautomating debiased estimation and inference procedures in causal inference and\nsemiparametric statistics, our framework enables practitioners to construct\nvalid estimators for complex parameters without requiring specialized\nexpertise. The framework supports Neyman-orthogonal loss functions with unknown\nnuisance parameters requiring data-driven estimation, as well as vector-valued\nM-estimands involving simultaneous loss minimization across multiple Hilbert\nspace models. We formalize the class of parameters efficiently estimable by\nautoDML as a novel class of nonparametric projection parameters, defined via\northogonal minimum loss objectives. We introduce three autoDML estimators based\non one-step estimation, targeted minimum loss-based estimation, and the method\nof sieves. For data-driven model selection, we derive a novel decomposition of\nmodel approximation error for smooth functionals of M-estimands and propose\nadaptive debiased machine learning estimators that are superefficient and\nadaptive to the functional form of the M-estimand. Finally, we illustrate the\nflexibility of our framework by constructing autoDML estimators for the\nlong-term survival under a beta-geometric model."
                },
                "authors": [
                    {
                        "name": "Lars van der Laan"
                    },
                    {
                        "name": "Aurelien Bibaut"
                    },
                    {
                        "name": "Nathan Kallus"
                    },
                    {
                        "name": "Alex Luedtke"
                    }
                ],
                "author_detail": {
                    "name": "Alex Luedtke"
                },
                "author": "Alex Luedtke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11865v1",
                "updated": "2025-01-21T03:44:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    44,
                    40,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:44:40Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    44,
                    40,
                    1,
                    21,
                    0
                ],
                "title": "A White Dwarf Binary Candidate Discovered by LAMOST Using Dynamical\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A White Dwarf Binary Candidate Discovered by LAMOST Using Dynamical\n  Method"
                },
                "summary": "We present the discovery of a binary system containing a white dwarf\ncandidate using data from the LAMOST. Our analysis of the radial velocity data\nallowed us to determine an orbital period of approximately 0.953 days and a\nmass function of 0.129 $M_\\odot$. Through spectral energy distribution (SED)\nfitting, we obtained the stellar parameters of the visible star. By combining\nthese results with the mass function, we established a relationship between the\nmass of the invisible star and the system's inclination angle, along with the\nRoche lobe radius. We find that the mass of the invisible star is below the\nChandrasekhar limit when the inclination angle exceeds $35^\\circ$. Given that\nsystems with large variations in radial velocity typically have high\ninclination angles, we classify the invisible star as a white dwarf candidate.\nThe Roche lobe radius exceeds the physical radius of the visible star,\nindicating that no mass transfer occurs, which results in a weak ellipsoidal\nmodulation effect. Additionally, we obtained light curves from the TESS,\nASAS-SN, and CRTS surveys. The light curves also exhibit a periodicity of\napproximately 0.95 days, with ellipsoidal modulation only in the 2019 TESS\nobservations. Coupled with the strong $\\rm H_{\\alpha}$ emission line observed\nin the LAMOST MRS spectrum, we infer that the surface of the visible star\ncontains significant hot spots. This obscures the system's inherently weak\nellipsoidal modulation, resulting in a manifestation of rotational variables.\nFurthermore, an analysis of the dynamical characteristics of this system\nindicates that it has a high inclination angle ($>60$ degrees) and its orbital\nproperties are consistent with those of typical thin disk stars, supporting the\nhypothesis that the invisible object is a white dwarf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the discovery of a binary system containing a white dwarf\ncandidate using data from the LAMOST. Our analysis of the radial velocity data\nallowed us to determine an orbital period of approximately 0.953 days and a\nmass function of 0.129 $M_\\odot$. Through spectral energy distribution (SED)\nfitting, we obtained the stellar parameters of the visible star. By combining\nthese results with the mass function, we established a relationship between the\nmass of the invisible star and the system's inclination angle, along with the\nRoche lobe radius. We find that the mass of the invisible star is below the\nChandrasekhar limit when the inclination angle exceeds $35^\\circ$. Given that\nsystems with large variations in radial velocity typically have high\ninclination angles, we classify the invisible star as a white dwarf candidate.\nThe Roche lobe radius exceeds the physical radius of the visible star,\nindicating that no mass transfer occurs, which results in a weak ellipsoidal\nmodulation effect. Additionally, we obtained light curves from the TESS,\nASAS-SN, and CRTS surveys. The light curves also exhibit a periodicity of\napproximately 0.95 days, with ellipsoidal modulation only in the 2019 TESS\nobservations. Coupled with the strong $\\rm H_{\\alpha}$ emission line observed\nin the LAMOST MRS spectrum, we infer that the surface of the visible star\ncontains significant hot spots. This obscures the system's inherently weak\nellipsoidal modulation, resulting in a manifestation of rotational variables.\nFurthermore, an analysis of the dynamical characteristics of this system\nindicates that it has a high inclination angle ($>60$ degrees) and its orbital\nproperties are consistent with those of typical thin disk stars, supporting the\nhypothesis that the invisible object is a white dwarf."
                },
                "authors": [
                    {
                        "name": "Haifan Zhu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Xue Lib"
                    },
                    {
                        "name": "Jia-jia Li"
                    },
                    {
                        "name": "Pengfu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Pengfu Tian"
                },
                "author": "Pengfu Tian",
                "arxiv_comment": "13pages, 10figures. Accepted by the Journal of High Energy\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11864v1",
                "updated": "2025-01-21T03:42:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    42,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:42:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    42,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "LLM-Agents Driven Automated Simulation Testing and Analysis of small\n  Uncrewed Aerial Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agents Driven Automated Simulation Testing and Analysis of small\n  Uncrewed Aerial Systems"
                },
                "summary": "Thorough simulation testing is crucial for validating the correct behavior of\nsmall Uncrewed Aerial Systems (sUAS) across multiple scenarios, including\nadverse weather conditions (such as wind, and fog), diverse settings (hilly\nterrain, or urban areas), and varying mission profiles (surveillance,\ntracking). While various sUAS simulation tools exist to support developers, the\nentire process of creating, executing, and analyzing simulation tests remains a\nlargely manual and cumbersome task. Developers must identify test scenarios,\nset up the simulation environment, integrate the System under Test (SuT) with\nsimulation tools, formulate mission plans, and collect and analyze results.\nThese labor-intensive tasks limit the ability of developers to conduct\nexhaustive testing across a wide range of scenarios. To alleviate this problem,\nin this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven\nframework, where multiple LLM agents collaborate to support the sUAS simulation\ntesting process. This includes: (1) creating test scenarios that subject the\nSuT to unique environmental contexts; (2) preparing the simulation environment\nas per the test scenario; (3) generating diverse sUAS missions for the SuT to\nexecute; and (4) analyzing simulation results and providing an interactive\nanalytics interface. Further, the design of the framework is flexible for\ncreating and testing scenarios for a variety of sUAS use cases, simulation\ntools, and SuT input requirements. We evaluated our approach by (a) conducting\nsimulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)\nanalyzing the performance of each agent, and (c) gathering feedback from sUAS\ndevelopers. Our findings indicate that AutoSimTest significantly improves the\nefficiency and scope of the sUAS testing process, allowing for more\ncomprehensive and varied scenario evaluations while reducing the manual effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thorough simulation testing is crucial for validating the correct behavior of\nsmall Uncrewed Aerial Systems (sUAS) across multiple scenarios, including\nadverse weather conditions (such as wind, and fog), diverse settings (hilly\nterrain, or urban areas), and varying mission profiles (surveillance,\ntracking). While various sUAS simulation tools exist to support developers, the\nentire process of creating, executing, and analyzing simulation tests remains a\nlargely manual and cumbersome task. Developers must identify test scenarios,\nset up the simulation environment, integrate the System under Test (SuT) with\nsimulation tools, formulate mission plans, and collect and analyze results.\nThese labor-intensive tasks limit the ability of developers to conduct\nexhaustive testing across a wide range of scenarios. To alleviate this problem,\nin this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven\nframework, where multiple LLM agents collaborate to support the sUAS simulation\ntesting process. This includes: (1) creating test scenarios that subject the\nSuT to unique environmental contexts; (2) preparing the simulation environment\nas per the test scenario; (3) generating diverse sUAS missions for the SuT to\nexecute; and (4) analyzing simulation results and providing an interactive\nanalytics interface. Further, the design of the framework is flexible for\ncreating and testing scenarios for a variety of sUAS use cases, simulation\ntools, and SuT input requirements. We evaluated our approach by (a) conducting\nsimulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)\nanalyzing the performance of each agent, and (c) gathering feedback from sUAS\ndevelopers. Our findings indicate that AutoSimTest significantly improves the\nefficiency and scope of the sUAS testing process, allowing for more\ncomprehensive and varied scenario evaluations while reducing the manual effort."
                },
                "authors": [
                    {
                        "name": "Venkata Sai Aswath Duvvuru"
                    },
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Michael Vierhauser"
                    },
                    {
                        "name": "Ankit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ankit Agrawal"
                },
                "author": "Ankit Agrawal",
                "arxiv_comment": "Accepted as full paper at ICSE-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10159v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10159v4",
                "updated": "2025-01-21T03:40:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    40,
                    14,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-19T17:09:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_comment": "NeurIPS 2024 poster",
                "arxiv_journal_ref": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10159v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10159v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04192v2",
                "updated": "2025-01-21T03:40:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    40,
                    1,
                    1,
                    21,
                    0
                ],
                "published": "2024-11-06T19:00:07Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    19,
                    0,
                    7,
                    2,
                    311,
                    0
                ],
                "title": "A Planet Candidate Orbiting near the Hot Jupiter TOI-2818 b Inferred\n  through Transit Timing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Planet Candidate Orbiting near the Hot Jupiter TOI-2818 b Inferred\n  through Transit Timing"
                },
                "summary": "TOI-2818 b is a hot Jupiter orbiting a slightly evolved G-type star on a\n4.04-day orbit that shows transit timing variations (TTVs) suggestive of a\ndecreasing orbital period. In the most recent year of TESS observations,\ntransits were observed $\\sim$8 minutes earlier than expected for a constant\nperiod. The implied orbital decay rate is $1.35 \\pm 0.25$ s yr$^{-1}$, too fast\nto be explained by tidal dissipation even considering the evolved nature of the\nhost star. Radial velocity monitoring rules out the possibility that the\napparent change in period is due to a steady acceleration of the star by a\nlong-period companion. Apsidal precession due to the tidal distortion of the\nplanet is also physically implausible. The most plausible explanation for the\nTTVs appears to be gravitational perturbations from a hitherto undetected\nplanet with mass $\\lesssim$$10\\,M_\\oplus$ that is in (or near) a mean-motion\nresonance with the hot Jupiter. Such a planet could be responsible for the\nobserved TTVs while avoiding detection with the available radial velocity and\ntransit data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TOI-2818 b is a hot Jupiter orbiting a slightly evolved G-type star on a\n4.04-day orbit that shows transit timing variations (TTVs) suggestive of a\ndecreasing orbital period. In the most recent year of TESS observations,\ntransits were observed $\\sim$8 minutes earlier than expected for a constant\nperiod. The implied orbital decay rate is $1.35 \\pm 0.25$ s yr$^{-1}$, too fast\nto be explained by tidal dissipation even considering the evolved nature of the\nhost star. Radial velocity monitoring rules out the possibility that the\napparent change in period is due to a steady acceleration of the star by a\nlong-period companion. Apsidal precession due to the tidal distortion of the\nplanet is also physically implausible. The most plausible explanation for the\nTTVs appears to be gravitational perturbations from a hitherto undetected\nplanet with mass $\\lesssim$$10\\,M_\\oplus$ that is in (or near) a mean-motion\nresonance with the hot Jupiter. Such a planet could be responsible for the\nobserved TTVs while avoiding detection with the available radial velocity and\ntransit data."
                },
                "authors": [
                    {
                        "name": "Brendan J. McKee"
                    },
                    {
                        "name": "Benjamin T. Montet"
                    },
                    {
                        "name": "Samuel W. Yee"
                    },
                    {
                        "name": "Joel D. Hartman"
                    },
                    {
                        "name": "Joshua N. Winn"
                    },
                    {
                        "name": "Jorge H. C. Martins"
                    },
                    {
                        "name": "André M. Silva"
                    },
                    {
                        "name": "Alexander L. Wallace"
                    }
                ],
                "author_detail": {
                    "name": "Alexander L. Wallace"
                },
                "author": "Alexander L. Wallace",
                "arxiv_comment": "11 pages, 2 figures, 3 tables, accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09701v3",
                "updated": "2025-01-21T03:27:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    27,
                    58,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-14T04:01:25Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    4,
                    1,
                    25,
                    4,
                    166,
                    0
                ],
                "title": "Towards Explainable Vulnerability Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Vulnerability Detection with Large Language Models"
                },
                "summary": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security."
                },
                "authors": [
                    {
                        "name": "Qiheng Mao"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11858v1",
                "updated": "2025-01-21T03:22:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    22,
                    10,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:22:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    22,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval."
                },
                "authors": [
                    {
                        "name": "Zhili Cheng"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Shiqi Dai"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Tianyu Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11849v1",
                "updated": "2025-01-21T03:07:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    7,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:07:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    7,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "Network-informed Prompt Engineering against Organized Astroturf\n  Campaigns under Extreme Class Imbalance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-informed Prompt Engineering against Organized Astroturf\n  Campaigns under Extreme Class Imbalance"
                },
                "summary": "Detecting organized political campaigns is of paramount importance in\nfighting against disinformation on social media. Existing approaches for the\nidentification of such organized actions employ techniques mostly from network\nscience, graph machine learning and natural language processing. Their ultimate\ngoal is to analyze the relationships and interactions (e.g. re-posting) among\nusers and the textual similarities of their posts. Despite their effectiveness\nin recognizing astroturf campaigns, these methods face significant challenges,\nnotably the class imbalance in available training datasets. To mitigate this\nissue, recent methods usually resort to data augmentation or increasing the\nnumber of positive samples, which may not always be feasible or sufficient in\nreal-world settings. Following a different path, in this paper, we propose a\nnovel framework for identifying astroturf campaigns based solely on large\nlanguage models (LLMs), introducing a Balanced Retrieval-Augmented Generation\n(Balanced RAG) component. Our approach first gives both textual information\nconcerning the posts (in our case tweets) and the user interactions of the\nsocial network as input to a language model. Then, through prompt engineering\nand the proposed Balanced RAG method, it effectively detects coordinated\ndisinformation campaigns on X (Twitter). The proposed framework does not\nrequire any training or fine-tuning of the language model. Instead, by\nstrategically harnessing the strengths of prompt engineering and Balanced RAG,\nit facilitates LLMs to overcome the effects of class imbalance and effectively\nidentify coordinated political campaigns. The experimental results demonstrate\nthat by incorporating the proposed prompt engineering and Balanced RAG methods,\nour framework outperforms the traditional graph-based baselines, achieving\n2x-3x improvements in terms of precision, recall and F1 scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting organized political campaigns is of paramount importance in\nfighting against disinformation on social media. Existing approaches for the\nidentification of such organized actions employ techniques mostly from network\nscience, graph machine learning and natural language processing. Their ultimate\ngoal is to analyze the relationships and interactions (e.g. re-posting) among\nusers and the textual similarities of their posts. Despite their effectiveness\nin recognizing astroturf campaigns, these methods face significant challenges,\nnotably the class imbalance in available training datasets. To mitigate this\nissue, recent methods usually resort to data augmentation or increasing the\nnumber of positive samples, which may not always be feasible or sufficient in\nreal-world settings. Following a different path, in this paper, we propose a\nnovel framework for identifying astroturf campaigns based solely on large\nlanguage models (LLMs), introducing a Balanced Retrieval-Augmented Generation\n(Balanced RAG) component. Our approach first gives both textual information\nconcerning the posts (in our case tweets) and the user interactions of the\nsocial network as input to a language model. Then, through prompt engineering\nand the proposed Balanced RAG method, it effectively detects coordinated\ndisinformation campaigns on X (Twitter). The proposed framework does not\nrequire any training or fine-tuning of the language model. Instead, by\nstrategically harnessing the strengths of prompt engineering and Balanced RAG,\nit facilitates LLMs to overcome the effects of class imbalance and effectively\nidentify coordinated political campaigns. The experimental results demonstrate\nthat by incorporating the proposed prompt engineering and Balanced RAG methods,\nour framework outperforms the traditional graph-based baselines, achieving\n2x-3x improvements in terms of precision, recall and F1 scores."
                },
                "authors": [
                    {
                        "name": "Nikos Kanakaris"
                    },
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Xiongye Xiao"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    },
                    {
                        "name": "Luca Luceri"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11847v1",
                "updated": "2025-01-21T03:06:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    6,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:06:30Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    6,
                    30,
                    1,
                    21,
                    0
                ],
                "title": "A Survey on Memory-Efficient Large-Scale Model Training in AI for\n  Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Memory-Efficient Large-Scale Model Training in AI for\n  Science"
                },
                "summary": "Scientific research faces high costs and inefficiencies with traditional\nmethods, but the rise of deep learning and large language models (LLMs) offers\ninnovative solutions. This survey reviews LLM applications across scientific\nfields such as biology, medicine, chemistry, and meteorology, underscoring\ntheir role in advancing research. However, the continuous expansion of model\nsize has led to significant memory demands, hindering further development and\napplication of LLMs for science. To address this, we review memory-efficient\ntraining techniques for LLMs based on the transformer architecture, including\ndistributed training, mixed precision training, and gradient checkpointing.\nUsing AlphaFold 2 as an example, we demonstrate how tailored memory\noptimization methods can reduce storage needs while preserving prediction\naccuracy. We also discuss the challenges of memory optimization in practice and\npotential future directions, hoping to provide valuable insights for\nresearchers and engineers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research faces high costs and inefficiencies with traditional\nmethods, but the rise of deep learning and large language models (LLMs) offers\ninnovative solutions. This survey reviews LLM applications across scientific\nfields such as biology, medicine, chemistry, and meteorology, underscoring\ntheir role in advancing research. However, the continuous expansion of model\nsize has led to significant memory demands, hindering further development and\napplication of LLMs for science. To address this, we review memory-efficient\ntraining techniques for LLMs based on the transformer architecture, including\ndistributed training, mixed precision training, and gradient checkpointing.\nUsing AlphaFold 2 as an example, we demonstrate how tailored memory\noptimization methods can reduce storage needs while preserving prediction\naccuracy. We also discuss the challenges of memory optimization in practice and\npotential future directions, hoping to provide valuable insights for\nresearchers and engineers."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Tian"
                    },
                    {
                        "name": "Linbo Qiao"
                    },
                    {
                        "name": "Baihui Liu"
                    },
                    {
                        "name": "Gongqingjian Jiang"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11841v1",
                "updated": "2025-01-21T02:51:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    51,
                    10,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:51:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    51,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "Survey on Monocular Metric Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Monocular Metric Depth Estimation"
                },
                "summary": "Monocular Depth Estimation (MDE) is a fundamental computer vision task\nunderpinning applications such as spatial understanding, 3D reconstruction, and\nautonomous driving. While deep learning-based MDE methods can predict relative\ndepth from a single image, their lack of metric scale information often results\nin scale inconsistencies, limiting their utility in downstream tasks like\nvisual SLAM, 3D reconstruction, and novel view synthesis. Monocular Metric\nDepth Estimation (MMDE) addresses these challenges by enabling precise,\nscene-scale depth inference. MMDE improves depth consistency, enhances\nsequential task stability, simplifies integration into downstream applications,\nand broadens practical use cases. This paper provides a comprehensive review of\ndepth estimation technologies, highlighting the evolution from geometry-based\nmethods to state-of-the-art deep learning approaches. It emphasizes\nadvancements in scale-agnostic methods, which are crucial for enabling\nzero-shot generalization as the foundational capability for MMDE. Recent\nprogress in zero-shot MMDE research is explored, focusing on challenges such as\nmodel generalization and the loss of detail at scene boundaries. Innovative\nstrategies to address these issues include unlabelled data augmentation, image\npatching, architectural optimization, and generative techniques. These\nadvancements, analyzed in detail, demonstrate significant contributions to\novercoming existing limitations. Finally, this paper synthesizes recent\ndevelopments in zero-shot MMDE, identifies unresolved challenges, and outlines\nfuture research directions. By offering a clear roadmap and cutting-edge\ninsights, this work aims to deepen understanding of MMDE, inspire novel\napplications, and drive technological innovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Depth Estimation (MDE) is a fundamental computer vision task\nunderpinning applications such as spatial understanding, 3D reconstruction, and\nautonomous driving. While deep learning-based MDE methods can predict relative\ndepth from a single image, their lack of metric scale information often results\nin scale inconsistencies, limiting their utility in downstream tasks like\nvisual SLAM, 3D reconstruction, and novel view synthesis. Monocular Metric\nDepth Estimation (MMDE) addresses these challenges by enabling precise,\nscene-scale depth inference. MMDE improves depth consistency, enhances\nsequential task stability, simplifies integration into downstream applications,\nand broadens practical use cases. This paper provides a comprehensive review of\ndepth estimation technologies, highlighting the evolution from geometry-based\nmethods to state-of-the-art deep learning approaches. It emphasizes\nadvancements in scale-agnostic methods, which are crucial for enabling\nzero-shot generalization as the foundational capability for MMDE. Recent\nprogress in zero-shot MMDE research is explored, focusing on challenges such as\nmodel generalization and the loss of detail at scene boundaries. Innovative\nstrategies to address these issues include unlabelled data augmentation, image\npatching, architectural optimization, and generative techniques. These\nadvancements, analyzed in detail, demonstrate significant contributions to\novercoming existing limitations. Finally, this paper synthesizes recent\ndevelopments in zero-shot MMDE, identifies unresolved challenges, and outlines\nfuture research directions. By offering a clear roadmap and cutting-edge\ninsights, this work aims to deepen understanding of MMDE, inspire novel\napplications, and drive technological innovation."
                },
                "authors": [
                    {
                        "name": "Jiuling Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiuling Zhang"
                },
                "author": "Jiuling Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11840v1",
                "updated": "2025-01-21T02:49:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    49,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:49:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    49,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction"
                },
                "summary": "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs."
                },
                "authors": [
                    {
                        "name": "Noah L. Schroeder"
                    },
                    {
                        "name": "Chris Davis Jaldi"
                    },
                    {
                        "name": "Shan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shan Zhang"
                },
                "author": "Shan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00253v4",
                "updated": "2025-01-21T02:45:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    45,
                    49,
                    1,
                    21,
                    0
                ],
                "published": "2024-04-30T23:56:38Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    23,
                    56,
                    38,
                    1,
                    121,
                    0
                ],
                "title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based\n  Verification"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in code\ngeneration, offering developers groundbreaking automated programming support.\nHowever, LLMs often generate code that is syntactically correct and even\nsemantically plausible, but may not execute as expected or fulfill specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To advance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we present a\ndynamic detection algorithm called CodeHalu designed to detect and quantify\ncode hallucinations. We also introduce the CodeHaluEval benchmark, which\nincludes 8,883 samples from 699 tasks, to systematically and quantitatively\nevaluate code hallucinations. By evaluating 17 popular LLMs using this\nbenchmark, we reveal significant differences in their accuracy and reliability\nin code generation, offering detailed insights for further improving the code\ngeneration capabilities of LLMs. The CodeHalu benchmark and code are publicly\navailable at https://github.com/yuchen814/CodeHalu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in code\ngeneration, offering developers groundbreaking automated programming support.\nHowever, LLMs often generate code that is syntactically correct and even\nsemantically plausible, but may not execute as expected or fulfill specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To advance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we present a\ndynamic detection algorithm called CodeHalu designed to detect and quantify\ncode hallucinations. We also introduce the CodeHaluEval benchmark, which\nincludes 8,883 samples from 699 tasks, to systematically and quantitatively\nevaluate code hallucinations. By evaluating 17 popular LLMs using this\nbenchmark, we reveal significant differences in their accuracy and reliability\nin code generation, offering detailed insights for further improving the code\ngeneration capabilities of LLMs. The CodeHalu benchmark and code are publicly\navailable at https://github.com/yuchen814/CodeHalu."
                },
                "authors": [
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Qian Yang"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "Accepted by AAAI 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.12372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12372v1",
                "updated": "2025-01-21T18:52:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:52:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    52,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Long Context All You Need? Leveraging LLM's Extended Context for\n  NL2SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\na range of natural language processing tasks. In particular, improvements in\nreasoning abilities and the expansion of context windows have opened new\navenues for leveraging these powerful models. NL2SQL is challenging in that the\nnatural language question is inherently ambiguous, while the SQL generation\nrequires a precise understanding of complex data schema and semantics. One\napproach to this semantic ambiguous problem is to provide more and sufficient\ncontextual information.\n  In this work, we explore the performance and the latency trade-offs of the\nextended context window (a.k.a., long context) offered by Google's\nstate-of-the-art LLM (\\textit{gemini-1.5-pro}). We study the impact of various\ncontextual information, including column example values, question and SQL query\npairs, user-provided hints, SQL documentation, and schema. To the best of our\nknowledge, this is the first work to study how the extended context window and\nextra contextual information can help NL2SQL generation with respect to both\naccuracy and latency cost. We show that long context LLMs are robust and do not\nget lost in the extended contextual information. Additionally, our long-context\nNL2SQL pipeline based on Google's \\textit{gemini-pro-1.5} achieve a strong\nperformance with 67.41\\% on BIRD benchmark (dev) without finetuning and\nexpensive self-consistency based techniques."
                },
                "authors": [
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Gaurav T. Kakkar"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Brenton Milne"
                    },
                    {
                        "name": "Fatma Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Fatma Ozcan"
                },
                "author": "Fatma Ozcan",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14655v2",
                "updated": "2025-01-21T18:14:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    14,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-18T17:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens"
                },
                "summary": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Rasool Fakoor"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Fakoor"
                },
                "author": "Rasool Fakoor",
                "arxiv_comment": "Published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12339v1",
                "updated": "2025-01-21T18:13:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:13:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    13,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Treefix: Enabling Execution with a Tree of Prefixes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treefix: Enabling Execution with a Tree of Prefixes"
                },
                "summary": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute code is a prerequisite for various dynamic program\nanalyses. Learning-guided execution has been proposed as an approach to enable\nthe execution of arbitrary code snippets by letting a neural model predict\nlikely values for any missing variables. Although state-of-the-art\nlearning-guided execution approaches, such as LExecutor, can enable the\nexecution of a relative high amount of code, they are limited to predicting a\nrestricted set of possible values and do not use any feedback from previous\nexecutions to execute even more code. This paper presents Treefix, a novel\nlearning-guided execution approach that leverages LLMs to iteratively create\ncode prefixes that enable the execution of a given code snippet. The approach\naddresses the problem in a multi-step fashion, where each step uses feedback\nabout the code snippet and its execution to instruct an LLM to improve a\npreviously generated prefix. This process iteratively creates a tree of\nprefixes, a subset of which is returned to the user as prefixes that maximize\nthe number of executed lines in the code snippet. In our experiments with two\ndatasets of Python code snippets, Treefix achieves 25% and 7% more coverage\nrelative to the current state of the art in learning-guided execution, covering\na total of 84% and 82% of all lines in the code snippets."
                },
                "authors": [
                    {
                        "name": "Beatriz Souza"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "arxiv_comment": "Accepted in research track of the EEE/ACM International Conference on\n  Software Engineering (ICSE) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12335v1",
                "updated": "2025-01-21T18:10:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    10,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:10:03Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    10,
                    3,
                    1,
                    21,
                    0
                ],
                "title": "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration"
                },
                "summary": "Compressive sensing is a signal processing technique that enables the\nreconstruction of sparse signals from a limited number of measurements,\nleveraging the signal's inherent sparsity to facilitate efficient recovery.\nRecent works on the Quantum Compressive Sensing (QCS) architecture, a quantum\ndata-driven approach to compressive sensing where the state of the tensor\nnetwork is represented by a quantum state over a set of entangled qubits, have\nshown promise in advancing quantum data-driven methods for compressive sensing.\nHowever, the QCS framework has remained largely untested on quantum computing\nresources or in the presence of quantum noise. In this work, we present a\npractical implementation of QCS on Amazon Braket, utilizing the Quantum\nImaginary Time Evolution (QITE) projection technique to assess the framework's\ncapabilities under quantum noise. We outline the necessary modifications to the\nQCS framework for deployment on Amazon Braket, followed by results under four\ntypes of quantum noise. Finally, we discuss potential long-term directions\naimed at unlocking the full potential of quantum compressive sensing for\napplications such as signal recovery and image processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressive sensing is a signal processing technique that enables the\nreconstruction of sparse signals from a limited number of measurements,\nleveraging the signal's inherent sparsity to facilitate efficient recovery.\nRecent works on the Quantum Compressive Sensing (QCS) architecture, a quantum\ndata-driven approach to compressive sensing where the state of the tensor\nnetwork is represented by a quantum state over a set of entangled qubits, have\nshown promise in advancing quantum data-driven methods for compressive sensing.\nHowever, the QCS framework has remained largely untested on quantum computing\nresources or in the presence of quantum noise. In this work, we present a\npractical implementation of QCS on Amazon Braket, utilizing the Quantum\nImaginary Time Evolution (QITE) projection technique to assess the framework's\ncapabilities under quantum noise. We outline the necessary modifications to the\nQCS framework for deployment on Amazon Braket, followed by results under four\ntypes of quantum noise. Finally, we discuss potential long-term directions\naimed at unlocking the full potential of quantum compressive sensing for\napplications such as signal recovery and image processing."
                },
                "authors": [
                    {
                        "name": "Naveed Naimipour"
                    },
                    {
                        "name": "Collin Frink"
                    },
                    {
                        "name": "Harry Shaw"
                    },
                    {
                        "name": "Haleh Safavi"
                    },
                    {
                        "name": "Mojtaba Soltanalian"
                    }
                ],
                "author_detail": {
                    "name": "Mojtaba Soltanalian"
                },
                "author": "Mojtaba Soltanalian",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12332v1",
                "updated": "2025-01-21T18:06:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    6,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T18:06:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    18,
                    6,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "Automatic Labelling with Open-source LLMs using Dynamic Label Schema\n  Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Labelling with Open-source LLMs using Dynamic Label Schema\n  Integration"
                },
                "summary": "Acquiring labelled training data remains a costly task in real world machine\nlearning projects to meet quantity and quality requirements. Recently Large\nLanguage Models (LLMs), notably GPT-4, have shown great promises in labelling\ndata with high accuracy. However, privacy and cost concerns prevent the\nubiquitous use of GPT-4. In this work, we explore effectively leveraging\nopen-source models for automatic labelling. We identify integrating label\nschema as a promising technology but found that naively using the label\ndescription for classification leads to poor performance on high cardinality\ntasks. To address this, we propose Retrieval Augmented Classification (RAC) for\nwhich LLM performs inferences for one label at a time using corresponding label\nschema; we start with the most related label and iterates until a label is\nchosen by the LLM. We show that our method, which dynamically integrates label\ndescription, leads to performance improvements in labelling tasks. We further\nshow that by focusing only on the most promising labels, RAC can trade off\nbetween label quality and coverage - a property we leverage to automatically\nlabel our internal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiring labelled training data remains a costly task in real world machine\nlearning projects to meet quantity and quality requirements. Recently Large\nLanguage Models (LLMs), notably GPT-4, have shown great promises in labelling\ndata with high accuracy. However, privacy and cost concerns prevent the\nubiquitous use of GPT-4. In this work, we explore effectively leveraging\nopen-source models for automatic labelling. We identify integrating label\nschema as a promising technology but found that naively using the label\ndescription for classification leads to poor performance on high cardinality\ntasks. To address this, we propose Retrieval Augmented Classification (RAC) for\nwhich LLM performs inferences for one label at a time using corresponding label\nschema; we start with the most related label and iterates until a label is\nchosen by the LLM. We show that our method, which dynamically integrates label\ndescription, leads to performance improvements in labelling tasks. We further\nshow that by focusing only on the most promising labels, RAC can trade off\nbetween label quality and coverage - a property we leverage to automatically\nlabel our internal datasets."
                },
                "authors": [
                    {
                        "name": "Thomas Walshe"
                    },
                    {
                        "name": "Sae Young Moon"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Yawwani Gunawardana"
                    },
                    {
                        "name": "Fran Silavong"
                    }
                ],
                "author_detail": {
                    "name": "Fran Silavong"
                },
                "author": "Fran Silavong",
                "arxiv_comment": "11 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10622v3",
                "updated": "2025-01-21T17:20:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    20,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-14T00:05:42Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    0,
                    5,
                    42,
                    5,
                    349,
                    0
                ],
                "title": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options"
                },
                "summary": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple-choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet -- with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\nability, the correct answer options in the questions were replaced with \"None\nof the above.\" Then, the explain-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning ability. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answer options with 'None of the above', all models\nexhibited a considerable decline in performance, suggesting room for\nimprovement. The explain-first and step-by-step instruction prompts helped\nenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and\nClaude 3.5 Sonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics education and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple-choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet -- with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\nability, the correct answer options in the questions were replaced with \"None\nof the above.\" Then, the explain-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning ability. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answer options with 'None of the above', all models\nexhibited a considerable decline in performance, suggesting room for\nimprovement. The explain-first and step-by-step instruction prompts helped\nenhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and\nClaude 3.5 Sonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics education and training."
                },
                "authors": [
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Dequan Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Jiajian Shen"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12300v1",
                "updated": "2025-01-21T17:13:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    13,
                    13,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T17:13:13Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    13,
                    13,
                    1,
                    21,
                    0
                ],
                "title": "LLM-Assisted Knowledge Graph Completion for Curriculum and Domain\n  Modelling in Personalized Higher Education Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Knowledge Graph Completion for Curriculum and Domain\n  Modelling in Personalized Higher Education Recommendations"
                },
                "summary": "While learning personalization offers great potential for learners, modern\npractices in higher education require a deeper consideration of domain models\nand learning contexts, to develop effective personalization algorithms. This\npaper introduces an innovative approach to higher education curriculum\nmodelling that utilizes large language models (LLMs) for knowledge graph (KG)\ncompletion, with the goal of creating personalized learning-path\nrecommendations. Our research focuses on modelling university subjects and\nlinking their topics to corresponding domain models, enabling the integration\nof learning modules from different faculties and institutions in the student's\nlearning path. Central to our approach is a collaborative process, where LLMs\nassist human experts in extracting high-quality, fine-grained topics from\nlecture materials. We develop a domain, curriculum, and user models for\nuniversity modules and stakeholders. We implement this model to create the KG\nfrom two study modules: Embedded Systems and Development of Embedded Systems\nUsing FPGA. The resulting KG structures the curriculum and links it to the\ndomain models. We evaluate our approach through qualitative expert feedback and\nquantitative graph quality metrics. Domain experts validated the relevance and\naccuracy of the model, while the graph quality metrics measured the structural\nproperties of our KG. Our results show that the LLM-assisted graph completion\napproach enhances the ability to connect related courses across disciplines to\npersonalize the learning experience. Expert feedback also showed high\nacceptance of the proposed collaborative approach for concept extraction and\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While learning personalization offers great potential for learners, modern\npractices in higher education require a deeper consideration of domain models\nand learning contexts, to develop effective personalization algorithms. This\npaper introduces an innovative approach to higher education curriculum\nmodelling that utilizes large language models (LLMs) for knowledge graph (KG)\ncompletion, with the goal of creating personalized learning-path\nrecommendations. Our research focuses on modelling university subjects and\nlinking their topics to corresponding domain models, enabling the integration\nof learning modules from different faculties and institutions in the student's\nlearning path. Central to our approach is a collaborative process, where LLMs\nassist human experts in extracting high-quality, fine-grained topics from\nlecture materials. We develop a domain, curriculum, and user models for\nuniversity modules and stakeholders. We implement this model to create the KG\nfrom two study modules: Embedded Systems and Development of Embedded Systems\nUsing FPGA. The resulting KG structures the curriculum and links it to the\ndomain models. We evaluate our approach through qualitative expert feedback and\nquantitative graph quality metrics. Domain experts validated the relevance and\naccuracy of the model, while the graph quality metrics measured the structural\nproperties of our KG. Our results show that the LLM-assisted graph completion\napproach enhances the ability to connect related courses across disciplines to\npersonalize the learning experience. Expert feedback also showed high\nacceptance of the proposed collaborative approach for concept extraction and\nclassification."
                },
                "authors": [
                    {
                        "name": "Hasan Abu-Rasheed"
                    },
                    {
                        "name": "Constance Jumbo"
                    },
                    {
                        "name": "Rashed Al Amin"
                    },
                    {
                        "name": "Christian Weber"
                    },
                    {
                        "name": "Veit Wiese"
                    },
                    {
                        "name": "Roman Obermaisser"
                    },
                    {
                        "name": "Madjid Fathi"
                    }
                ],
                "author_detail": {
                    "name": "Madjid Fathi"
                },
                "author": "Madjid Fathi",
                "arxiv_comment": "Accepted in the IEEE Global Engineering Education Conference\n  (EDUCON2025), London, UK, 22-25 April, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12273v1",
                "updated": "2025-01-21T16:44:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    44,
                    12,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T16:44:12Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    44,
                    12,
                    1,
                    21,
                    0
                ],
                "title": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and\n  Refinement"
                },
                "summary": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of Supervised Fine-Tuning (SFT) data plays a critical role in\nenhancing the conversational capabilities of Large Language Models (LLMs).\nHowever, as LLMs become more advanced, the availability of high-quality\nhuman-annotated SFT data has become a significant bottleneck, necessitating a\ngreater reliance on synthetic training data. In this work, we introduce Condor,\na novel two-stage synthetic data generation framework that incorporates World\nKnowledge Tree and Self-Reflection Refinement to produce high-quality SFT data\nat scale. Our experimental results demonstrate that a base model fine-tuned on\nonly 20K Condor-generated samples achieves superior performance compared to\ncounterparts. The additional refinement stage in Condor further enables\niterative self-improvement for LLMs at various scales (up to 72B), validating\nthe effectiveness of our approach. Furthermore, our investigation into the\nscaling for synthetic data in post-training reveals substantial unexplored\npotential for performance improvements, opening promising avenues for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Maosong Cao"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Chuyu Zhang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "Tech Report. Github: https://github.com/InternLM/Condor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12263v1",
                "updated": "2025-01-21T16:34:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    34,
                    16,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T16:34:16Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    34,
                    16,
                    1,
                    21,
                    0
                ],
                "title": "mmCooper: A Multi-agent Multi-stage Communication-efficient and\n  Collaboration-robust Cooperative Perception Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mmCooper: A Multi-agent Multi-stage Communication-efficient and\n  Collaboration-robust Cooperative Perception Framework"
                },
                "summary": "Collaborative perception significantly enhances individual vehicle perception\nperformance through the exchange of sensory information among agents. However,\nreal-world deployment faces challenges due to bandwidth constraints and\ninevitable calibration errors during information exchange. To address these\nissues, we propose mmCooper, a novel multi-agent, multi-stage,\ncommunication-efficient, and collaboration-robust cooperative perception\nframework. Our framework leverages a multi-stage collaboration strategy that\ndynamically and adaptively balances intermediate- and late-stage information to\nshare among agents, enhancing perceptual performance while maintaining\ncommunication efficiency. To support robust collaboration despite potential\nmisalignments and calibration errors, our framework captures multi-scale\ncontextual information for robust fusion in the intermediate stage and\ncalibrates the received detection results to improve accuracy in the late\nstage. We validate the effectiveness of mmCooper through extensive experiments\non real-world and simulated datasets. The results demonstrate the superiority\nof our proposed framework and the effectiveness of each component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative perception significantly enhances individual vehicle perception\nperformance through the exchange of sensory information among agents. However,\nreal-world deployment faces challenges due to bandwidth constraints and\ninevitable calibration errors during information exchange. To address these\nissues, we propose mmCooper, a novel multi-agent, multi-stage,\ncommunication-efficient, and collaboration-robust cooperative perception\nframework. Our framework leverages a multi-stage collaboration strategy that\ndynamically and adaptively balances intermediate- and late-stage information to\nshare among agents, enhancing perceptual performance while maintaining\ncommunication efficiency. To support robust collaboration despite potential\nmisalignments and calibration errors, our framework captures multi-scale\ncontextual information for robust fusion in the intermediate stage and\ncalibrates the received detection results to improve accuracy in the late\nstage. We validate the effectiveness of mmCooper through extensive experiments\non real-world and simulated datasets. The results demonstrate the superiority\nof our proposed framework and the effectiveness of each component."
                },
                "authors": [
                    {
                        "name": "Bingyi Liu"
                    },
                    {
                        "name": "Jian Teng"
                    },
                    {
                        "name": "Hongfei Xue"
                    },
                    {
                        "name": "Enshu Wang"
                    },
                    {
                        "name": "Chuanhui Zhu"
                    },
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Libing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Libing Wu"
                },
                "author": "Libing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06687v3",
                "updated": "2025-01-21T16:15:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    15,
                    37,
                    1,
                    21,
                    0
                ],
                "published": "2024-05-06T18:09:32Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    18,
                    9,
                    32,
                    0,
                    127,
                    0
                ],
                "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes"
                },
                "summary": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes."
                },
                "authors": [
                    {
                        "name": "Damin Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Geetanjali Bihani"
                    },
                    {
                        "name": "Julia Rayz"
                    }
                ],
                "author_detail": {
                    "name": "Julia Rayz"
                },
                "author": "Julia Rayz",
                "arxiv_comment": "COLING 2025",
                "arxiv_journal_ref": "Proceedings of the 31st International Conference on Computational\n  Linguistics (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11414v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11414v2",
                "updated": "2025-01-21T16:05:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    5,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-15T09:02:09Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    2,
                    9,
                    1,
                    289,
                    0
                ],
                "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability"
                },
                "summary": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads."
                },
                "authors": [
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "23pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11414v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11414v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12243v1",
                "updated": "2025-01-21T16:03:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    3,
                    42,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T16:03:42Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    16,
                    3,
                    42,
                    1,
                    21,
                    0
                ],
                "title": "FOCUS: First Order Concentrated Updating Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: First Order Concentrated Updating Scheme"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance, and\nimproving their pre-training process appears to be key to enhancing their\ncapabilities further. Based on the documented success of Adam, learning rate\ndecay, and weight decay, we hypothesize that the pre-training loss landscape\nfeatures a narrowing valley structure. Through experiments with synthetic loss\nfunctions, we discover that when gradient query noise is high relative to the\nvalley's sharpness, Adam's performance falls behind that of Signum because Adam\nreduces the effective step size too drastically. This observation led us to\ndevelop FOCUS, an optimizer that enhances Signum by incorporating attraction\ntoward moving averaged parameters, allowing it to handle noise better while\nmaintaining larger step sizes. In training GPT-2, FOCUS proves to be more\nstable than Signum and faster than Adam. These results suggest that gradient\nnoise may be an underappreciated limiting factor in LLM training, and FOCUS\noffers promising solutions."
                },
                "authors": [
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Jeff Gore"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Gore"
                },
                "author": "Jeff Gore",
                "arxiv_comment": "19 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12226v1",
                "updated": "2025-01-21T15:51:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    51,
                    7,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:51:07Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    51,
                    7,
                    1,
                    21,
                    0
                ],
                "title": "CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning"
                },
                "summary": "Large Language Models (LLMs) have recently achieved impressive results in\ncomplex reasoning tasks through Chain of Thought (CoT) prompting. However, most\nexisting CoT methods rely on using the same prompts, whether manually designed\nor automatically generated, to handle the entire dataset. This\none-size-fits-all approach may fail to meet the specific needs arising from the\ndiversities within a single dataset. To solve this problem, we propose the\nClustered Distance-Weighted Chain of Thought (CDW-CoT) method, which\ndynamically constructs prompts tailored to the characteristics of each data\ninstance by integrating clustering and prompt optimization techniques. Our\nmethod employs clustering algorithms to categorize the dataset into distinct\ngroups, from which a candidate pool of prompts is selected to reflect the\ninherent diversity within the dataset. For each cluster, CDW-CoT trains the\noptimal prompt probability distribution tailored to their specific\ncharacteristics. Finally, it dynamically constructs a unique prompt probability\ndistribution for each test instance, based on its proximity to cluster centers,\nfrom which prompts are selected for reasoning. CDW-CoT consistently outperforms\ntraditional CoT methods across six datasets, including commonsense, symbolic,\nand mathematical reasoning tasks. Specifically, when compared to manual CoT,\nCDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and\n15.72% on LLaMA3 (8B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently achieved impressive results in\ncomplex reasoning tasks through Chain of Thought (CoT) prompting. However, most\nexisting CoT methods rely on using the same prompts, whether manually designed\nor automatically generated, to handle the entire dataset. This\none-size-fits-all approach may fail to meet the specific needs arising from the\ndiversities within a single dataset. To solve this problem, we propose the\nClustered Distance-Weighted Chain of Thought (CDW-CoT) method, which\ndynamically constructs prompts tailored to the characteristics of each data\ninstance by integrating clustering and prompt optimization techniques. Our\nmethod employs clustering algorithms to categorize the dataset into distinct\ngroups, from which a candidate pool of prompts is selected to reflect the\ninherent diversity within the dataset. For each cluster, CDW-CoT trains the\noptimal prompt probability distribution tailored to their specific\ncharacteristics. Finally, it dynamically constructs a unique prompt probability\ndistribution for each test instance, based on its proximity to cluster centers,\nfrom which prompts are selected for reasoning. CDW-CoT consistently outperforms\ntraditional CoT methods across six datasets, including commonsense, symbolic,\nand mathematical reasoning tasks. Specifically, when compared to manual CoT,\nCDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and\n15.72% on LLaMA3 (8B)."
                },
                "authors": [
                    {
                        "name": "Yuanheng Fang"
                    },
                    {
                        "name": "Guoqing Chao"
                    },
                    {
                        "name": "Wenqiang Lei"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Dianhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Dianhui Chu"
                },
                "author": "Dianhui Chu",
                "arxiv_comment": "aaai25(poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12221v1",
                "updated": "2025-01-21T15:47:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    47,
                    32,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:47:32Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    47,
                    32,
                    1,
                    21,
                    0
                ],
                "title": "Leveraging Large Language Models for Realizing Truly Intelligent User\n  Interfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models for Realizing Truly Intelligent User\n  Interfaces"
                },
                "summary": "The number of published scholarly articles is growing at a significant rate,\nmaking scholarly knowledge organization increasingly important. Various\napproaches have been proposed to organize scholarly information, including\ndescribing scholarly knowledge semantically leveraging knowledge graphs.\nTransforming unstructured knowledge, presented within articles, to structured\nand semantically represented knowledge generally requires human intelligence\nand labor since natural language processing methods alone typically do not\nrender sufficient precision and recall for many applications. With the recent\ndevelopments of Large Language Models (LLMs), it becomes increasingly possible\nto provide truly intelligent user interfaces guiding humans in the\ntransformation process. We present an approach to integrate non-intrusive LLMs\nguidance into existing user interfaces. More specifically, we integrate\nLLM-supported user interface components into an existing scholarly knowledge\ninfrastructure. Additionally, we provide our experiences with LLM integration,\ndetailing best practices and obstacles. Finally, we evaluate the approach using\na small-scale user evaluation with domain experts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of published scholarly articles is growing at a significant rate,\nmaking scholarly knowledge organization increasingly important. Various\napproaches have been proposed to organize scholarly information, including\ndescribing scholarly knowledge semantically leveraging knowledge graphs.\nTransforming unstructured knowledge, presented within articles, to structured\nand semantically represented knowledge generally requires human intelligence\nand labor since natural language processing methods alone typically do not\nrender sufficient precision and recall for many applications. With the recent\ndevelopments of Large Language Models (LLMs), it becomes increasingly possible\nto provide truly intelligent user interfaces guiding humans in the\ntransformation process. We present an approach to integrate non-intrusive LLMs\nguidance into existing user interfaces. More specifically, we integrate\nLLM-supported user interface components into an existing scholarly knowledge\ninfrastructure. Additionally, we provide our experiences with LLM integration,\ndetailing best practices and obstacles. Finally, we evaluate the approach using\na small-scale user evaluation with domain experts."
                },
                "authors": [
                    {
                        "name": "Allard Oelen"
                    },
                    {
                        "name": "Sören Auer"
                    }
                ],
                "author_detail": {
                    "name": "Sören Auer"
                },
                "author": "Sören Auer",
                "arxiv_doi": "10.1145/3613905.3650949",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3613905.3650949",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16789v2",
                "updated": "2025-01-21T15:40:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2024-05-27T03:24:01Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    3,
                    24,
                    1,
                    0,
                    148,
                    0
                ],
                "title": "NoteLLM-2: Multimodal Large Representation Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoteLLM-2: Multimodal Large Representation Models for Recommendation"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ntext understanding and embedding tasks. However, their potential in multimodal\nrepresentation, particularly for item-to-item (I2I) recommendations, remains\nunderexplored. While leveraging existing Multimodal Large Language Models\n(MLLMs) for such tasks is promising, challenges arise due to their delayed\nrelease compared to corresponding LLMs and the inefficiency in representation\ntasks. To address these issues, we propose an end-to-end fine-tuning method\nthat customizes the integration of any existing LLMs and vision encoders for\nefficient multimodal representation. Preliminary experiments revealed that\nfine-tuned LLMs often neglect image content. To counteract this, we propose\nNoteLLM-2, a novel framework that enhances visual information. Specifically, we\npropose two approaches: first, a prompt-based method that segregates visual and\ntextual content, employing a multimodal In-Context Learning strategy to balance\nfocus across modalities; second, a late fusion technique that directly\nintegrates visual information into the final representations. Extensive\nexperiments, both online and offline, demonstrate the effectiveness of our\napproach. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/NoteLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional proficiency in\ntext understanding and embedding tasks. However, their potential in multimodal\nrepresentation, particularly for item-to-item (I2I) recommendations, remains\nunderexplored. While leveraging existing Multimodal Large Language Models\n(MLLMs) for such tasks is promising, challenges arise due to their delayed\nrelease compared to corresponding LLMs and the inefficiency in representation\ntasks. To address these issues, we propose an end-to-end fine-tuning method\nthat customizes the integration of any existing LLMs and vision encoders for\nefficient multimodal representation. Preliminary experiments revealed that\nfine-tuned LLMs often neglect image content. To counteract this, we propose\nNoteLLM-2, a novel framework that enhances visual information. Specifically, we\npropose two approaches: first, a prompt-based method that segregates visual and\ntextual content, employing a multimodal In-Context Learning strategy to balance\nfocus across modalities; second, a late fusion technique that directly\nintegrates visual information into the final representations. Extensive\nexperiments, both online and offline, demonstrate the effectiveness of our\napproach. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/NoteLLM."
                },
                "authors": [
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "Accepted by KDD'25 ADS track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01957v3",
                "updated": "2025-01-21T15:36:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    36,
                    41,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-03T18:59:52Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Haoyu Cao"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Long Ma"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "https://github.com/VITA-MLLM/VITA (2K+ Stars by now)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12210v1",
                "updated": "2025-01-21T15:24:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    24,
                    29,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T15:24:29Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    24,
                    29,
                    1,
                    21,
                    0
                ],
                "title": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Can't Eat Your Cake and Have It Too: The Performance Degradation of\n  LLMs with Jailbreak Defense"
                },
                "summary": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of generative large language models (LLMs) like LLaMA and\nChatGPT, these models have significantly transformed daily life and work by\nproviding advanced insights. However, as jailbreak attacks continue to\ncircumvent built-in safety mechanisms, exploiting carefully crafted scenarios\nor tokens, the safety risks of LLMs have come into focus. While numerous\ndefense strategies--such as prompt detection, modification, and model\nfine-tuning--have been proposed to counter these attacks, a critical question\narises: do these defenses compromise the utility and usability of LLMs for\nlegitimate users? Existing research predominantly focuses on the effectiveness\nof defense strategies without thoroughly examining their impact on performance,\nleaving a gap in understanding the trade-offs between LLM safety and\nperformance. Our research addresses this gap by conducting a comprehensive\nstudy on the utility degradation, safety elevation, and exaggerated-safety\nescalation of LLMs with jailbreak defense strategies. We propose USEBench, a\nnovel benchmark designed to evaluate these aspects, along with USEIndex, a\ncomprehensive metric for assessing overall model performance. Through\nexperiments on seven state-of-the-art LLMs, we found that mainstream jailbreak\ndefenses fail to ensure both safety and performance simultaneously. Although\nmodel-finetuning performs the best overall, their effectiveness varies across\nLLMs. Furthermore, vertical comparisons reveal that developers commonly\nprioritize performance over safety when iterating or fine-tuning their LLMs."
                },
                "authors": [
                    {
                        "name": "Wuyuao Mai"
                    },
                    {
                        "name": "Geng Hong"
                    },
                    {
                        "name": "Pei Chen"
                    },
                    {
                        "name": "Xudong Pan"
                    },
                    {
                        "name": "Baojun Liu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Haixin Duan"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10178v2",
                "updated": "2025-01-21T15:11:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    15,
                    11,
                    41,
                    1,
                    21,
                    0
                ],
                "published": "2024-02-15T18:27:37Z",
                "published_parsed": [
                    2024,
                    2,
                    15,
                    18,
                    27,
                    37,
                    3,
                    46,
                    0
                ],
                "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and\n  Agent Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and\n  Agent Generation"
                },
                "summary": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the\ndevelopment of LLM-based agents capable of addressing complex, real-world\ntasks. However, these agents often struggle during task execution due to\nmethodological constraints, such as error propagation and limited adaptability.\nTo address this issue, we propose a multi-agent framework based on dynamic Task\nDecomposition and Agent Generation (TDAG). This framework dynamically\ndecomposes complex tasks into smaller subtasks and assigns each to a\nspecifically generated subagent, thereby enhancing adaptability in diverse and\nunpredictable real-world tasks. Simultaneously, existing benchmarks often lack\nthe granularity needed to evaluate incremental progress in complex, multi-step\ntasks. In response, we introduce ItineraryBench in the context of travel\nplanning, featuring interconnected, progressively complex tasks with a\nfine-grained evaluation system. ItineraryBench is designed to assess agents'\nabilities in memory, planning, and tool usage across tasks of varying\ncomplexity. Our experimental results reveal that TDAG significantly outperforms\nestablished baselines, showcasing its superior adaptability and context\nawareness in complex task scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the\ndevelopment of LLM-based agents capable of addressing complex, real-world\ntasks. However, these agents often struggle during task execution due to\nmethodological constraints, such as error propagation and limited adaptability.\nTo address this issue, we propose a multi-agent framework based on dynamic Task\nDecomposition and Agent Generation (TDAG). This framework dynamically\ndecomposes complex tasks into smaller subtasks and assigns each to a\nspecifically generated subagent, thereby enhancing adaptability in diverse and\nunpredictable real-world tasks. Simultaneously, existing benchmarks often lack\nthe granularity needed to evaluate incremental progress in complex, multi-step\ntasks. In response, we introduce ItineraryBench in the context of travel\nplanning, featuring interconnected, progressively complex tasks with a\nfine-grained evaluation system. ItineraryBench is designed to assess agents'\nabilities in memory, planning, and tool usage across tasks of varying\ncomplexity. Our experimental results reveal that TDAG significantly outperforms\nestablished baselines, showcasing its superior adaptability and context\nawareness in complex task scenarios."
                },
                "authors": [
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Zhiyong Wu"
                    },
                    {
                        "name": "Junfeng Yao"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted by Neural Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11900v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11900v4",
                "updated": "2025-01-21T14:57:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    57,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-14T19:39:11Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    39,
                    11,
                    0,
                    288,
                    0
                ],
                "title": "FLARE: Faithful Logic-Aided Reasoning and Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLARE: Faithful Logic-Aided Reasoning and Exploration"
                },
                "summary": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Question Answering (QA) and Reasoning approaches based on Large\nLanguage Models (LLMs) commonly use prompting techniques, such as\nChain-of-Thought (CoT), assuming the resulting generation will have a more\ngranular exploration and reasoning over the question space and scope. However,\nsuch methods struggle with generating outputs that are faithful to the\nintermediate chain of reasoning produced by the model. On the other end of the\nspectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to\ncombine LLMs with external symbolic solvers. While such approaches boast a high\ndegree of faithfulness, they usually require a model trained for code\ngeneration and struggle with tasks that are ambiguous or hard to formalise\nstrictly. We introduce $\\textbf{F}$aithful $\\textbf{L}$ogic-$\\textbf{A}$ided\n$\\textbf{R}$easoning and $\\textbf{E}$xploration ($\\textbf{FLARE}$), a novel\ninterpretable approach for traversing the problem space using task\ndecompositions. We use the LLM to plan a solution, soft-formalise the query\ninto facts and predicates using a logic programming code and simulate that code\nexecution using an exhaustive multi-hop search over the defined space. Our\nmethod allows us to compute the faithfulness of the reasoning process w.r.t.\nthe generated code and analyse the steps of the multi-hop search without\nrelying on external solvers. Our methods achieve SOTA results on $\\mathbf{7}$\nout of $\\mathbf{9}$ diverse reasoning benchmarks. We also show that model\nfaithfulness positively correlates with overall performance and further\ndemonstrate that $\\textbf{FLARE}$ allows pinpointing the decisive factors\nsufficient for and leading to the correct answer with optimal reasoning during\nthe multi-hop search."
                },
                "authors": [
                    {
                        "name": "Erik Arakelyan"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Pat Verga"
                    },
                    {
                        "name": "Patrick Lewis"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11900v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11900v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07455v2",
                "updated": "2025-01-21T14:53:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    53,
                    36,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-11T17:01:41Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    1,
                    41,
                    1,
                    163,
                    0
                ],
                "title": "Reinforcement Learning from Human Feedback without Reward Inference:\n  Model-Free Algorithm and Instance-Dependent Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback without Reward Inference:\n  Model-Free Algorithm and Instance-Dependent Analysis"
                },
                "summary": "In this paper, we study reinforcement learning from human feedback (RLHF)\nunder an episodic Markov decision process with a general trajectory-wise reward\nmodel. We developed a model-free RLHF best policy identification algorithm,\ncalled $\\mathsf{BSAD}$, without explicit reward model inference, which is a\ncritical intermediate step in the contemporary RLHF paradigms for training\nlarge language models (LLM). The algorithm identifies the optimal policy\ndirectly from human preference information in a backward manner, employing a\ndueling bandit sub-routine that constantly duels actions to identify the\nsuperior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and\nbest-arm-identification-like adaptive stopping criteria to equalize the\nvisitation among all states in the same decision step while moving to the\nprevious step as soon as the optimal action is identifiable, leading to a\nprovable, instance-dependent sample complexity\n$\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which\nresembles the result in classic RL, where $c_{\\mathcal{M}}$ is the\ninstance-dependent constant and $M$ is the batch size. Moreover,\n$\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with\nlogarithmic regret and generalized to discounted MDPs using a frame-based\napproach. Our results show: (i) sample-complexity-wise, RLHF is not\nsignificantly harder than classic RL and (ii) end-to-end RLHF may deliver\nimproved performance by avoiding pitfalls in reward inferring such as overfit\nand distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study reinforcement learning from human feedback (RLHF)\nunder an episodic Markov decision process with a general trajectory-wise reward\nmodel. We developed a model-free RLHF best policy identification algorithm,\ncalled $\\mathsf{BSAD}$, without explicit reward model inference, which is a\ncritical intermediate step in the contemporary RLHF paradigms for training\nlarge language models (LLM). The algorithm identifies the optimal policy\ndirectly from human preference information in a backward manner, employing a\ndueling bandit sub-routine that constantly duels actions to identify the\nsuperior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and\nbest-arm-identification-like adaptive stopping criteria to equalize the\nvisitation among all states in the same decision step while moving to the\nprevious step as soon as the optimal action is identifiable, leading to a\nprovable, instance-dependent sample complexity\n$\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which\nresembles the result in classic RL, where $c_{\\mathcal{M}}$ is the\ninstance-dependent constant and $M$ is the batch size. Moreover,\n$\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with\nlogarithmic regret and generalized to discounted MDPs using a frame-based\napproach. Our results show: (i) sample-complexity-wise, RLHF is not\nsignificantly harder than classic RL and (ii) end-to-end RLHF may deliver\nimproved performance by avoiding pitfalls in reward inferring such as overfit\nand distribution shift."
                },
                "authors": [
                    {
                        "name": "Qining Zhang"
                    },
                    {
                        "name": "Honghao Wei"
                    },
                    {
                        "name": "Lei Ying"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ying"
                },
                "author": "Lei Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09460v2",
                "updated": "2025-01-21T14:38:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    38,
                    35,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-12T17:11:22Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    11,
                    22,
                    3,
                    347,
                    0
                ],
                "title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Copyrighted Material on Large Language Models: A Norwegian\n  Perspective"
                },
                "summary": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of copyrighted materials in training language models raises critical\nlegal and ethical questions. This paper presents a framework for and the\nresults of empirically assessing the impact of publisher-controlled copyrighted\ncorpora on the performance of generative large language models (LLMs) for\nNorwegian. When evaluated on a diverse set of tasks, we found that adding both\nbooks and newspapers to the data mixture of LLMs tend to improve their\nperformance, while the addition of fiction works seems to be detrimental. Our\nexperiments could inform the creation of a compensation scheme for authors\nwhose works contribute to AI development."
                },
                "authors": [
                    {
                        "name": "Javier de la Rosa"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Lemei Zhang"
                    },
                    {
                        "name": "Freddy Wetjen"
                    },
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Rolv-Arild Braaten"
                    },
                    {
                        "name": "Petter Mæhlum"
                    },
                    {
                        "name": "Magnus Breder Birkenes"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Tita Enstad"
                    },
                    {
                        "name": "Hans Christian Farsethås"
                    },
                    {
                        "name": "Svein Arne Brygfjeld"
                    },
                    {
                        "name": "Jon Atle Gulla"
                    },
                    {
                        "name": "Stephan Oepen"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Wilfred Østgulen"
                    },
                    {
                        "name": "Liljia Øvrelid"
                    },
                    {
                        "name": "Aslak Sira Myhre"
                    }
                ],
                "author_detail": {
                    "name": "Aslak Sira Myhre"
                },
                "author": "Aslak Sira Myhre",
                "arxiv_comment": "17 pages, 5 figures, 8 tables. Accepted at NoDaLiDa/Baltic-HLT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09509v2",
                "updated": "2025-01-21T14:34:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    34,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-16T12:43:23Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    12,
                    43,
                    23,
                    3,
                    16,
                    0
                ],
                "title": "Power-Efficient RAN Intelligent Controllers Through Optimized KPI\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Power-Efficient RAN Intelligent Controllers Through Optimized KPI\n  Monitoring"
                },
                "summary": "The Open Radio Access Network (RAN) paradigm envisions a more flexible,\ninteroperable, and intelligent RAN ecosystem via new open interfaces and\nelements like the RAN Intelligent Controller (RIC). However, the impact of\nthese elements on Open RAN's power consumption remains heavily unexplored. This\nwork for the first time evaluates the impact of Key Performance Indicator (KPI)\nmonitoring on RIC's power consumption using real traffic and power\nmeasurements. By analyzing various RIC-RAN communication scenarios, we identify\nthat RIC's power consumption can become a scalability bottleneck, particularly\nin large-scale deployments, even when RIC is limited to its core operational\nfunctionalities and without incorporating application-specific processes. In\nthis context, also for the first time we explore potential power savings\nthrough the elimination of redundant KPI transmissions, extending existing\ntechniques for identical subscription removal and KPI selection, achieving\nsignificant power consumption gains exceeding 87\\% of the overall RIC power\nconsumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Radio Access Network (RAN) paradigm envisions a more flexible,\ninteroperable, and intelligent RAN ecosystem via new open interfaces and\nelements like the RAN Intelligent Controller (RIC). However, the impact of\nthese elements on Open RAN's power consumption remains heavily unexplored. This\nwork for the first time evaluates the impact of Key Performance Indicator (KPI)\nmonitoring on RIC's power consumption using real traffic and power\nmeasurements. By analyzing various RIC-RAN communication scenarios, we identify\nthat RIC's power consumption can become a scalability bottleneck, particularly\nin large-scale deployments, even when RIC is limited to its core operational\nfunctionalities and without incorporating application-specific processes. In\nthis context, also for the first time we explore potential power savings\nthrough the elimination of redundant KPI transmissions, extending existing\ntechniques for identical subscription removal and KPI selection, achieving\nsignificant power consumption gains exceeding 87\\% of the overall RIC power\nconsumption."
                },
                "authors": [
                    {
                        "name": "João Paulo S. H. Lima"
                    },
                    {
                        "name": "George N. Katsaros"
                    },
                    {
                        "name": "Konstantinos Nikitopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Nikitopoulos"
                },
                "author": "Konstantinos Nikitopoulos",
                "arxiv_comment": "Accepted for publication and presentation at IEEE WCNC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M12, 90B18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.3; D.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12174v1",
                "updated": "2025-01-21T14:32:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    32,
                    50,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    32,
                    50,
                    1,
                    21,
                    0
                ],
                "title": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiMarker: Enhancing Text Watermark Detection for Large Language Models\n  with Bipolar Watermarks"
                },
                "summary": "The rapid proliferation of Large Language Models (LLMs) has raised concerns\nabout misuse and the challenges of distinguishing AI-generated text from\nhuman-written content. Existing watermarking techniques, such as \\kgw, still\nface limitations under low watermark strength, stringent false-positive\nrequirements, and low-entropy scenarios. Our analysis reveals that current\ndetection methods rely on coarse estimates of non-watermarked text, which\nconstrains watermark detectability. We propose the Bipolar Watermark\n(BiMarker), a novel approach that divides generated text into positive and\nnegative poles, leveraging the difference in green token counts for detection.\nThis differential mechanism significantly enhances the detectability of\nwatermarked text. Theoretical analysis and experimental results demonstrate\nBiMarker's effectiveness and compatibility with existing optimization\ntechniques, offering a new optimization dimension for watermarking in\nLLM-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of Large Language Models (LLMs) has raised concerns\nabout misuse and the challenges of distinguishing AI-generated text from\nhuman-written content. Existing watermarking techniques, such as \\kgw, still\nface limitations under low watermark strength, stringent false-positive\nrequirements, and low-entropy scenarios. Our analysis reveals that current\ndetection methods rely on coarse estimates of non-watermarked text, which\nconstrains watermark detectability. We propose the Bipolar Watermark\n(BiMarker), a novel approach that divides generated text into positive and\nnegative poles, leveraging the difference in green token counts for detection.\nThis differential mechanism significantly enhances the detectability of\nwatermarked text. Theoretical analysis and experimental results demonstrate\nBiMarker's effectiveness and compatibility with existing optimization\ntechniques, offering a new optimization dimension for watermarking in\nLLM-generated content."
                },
                "authors": [
                    {
                        "name": "Zhuang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Li"
                },
                "author": "Zhuang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12162v1",
                "updated": "2025-01-21T14:15:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    15,
                    1,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:15:01Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    15,
                    1,
                    1,
                    21,
                    0
                ],
                "title": "AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative\n  Decoding"
                },
                "summary": "This paper introduces AdaServe, the first LLM serving system to support SLO\ncustomization through fine-grained speculative decoding. AdaServe leverages the\nlogits of a draft model to predict the speculative accuracy of tokens and\nemploys a theoretically optimal algorithm to construct token trees for\nverification. To accommodate diverse SLO requirements without compromising\nthroughput, AdaServe employs a speculation-and-selection scheme that first\nconstructs candidate token trees for each request and then dynamically selects\ntokens to meet individual SLO constraints while optimizing throughput.\nComprehensive evaluations demonstrate that AdaServe achieves up to 73% higher\nSLO attainment and 74% higher goodput compared to state-of-the-art systems.\nThese results underscore AdaServe's potential to enhance the efficiency and\nadaptability of LLM deployments across varied application scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AdaServe, the first LLM serving system to support SLO\ncustomization through fine-grained speculative decoding. AdaServe leverages the\nlogits of a draft model to predict the speculative accuracy of tokens and\nemploys a theoretically optimal algorithm to construct token trees for\nverification. To accommodate diverse SLO requirements without compromising\nthroughput, AdaServe employs a speculation-and-selection scheme that first\nconstructs candidate token trees for each request and then dynamically selects\ntokens to meet individual SLO constraints while optimizing throughput.\nComprehensive evaluations demonstrate that AdaServe achieves up to 73% higher\nSLO attainment and 74% higher goodput compared to state-of-the-art systems.\nThese results underscore AdaServe's potential to enhance the efficiency and\nadaptability of LLM deployments across varied application scenarios."
                },
                "authors": [
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Remi Delacourt"
                    },
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Qinghan Chen"
                    },
                    {
                        "name": "Shuhuai Lin"
                    },
                    {
                        "name": "April Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Sean Lai"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12152v1",
                "updated": "2025-01-21T14:02:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:02:39Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "title": "Contextualizing Recommendation Explanations with LLMs: A User Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualizing Recommendation Explanations with LLMs: A User Study"
                },
                "summary": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly prevalent in recommender\nsystems, where LLMs can be used to generate personalized recommendations. Here,\nwe examine how different LLM-generated explanations for movie recommendations\naffect users' perceptions of cognitive, affective, and utilitarian needs and\nconsumption intentions. In a pre-registered, between-subject online experiment\n(N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic\nexplanations, and (b) LLM-generated contextualized explanations. Our findings\nshow that contextualized explanations (i.e., explanations that incorporate\nusers' past behaviors) effectively meet users' cognitive needs while increasing\nusers' intentions to watch recommended movies. However, adding explanations\noffers limited benefits in meeting users' utilitarian and affective needs,\nraising concerns about the proper design and implications of LLM-generated\nexplanations. Qualitative insights from interviews reveal that referencing\nusers' past preferences enhances trust and understanding but can feel excessive\nif overused. Furthermore, users with more active and positive engagement with\nthe recommender system and movie-watching get substantial gains from\ncontextualized explanations. Overall, our research clarifies how LLM-generated\nrecommendations influence users' motivations and behaviors, providing valuable\ninsights for the future development of user-centric recommender systems, a key\nelement in social media platforms and online ecosystems."
                },
                "authors": [
                    {
                        "name": "Yuanjun Feng"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Yash Raj Shrestha"
                    }
                ],
                "author_detail": {
                    "name": "Yash Raj Shrestha"
                },
                "author": "Yash Raj Shrestha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12147v1",
                "updated": "2025-01-21T14:00:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    0,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T14:00:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    14,
                    0,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Influence-based Instruction Tuning Data Selection for Balanced\n  Learning of Diverse Capabilities"
                },
                "summary": "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selecting appropriate training data is crucial for effective instruction\nfine-tuning of large language models (LLMs), which aims to (1) elicit strong\ncapabilities, and (2) achieve balanced performance across a diverse range of\ntasks. Influence-based methods show promise in achieving (1) by estimating the\ncontribution of each training example to the model's predictions, but often\nstruggle with (2). Our systematic investigation reveals that this\nunderperformance can be attributed to an inherent bias where certain tasks\nintrinsically have greater influence than others. As a result, data selection\nis often biased towards these tasks, not only hurting the model's performance\non others but also, counterintuitively, harms performance on these\nhigh-influence tasks themselves.\n  As a remedy, we propose BIDS, a Balanced and Influential Data Selection\nalgorithm. BIDS first normalizes influence scores of the training data, and\nthen iteratively balances data selection by choosing the training example with\nthe highest influence on the most underrepresented task. Experiments with both\nLlama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities\nshow that BIDS consistently outperforms both state-of-the-art influence-based\nalgorithms and other non-influence-based selection frameworks. Surprisingly,\ntraining on a 15% subset selected by BIDS can even outperform full-dataset\ntraining with a much more balanced performance. Our analysis further highlights\nthe importance of both instance-level normalization and iterative optimization\nof selected data for balanced learning of diverse capabilities."
                },
                "authors": [
                    {
                        "name": "Qirun Dai"
                    },
                    {
                        "name": "Dylan Zhang"
                    },
                    {
                        "name": "Jiaqi W. Ma"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12134v1",
                "updated": "2025-01-21T13:47:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    47,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T13:47:22Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    47,
                    22,
                    1,
                    21,
                    0
                ],
                "title": "Do LLMs Provide Links to Code Similar to what they Generate? A Study\n  with Gemini and Bing CoPilot",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Provide Links to Code Similar to what they Generate? A Study\n  with Gemini and Bing CoPilot"
                },
                "summary": "Large Language Models (LLMs) are currently used for various software\ndevelopment tasks, including generating code snippets to solve specific\nproblems. Unlike reuse from the Web, LLMs are limited in providing provenance\ninformation about the generated code, which may have important trustworthiness\nand legal consequences. While LLM-based assistants may provide external links\nthat are \"related\" to the generated code, we do not know how relevant such\nlinks are. This paper presents the findings of an empirical study assessing the\nextent to which 243 and 194 code snippets, across six programming languages,\ngenerated by Bing CoPilot and Google Gemini, likely originate from the links\nprovided by these two LLM-based assistants. The study leverages automated code\nsimilarity assessments with thorough manual analysis. The study's findings\nindicate that the LLM-based assistants provide a mix of relevant and irrelevant\nlinks having a different nature. Specifically, although 66% of the links from\nBing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants\nstill suffer from serious \"provenance debt\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are currently used for various software\ndevelopment tasks, including generating code snippets to solve specific\nproblems. Unlike reuse from the Web, LLMs are limited in providing provenance\ninformation about the generated code, which may have important trustworthiness\nand legal consequences. While LLM-based assistants may provide external links\nthat are \"related\" to the generated code, we do not know how relevant such\nlinks are. This paper presents the findings of an empirical study assessing the\nextent to which 243 and 194 code snippets, across six programming languages,\ngenerated by Bing CoPilot and Google Gemini, likely originate from the links\nprovided by these two LLM-based assistants. The study leverages automated code\nsimilarity assessments with thorough manual analysis. The study's findings\nindicate that the LLM-based assistants provide a mix of relevant and irrelevant\nlinks having a different nature. Specifically, although 66% of the links from\nBing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants\nstill suffer from serious \"provenance debt\"."
                },
                "authors": [
                    {
                        "name": "Daniele Bifolco"
                    },
                    {
                        "name": "Pietro Cassieri"
                    },
                    {
                        "name": "Giuseppe Scanniello"
                    },
                    {
                        "name": "Massimiliano Di Penta"
                    },
                    {
                        "name": "Fiorella Zampetti"
                    }
                ],
                "author_detail": {
                    "name": "Fiorella Zampetti"
                },
                "author": "Fiorella Zampetti",
                "arxiv_journal_ref": "Proceedings of the 22nd ACM/IEEE International Conference on\n  Mining Software Repositories (MSR 2025), April 28-29 2025, Ottawa, ON, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12128v1",
                "updated": "2025-01-21T13:42:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    42,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T13:42:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    42,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced\n  Human-Robot Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced\n  Human-Robot Interactions"
                },
                "summary": "To achieve natural and intuitive interaction with people, HRI frameworks\ncombine a wide array of methods for human perception, intention communication,\nhuman-aware navigation and collaborative action. In practice, when encountering\nunpredictable behavior of people or unexpected states of the environment, these\nframeworks may lack the ability to dynamically recognize such states, adapt and\nrecover to resume the interaction. Large Language Models (LLMs), owing to their\nadvanced reasoning capabilities and context retention, present a promising\nsolution for enhancing robot adaptability. This potential, however, may not\ndirectly translate to improved interaction metrics. This paper considers a\nrepresentative interaction with an industrial robot involving approach,\ninstruction, and object manipulation, implemented in two conditions: (1) fully\nscripted and (2) including LLM-enhanced responses. We use gaze tracking and\nquestionnaires to measure the participants' task efficiency, engagement, and\nrobot perception. The results indicate higher subjective ratings for the LLM\ncondition, but objective metrics show that the scripted condition performs\ncomparably, particularly in efficiency and focus during simple tasks. We also\nnote that the scripted condition may have an edge over LLM-enhanced responses\nin terms of response latency and energy consumption, especially for trivial and\nrepetitive interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve natural and intuitive interaction with people, HRI frameworks\ncombine a wide array of methods for human perception, intention communication,\nhuman-aware navigation and collaborative action. In practice, when encountering\nunpredictable behavior of people or unexpected states of the environment, these\nframeworks may lack the ability to dynamically recognize such states, adapt and\nrecover to resume the interaction. Large Language Models (LLMs), owing to their\nadvanced reasoning capabilities and context retention, present a promising\nsolution for enhancing robot adaptability. This potential, however, may not\ndirectly translate to improved interaction metrics. This paper considers a\nrepresentative interaction with an industrial robot involving approach,\ninstruction, and object manipulation, implemented in two conditions: (1) fully\nscripted and (2) including LLM-enhanced responses. We use gaze tracking and\nquestionnaires to measure the participants' task efficiency, engagement, and\nrobot perception. The results indicate higher subjective ratings for the LLM\ncondition, but objective metrics show that the scripted condition performs\ncomparably, particularly in efficiency and focus during simple tasks. We also\nnote that the scripted condition may have an edge over LLM-enhanced responses\nin terms of response latency and energy consumption, especially for trivial and\nrepetitive interactions."
                },
                "authors": [
                    {
                        "name": "Tim Schreiter"
                    },
                    {
                        "name": "Jens V. Rüppel"
                    },
                    {
                        "name": "Rishi Hazra"
                    },
                    {
                        "name": "Andrey Rudenko"
                    },
                    {
                        "name": "Martin Magnusson"
                    },
                    {
                        "name": "Achim J. Lilienthal"
                    }
                ],
                "author_detail": {
                    "name": "Achim J. Lilienthal"
                },
                "author": "Achim J. Lilienthal",
                "arxiv_comment": "Accepted as a Late-Breaking Report to the 2025, 20th ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13340v2",
                "updated": "2025-01-21T13:07:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    13,
                    7,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2024-07-18T09:37:05Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    37,
                    5,
                    3,
                    200,
                    0
                ],
                "title": "TwinRAN: Twinning the 5G RAN in Azure Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TwinRAN: Twinning the 5G RAN in Azure Cloud"
                },
                "summary": "The proliferation of 5G technology necessitates advanced network management\nstrategies to ensure optimal performance and reliability. Digital Twin (DT)s\nhave emerged as a promising paradigm for modeling and simulating complex\nsystems like the 5G Radio Access Network (RAN). In this paper, we present\nTwinRAN, a DT of the 5G RAN built leveraging the Azure DT platform. TwinRAN is\nbuilt on top of the Open RAN (O-RAN) architecture and is agnostic to the vendor\nof the underlying equipment. We demonstrate three applications using TwinRAN\nand evaluate the required resources and their performance for a network with\n800 users and eight gNBs. We first evaluate the performance and limitations of\nthe Azure DT platform, measuring the latency under different conditions. The\nresults from this evaluation allow us to optimize TwinRAN for the DT platform\nit uses. Then, we present the system's architectural design, emphasizing its\ncomponents and interactions. We propose that two types of twin graphs be\nsimultaneously maintained on the cloud: one for intercell operations, keeping a\nbroad overview of all the cells in the network, and another where each cell is\nspawned in a separate Azure DT instance for more granular operation and\nmonitoring of intracell tasks. We evaluate the performance and operating costs\nof TwinRAN for each of the three applications. The TwinRAN DT in the cloud can\nkeep track of its physical twin within a few hundred milliseconds, extending\nits utility to many 5G network management tasks, some of which are shown in\nthis paper. The novel framework for building and maintaining a DT of the 5G RAN\npresented in this paper offers network operators enhanced capabilities,\nempowering efficient deployments and management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of 5G technology necessitates advanced network management\nstrategies to ensure optimal performance and reliability. Digital Twin (DT)s\nhave emerged as a promising paradigm for modeling and simulating complex\nsystems like the 5G Radio Access Network (RAN). In this paper, we present\nTwinRAN, a DT of the 5G RAN built leveraging the Azure DT platform. TwinRAN is\nbuilt on top of the Open RAN (O-RAN) architecture and is agnostic to the vendor\nof the underlying equipment. We demonstrate three applications using TwinRAN\nand evaluate the required resources and their performance for a network with\n800 users and eight gNBs. We first evaluate the performance and limitations of\nthe Azure DT platform, measuring the latency under different conditions. The\nresults from this evaluation allow us to optimize TwinRAN for the DT platform\nit uses. Then, we present the system's architectural design, emphasizing its\ncomponents and interactions. We propose that two types of twin graphs be\nsimultaneously maintained on the cloud: one for intercell operations, keeping a\nbroad overview of all the cells in the network, and another where each cell is\nspawned in a separate Azure DT instance for more granular operation and\nmonitoring of intracell tasks. We evaluate the performance and operating costs\nof TwinRAN for each of the three applications. The TwinRAN DT in the cloud can\nkeep track of its physical twin within a few hundred milliseconds, extending\nits utility to many 5G network management tasks, some of which are shown in\nthis paper. The novel framework for building and maintaining a DT of the 5G RAN\npresented in this paper offers network operators enhanced capabilities,\nempowering efficient deployments and management."
                },
                "authors": [
                    {
                        "name": "Yash Deshpande"
                    },
                    {
                        "name": "Eni Sulkaj"
                    },
                    {
                        "name": "Wolfgang Kellerer"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kellerer"
                },
                "author": "Wolfgang Kellerer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v1",
                "updated": "2025-01-21T12:56:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "48 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12067v1",
                "updated": "2025-01-21T11:42:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    42,
                    9,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:42:09Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    42,
                    9,
                    1,
                    21,
                    0
                ],
                "title": "EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular\n  Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular\n  Value Decomposition"
                },
                "summary": "Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of\ntrainable parameters. However, they often suffer from scalability issues and\ndifferences between their learning pattern and full fine-tuning. To overcome\nthese limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation\n(EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude\nand directional components. By freezing low-rank matrices, initializing them by\nsingular value decomposition, and introducing a small trainable matrix between\nthem, EDoRA achieves substantial reduction in trainable parameters while\nmaintaining learning capacity. Experimental results on the GLUE benchmark\ndemonstrate that EDoRA achieves competitive or superior performance compared to\nstate-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable\nparameters. This makes EDoRA a highly efficient solution for adapting LLMs to\ndiverse tasks under memory-constrained settings. Code is available at\nhttps://github.com/Hamid-Nasiri/EDoRA .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of\ntrainable parameters. However, they often suffer from scalability issues and\ndifferences between their learning pattern and full fine-tuning. To overcome\nthese limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation\n(EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude\nand directional components. By freezing low-rank matrices, initializing them by\nsingular value decomposition, and introducing a small trainable matrix between\nthem, EDoRA achieves substantial reduction in trainable parameters while\nmaintaining learning capacity. Experimental results on the GLUE benchmark\ndemonstrate that EDoRA achieves competitive or superior performance compared to\nstate-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable\nparameters. This makes EDoRA a highly efficient solution for adapting LLMs to\ndiverse tasks under memory-constrained settings. Code is available at\nhttps://github.com/Hamid-Nasiri/EDoRA ."
                },
                "authors": [
                    {
                        "name": "Hamid Nasiri"
                    },
                    {
                        "name": "Peter Garraghan"
                    }
                ],
                "author_detail": {
                    "name": "Peter Garraghan"
                },
                "author": "Peter Garraghan",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v7",
                "updated": "2025-01-21T11:36:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    36,
                    3,
                    1,
                    21,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12053v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12053v1",
                "updated": "2025-01-21T11:26:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    26,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:26:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    26,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "PINNsAgent: Automated PDE Surrogation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINNsAgent: Automated PDE Surrogation with Large Language Models"
                },
                "summary": "Solving partial differential equations (PDEs) using neural methods has been a\nlong-standing scientific and engineering research pursuit. Physics-Informed\nNeural Networks (PINNs) have emerged as a promising alternative to traditional\nnumerical methods for solving PDEs. However, the gap between domain-specific\nknowledge and deep learning expertise often limits the practical application of\nPINNs. Previous works typically involve manually conducting extensive PINNs\nexperiments and summarizing heuristic rules for hyperparameter tuning. In this\nwork, we introduce PINNsAgent, a novel surrogation framework that leverages\nlarge language models (LLMs) and utilizes PINNs as a foundation to bridge the\ngap between domain-specific knowledge and deep learning. Specifically,\nPINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes\nthe essential characteristics of PDEs and their associated best-performing\nPINNs configurations into a structured format, enabling efficient knowledge\ntransfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a\nstrategy that effectively explores the search space for optimal PINNs\narchitectures. By leveraging LLMs and exploration strategies, PINNsAgent\nenhances the automation and efficiency of PINNs-based solutions. We evaluate\nPINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating\nthe surrogation process and significantly improving the accuracy of PINNs-based\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving partial differential equations (PDEs) using neural methods has been a\nlong-standing scientific and engineering research pursuit. Physics-Informed\nNeural Networks (PINNs) have emerged as a promising alternative to traditional\nnumerical methods for solving PDEs. However, the gap between domain-specific\nknowledge and deep learning expertise often limits the practical application of\nPINNs. Previous works typically involve manually conducting extensive PINNs\nexperiments and summarizing heuristic rules for hyperparameter tuning. In this\nwork, we introduce PINNsAgent, a novel surrogation framework that leverages\nlarge language models (LLMs) and utilizes PINNs as a foundation to bridge the\ngap between domain-specific knowledge and deep learning. Specifically,\nPINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes\nthe essential characteristics of PDEs and their associated best-performing\nPINNs configurations into a structured format, enabling efficient knowledge\ntransfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a\nstrategy that effectively explores the search space for optimal PINNs\narchitectures. By leveraging LLMs and exploration strategies, PINNsAgent\nenhances the automation and efficiency of PINNs-based solutions. We evaluate\nPINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating\nthe surrogation process and significantly improving the accuracy of PINNs-based\nsolutions."
                },
                "authors": [
                    {
                        "name": "Qingpo Wuwu"
                    },
                    {
                        "name": "Chonghan Gao"
                    },
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Yihang Huang"
                    },
                    {
                        "name": "Yuekai Zhang"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Jianxin Li"
                    },
                    {
                        "name": "Haoyi Zhou"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "9 pages, 3 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12053v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12053v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12051v1",
                "updated": "2025-01-21T11:24:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    24,
                    55,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:24:55Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    24,
                    55,
                    1,
                    21,
                    0
                ],
                "title": "MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow\n  Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow\n  Thinking"
                },
                "summary": "Medical language models (MLMs) have become pivotal in advancing medical\nnatural language processing. However, prior models that rely on pre-training or\nsupervised fine-tuning often exhibit low data efficiency and limited\npracticality in real-world clinical applications. While OpenAIs O1 highlights\ntest-time scaling in mathematics, attempts to replicate this approach in\nmedicine typically distill responses from GPT-series models to open-source\nmodels, focusing primarily on multiple-choice tasks. This strategy, though\nstraightforward, neglects critical concerns like data privacy and realistic\ndeployment in clinical settings. In this work, we present a deployable,\nsmall-scale medical language model, \\mone, designed for long-chain reasoning in\nclinical tasks using a self-evolution paradigm. Starting with a seed dataset of\naround 8,000 instances spanning five domains and 16 datasets, we prompt a base\npolicy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable\nreasoning chains. Each reasoning step is assigned an evolution rollout value,\nallowing verified trajectories to train the policy model and the reward model.\nDuring inference, the policy model generates multiple responses, and the reward\nmodel selects the one with the highest reward score. Experiments on eleven\nevaluation datasets demonstrate that \\mone outperforms prior open-source models\nby 2 points, with the addition of the reward model further boosting performance\n($\\sim$13 points), surpassing GPT-4o-mini. Code and data are available at\n\\url{https://github.com/pixas/MedSSS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical language models (MLMs) have become pivotal in advancing medical\nnatural language processing. However, prior models that rely on pre-training or\nsupervised fine-tuning often exhibit low data efficiency and limited\npracticality in real-world clinical applications. While OpenAIs O1 highlights\ntest-time scaling in mathematics, attempts to replicate this approach in\nmedicine typically distill responses from GPT-series models to open-source\nmodels, focusing primarily on multiple-choice tasks. This strategy, though\nstraightforward, neglects critical concerns like data privacy and realistic\ndeployment in clinical settings. In this work, we present a deployable,\nsmall-scale medical language model, \\mone, designed for long-chain reasoning in\nclinical tasks using a self-evolution paradigm. Starting with a seed dataset of\naround 8,000 instances spanning five domains and 16 datasets, we prompt a base\npolicy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable\nreasoning chains. Each reasoning step is assigned an evolution rollout value,\nallowing verified trajectories to train the policy model and the reward model.\nDuring inference, the policy model generates multiple responses, and the reward\nmodel selects the one with the highest reward score. Experiments on eleven\nevaluation datasets demonstrate that \\mone outperforms prior open-source models\nby 2 points, with the addition of the reward model further boosting performance\n($\\sim$13 points), surpassing GPT-4o-mini. Code and data are available at\n\\url{https://github.com/pixas/MedSSS}."
                },
                "authors": [
                    {
                        "name": "Shuyang Jiang"
                    },
                    {
                        "name": "Yusheng Liao"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "19 pages; technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12043v1",
                "updated": "2025-01-21T11:03:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    3,
                    59,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T11:03:59Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    11,
                    3,
                    59,
                    1,
                    21,
                    0
                ],
                "title": "High-Fidelity Coherent-One-Way QKD Simulation Framework for 6G Networks:\n  Bridging Theory and Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Fidelity Coherent-One-Way QKD Simulation Framework for 6G Networks:\n  Bridging Theory and Reality"
                },
                "summary": "Quantum key distribution (QKD) has been emerged as a promising solution for\nguaranteeing information-theoretic security. Inspired by this, a great amount\nof research effort has been recently put on designing and testing QKD systems\nas well as articulating preliminary application scenarios. However, due to the\nconsiderable high-cost of QKD equipment, a lack of QKD communication system\ndesign tools, wide deployment of such systems and networks is challenging.\nMotivated by this, this paper introduces a QKD communication system design\ntool. First we articulate key operation elements of the QKD, and explain the\nfeasibility and applicability of coherent-one-way (COW) QKD solutions. Next, we\nfocus on documenting the corresponding simulation framework as well as defining\nthe key performance metrics, i.e., quantum bit error rate (QBER), and secrecy\nkey rate. To verify the accuracy of the simulation framework, we design and\ndeploy a real-world QKD setup. We perform extensive experiments for three\ndeployments of diverse transmission distance in the presence or absence of a\nQKD eavesdropper. The results reveal an acceptable match between simulations\nand experiments rendering the simulation framework a suitable tool for QKD\ncommunication system design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum key distribution (QKD) has been emerged as a promising solution for\nguaranteeing information-theoretic security. Inspired by this, a great amount\nof research effort has been recently put on designing and testing QKD systems\nas well as articulating preliminary application scenarios. However, due to the\nconsiderable high-cost of QKD equipment, a lack of QKD communication system\ndesign tools, wide deployment of such systems and networks is challenging.\nMotivated by this, this paper introduces a QKD communication system design\ntool. First we articulate key operation elements of the QKD, and explain the\nfeasibility and applicability of coherent-one-way (COW) QKD solutions. Next, we\nfocus on documenting the corresponding simulation framework as well as defining\nthe key performance metrics, i.e., quantum bit error rate (QBER), and secrecy\nkey rate. To verify the accuracy of the simulation framework, we design and\ndeploy a real-world QKD setup. We perform extensive experiments for three\ndeployments of diverse transmission distance in the presence or absence of a\nQKD eavesdropper. The results reveal an acceptable match between simulations\nand experiments rendering the simulation framework a suitable tool for QKD\ncommunication system design."
                },
                "authors": [
                    {
                        "name": "Aitor Brazaola-Vicario"
                    },
                    {
                        "name": "Vasileios Kouvakis"
                    },
                    {
                        "name": "Stylianos E. Trevlakis"
                    },
                    {
                        "name": "Alejandra Ruiz"
                    },
                    {
                        "name": "Alexandros-Apostolos A. Boulogeorgos"
                    },
                    {
                        "name": "Theodoros Tsiftsis"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12032v1",
                "updated": "2025-01-21T10:53:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    53,
                    17,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T10:53:17Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    53,
                    17,
                    1,
                    21,
                    0
                ],
                "title": "In-Network Preprocessing of Recommender Systems on Multi-Tenant\n  SmartNICs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Network Preprocessing of Recommender Systems on Multi-Tenant\n  SmartNICs"
                },
                "summary": "Keeping ML-based recommender models up-to-date as data drifts and evolves is\nessential to maintain accuracy. As a result, online data preprocessing plays an\nincreasingly important role in serving recommender systems. Existing solutions\nemploy multiple CPU workers to saturate the input bandwidth of a single\ntraining node. Such an approach results in high deployment costs and energy\nconsumption. For instance, a recent report from industrial deployments shows\nthat data storage and ingestion pipelines can account for over 60\\% of the\npower consumption in a recommender system. In this paper, we tackle the issue\nfrom a hardware perspective by introducing Piper, a flexible and\nnetwork-attached accelerator that executes data loading and preprocessing\npipelines in a streaming fashion. As part of the design, we define MiniPipe,\nthe smallest pipeline unit enabling multi-pipeline implementation by executing\nvarious data preprocessing tasks across the single board, giving Piper the\nability to be reconfigured at runtime. Our results, using publicly released\ncommercial pipelines, show that Piper, prototyped on a power-efficient FPGA,\nachieves a 39$\\sim$105$\\times$ speedup over a server-grade, 128-core CPU and\n3$\\sim$17$\\times$ speedup over GPUs like RTX 3090 and A100 in multiple\npipelines. The experimental analysis demonstrates that Piper provides\nadvantages in both latency and energy efficiency for preprocessing tasks in\nrecommender systems, providing an alternative design point for systems that\ntoday are in very high demand.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keeping ML-based recommender models up-to-date as data drifts and evolves is\nessential to maintain accuracy. As a result, online data preprocessing plays an\nincreasingly important role in serving recommender systems. Existing solutions\nemploy multiple CPU workers to saturate the input bandwidth of a single\ntraining node. Such an approach results in high deployment costs and energy\nconsumption. For instance, a recent report from industrial deployments shows\nthat data storage and ingestion pipelines can account for over 60\\% of the\npower consumption in a recommender system. In this paper, we tackle the issue\nfrom a hardware perspective by introducing Piper, a flexible and\nnetwork-attached accelerator that executes data loading and preprocessing\npipelines in a streaming fashion. As part of the design, we define MiniPipe,\nthe smallest pipeline unit enabling multi-pipeline implementation by executing\nvarious data preprocessing tasks across the single board, giving Piper the\nability to be reconfigured at runtime. Our results, using publicly released\ncommercial pipelines, show that Piper, prototyped on a power-efficient FPGA,\nachieves a 39$\\sim$105$\\times$ speedup over a server-grade, 128-core CPU and\n3$\\sim$17$\\times$ speedup over GPUs like RTX 3090 and A100 in multiple\npipelines. The experimental analysis demonstrates that Piper provides\nadvantages in both latency and energy efficiency for preprocessing tasks in\nrecommender systems, providing an alternative design point for systems that\ntoday are in very high demand."
                },
                "authors": [
                    {
                        "name": "Yu Zhu"
                    },
                    {
                        "name": "Wenqi Jiang"
                    },
                    {
                        "name": "Gustavo Alonso"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Alonso"
                },
                "author": "Gustavo Alonso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06066v2",
                "updated": "2025-01-21T10:48:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    10,
                    48,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-10T15:57:23Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    15,
                    57,
                    23,
                    4,
                    10,
                    0
                ],
                "title": "Distilling Calibration via Conformalized Credal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Calibration via Conformalized Credal Inference"
                },
                "summary": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying artificial intelligence (AI) models on edge devices involves a\ndelicate balance between meeting stringent complexity constraints, such as\nlimited memory and energy resources, and ensuring reliable performance in\nsensitive decision-making tasks. One way to enhance reliability is through\nuncertainty quantification via Bayesian inference. This approach, however,\ntypically necessitates maintaining and running multiple models in an ensemble,\nwhich may exceed the computational limits of edge devices. This paper\nintroduces a low-complexity methodology to address this challenge by distilling\ncalibration information from a more complex model. In an offline phase,\npredictive probabilities generated by a high-complexity cloud-based model are\nleveraged to determine a threshold based on the typical divergence between the\ncloud and edge models. At run time, this threshold is used to construct credal\nsets -- ranges of predictive probabilities that are guaranteed, with a\nuser-selected confidence level, to include the predictions of the cloud model.\nThe credal sets are obtained through thresholding of a divergence measure in\nthe simplex of predictive probabilities. Experiments on visual and language\ntasks demonstrate that the proposed approach, termed Conformalized Distillation\nfor Credal Inference (CD-CI), significantly improves calibration performance\ncompared to low-complexity Bayesian methods, such as Laplace approximation,\nmaking it a practical and efficient solution for edge AI deployments."
                },
                "authors": [
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Nicola Paoletti"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20135v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20135v5",
                "updated": "2025-01-21T09:25:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    9,
                    25,
                    25,
                    1,
                    21,
                    0
                ],
                "published": "2024-09-30T09:34:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    34,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Instruction Tuning of LLMs with Domain Coverage Augmentation"
                },
                "summary": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with various strategies of instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. At its core, the greedy selection procedure iteratively picks\nclient centers that maximize the diversity and coverage of the instruction\nspace while avoiding redundancy with previously selected centers. This ensures\nbroad yet efficient coverage of the domain distribution across clients. For\nclient-side computational efficiency and system scalability, FedDCA$^*$, the\nvariant of FedDCA, utilizes heterogeneous encoders with server-side feature\nalignment. Extensive experiments across code, medical, financial, and\nmathematical domains substantiate the effectiveness of both methods, as well as\nplug-and-play capability. We further analyze privacy preservation against\nmemory extraction attacks, showing that while privacy leakage risk is\nindependent of augmented public data ratio, it decreases or converges as\ntraining progresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited\ncross-client private data together with various strategies of instruction\naugmentation, ultimately boosting model performance within specific domains. To\ndate, the factors affecting FedDIT remain unclear, and existing instruction\naugmentation methods primarily focus on the centralized setting without\nconsidering distributed environments. Our experiments reveal that the\ncross-client domain coverage, rather than data heterogeneity, drives model\nperformance in FedDIT. In response, we propose FedDCA, which optimizes domain\ncoverage through greedy client center selection and retrieval-based\naugmentation. At its core, the greedy selection procedure iteratively picks\nclient centers that maximize the diversity and coverage of the instruction\nspace while avoiding redundancy with previously selected centers. This ensures\nbroad yet efficient coverage of the domain distribution across clients. For\nclient-side computational efficiency and system scalability, FedDCA$^*$, the\nvariant of FedDCA, utilizes heterogeneous encoders with server-side feature\nalignment. Extensive experiments across code, medical, financial, and\nmathematical domains substantiate the effectiveness of both methods, as well as\nplug-and-play capability. We further analyze privacy preservation against\nmemory extraction attacks, showing that while privacy leakage risk is\nindependent of augmented public data ratio, it decreases or converges as\ntraining progresses."
                },
                "authors": [
                    {
                        "name": "Zezhou Wang"
                    },
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yugang Jiang"
                    },
                    {
                        "name": "Zhuzhong Qian"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20135v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20135v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11979v1",
                "updated": "2025-01-21T08:52:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    52,
                    47,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:52:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    52,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Linear Feedback Control Systems for Iterative Prompt Optimization in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Feedback Control Systems for Iterative Prompt Optimization in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized various applications by\ngenerating outputs based on given prompts. However, achieving the desired\noutput requires iterative prompt refinement. This paper presents a novel\napproach that draws parallels between the iterative prompt optimization process\nin LLMs and feedback control systems. We iteratively refine the prompt by\ntreating the deviation between the LLM output and the desired result as an\nerror term until the output criteria are met. This process is akin to a\nfeedback control system, where the LLM, despite being non-linear and\nnon-deterministic, is managed using principles from linear feedback control\nsystems. We explore the application of different types of controllers within\nthis framework, providing a mathematical foundation for integrating linear\nfeedback control mechanisms with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized various applications by\ngenerating outputs based on given prompts. However, achieving the desired\noutput requires iterative prompt refinement. This paper presents a novel\napproach that draws parallels between the iterative prompt optimization process\nin LLMs and feedback control systems. We iteratively refine the prompt by\ntreating the deviation between the LLM output and the desired result as an\nerror term until the output criteria are met. This process is akin to a\nfeedback control system, where the LLM, despite being non-linear and\nnon-deterministic, is managed using principles from linear feedback control\nsystems. We explore the application of different types of controllers within\nthis framework, providing a mathematical foundation for integrating linear\nfeedback control mechanisms with LLMs."
                },
                "authors": [
                    {
                        "name": "Rupesh Raj Karn"
                    }
                ],
                "author_detail": {
                    "name": "Rupesh Raj Karn"
                },
                "author": "Rupesh Raj Karn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11977v1",
                "updated": "2025-01-21T08:51:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    51,
                    12,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:51:12Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    51,
                    12,
                    1,
                    21,
                    0
                ],
                "title": "Leveraging Graph Structures and Large Language Models for End-to-End\n  Synthetic Task-Oriented Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Graph Structures and Large Language Models for End-to-End\n  Synthetic Task-Oriented Dialogues"
                },
                "summary": "Training task-oriented dialogue systems is both costly and time-consuming,\ndue to the need for high-quality datasets encompassing diverse intents.\nTraditional methods depend on extensive human annotation, while recent\nadvancements leverage large language models (LLMs) to generate synthetic data.\nHowever, these approaches often require custom prompts or code, limiting\naccessibility for non-technical users. We introduce GraphTOD, an end-to-end\nframework that simplifies the generation of task-oriented dialogues. Users can\ncreate dialogues by specifying transition graphs in JSON format. Our evaluation\ndemonstrates that GraphTOD generates high-quality dialogues across various\ndomains, significantly lowering the cost and complexity of dataset creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training task-oriented dialogue systems is both costly and time-consuming,\ndue to the need for high-quality datasets encompassing diverse intents.\nTraditional methods depend on extensive human annotation, while recent\nadvancements leverage large language models (LLMs) to generate synthetic data.\nHowever, these approaches often require custom prompts or code, limiting\naccessibility for non-technical users. We introduce GraphTOD, an end-to-end\nframework that simplifies the generation of task-oriented dialogues. Users can\ncreate dialogues by specifying transition graphs in JSON format. Our evaluation\ndemonstrates that GraphTOD generates high-quality dialogues across various\ndomains, significantly lowering the cost and complexity of dataset creation."
                },
                "authors": [
                    {
                        "name": "Maya Medjad"
                    },
                    {
                        "name": "Hugo Imbert"
                    },
                    {
                        "name": "Bruno Yun"
                    },
                    {
                        "name": "Raphaël Szymocha"
                    },
                    {
                        "name": "Frédéric Armetta"
                    }
                ],
                "author_detail": {
                    "name": "Frédéric Armetta"
                },
                "author": "Frédéric Armetta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20954v2",
                "updated": "2025-01-21T08:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    38,
                    1,
                    1,
                    21,
                    0
                ],
                "published": "2024-12-30T13:50:20Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "title": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents"
                },
                "summary": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort."
                },
                "authors": [
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Tianyun Ma"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Guanglin Xu"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xiaqing Li"
                    },
                    {
                        "name": "Yuanbo Wen"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11967v1",
                "updated": "2025-01-21T08:26:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    26,
                    20,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:26:20Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    26,
                    20,
                    1,
                    21,
                    0
                ],
                "title": "A Hybrid Attention Framework for Fake News Detection with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Attention Framework for Fake News Detection with Large Language\n  Models"
                },
                "summary": "With the rapid growth of online information, the spread of fake news has\nbecome a serious social challenge. In this study, we propose a novel detection\nframework based on Large Language Models (LLMs) to identify and classify fake\nnews by integrating textual statistical features and deep semantic features.\nOur approach utilizes the contextual understanding capability of the large\nlanguage model for text analysis and introduces a hybrid attention mechanism to\nfocus on feature combinations that are particularly important for fake news\nidentification. Extensive experiments on the WELFake news dataset show that our\nmodel significantly outperforms existing methods, with a 1.5\\% improvement in\nF1 score. In addition, we assess the interpretability of the model through\nattention heat maps and SHAP values, providing actionable insights for content\nreview strategies. Our framework provides a scalable and efficient solution to\ndeal with the spread of fake news and helps build a more reliable online\ninformation ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of online information, the spread of fake news has\nbecome a serious social challenge. In this study, we propose a novel detection\nframework based on Large Language Models (LLMs) to identify and classify fake\nnews by integrating textual statistical features and deep semantic features.\nOur approach utilizes the contextual understanding capability of the large\nlanguage model for text analysis and introduces a hybrid attention mechanism to\nfocus on feature combinations that are particularly important for fake news\nidentification. Extensive experiments on the WELFake news dataset show that our\nmodel significantly outperforms existing methods, with a 1.5\\% improvement in\nF1 score. In addition, we assess the interpretability of the model through\nattention heat maps and SHAP values, providing actionable insights for content\nreview strategies. Our framework provides a scalable and efficient solution to\ndeal with the spread of fake news and helps build a more reliable online\ninformation ecosystem."
                },
                "authors": [
                    {
                        "name": "Xiaochuan Xu"
                    },
                    {
                        "name": "Peiyang Yu"
                    },
                    {
                        "name": "Zeqiu Xu"
                    },
                    {
                        "name": "Jiani Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiani Wang"
                },
                "author": "Jiani Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02044v2",
                "updated": "2025-01-21T08:17:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    17,
                    27,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-04T07:27:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    27,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "QROA: A Black-Box Query-Response Optimization Attack on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QROA: A Black-Box Query-Response Optimization Attack on LLMs"
                },
                "summary": "Large Language Models (LLMs) have surged in popularity in recent months, yet\nthey possess concerning capabilities for generating harmful content when\nmanipulated. This study introduces the Query-Response Optimization Attack\n(QROA), an optimization-based strategy designed to exploit LLMs through a\nblack-box, query-only interaction. QROA adds an optimized trigger to a\nmalicious instruction to compel the LLM to generate harmful content. Unlike\nprevious approaches, QROA does not require access to the model's logit\ninformation or any other internal data and operates solely through the standard\nquery-response interface of LLMs. Inspired by deep Q-learning and Greedy\ncoordinate descent, the method iteratively updates tokens to maximize a\ndesigned reward function. We tested our method on various LLMs such as Vicuna,\nFalcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%. We also\ntested the model against Llama2-chat, the fine-tuned version of Llama2 designed\nto resist Jailbreak attacks, achieving good ASR with a suboptimal initial\ntrigger seed. This study demonstrates the feasibility of generating jailbreak\nattacks against deployed LLMs in the public domain using black-box optimization\nmethods, enabling more comprehensive safety testing of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have surged in popularity in recent months, yet\nthey possess concerning capabilities for generating harmful content when\nmanipulated. This study introduces the Query-Response Optimization Attack\n(QROA), an optimization-based strategy designed to exploit LLMs through a\nblack-box, query-only interaction. QROA adds an optimized trigger to a\nmalicious instruction to compel the LLM to generate harmful content. Unlike\nprevious approaches, QROA does not require access to the model's logit\ninformation or any other internal data and operates solely through the standard\nquery-response interface of LLMs. Inspired by deep Q-learning and Greedy\ncoordinate descent, the method iteratively updates tokens to maximize a\ndesigned reward function. We tested our method on various LLMs such as Vicuna,\nFalcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\\%. We also\ntested the model against Llama2-chat, the fine-tuned version of Llama2 designed\nto resist Jailbreak attacks, achieving good ASR with a suboptimal initial\ntrigger seed. This study demonstrates the feasibility of generating jailbreak\nattacks against deployed LLMs in the public domain using black-box optimization\nmethods, enabling more comprehensive safety testing of LLMs."
                },
                "authors": [
                    {
                        "name": "Hussein Jawad"
                    },
                    {
                        "name": "Nicolas J. -B. BRUNEL"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas J. -B. BRUNEL"
                },
                "arxiv_affiliation": "LaMME",
                "author": "Nicolas J. -B. BRUNEL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11955v1",
                "updated": "2025-01-21T08:02:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    2,
                    17,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T08:02:17Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    8,
                    2,
                    17,
                    1,
                    21,
                    0
                ],
                "title": "Simultaneously decoding the unknown stationary state and function\n  parameters for mean field games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneously decoding the unknown stationary state and function\n  parameters for mean field games"
                },
                "summary": "Mean field games (MFGs) offer a versatile framework for modeling large-scale\ninteractive systems across multiple domains. This paper builds upon a previous\nwork, by developing a state-of-the-art unified approach to decode or design the\nunknown stationary state of MFGs, in addition to the underlying parameter\nfunctions governing their behavior. This result is novel, even in the general\nrealm of inverse problems for nonlinear PDEs. By enabling agents to distill\ncrucial insights from observed data and unveil intricate hidden structures and\nunknown states within MFG systems, our approach surmounts a significant\nobstacle, enhancing the applicability of MFGs in real-world scenarios. This\nadvancement not only enriches our understanding of MFG dynamics but also\nbroadens the scope for their practical deployment in various contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean field games (MFGs) offer a versatile framework for modeling large-scale\ninteractive systems across multiple domains. This paper builds upon a previous\nwork, by developing a state-of-the-art unified approach to decode or design the\nunknown stationary state of MFGs, in addition to the underlying parameter\nfunctions governing their behavior. This result is novel, even in the general\nrealm of inverse problems for nonlinear PDEs. By enabling agents to distill\ncrucial insights from observed data and unveil intricate hidden structures and\nunknown states within MFG systems, our approach surmounts a significant\nobstacle, enhancing the applicability of MFGs in real-world scenarios. This\nadvancement not only enriches our understanding of MFG dynamics but also\nbroadens the scope for their practical deployment in various contexts."
                },
                "authors": [
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Catharine W. K. Lo"
                    }
                ],
                "author_detail": {
                    "name": "Catharine W. K. Lo"
                },
                "author": "Catharine W. K. Lo",
                "arxiv_comment": "Keywords: Mean field games, inverse problems, Cauchy dataset, unique\n  continuation principle, unique identifiability, unknown stationary solutions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 35Q89, 35R30, secondary 91A16, 35R35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11953v1",
                "updated": "2025-01-21T07:54:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    54,
                    22,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:54:22Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    54,
                    22,
                    1,
                    21,
                    0
                ],
                "title": "Proverbs Run in Pairs: Evaluating Proverb Translation Capability of\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proverbs Run in Pairs: Evaluating Proverb Translation Capability of\n  Large Language Model"
                },
                "summary": "Despite achieving remarkable performance, machine translation (MT) research\nremains underexplored in terms of translating cultural elements in languages,\nsuch as idioms, proverbs, and colloquial expressions. This paper investigates\nthe capability of state-of-the-art neural machine translation (NMT) and large\nlanguage models (LLMs) in translating proverbs, which are deeply rooted in\ncultural contexts. We construct a translation dataset of standalone proverbs\nand proverbs in conversation for four language pairs. Our experiments show that\nthe studied models can achieve good translation between languages with similar\ncultural backgrounds, and LLMs generally outperform NMT models in proverb\ntranslation. Furthermore, we find that current automatic evaluation metrics\nsuch as BLEU, CHRF++ and COMET are inadequate for reliably assessing the\nquality of proverb translation, highlighting the need for more culturally aware\nevaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite achieving remarkable performance, machine translation (MT) research\nremains underexplored in terms of translating cultural elements in languages,\nsuch as idioms, proverbs, and colloquial expressions. This paper investigates\nthe capability of state-of-the-art neural machine translation (NMT) and large\nlanguage models (LLMs) in translating proverbs, which are deeply rooted in\ncultural contexts. We construct a translation dataset of standalone proverbs\nand proverbs in conversation for four language pairs. Our experiments show that\nthe studied models can achieve good translation between languages with similar\ncultural backgrounds, and LLMs generally outperform NMT models in proverb\ntranslation. Furthermore, we find that current automatic evaluation metrics\nsuch as BLEU, CHRF++ and COMET are inadequate for reliably assessing the\nquality of proverb translation, highlighting the need for more culturally aware\nevaluation metrics."
                },
                "authors": [
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Viet-Thanh Pham"
                    },
                    {
                        "name": "Farhad Moghimifar"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    }
                ],
                "author_detail": {
                    "name": "Thuy-Trang Vu"
                },
                "author": "Thuy-Trang Vu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v3",
                "updated": "2025-01-21T07:34:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    34,
                    54,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference"
                },
                "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13849v2",
                "updated": "2025-01-21T07:28:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    28,
                    48,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-25T14:38:13Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    14,
                    38,
                    13,
                    6,
                    238,
                    0
                ],
                "title": "Sample-Independent Federated Learning Backdoor Attack in Speaker\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample-Independent Federated Learning Backdoor Attack in Speaker\n  Recognition"
                },
                "summary": "In federated learning, backdoor attacks embed triggers in the adversarial\nclient's data to inject a backdoor into the model. In order to enhance the\nstealth, an attack method based on the dropout layer has been proposed, which\ncan implant the backdoor without modifying the sample. However, these methods\nstruggle to covertly utilize dropout in evaluation mode, thus hindering their\ndeployment in real-world scenarios. To address these, this paper introduces\nGhostB, a novel approach to federated learning backdoor attacks in speaker\nrecognition that neither alters samples nor relies on dropout. This method\nemploys the behavior of neurons producing specific values as triggers. By\nmapping these neuronal values to categories specified by the adversary, the\nbackdoor is implanted and activated when particular feature values are detected\nat designated neurons. Our experiments conducted on TIMIT, LibriSpeech, and\nVoxCeleb2 databases in both Closed Set Identification (CSI) and Open Set\nIdentification (OSI) scenarios demonstrate that GhostB achieves a 100% success\nrate upon activation in speaker recognition, with this rate maintained across\nexperiments involving 1 to 50 ghost neurons. This paper investigates how the\ndispersion of neurons and their depth within hidden layers affect the success\nrate, revealing that increased dispersion and positioning of neurons can\nsignificantly decrease effectiveness, potentially rendering the attack\nunsuccessful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In federated learning, backdoor attacks embed triggers in the adversarial\nclient's data to inject a backdoor into the model. In order to enhance the\nstealth, an attack method based on the dropout layer has been proposed, which\ncan implant the backdoor without modifying the sample. However, these methods\nstruggle to covertly utilize dropout in evaluation mode, thus hindering their\ndeployment in real-world scenarios. To address these, this paper introduces\nGhostB, a novel approach to federated learning backdoor attacks in speaker\nrecognition that neither alters samples nor relies on dropout. This method\nemploys the behavior of neurons producing specific values as triggers. By\nmapping these neuronal values to categories specified by the adversary, the\nbackdoor is implanted and activated when particular feature values are detected\nat designated neurons. Our experiments conducted on TIMIT, LibriSpeech, and\nVoxCeleb2 databases in both Closed Set Identification (CSI) and Open Set\nIdentification (OSI) scenarios demonstrate that GhostB achieves a 100% success\nrate upon activation in speaker recognition, with this rate maintained across\nexperiments involving 1 to 50 ghost neurons. This paper investigates how the\ndispersion of neurons and their depth within hidden layers affect the success\nrate, revealing that increased dispersion and positioning of neurons can\nsignificantly decrease effectiveness, potentially rendering the attack\nunsuccessful."
                },
                "authors": [
                    {
                        "name": "Weida Xu"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Sicong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Sicong Zhang"
                },
                "author": "Sicong Zhang",
                "arxiv_doi": "10.1007/s10586-024-04837-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04837-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.13849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Cluster Comput 28, 158 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11935v1",
                "updated": "2025-01-21T07:16:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    16,
                    18,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:16:18Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    16,
                    18,
                    1,
                    21,
                    0
                ],
                "title": "Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students"
                },
                "summary": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs such as ChatGPT have been widely adopted by students in higher education\nas tools for learning programming and related concepts. However, it remains\nunclear how effective students are and what strategies students use while\nlearning with LLMs. Since the majority of students' experiences in online\nself-learning have come through using search engines such as Google, evaluating\nAI tools in this context can help us address these gaps. In this mixed methods\nresearch, we conducted an exploratory within-subjects study to understand how\nCS2 students learn programming concepts using both LLMs as well as traditional\nonline methods such as educational websites and videos to examine how students\napproach learning within and across both scenarios. We discovered that students\nfound it easier to learn a more difficult concept using traditional methods\nthan using ChatGPT. We also found that students ask fewer follow-ups and use\nmore keyword-based queries for search engines while their prompts to LLMs tend\nto explicitly ask for information."
                },
                "authors": [
                    {
                        "name": "Aayush Kumar"
                    },
                    {
                        "name": "Daniel Prol"
                    },
                    {
                        "name": "Amin Alipour"
                    },
                    {
                        "name": "Sruti Srinivasa Ragavan"
                    }
                ],
                "author_detail": {
                    "name": "Sruti Srinivasa Ragavan"
                },
                "author": "Sruti Srinivasa Ragavan",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11673v2",
                "updated": "2025-01-21T07:09:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    9,
                    35,
                    1,
                    21,
                    0
                ],
                "published": "2024-09-18T03:20:04Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    3,
                    20,
                    4,
                    2,
                    262,
                    0
                ],
                "title": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUIE: Retrieval-based Unified Information Extraction using Large\n  Language Model"
                },
                "summary": "Unified information extraction (UIE) aims to extract diverse structured\ninformation from unstructured text. While large language models (LLMs) have\nshown promise for UIE, they require significant computational resources and\noften struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based\nUnified Information Extraction), a framework that leverages in-context learning\nfor efficient task generalization. RUIE introduces a novel demonstration\nselection mechanism combining LLM preferences with a keyword-enhanced reward\nmodel, and employs a bi-encoder retriever trained through contrastive learning\nand knowledge distillation. As the first trainable retrieval framework for UIE,\nRUIE serves as a universal plugin for various LLMs. Experimental results on\neight held-out datasets demonstrate RUIE's effectiveness, with average F1-score\nimprovements of 19.22 and 3.22 compared to instruction-tuning methods and other\nretrievers, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified information extraction (UIE) aims to extract diverse structured\ninformation from unstructured text. While large language models (LLMs) have\nshown promise for UIE, they require significant computational resources and\noften struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based\nUnified Information Extraction), a framework that leverages in-context learning\nfor efficient task generalization. RUIE introduces a novel demonstration\nselection mechanism combining LLM preferences with a keyword-enhanced reward\nmodel, and employs a bi-encoder retriever trained through contrastive learning\nand knowledge distillation. As the first trainable retrieval framework for UIE,\nRUIE serves as a universal plugin for various LLMs. Experimental results on\neight held-out datasets demonstrate RUIE's effectiveness, with average F1-score\nimprovements of 19.22 and 3.22 compared to instruction-tuning methods and other\nretrievers, respectively."
                },
                "authors": [
                    {
                        "name": "Xincheng Liao"
                    },
                    {
                        "name": "Junwen Duan"
                    },
                    {
                        "name": "Yixi Huang"
                    },
                    {
                        "name": "Jianxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Wang"
                },
                "author": "Jianxin Wang",
                "arxiv_comment": "To appear in COLING 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11929v1",
                "updated": "2025-01-21T07:07:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    7,
                    58,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:07:58Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    7,
                    58,
                    1,
                    21,
                    0
                ],
                "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) systems have been shown to improve the\naccuracy of Large Language Model (LLM) outputs. However, these models can often\nachieve low accuracy when applied to new data domains.\n  We introduce the Automatic Local Fine Tuning of Retrieval Augmented\nGeneration models (ALoFTRAG) framework, designed to improve the accuracy of RAG\nsystems on a given domain by training LLMs without manually labeled data or\nusing larger teacher models.\n  By generating and filtering synthetic training data and performing LoRA\nfine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets\nin 26 languages by, on average, 8.3% and 3.0% respectively.\n  Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and\ndata-secure solution for improving RAG accuracy, making it particularly\napplicable to sensitive domains such as healthcare and finance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) systems have been shown to improve the\naccuracy of Large Language Model (LLM) outputs. However, these models can often\nachieve low accuracy when applied to new data domains.\n  We introduce the Automatic Local Fine Tuning of Retrieval Augmented\nGeneration models (ALoFTRAG) framework, designed to improve the accuracy of RAG\nsystems on a given domain by training LLMs without manually labeled data or\nusing larger teacher models.\n  By generating and filtering synthetic training data and performing LoRA\nfine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets\nin 26 languages by, on average, 8.3% and 3.0% respectively.\n  Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and\ndata-secure solution for improving RAG accuracy, making it particularly\napplicable to sensitive domains such as healthcare and finance."
                },
                "authors": [
                    {
                        "name": "Peter Devine"
                    }
                ],
                "author_detail": {
                    "name": "Peter Devine"
                },
                "author": "Peter Devine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.20151v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.20151v2",
                "updated": "2025-01-21T06:26:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    26,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2023-10-31T03:37:11Z",
                "published_parsed": [
                    2023,
                    10,
                    31,
                    3,
                    37,
                    11,
                    1,
                    304,
                    0
                ],
                "title": "Multi-Agent Consensus Seeking via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Consensus Seeking via Large Language Models"
                },
                "summary": "Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: windylab.github.io/ConsensusLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: windylab.github.io/ConsensusLLM/."
                },
                "authors": [
                    {
                        "name": "Huaben Chen"
                    },
                    {
                        "name": "Wenkang Ji"
                    },
                    {
                        "name": "Lufeng Xu"
                    },
                    {
                        "name": "Shiyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Zhao"
                },
                "author": "Shiyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.20151v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.20151v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11911v1",
                "updated": "2025-01-21T06:12:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    12,
                    49,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T06:12:49Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    12,
                    49,
                    1,
                    21,
                    0
                ],
                "title": "Integrate Temporal Graph Learning into LLM-based Temporal Knowledge\n  Graph Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrate Temporal Graph Learning into LLM-based Temporal Knowledge\n  Graph Model"
                },
                "summary": "Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events\nbased on the observed events in history. Recently, Large Language Models (LLMs)\nhave exhibited remarkable capabilities, generating significant research\ninterest in their application for reasoning over temporal knowledge graphs\n(TKGs). Existing LLM-based methods have integrated retrieved historical facts\nor static graph representations into LLMs. Despite the notable performance of\nLLM-based methods, they are limited by the insufficient modeling of temporal\npatterns and ineffective cross-modal alignment between graph and language,\nhindering the ability of LLMs to fully grasp the temporal and structural\ninformation in TKGs. To tackle these issues, we propose a novel framework\nTGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge\ngraph model. Specifically, we introduce temporal graph learning to capture the\ntemporal and relational patterns and obtain the historical graph embedding.\nFurthermore, we design a hybrid graph tokenization to sufficiently model the\ntemporal patterns within LLMs. To achieve better alignment between graph and\nlanguage, we employ a two-stage training paradigm to finetune LLMs on\nhigh-quality and diverse data, thereby resulting in better performance.\nExtensive experiments on three real-world datasets show that our approach\noutperforms a range of state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events\nbased on the observed events in history. Recently, Large Language Models (LLMs)\nhave exhibited remarkable capabilities, generating significant research\ninterest in their application for reasoning over temporal knowledge graphs\n(TKGs). Existing LLM-based methods have integrated retrieved historical facts\nor static graph representations into LLMs. Despite the notable performance of\nLLM-based methods, they are limited by the insufficient modeling of temporal\npatterns and ineffective cross-modal alignment between graph and language,\nhindering the ability of LLMs to fully grasp the temporal and structural\ninformation in TKGs. To tackle these issues, we propose a novel framework\nTGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge\ngraph model. Specifically, we introduce temporal graph learning to capture the\ntemporal and relational patterns and obtain the historical graph embedding.\nFurthermore, we design a hybrid graph tokenization to sufficiently model the\ntemporal patterns within LLMs. To achieve better alignment between graph and\nlanguage, we employ a two-stage training paradigm to finetune LLMs on\nhigh-quality and diverse data, thereby resulting in better performance.\nExtensive experiments on three real-world datasets show that our approach\noutperforms a range of state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "He Chang"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Zhulin Tao"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Xianglin Huang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08561v2",
                "updated": "2025-01-21T06:12:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    12,
                    26,
                    1,
                    21,
                    0
                ],
                "published": "2024-11-13T12:18:00Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    12,
                    18,
                    0,
                    2,
                    318,
                    0
                ],
                "title": "LogLLM: Log-based Anomaly Detection Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogLLM: Log-based Anomaly Detection Using Large Language Models"
                },
                "summary": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately."
                },
                "authors": [
                    {
                        "name": "Wei Guan"
                    },
                    {
                        "name": "Jian Cao"
                    },
                    {
                        "name": "Shiyou Qian"
                    },
                    {
                        "name": "Jianqi Gao"
                    },
                    {
                        "name": "Chun Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Ouyang"
                },
                "author": "Chun Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02253v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02253v2",
                "updated": "2025-01-21T06:03:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    6,
                    3,
                    7,
                    1,
                    21,
                    0
                ],
                "published": "2023-12-04T18:35:27Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    18,
                    35,
                    27,
                    0,
                    338,
                    0
                ],
                "title": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with\n  Synthetic Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with\n  Synthetic Images"
                },
                "summary": "Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative deep learning have enabled the creation of\nhigh-quality synthetic images in text-to-image generation. Prior work shows\nthat fine-tuning a pretrained diffusion model on ImageNet and generating\nsynthetic training images from the finetuned model can enhance an ImageNet\nclassifier's performance. However, performance degrades as synthetic images\noutnumber real ones. In this paper, we explore whether generative fine-tuning\nis essential for this improvement and whether it is possible to further scale\nup training using more synthetic data. We present a new framework leveraging\noff-the-shelf generative models to generate synthetic training images,\naddressing multiple challenges: class name ambiguity, lack of diversity in\nnaive prompts, and domain shifts. Specifically, we leverage large language\nmodels (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we\npropose contextualized diversification (CD) and stylized diversification (SD)\nmethods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage\ndomain adaptation techniques with auxiliary batch normalization for synthetic\nimages. Our framework consistently enhances recognition model performance with\nmore synthetic data, up to 6x of original ImageNet size showcasing the\npotential of synthetic data for improved recognition models and strong\nout-of-domain generalization."
                },
                "authors": [
                    {
                        "name": "Zhuoran Yu"
                    },
                    {
                        "name": "Chenchen Zhu"
                    },
                    {
                        "name": "Sean Culatana"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Fanyi Xiao"
                    },
                    {
                        "name": "Yong Jae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yong Jae Lee"
                },
                "author": "Yong Jae Lee",
                "arxiv_comment": "Accepted by Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02253v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02253v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13265v2",
                "updated": "2025-01-21T05:50:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    50,
                    19,
                    1,
                    21,
                    0
                ],
                "published": "2024-09-20T06:54:00Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    6,
                    54,
                    0,
                    4,
                    264,
                    0
                ],
                "title": "Towards LifeSpan Cognitive Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards LifeSpan Cognitive Systems"
                },
                "summary": "Building a human-like system that continuously interacts with complex\nenvironments -- whether simulated digital worlds or human society -- presents\nseveral key challenges. Central to this is enabling continuous, high-frequency\ninteractions, where the interactions are termed experiences. We refer to this\nenvisioned system as the LifeSpan Cognitive System (LSCS). A critical feature\nof LSCS is its ability to engage in incremental and rapid updates while\nretaining and accurately recalling past experiences. In this paper we focus on\nthe domain of Large Language Models (LLMs), where we identify two major\nchallenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention\nwith Accurate Recall. These properties are essential for storing new\nexperiences, organizing past experiences, and responding to the environment in\nways that leverage relevant historical data. Unlike language models with\ncontinual learning, which typically rely on large corpora for fine-tuning and\nfocus on improving performance within specific domains or tasks, LSCS must\nrapidly and incrementally update with new information from its environment at a\nhigh frequency. Existing technologies with the potential of solving the above\ntwo major challenges can be classified into four classes based on a conceptual\nmetric called Storage Complexity, which measures the relative space required to\nstore past experiences. Each of these four classes of technologies has its own\nstrengths and limitations while we argue none of them alone can achieve LSCS\nalone. To this end, we propose a potential instantiation for LSCS that can\nintegrate all four classes of technologies. The new instantiation, serving as a\nconjecture, operates through two core processes: Absorbing Experiences and\nGenerating Responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a human-like system that continuously interacts with complex\nenvironments -- whether simulated digital worlds or human society -- presents\nseveral key challenges. Central to this is enabling continuous, high-frequency\ninteractions, where the interactions are termed experiences. We refer to this\nenvisioned system as the LifeSpan Cognitive System (LSCS). A critical feature\nof LSCS is its ability to engage in incremental and rapid updates while\nretaining and accurately recalling past experiences. In this paper we focus on\nthe domain of Large Language Models (LLMs), where we identify two major\nchallenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention\nwith Accurate Recall. These properties are essential for storing new\nexperiences, organizing past experiences, and responding to the environment in\nways that leverage relevant historical data. Unlike language models with\ncontinual learning, which typically rely on large corpora for fine-tuning and\nfocus on improving performance within specific domains or tasks, LSCS must\nrapidly and incrementally update with new information from its environment at a\nhigh frequency. Existing technologies with the potential of solving the above\ntwo major challenges can be classified into four classes based on a conceptual\nmetric called Storage Complexity, which measures the relative space required to\nstore past experiences. Each of these four classes of technologies has its own\nstrengths and limitations while we argue none of them alone can achieve LSCS\nalone. To this end, we propose a potential instantiation for LSCS that can\nintegrate all four classes of technologies. The new instantiation, serving as a\nconjecture, operates through two core processes: Absorbing Experiences and\nGenerating Responses."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Xiaoxin He"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Nafis Sadeq"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Zexue He"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11900v1",
                "updated": "2025-01-21T05:30:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    30,
                    20,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T05:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    5,
                    30,
                    20,
                    1,
                    21,
                    0
                ],
                "title": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panoramic Interests: Stylistic-Content Aware Personalized Headline\n  Generation"
                },
                "summary": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized news headline generation aims to provide users with\nattention-grabbing headlines that are tailored to their preferences. Prevailing\nmethods focus on user-oriented content preferences, but most of them overlook\nthe fact that diverse stylistic preferences are integral to users' panoramic\ninterests, leading to suboptimal personalization. In view of this, we propose a\nnovel Stylistic-Content Aware Personalized Headline Generation (SCAPE)\nframework. SCAPE extracts both content and stylistic features from headlines\nwith the aid of large language model (LLM) collaboration. It further adaptively\nintegrates users' long- and short-term interests through a contrastive\nlearning-based hierarchical fusion network. By incorporating the panoramic\ninterests into the headline generator, SCAPE reflects users' stylistic-content\npreferences during the generation process. Extensive experiments on the\nreal-world dataset PENS demonstrate the superiority of SCAPE over baselines."
                },
                "authors": [
                    {
                        "name": "Junhong Lian"
                    },
                    {
                        "name": "Xiang Ao"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qing He"
                    }
                ],
                "author_detail": {
                    "name": "Qing He"
                },
                "author": "Qing He",
                "arxiv_comment": "Accepted to The ACM Web Conference 2025 (WWW'25, short paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11885v1",
                "updated": "2025-01-21T04:40:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T04:40:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    40,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and\n  Reasoning of Evidence-Based Medicine"
                },
                "summary": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have exhibited remarkable\ncapabilities in clinical scenarios. However, despite their potential, existing\nworks face challenges when applying LLMs to medical settings. Strategies\nrelying on training with medical datasets are highly cost-intensive and may\nsuffer from outdated training data. Leveraging external knowledge bases is a\nsuitable alternative, yet it faces obstacles such as limited retrieval\nprecision and poor effectiveness in answer extraction. These issues\ncollectively prevent LLMs from demonstrating the expected level of proficiency\nin mastering medical expertise. To address these challenges, we introduce\nMed-R^2, a novel LLM physician framework that adheres to the Evidence-Based\nMedicine (EBM) process, efficiently integrating retrieval mechanisms as well as\nthe selection and reasoning processes of evidence, thereby enhancing the\nproblem-solving capabilities of LLMs in healthcare scenarios and fostering a\ntrustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2\nachieves a 14.87\\% improvement over vanilla RAG methods and even a 3.59\\%\nenhancement compared to fine-tuning strategies, without incurring additional\ntraining costs."
                },
                "authors": [
                    {
                        "name": "Keer Lu"
                    },
                    {
                        "name": "Zheng Liang"
                    },
                    {
                        "name": "Da Pan"
                    },
                    {
                        "name": "Shusen Zhang"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zenan Zhou"
                    },
                    {
                        "name": "Guosheng Dong"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11877v1",
                "updated": "2025-01-21T04:11:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    11,
                    59,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T04:11:59Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    11,
                    59,
                    1,
                    21,
                    0
                ],
                "title": "From Drafts to Answers: Unlocking LLM Potential via Aggregation\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Drafts to Answers: Unlocking LLM Potential via Aggregation\n  Fine-Tuning"
                },
                "summary": "Scaling data and model size has been proven effective for boosting the\nperformance of large language models. In addition to training-time scaling,\nrecent studies have revealed that increasing test-time computational resources\ncan further improve performance. In this work, we introduce Aggregation\nFine-Tuning (AFT), a supervised finetuning paradigm where the model learns to\nsynthesize multiple draft responses, referred to as proposals, into a single,\nrefined answer, termed aggregation. At inference time, a propose-and-aggregate\nstrategy further boosts performance by iteratively generating proposals and\naggregating them. Empirical evaluations on benchmark datasets show that\nAFT-trained models substantially outperform standard SFT. Notably, an AFT\nmodel, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC\nwin rate on AlpacaEval 2, surpassing significantly larger LLMs such as\nLlama3.1-405B-Instruct and GPT4. By combining sequential refinement and\nparallel sampling, the propose-and-aggregate framework scales inference-time\ncomputation in a flexible manner. Overall, These findings position AFT as a\npromising approach to unlocking additional capabilities of LLMs without\nresorting to increasing data volume or model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling data and model size has been proven effective for boosting the\nperformance of large language models. In addition to training-time scaling,\nrecent studies have revealed that increasing test-time computational resources\ncan further improve performance. In this work, we introduce Aggregation\nFine-Tuning (AFT), a supervised finetuning paradigm where the model learns to\nsynthesize multiple draft responses, referred to as proposals, into a single,\nrefined answer, termed aggregation. At inference time, a propose-and-aggregate\nstrategy further boosts performance by iteratively generating proposals and\naggregating them. Empirical evaluations on benchmark datasets show that\nAFT-trained models substantially outperform standard SFT. Notably, an AFT\nmodel, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC\nwin rate on AlpacaEval 2, surpassing significantly larger LLMs such as\nLlama3.1-405B-Instruct and GPT4. By combining sequential refinement and\nparallel sampling, the propose-and-aggregate framework scales inference-time\ncomputation in a flexible manner. Overall, These findings position AFT as a\npromising approach to unlocking additional capabilities of LLMs without\nresorting to increasing data volume or model size."
                },
                "authors": [
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Tingchen Fu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "20 pages; work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12624v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12624v5",
                "updated": "2025-01-21T04:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    10,
                    13,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-18T13:49:54Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    49,
                    54,
                    1,
                    170,
                    0
                ],
                "title": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: Evaluating Alignment and Vulnerabilities in\n  LLMs-as-Judges"
                },
                "summary": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offering a promising solution to the scalability challenges associated with\nhuman evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an\napproach to evaluating large language models (LLMs). However, there are still\nmany open questions about the strengths and weaknesses of this paradigm, and\nwhat potential biases it may hold. In this paper, we present a comprehensive\nstudy of the performance of various LLMs acting as judges, focusing on a clean\nscenario in which inter-human agreement is high. Investigating thirteen judge\nmodels of different model sizes and families, judging answers of nine different\n'examtaker models' - both base and instruction-tuned - we find that only the\nbest (and largest) models achieve reasonable alignment with humans. However,\nthey are still quite far behind inter-human agreement and their assigned scores\nmay still differ with up to 5 points from human-assigned scores. In terms of\ntheir ranking of the nine exam-taker models, instead, also smaller models and\neven the lexical metric contains may provide a reasonable signal. Through error\nanalysis and other studies, we identify vulnerabilities in judge models, such\nas their sensitivity to prompt complexity and length, and a tendency toward\nleniency. The fact that even the best judges differ from humans in this\ncomparatively simple setup suggest that caution may be wise when using judges\nin more complex setups. Lastly, our research rediscovers the importance of\nusing alignment metrics beyond simple percent alignment, showing that judges\nwith high percent agreement can still assign vastly different scores."
                },
                "authors": [
                    {
                        "name": "Aman Singh Thakur"
                    },
                    {
                        "name": "Kartik Choudhary"
                    },
                    {
                        "name": "Venkat Srinik Ramayapally"
                    },
                    {
                        "name": "Sankaran Vaidyanathan"
                    },
                    {
                        "name": "Dieuwke Hupkes"
                    }
                ],
                "author_detail": {
                    "name": "Dieuwke Hupkes"
                },
                "author": "Dieuwke Hupkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12624v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12624v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11051v2",
                "updated": "2025-01-21T04:06:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    6,
                    9,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-20T17:57:46Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    46,
                    1,
                    233,
                    0
                ],
                "title": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLAME: Learning to Navigate with Multimodal LLM in Urban Environments"
                },
                "summary": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for route summarization, and end-to-end\ntraining on VLN datasets. The augmented datasets are synthesized automatically.\nExperimental results demonstrate FLAME's superiority over existing methods,\nsurpassing state-of-the-art methods by a 7.3% increase in task completion on\nTouchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)\nin complex navigation tasks, representing an advancement towards applications\nof MLLMs in the field of embodied intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated potential in\nVision-and-Language Navigation (VLN) tasks, yet current applications face\nchallenges. While LLMs excel in general conversation scenarios, they struggle\nwith specialized navigation tasks, yielding suboptimal performance compared to\nspecialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied\nAgent), a novel Multimodal LLM-based agent and architecture designed for urban\nVLN tasks that efficiently handles multiple observations. Our approach\nimplements a three-phase tuning technique for effective adaptation to\nnavigation tasks, including single perception tuning for street view\ndescription, multiple perception tuning for route summarization, and end-to-end\ntraining on VLN datasets. The augmented datasets are synthesized automatically.\nExperimental results demonstrate FLAME's superiority over existing methods,\nsurpassing state-of-the-art methods by a 7.3% increase in task completion on\nTouchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs)\nin complex navigation tasks, representing an advancement towards applications\nof MLLMs in the field of embodied intelligence."
                },
                "authors": [
                    {
                        "name": "Yunzhe Xu"
                    },
                    {
                        "name": "Yiyuan Pan"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hesheng Wang"
                },
                "author": "Hesheng Wang",
                "arxiv_comment": "Accepted to AAAI 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11873v1",
                "updated": "2025-01-21T04:04:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    4,
                    39,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T04:04:39Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    4,
                    39,
                    1,
                    21,
                    0
                ],
                "title": "Demons in the Detail: On Implementing Load Balancing Loss for Training\n  Specialized Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demons in the Detail: On Implementing Load Balancing Loss for Training\n  Specialized Mixture-of-Expert Models"
                },
                "summary": "This paper revisits the implementation of\n$\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E\n\\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$\nrepresents the frequency of expert $i$ being selected, and $p_i$ denotes the\naverage gating score of the expert $i$. Existing MoE training frameworks\nusually employ the parallel training strategy so that $f_i$ and the LBL are\ncalculated within a $\\textbf{micro-batch}$ and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a $\\textbf{global-batch}$ to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize $f_i$ across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n$\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the implementation of\n$\\textbf{L}$oad-$\\textbf{b}$alancing $\\textbf{L}$oss (LBL) when training\nMixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E\n\\sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$\nrepresents the frequency of expert $i$ being selected, and $p_i$ denotes the\naverage gating score of the expert $i$. Existing MoE training frameworks\nusually employ the parallel training strategy so that $f_i$ and the LBL are\ncalculated within a $\\textbf{micro-batch}$ and then averaged across parallel\ngroups. In essence, a micro-batch for training billion-scale LLMs normally\ncontains very few sequences. So, the micro-batch LBL is almost at the sequence\nlevel, and the router is pushed to distribute the token evenly within each\nsequence. Under this strict constraint, even tokens from a domain-specific\nsequence ($\\textit{e.g.}$, code) are uniformly routed to all experts, thereby\ninhibiting expert specialization. In this work, we propose calculating LBL\nusing a $\\textbf{global-batch}$ to loose this constraint. Because a\nglobal-batch contains much more diverse sequences than a micro-batch, which\nwill encourage load balance at the corpus level. Specifically, we introduce an\nextra communication step to synchronize $f_i$ across micro-batches and then use\nit to calculate the LBL. Through experiments on training MoEs-based LLMs (up to\n$\\textbf{42.8B}$ total parameters and $\\textbf{400B}$ tokens), we surprisingly\nfind that the global-batch LBL strategy yields excellent performance gains in\nboth pre-training perplexity and downstream tasks. Our analysis reveals that\nthe global-batch LBL also greatly improves the domain specialization of MoE\nexperts."
                },
                "authors": [
                    {
                        "name": "Zihan Qiu"
                    },
                    {
                        "name": "Zeyu Huang"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Kaiyue Wen"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Ivan Titov"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11864v1",
                "updated": "2025-01-21T03:42:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    42,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:42:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    42,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "LLM-Agents Driven Automated Simulation Testing and Analysis of small\n  Uncrewed Aerial Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Agents Driven Automated Simulation Testing and Analysis of small\n  Uncrewed Aerial Systems"
                },
                "summary": "Thorough simulation testing is crucial for validating the correct behavior of\nsmall Uncrewed Aerial Systems (sUAS) across multiple scenarios, including\nadverse weather conditions (such as wind, and fog), diverse settings (hilly\nterrain, or urban areas), and varying mission profiles (surveillance,\ntracking). While various sUAS simulation tools exist to support developers, the\nentire process of creating, executing, and analyzing simulation tests remains a\nlargely manual and cumbersome task. Developers must identify test scenarios,\nset up the simulation environment, integrate the System under Test (SuT) with\nsimulation tools, formulate mission plans, and collect and analyze results.\nThese labor-intensive tasks limit the ability of developers to conduct\nexhaustive testing across a wide range of scenarios. To alleviate this problem,\nin this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven\nframework, where multiple LLM agents collaborate to support the sUAS simulation\ntesting process. This includes: (1) creating test scenarios that subject the\nSuT to unique environmental contexts; (2) preparing the simulation environment\nas per the test scenario; (3) generating diverse sUAS missions for the SuT to\nexecute; and (4) analyzing simulation results and providing an interactive\nanalytics interface. Further, the design of the framework is flexible for\ncreating and testing scenarios for a variety of sUAS use cases, simulation\ntools, and SuT input requirements. We evaluated our approach by (a) conducting\nsimulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)\nanalyzing the performance of each agent, and (c) gathering feedback from sUAS\ndevelopers. Our findings indicate that AutoSimTest significantly improves the\nefficiency and scope of the sUAS testing process, allowing for more\ncomprehensive and varied scenario evaluations while reducing the manual effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thorough simulation testing is crucial for validating the correct behavior of\nsmall Uncrewed Aerial Systems (sUAS) across multiple scenarios, including\nadverse weather conditions (such as wind, and fog), diverse settings (hilly\nterrain, or urban areas), and varying mission profiles (surveillance,\ntracking). While various sUAS simulation tools exist to support developers, the\nentire process of creating, executing, and analyzing simulation tests remains a\nlargely manual and cumbersome task. Developers must identify test scenarios,\nset up the simulation environment, integrate the System under Test (SuT) with\nsimulation tools, formulate mission plans, and collect and analyze results.\nThese labor-intensive tasks limit the ability of developers to conduct\nexhaustive testing across a wide range of scenarios. To alleviate this problem,\nin this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven\nframework, where multiple LLM agents collaborate to support the sUAS simulation\ntesting process. This includes: (1) creating test scenarios that subject the\nSuT to unique environmental contexts; (2) preparing the simulation environment\nas per the test scenario; (3) generating diverse sUAS missions for the SuT to\nexecute; and (4) analyzing simulation results and providing an interactive\nanalytics interface. Further, the design of the framework is flexible for\ncreating and testing scenarios for a variety of sUAS use cases, simulation\ntools, and SuT input requirements. We evaluated our approach by (a) conducting\nsimulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b)\nanalyzing the performance of each agent, and (c) gathering feedback from sUAS\ndevelopers. Our findings indicate that AutoSimTest significantly improves the\nefficiency and scope of the sUAS testing process, allowing for more\ncomprehensive and varied scenario evaluations while reducing the manual effort."
                },
                "authors": [
                    {
                        "name": "Venkata Sai Aswath Duvvuru"
                    },
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Michael Vierhauser"
                    },
                    {
                        "name": "Ankit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ankit Agrawal"
                },
                "author": "Ankit Agrawal",
                "arxiv_comment": "Accepted as full paper at ICSE-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10159v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10159v4",
                "updated": "2025-01-21T03:40:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    40,
                    14,
                    1,
                    21,
                    0
                ],
                "published": "2024-08-19T17:09:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    17,
                    9,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizing Language Models with Instance-wise LoRA for Sequential\n  Recommendation"
                },
                "summary": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems predict the next interaction item based on\nusers' past interactions, aligning recommendations with individual preferences.\nLeveraging the strengths of Large Language Models (LLMs) in knowledge\ncomprehension and reasoning, recent approaches are eager to apply LLMs to\nsequential recommendation. A common paradigm is converting user behavior\nsequences into instruction data, and fine-tuning the LLM with\nparameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA).\nHowever, the uniform application of LoRA across diverse user behaviors is\ninsufficient to capture individual variability, resulting in negative transfer\nbetween disparate sequences. To address these challenges, we propose\nInstance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation\ntask as a form of multi-task learning, integrating LoRA with the Mixture of\nExperts (MoE) framework. This approach encourages different experts to capture\nvarious aspects of user behavior. Additionally, we introduce a sequence\nrepresentation guided gate function that generates customized expert\nparticipation weights for each user sequence, which allows dynamic parameter\nadjustment for instance-wise recommendations. In sequential recommendation,\niLoRA achieves an average relative improvement of 11.4\\% over basic LoRA in the\nhit ratio metric, with less than a 1\\% relative increase in trainable\nparameters. Extensive experiments on three benchmark datasets demonstrate the\neffectiveness of iLoRA, highlighting its superior performance compared to\nexisting methods in mitigating negative transfer and improving recommendation\naccuracy. Our data and code are available at\nhttps://github.com/AkaliKong/iLoRA."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Kong"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Leheng Sheng"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_comment": "NeurIPS 2024 poster",
                "arxiv_journal_ref": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10159v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10159v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09701v3",
                "updated": "2025-01-21T03:27:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    27,
                    58,
                    1,
                    21,
                    0
                ],
                "published": "2024-06-14T04:01:25Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    4,
                    1,
                    25,
                    4,
                    166,
                    0
                ],
                "title": "Towards Explainable Vulnerability Detection with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Explainable Vulnerability Detection with Large Language Models"
                },
                "summary": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities pose significant risks to the security and integrity\nof software systems. Although prior studies have explored vulnerability\ndetection using deep learning and pre-trained models, these approaches often\nfail to provide the detailed explanations necessary for developers to\nunderstand and remediate vulnerabilities effectively. The advent of large\nlanguage models (LLMs) has introduced transformative potential due to their\nadvanced generative capabilities and ability to comprehend complex contexts,\noffering new possibilities for addressing these challenges. In this paper, we\npropose LLMVulExp, an automated framework designed to specialize LLMs for the\ndual tasks of vulnerability detection and explanation. To address the\nchallenges of acquiring high-quality annotated data and injecting\ndomain-specific knowledge, LLMVulExp leverages prompt-based techniques for\nannotating vulnerability explanations and finetunes LLMs using instruction\ntuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect\nvulnerability types in code while generating detailed explanations, including\nthe cause, location, and repair suggestions. Additionally, we employ a\nChain-of-Thought (CoT) based key code extraction strategy to focus LLMs on\nanalyzing vulnerability-prone code, further enhancing detection accuracy and\nexplanatory depth. Our experimental results demonstrate that LLMVulExp achieves\nover a 90% F1 score on the SeVC dataset, effectively combining high detection\naccuracy with actionable and coherent explanations. This study highlights the\nfeasibility of utilizing LLMs for real-world vulnerability detection and\nexplanation tasks, providing critical insights into their adaptation and\napplication in software security."
                },
                "authors": [
                    {
                        "name": "Qiheng Mao"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Jianling Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jianling Sun"
                },
                "author": "Jianling Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11858v1",
                "updated": "2025-01-21T03:22:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    22,
                    10,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:22:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    22,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval."
                },
                "authors": [
                    {
                        "name": "Zhili Cheng"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Shiqi Dai"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Tianyu Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11849v1",
                "updated": "2025-01-21T03:07:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    7,
                    21,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:07:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    7,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "Network-informed Prompt Engineering against Organized Astroturf\n  Campaigns under Extreme Class Imbalance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-informed Prompt Engineering against Organized Astroturf\n  Campaigns under Extreme Class Imbalance"
                },
                "summary": "Detecting organized political campaigns is of paramount importance in\nfighting against disinformation on social media. Existing approaches for the\nidentification of such organized actions employ techniques mostly from network\nscience, graph machine learning and natural language processing. Their ultimate\ngoal is to analyze the relationships and interactions (e.g. re-posting) among\nusers and the textual similarities of their posts. Despite their effectiveness\nin recognizing astroturf campaigns, these methods face significant challenges,\nnotably the class imbalance in available training datasets. To mitigate this\nissue, recent methods usually resort to data augmentation or increasing the\nnumber of positive samples, which may not always be feasible or sufficient in\nreal-world settings. Following a different path, in this paper, we propose a\nnovel framework for identifying astroturf campaigns based solely on large\nlanguage models (LLMs), introducing a Balanced Retrieval-Augmented Generation\n(Balanced RAG) component. Our approach first gives both textual information\nconcerning the posts (in our case tweets) and the user interactions of the\nsocial network as input to a language model. Then, through prompt engineering\nand the proposed Balanced RAG method, it effectively detects coordinated\ndisinformation campaigns on X (Twitter). The proposed framework does not\nrequire any training or fine-tuning of the language model. Instead, by\nstrategically harnessing the strengths of prompt engineering and Balanced RAG,\nit facilitates LLMs to overcome the effects of class imbalance and effectively\nidentify coordinated political campaigns. The experimental results demonstrate\nthat by incorporating the proposed prompt engineering and Balanced RAG methods,\nour framework outperforms the traditional graph-based baselines, achieving\n2x-3x improvements in terms of precision, recall and F1 scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting organized political campaigns is of paramount importance in\nfighting against disinformation on social media. Existing approaches for the\nidentification of such organized actions employ techniques mostly from network\nscience, graph machine learning and natural language processing. Their ultimate\ngoal is to analyze the relationships and interactions (e.g. re-posting) among\nusers and the textual similarities of their posts. Despite their effectiveness\nin recognizing astroturf campaigns, these methods face significant challenges,\nnotably the class imbalance in available training datasets. To mitigate this\nissue, recent methods usually resort to data augmentation or increasing the\nnumber of positive samples, which may not always be feasible or sufficient in\nreal-world settings. Following a different path, in this paper, we propose a\nnovel framework for identifying astroturf campaigns based solely on large\nlanguage models (LLMs), introducing a Balanced Retrieval-Augmented Generation\n(Balanced RAG) component. Our approach first gives both textual information\nconcerning the posts (in our case tweets) and the user interactions of the\nsocial network as input to a language model. Then, through prompt engineering\nand the proposed Balanced RAG method, it effectively detects coordinated\ndisinformation campaigns on X (Twitter). The proposed framework does not\nrequire any training or fine-tuning of the language model. Instead, by\nstrategically harnessing the strengths of prompt engineering and Balanced RAG,\nit facilitates LLMs to overcome the effects of class imbalance and effectively\nidentify coordinated political campaigns. The experimental results demonstrate\nthat by incorporating the proposed prompt engineering and Balanced RAG methods,\nour framework outperforms the traditional graph-based baselines, achieving\n2x-3x improvements in terms of precision, recall and F1 scores."
                },
                "authors": [
                    {
                        "name": "Nikos Kanakaris"
                    },
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Xiongye Xiao"
                    },
                    {
                        "name": "Nesreen K. Ahmed"
                    },
                    {
                        "name": "Luca Luceri"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11847v1",
                "updated": "2025-01-21T03:06:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    6,
                    30,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T03:06:30Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    6,
                    30,
                    1,
                    21,
                    0
                ],
                "title": "A Survey on Memory-Efficient Large-Scale Model Training in AI for\n  Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Memory-Efficient Large-Scale Model Training in AI for\n  Science"
                },
                "summary": "Scientific research faces high costs and inefficiencies with traditional\nmethods, but the rise of deep learning and large language models (LLMs) offers\ninnovative solutions. This survey reviews LLM applications across scientific\nfields such as biology, medicine, chemistry, and meteorology, underscoring\ntheir role in advancing research. However, the continuous expansion of model\nsize has led to significant memory demands, hindering further development and\napplication of LLMs for science. To address this, we review memory-efficient\ntraining techniques for LLMs based on the transformer architecture, including\ndistributed training, mixed precision training, and gradient checkpointing.\nUsing AlphaFold 2 as an example, we demonstrate how tailored memory\noptimization methods can reduce storage needs while preserving prediction\naccuracy. We also discuss the challenges of memory optimization in practice and\npotential future directions, hoping to provide valuable insights for\nresearchers and engineers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific research faces high costs and inefficiencies with traditional\nmethods, but the rise of deep learning and large language models (LLMs) offers\ninnovative solutions. This survey reviews LLM applications across scientific\nfields such as biology, medicine, chemistry, and meteorology, underscoring\ntheir role in advancing research. However, the continuous expansion of model\nsize has led to significant memory demands, hindering further development and\napplication of LLMs for science. To address this, we review memory-efficient\ntraining techniques for LLMs based on the transformer architecture, including\ndistributed training, mixed precision training, and gradient checkpointing.\nUsing AlphaFold 2 as an example, we demonstrate how tailored memory\noptimization methods can reduce storage needs while preserving prediction\naccuracy. We also discuss the challenges of memory optimization in practice and\npotential future directions, hoping to provide valuable insights for\nresearchers and engineers."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Tian"
                    },
                    {
                        "name": "Linbo Qiao"
                    },
                    {
                        "name": "Baihui Liu"
                    },
                    {
                        "name": "Gongqingjian Jiang"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11840v1",
                "updated": "2025-01-21T02:49:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    49,
                    43,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:49:43Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    49,
                    43,
                    1,
                    21,
                    0
                ],
                "title": "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models with Human-In-The-Loop Validation for Systematic\n  Review Data Extraction"
                },
                "summary": "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic reviews are time-consuming endeavors. Historically speaking,\nknowledgeable humans have had to screen and extract data from studies before it\ncan be analyzed. However, large language models (LLMs) hold promise to greatly\naccelerate this process. After a pilot study which showed great promise, we\ninvestigated the use of freely available LLMs for extracting data for\nsystematic reviews. Using three different LLMs, we extracted 24 types of data,\n9 explicitly stated variables and 15 derived categorical variables, from 112\nstudies that were included in a published scoping review. Overall we found that\nGemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably\nwell, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with\nhuman coding, respectively. While promising, these results highlight the dire\nneed for a human-in-the-loop (HIL) process for AI-assisted data extraction. As\na result, we present a free, open-source program we developed (AIDE) to\nfacilitate user-friendly, HIL data extraction with LLMs."
                },
                "authors": [
                    {
                        "name": "Noah L. Schroeder"
                    },
                    {
                        "name": "Chris Davis Jaldi"
                    },
                    {
                        "name": "Shan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shan Zhang"
                },
                "author": "Shan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00253v4",
                "updated": "2025-01-21T02:45:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    45,
                    49,
                    1,
                    21,
                    0
                ],
                "published": "2024-04-30T23:56:38Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    23,
                    56,
                    38,
                    1,
                    121,
                    0
                ],
                "title": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based\n  Verification"
                },
                "summary": "Large Language Models (LLMs) have made significant progress in code\ngeneration, offering developers groundbreaking automated programming support.\nHowever, LLMs often generate code that is syntactically correct and even\nsemantically plausible, but may not execute as expected or fulfill specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To advance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we present a\ndynamic detection algorithm called CodeHalu designed to detect and quantify\ncode hallucinations. We also introduce the CodeHaluEval benchmark, which\nincludes 8,883 samples from 699 tasks, to systematically and quantitatively\nevaluate code hallucinations. By evaluating 17 popular LLMs using this\nbenchmark, we reveal significant differences in their accuracy and reliability\nin code generation, offering detailed insights for further improving the code\ngeneration capabilities of LLMs. The CodeHalu benchmark and code are publicly\navailable at https://github.com/yuchen814/CodeHalu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant progress in code\ngeneration, offering developers groundbreaking automated programming support.\nHowever, LLMs often generate code that is syntactically correct and even\nsemantically plausible, but may not execute as expected or fulfill specified\nrequirements. This phenomenon of hallucinations in the code domain has not been\nsystematically explored. To advance the community's understanding and research\non this issue, we introduce the concept of code hallucinations and propose a\nclassification method for code hallucination based on execution verification.\nWe categorize code hallucinations into four main types: mapping, naming,\nresource, and logic hallucinations, with each category further divided into\ndifferent subcategories to understand and address the unique challenges faced\nby LLMs in code generation with finer granularity. Additionally, we present a\ndynamic detection algorithm called CodeHalu designed to detect and quantify\ncode hallucinations. We also introduce the CodeHaluEval benchmark, which\nincludes 8,883 samples from 699 tasks, to systematically and quantitatively\nevaluate code hallucinations. By evaluating 17 popular LLMs using this\nbenchmark, we reveal significant differences in their accuracy and reliability\nin code generation, offering detailed insights for further improving the code\ngeneration capabilities of LLMs. The CodeHalu benchmark and code are publicly\navailable at https://github.com/yuchen814/CodeHalu."
                },
                "authors": [
                    {
                        "name": "Yuchen Tian"
                    },
                    {
                        "name": "Weixiang Yan"
                    },
                    {
                        "name": "Qian Yang"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Ziyang Luo"
                    },
                    {
                        "name": "Lei Ma"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "Accepted by AAAI 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08067v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08067v3",
                "updated": "2025-01-21T02:40:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    40,
                    27,
                    1,
                    21,
                    0
                ],
                "published": "2024-10-10T16:01:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    1,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs"
                },
                "summary": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference alignment in Large Language Models (LLMs) has significantly\nimproved their ability to adhere to human instructions and intentions. However,\nexisting direct alignment algorithms primarily focus on relative preferences\nand often overlook the qualitative aspects of responses. Striving to maximize\nthe implicit reward gap between the chosen and the slightly inferior rejected\nresponses can cause overfitting and unnecessary unlearning of the high-quality\nrejected responses. The unawareness of the reward scores also drives the LLM to\nindiscriminately favor the low-quality chosen responses and fail to generalize\nto responses with the highest rewards, which are sparse in data. To overcome\nthese shortcomings, our study introduces reward-conditioned LLM policies that\ndiscern and learn from the entire spectrum of response quality within the\ndataset, helping extrapolate to more optimal regions. We propose an effective\nyet simple data relabeling method that conditions the preference pairs on\nquality scores to construct a reward-augmented dataset. This dataset is easily\nintegrated with existing direct alignment algorithms and is applicable to any\npreference dataset. The experimental results across instruction-following\nbenchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that\nour approach consistently boosts the performance of DPO by a considerable\nmargin across diverse models. Additionally, our method improves the average\naccuracy on various academic benchmarks. When applying our method to on-policy\ndata, the resulting DPO model achieves SOTA results on AlpacaEval. Through\nablation studies, we demonstrate that our method not only maximizes the utility\nof preference data but also mitigates the issue of unlearning, demonstrating\nits broad effectiveness beyond mere dataset expansion. Our code is available at\nhttps://github.com/shenao-zhang/reward-augmented-preference."
                },
                "authors": [
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yufeng Zhang"
                    },
                    {
                        "name": "Yingxiang Yang"
                    },
                    {
                        "name": "Yongfei Liu"
                    },
                    {
                        "name": "Liyu Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08067v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08067v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11835v1",
                "updated": "2025-01-21T02:38:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    38,
                    28,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:38:28Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    38,
                    28,
                    1,
                    21,
                    0
                ],
                "title": "Hybrid Adaptive Modeling using Neural Networks Trained with Nonlinear\n  Dynamics Based Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Adaptive Modeling using Neural Networks Trained with Nonlinear\n  Dynamics Based Features"
                },
                "summary": "Accurate models are essential for design, performance prediction, control,\nand diagnostics in complex engineering systems. Physics-based models excel\nduring the design phase but often become outdated during system deployment due\nto changing operational conditions, unknown interactions, excitations, and\nparametric drift. While data-based models can capture the current state of\ncomplex systems, they face significant challenges, including excessive data\ndependence, limited generalizability to changing conditions, and inability to\npredict parametric dependence. This has led to combining physics and data in\nmodeling, termed physics-infused machine learning, often using numerical\nsimulations from physics-based models. This paper introduces a novel approach\nthat departs from standard techniques by uncovering information from nonlinear\ndynamical modeling and embedding it in data-based models. The goal is to create\na hybrid adaptive modeling framework that integrates data-based modeling with\nnewly measured data and analytical nonlinear dynamical models for enhanced\naccuracy, parametric dependence, and improved generalizability. By explicitly\nincorporating nonlinear dynamic phenomena through perturbation methods, the\npredictive capabilities are more realistic and insightful compared to knowledge\nobtained from brute-force numerical simulations. In particular, perturbation\nmethods are utilized to derive asymptotic solutions which are parameterized to\ngenerate frequency responses. Frequency responses provide comprehensive\ninsights into dynamics and nonlinearity which are quantified and extracted as\nhigh-quality features. A machine-learning model, trained by these features,\ntracks parameter variations and updates the mismatched model. The results\ndemonstrate that this adaptive modeling method outperforms numerical gray box\nmodels in prediction accuracy and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate models are essential for design, performance prediction, control,\nand diagnostics in complex engineering systems. Physics-based models excel\nduring the design phase but often become outdated during system deployment due\nto changing operational conditions, unknown interactions, excitations, and\nparametric drift. While data-based models can capture the current state of\ncomplex systems, they face significant challenges, including excessive data\ndependence, limited generalizability to changing conditions, and inability to\npredict parametric dependence. This has led to combining physics and data in\nmodeling, termed physics-infused machine learning, often using numerical\nsimulations from physics-based models. This paper introduces a novel approach\nthat departs from standard techniques by uncovering information from nonlinear\ndynamical modeling and embedding it in data-based models. The goal is to create\na hybrid adaptive modeling framework that integrates data-based modeling with\nnewly measured data and analytical nonlinear dynamical models for enhanced\naccuracy, parametric dependence, and improved generalizability. By explicitly\nincorporating nonlinear dynamic phenomena through perturbation methods, the\npredictive capabilities are more realistic and insightful compared to knowledge\nobtained from brute-force numerical simulations. In particular, perturbation\nmethods are utilized to derive asymptotic solutions which are parameterized to\ngenerate frequency responses. Frequency responses provide comprehensive\ninsights into dynamics and nonlinearity which are quantified and extracted as\nhigh-quality features. A machine-learning model, trained by these features,\ntracks parameter variations and updates the mismatched model. The results\ndemonstrate that this adaptive modeling method outperforms numerical gray box\nmodels in prediction accuracy and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Prashant N. Kambali"
                    },
                    {
                        "name": "C. Nataraj"
                    }
                ],
                "author_detail": {
                    "name": "C. Nataraj"
                },
                "author": "C. Nataraj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11833v1",
                "updated": "2025-01-21T02:29:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    29,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:29:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    29,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Is your LLM trapped in a Mental Set? Investigative study on how mental\n  sets affect the reasoning capabilities of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is your LLM trapped in a Mental Set? Investigative study on how mental\n  sets affect the reasoning capabilities of LLMs"
                },
                "summary": "In this paper, we present an investigative study on how Mental Sets influence\nthe reasoning capabilities of LLMs. LLMs have excelled in diverse natural\nlanguage processing (NLP) tasks, driven by advancements in parameter-efficient\nfine-tuning (PEFT) and emergent capabilities like in-context learning (ICL).\nFor complex reasoning tasks, selecting the right model for PEFT or ICL is\ncritical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K.\nHowever, current evaluation methods, based on metrics like F1 Score or\nreasoning chain assessments by larger models, overlook a key dimension:\nadaptability to unfamiliar situations and overcoming entrenched thinking\npatterns. In cognitive psychology, Mental Set refers to the tendency to persist\nwith previously successful strategies, even when they become inefficient - a\nchallenge for problem solving and reasoning. We compare the performance of LLM\nmodels like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-4o in the\npresence of mental sets. To the best of our knowledge, this is the first study\nto integrate cognitive psychology concepts into the evaluation of LLMs for\ncomplex reasoning tasks, providing deeper insights into their adaptability and\nproblem-solving efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an investigative study on how Mental Sets influence\nthe reasoning capabilities of LLMs. LLMs have excelled in diverse natural\nlanguage processing (NLP) tasks, driven by advancements in parameter-efficient\nfine-tuning (PEFT) and emergent capabilities like in-context learning (ICL).\nFor complex reasoning tasks, selecting the right model for PEFT or ICL is\ncritical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K.\nHowever, current evaluation methods, based on metrics like F1 Score or\nreasoning chain assessments by larger models, overlook a key dimension:\nadaptability to unfamiliar situations and overcoming entrenched thinking\npatterns. In cognitive psychology, Mental Set refers to the tendency to persist\nwith previously successful strategies, even when they become inefficient - a\nchallenge for problem solving and reasoning. We compare the performance of LLM\nmodels like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-4o in the\npresence of mental sets. To the best of our knowledge, this is the first study\nto integrate cognitive psychology concepts into the evaluation of LLMs for\ncomplex reasoning tasks, providing deeper insights into their adaptability and\nproblem-solving efficacy."
                },
                "authors": [
                    {
                        "name": "Saiful Haq"
                    },
                    {
                        "name": "Niyati Chhaya"
                    },
                    {
                        "name": "Piyush Pandey"
                    },
                    {
                        "name": "Pushpak Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharya"
                },
                "author": "Pushpak Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10652v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10652v3",
                "updated": "2025-01-21T02:21:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    21,
                    4,
                    1,
                    21,
                    0
                ],
                "published": "2023-11-17T17:14:32Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    17,
                    14,
                    32,
                    4,
                    321,
                    0
                ],
                "title": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Lies Beneath? Exploring the Impact of Underlying AI Model Updates\n  in AI-Infused Systems"
                },
                "summary": "AI models are constantly evolving, with new versions released frequently.\nHuman-AI interaction guidelines encourage notifying users about changes in\nmodel capabilities, ideally supported by thorough benchmarking. However, as AI\nsystems integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical, often resulting in silent or minimally communicated\nupdates. This raises critical questions: Can users notice these updates? What\ncues do they rely on to distinguish between models? How do such changes affect\ntheir behavior and task performance? We address these questions through two\nstudies in the context of facial recognition for historical photo\nidentification: an online experiment examining users' ability to detect model\nupdates, followed by a diary study exploring perceptions in a real-world\ndeployment. Our findings highlight challenges in noticing AI model updates,\ntheir impact on downstream user behavior and performance, and how they lead\nusers to develop divergent folk theories. Drawing on these insights, we discuss\nstrategies for effectively communicating model updates in AI-infused systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models are constantly evolving, with new versions released frequently.\nHuman-AI interaction guidelines encourage notifying users about changes in\nmodel capabilities, ideally supported by thorough benchmarking. However, as AI\nsystems integrate into domain-specific workflows, exhaustive benchmarking can\nbecome impractical, often resulting in silent or minimally communicated\nupdates. This raises critical questions: Can users notice these updates? What\ncues do they rely on to distinguish between models? How do such changes affect\ntheir behavior and task performance? We address these questions through two\nstudies in the context of facial recognition for historical photo\nidentification: an online experiment examining users' ability to detect model\nupdates, followed by a diary study exploring perceptions in a real-world\ndeployment. Our findings highlight challenges in noticing AI model updates,\ntheir impact on downstream user behavior and performance, and how they lead\nusers to develop divergent folk theories. Drawing on these insights, we discuss\nstrategies for effectively communicating model updates in AI-infused systems."
                },
                "authors": [
                    {
                        "name": "Vikram Mohanty"
                    },
                    {
                        "name": "Jude Lim"
                    },
                    {
                        "name": "Kurt Luther"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Luther"
                },
                "author": "Kurt Luther",
                "arxiv_doi": "10.1145/3706598.3713751",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713751",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.10652v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10652v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACM CHI 2025 Conference on Human Factors in Computing\n  Systems",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11827v1",
                "updated": "2025-01-21T02:10:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    10,
                    50,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:10:50Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    10,
                    50,
                    1,
                    21,
                    0
                ],
                "title": "PXGen: A Post-hoc Explainable Method for Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PXGen: A Post-hoc Explainable Method for Generative Models"
                },
                "summary": "With the rapid growth of generative AI in numerous applications, explainable\nAI (XAI) plays a crucial role in ensuring the responsible development and\ndeployment of generative AI technologies. XAI has undergone notable\nadvancements and widespread adoption in recent years, reflecting a concerted\npush to enhance the transparency, interpretability, and credibility of AI\nsystems. Recent research emphasizes that a proficient XAI method should adhere\nto a set of criteria, primarily focusing on two key areas. Firstly, it should\nensure the quality and fluidity of explanations, encompassing aspects like\nfaithfulness, plausibility, completeness, and tailoring to individual needs.\nSecondly, the design principle of the XAI system or mechanism should cover the\nfollowing factors such as reliability, resilience, the verifiability of its\noutputs, and the transparency of its algorithm. However, research in XAI for\ngenerative models remains relatively scarce, with little exploration into how\nsuch methods can effectively meet these criteria in that domain. In this work,\nwe propose PXGen, a post-hoc explainable method for generative models. Given a\nmodel that needs to be explained, PXGen prepares two materials for the\nexplanation, the Anchor set and intrinsic & extrinsic criteria. Those materials\nare customizable by users according to their purpose and requirements. Via the\ncalculation of each criterion, each anchor has a set of feature values and\nPXGen provides examplebased explanation methods according to the feature values\namong all the anchors and illustrated and visualized to the users via tractable\nalgorithms such as k-dispersion or k-center.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of generative AI in numerous applications, explainable\nAI (XAI) plays a crucial role in ensuring the responsible development and\ndeployment of generative AI technologies. XAI has undergone notable\nadvancements and widespread adoption in recent years, reflecting a concerted\npush to enhance the transparency, interpretability, and credibility of AI\nsystems. Recent research emphasizes that a proficient XAI method should adhere\nto a set of criteria, primarily focusing on two key areas. Firstly, it should\nensure the quality and fluidity of explanations, encompassing aspects like\nfaithfulness, plausibility, completeness, and tailoring to individual needs.\nSecondly, the design principle of the XAI system or mechanism should cover the\nfollowing factors such as reliability, resilience, the verifiability of its\noutputs, and the transparency of its algorithm. However, research in XAI for\ngenerative models remains relatively scarce, with little exploration into how\nsuch methods can effectively meet these criteria in that domain. In this work,\nwe propose PXGen, a post-hoc explainable method for generative models. Given a\nmodel that needs to be explained, PXGen prepares two materials for the\nexplanation, the Anchor set and intrinsic & extrinsic criteria. Those materials\nare customizable by users according to their purpose and requirements. Via the\ncalculation of each criterion, each anchor has a set of feature values and\nPXGen provides examplebased explanation methods according to the feature values\namong all the anchors and illustrated and visualized to the users via tractable\nalgorithms such as k-dispersion or k-center."
                },
                "authors": [
                    {
                        "name": "Yen-Lung Huang"
                    },
                    {
                        "name": "Ming-Hsi Weng"
                    },
                    {
                        "name": "Hao-Tsung Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Tsung Yang"
                },
                "author": "Hao-Tsung Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08613v2",
                "updated": "2025-01-21T02:02:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    2,
                    39,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-15T06:22:35Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    6,
                    22,
                    35,
                    2,
                    15,
                    0
                ],
                "title": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Alignment of FOL Closeness Metrics with Human Judgement"
                },
                "summary": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent successful paradigm of solving logical reasoning problems with\ntool-augmented large language models (LLMs) leverages translation of natural\nlanguage statements into First-Order Logic~(FOL) and external theorem provers.\nHowever, the correctness of FOL statements, comprising operators and text\npredicates, often goes unverified due to the lack of a reliable evaluation\nmetric for comparing generated and ground-truth FOLs. In this paper, we present\na comprehensive study of sensitivity of existing metrics and their alignment\nwith human judgement on FOL evaluation. Using ground-truth FOLs, we carefully\ndesigned various perturbations on the ground-truth to assess metric\nsensitivity. We sample FOL translation candidates for natural language\nstatements and measure the ranking alignment between automatic metrics and\nhuman annotators. Our empirical findings highlight oversensitivity in the\nn-gram metric BLEU for text perturbations, the semantic graph metric Smatch++\nfor structural perturbations, and FOL metric for operator perturbation. We also\nobserve a closer alignment between BertScore and human judgement. Additionally,\nwe show that combining metrics enhances both alignment and sensitivity compared\nto using individual metrics."
                },
                "authors": [
                    {
                        "name": "Ramya Keerthy Thatikonda"
                    },
                    {
                        "name": "Wray Buntine"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    }
                ],
                "author_detail": {
                    "name": "Ehsan Shareghi"
                },
                "author": "Ehsan Shareghi",
                "arxiv_comment": "Code: https://github.com/RamyaKeerthy/AlignmentFOL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06465v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06465v2",
                "updated": "2025-01-21T01:56:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    1,
                    56,
                    11,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-11T07:35:51Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    7,
                    35,
                    51,
                    5,
                    11,
                    0
                ],
                "title": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCT: A Clinical Terminology Graph for Generative AI Applications in\n  Healthcare"
                },
                "summary": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the world's first clinical terminology for the Chinese\nhealthcare community, namely MedCT, accompanied by a clinical foundation model\nMedBERT and an entity linking model MedLink. The MedCT system enables\nstandardized and programmable representation of Chinese clinical data,\nsuccessively stimulating the development of new medicines, treatment pathways,\nand better patient outcomes for the populous Chinese community. Moreover, the\nMedCT knowledge graph provides a principled mechanism to minimize the\nhallucination problem of large language models (LLMs), therefore achieving\nsignificant levels of accuracy and safety in LLM-based clinical applications.\nBy leveraging the LLMs' emergent capabilities of generativeness and\nexpressiveness, we were able to rapidly built a production-quality terminology\nsystem and deployed to real-world clinical field within three months, while\nclassical terminologies like SNOMED CT have gone through more than twenty years\ndevelopment. Our experiments show that the MedCT system achieves\nstate-of-the-art (SOTA) performance in semantic matching and entity linking\ntasks, not only for Chinese but also for English. We also conducted a\nlongitudinal field experiment by applying MedCT and LLMs in a representative\nspectrum of clinical tasks, including electronic health record (EHR)\nauto-generation and medical document search for diagnostic decision making. Our\nstudy shows a multitude of values of MedCT for clinical workflows and patient\noutcomes, especially in the new genre of clinical LLM applications. We present\nour approach in sufficient engineering detail, such that implementing a\nclinical terminology for other non-English societies should be readily\nreproducible. We openly release our terminology, models and algorithms, along\nwith real-world clinical datasets for the development."
                },
                "authors": [
                    {
                        "name": "Ye Chen"
                    },
                    {
                        "name": "Dongdong Huang"
                    },
                    {
                        "name": "Haoyun Xu"
                    },
                    {
                        "name": "Cong Fu"
                    },
                    {
                        "name": "Lin Sheng"
                    },
                    {
                        "name": "Qingli Zhou"
                    },
                    {
                        "name": "Yuqiang Shen"
                    },
                    {
                        "name": "Kai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Kai Wang"
                },
                "author": "Kai Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06465v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09672v2",
                "updated": "2025-01-21T01:04:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    1,
                    4,
                    52,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-16T17:08:12Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    17,
                    8,
                    12,
                    3,
                    16,
                    0
                ],
                "title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP\n  Evaluation Benchmark"
                },
                "summary": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Vision-Language Models (VLMs) in the past several years\ncalls for rigorous and comprehensive evaluation methods and benchmarks. This\nwork analyzes existing VLM evaluation techniques, including automated metrics,\nAI-based assessments, and human evaluations across diverse tasks. We first\nintroduce Robin - a novel suite of VLMs that we built by combining Large\nLanguage Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use\nRobin to identify shortcomings of current evaluation approaches across scales.\nNext, to overcome the identified limitations, we introduce CHIRP - a new long\nform response benchmark we developed for more robust and complete VLM\nevaluation. We provide open access to the Robin training code, model suite, and\nCHIRP benchmark to promote reproducibility and advance VLM research."
                },
                "authors": [
                    {
                        "name": "Alexis Roger"
                    },
                    {
                        "name": "Prateek Humane"
                    },
                    {
                        "name": "Daniel Z. Kaplan"
                    },
                    {
                        "name": "Kshitij Gupta"
                    },
                    {
                        "name": "Qi Sun"
                    },
                    {
                        "name": "George Adamopoulos"
                    },
                    {
                        "name": "Jonathan Siu Chi Lim"
                    },
                    {
                        "name": "Quentin Anthony"
                    },
                    {
                        "name": "Edwin Fennell"
                    },
                    {
                        "name": "Irina Rish"
                    }
                ],
                "author_detail": {
                    "name": "Irina Rish"
                },
                "author": "Irina Rish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08496v2",
                "updated": "2025-01-21T01:01:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    1,
                    1,
                    37,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-14T23:59:23Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    59,
                    23,
                    1,
                    14,
                    0
                ],
                "title": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Importance of Data Alignment in Downstream Model\n  Performance"
                },
                "summary": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrary to the conventional emphasis on dataset size, we explore the role of\ndata alignment -- an often overlooked aspect of data quality -- in training\ncapable Large Language Models (LLMs). To do so, we use the Task2Vec-based\nalignment coefficient, a quantitative measure of the similarity between two\ndatasets, to quantify the impact of alignment between training data and\nevaluation data on downstream performance. In particular, we conduct controlled\n\\textit{interventional} experiments for two settings: 1. the impact of\nincreased alignment coefficients between various pre-training (pt) against\nevaluation datasets, and 2. the impact of increased alignment coefficients\nbetween domain specific fine-tuning (ft) against domain specific evaluation.\nThe domain specific task we explore is Autoformalization -- the machine\ntranslation task between natural language and code for formal verification. In\nboth settings, we find a strong, predictable negative correlation between the\nalignment coefficient of a model's training and evaluation data and the model's\nloss/perplexity on the respective downstream task. These findings suggest a\nre-evaluation of LLM training approaches, demonstrating the relevance of data\nalignment compared to data quantity, especially in specialized downstream tasks\nsuch as Autoformalization."
                },
                "authors": [
                    {
                        "name": "Krrish Chawla"
                    },
                    {
                        "name": "Aryan Sahai"
                    },
                    {
                        "name": "Mario DePavia"
                    },
                    {
                        "name": "Sudharsan Sundar"
                    },
                    {
                        "name": "Brando Miranda"
                    }
                ],
                "author_detail": {
                    "name": "Brando Miranda"
                },
                "author": "Brando Miranda",
                "arxiv_journal_ref": "ICLR DMLR Data-centric Machine Learning Research (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11790v1",
                "updated": "2025-01-20T23:41:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    41,
                    22,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:41:22Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    41,
                    22,
                    0,
                    20,
                    0
                ],
                "title": "Benchmarking Large Language Models via Random Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models via Random Variables"
                },
                "summary": "With the continuous advancement of large language models (LLMs) in\nmathematical reasoning, evaluating their performance in this domain has become\na prominent research focus. Recent studies have raised concerns about the\nreliability of current mathematical benchmarks, highlighting issues such as\nsimplistic design and potential data leakage. Therefore, creating a reliable\nbenchmark that effectively evaluates the genuine capabilities of LLMs in\nmathematical reasoning remains a significant challenge. To address this, we\npropose RV-Bench, a framework for Benchmarking LLMs via Random Variables in\nmathematical reasoning. Specifically, the background content of a random\nvariable question (RV question) mirrors the original problem in existing\nstandard benchmarks, but the variable combinations are randomized into\ndifferent values. LLMs must fully understand the problem-solving process for\nthe original problem to correctly answer RV questions with various combinations\nof variable values. As a result, the LLM's genuine capability in mathematical\nreasoning is reflected by its accuracy on RV-Bench. Extensive experiments are\nconducted with 29 representative LLMs across 900+ RV questions. A leaderboard\nfor RV-Bench ranks the genuine capability of these LLMs. Further analysis of\naccuracy dropping indicates that current LLMs still struggle with complex\nmathematical reasoning problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous advancement of large language models (LLMs) in\nmathematical reasoning, evaluating their performance in this domain has become\na prominent research focus. Recent studies have raised concerns about the\nreliability of current mathematical benchmarks, highlighting issues such as\nsimplistic design and potential data leakage. Therefore, creating a reliable\nbenchmark that effectively evaluates the genuine capabilities of LLMs in\nmathematical reasoning remains a significant challenge. To address this, we\npropose RV-Bench, a framework for Benchmarking LLMs via Random Variables in\nmathematical reasoning. Specifically, the background content of a random\nvariable question (RV question) mirrors the original problem in existing\nstandard benchmarks, but the variable combinations are randomized into\ndifferent values. LLMs must fully understand the problem-solving process for\nthe original problem to correctly answer RV questions with various combinations\nof variable values. As a result, the LLM's genuine capability in mathematical\nreasoning is reflected by its accuracy on RV-Bench. Extensive experiments are\nconducted with 29 representative LLMs across 900+ RV questions. A leaderboard\nfor RV-Bench ranks the genuine capability of these LLMs. Further analysis of\naccuracy dropping indicates that current LLMs still struggle with complex\nmathematical reasoning problems."
                },
                "authors": [
                    {
                        "name": "Zijin Hong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Su Dong"
                    },
                    {
                        "name": "Junnan Dong"
                    },
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Yujing Zhang"
                    },
                    {
                        "name": "Zhu Wang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14596v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14596v5",
                "updated": "2025-01-20T23:33:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    33,
                    33,
                    0,
                    20,
                    0
                ],
                "published": "2024-06-20T17:45:02Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    45,
                    2,
                    3,
                    172,
                    0
                ],
                "title": "VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLM Agents Generate Their Own Memories: Distilling Experience into\n  Embodied Programs of Thought"
                },
                "summary": "Large-scale LLMs and VLMs excel at few-shot learning but require high-quality\nexamples. We introduce In-Context Abstraction Learning (ICAL), which\niteratively refines suboptimal trajectories into high-quality data with\noptimized actions and detailed reasoning. Given an inefficient demonstration, a\nVLM corrects actions and annotates causal relationships, object states,\nsubgoals, and task-relevant visuals, forming \"programs of thought.\" With human\nfeedback, these programs are improved as the agent executes them in a similar\nenvironment. The resulting examples, used as prompt context or fine-tuning\ndata, significantly boost decision-making while reducing human feedback needs.\nICAL surpasses state-of-the-art in TEACh (dialogue-based instruction\nfollowing), VisualWebArena (multimodal web agents), and Ego4D (egocentric video\naction anticipation). In TEACh, combining fine-tuning and retrieval on ICAL\nexamples outperforms raw human demonstrations and expert examples, achieving a\n17.5% increase in goal-condition success. In VisualWebArena,\nretrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over\nGPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL\noutperforms few-shot GPT-4V and remains competitive with supervised models.\nOverall, ICAL scales 2x better than raw human demonstrations and reduces manual\nprompt engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale LLMs and VLMs excel at few-shot learning but require high-quality\nexamples. We introduce In-Context Abstraction Learning (ICAL), which\niteratively refines suboptimal trajectories into high-quality data with\noptimized actions and detailed reasoning. Given an inefficient demonstration, a\nVLM corrects actions and annotates causal relationships, object states,\nsubgoals, and task-relevant visuals, forming \"programs of thought.\" With human\nfeedback, these programs are improved as the agent executes them in a similar\nenvironment. The resulting examples, used as prompt context or fine-tuning\ndata, significantly boost decision-making while reducing human feedback needs.\nICAL surpasses state-of-the-art in TEACh (dialogue-based instruction\nfollowing), VisualWebArena (multimodal web agents), and Ego4D (egocentric video\naction anticipation). In TEACh, combining fine-tuning and retrieval on ICAL\nexamples outperforms raw human demonstrations and expert examples, achieving a\n17.5% increase in goal-condition success. In VisualWebArena,\nretrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over\nGPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL\noutperforms few-shot GPT-4V and remains competitive with supervised models.\nOverall, ICAL scales 2x better than raw human demonstrations and reduces manual\nprompt engineering."
                },
                "authors": [
                    {
                        "name": "Gabriel Sarch"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Michael J. Tarr"
                    },
                    {
                        "name": "William W. Cohen"
                    },
                    {
                        "name": "Kenneth Marino"
                    },
                    {
                        "name": "Katerina Fragkiadaki"
                    }
                ],
                "author_detail": {
                    "name": "Katerina Fragkiadaki"
                },
                "author": "Katerina Fragkiadaki",
                "arxiv_comment": "Project website: https://ical-learning.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14596v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14596v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11786v1",
                "updated": "2025-01-20T23:19:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    19,
                    15,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:19:15Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    19,
                    15,
                    0,
                    20,
                    0
                ],
                "title": "Synthetic Data Can Mislead Evaluations: Membership Inference as Machine\n  Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Can Mislead Evaluations: Membership Inference as Machine\n  Text Detection"
                },
                "summary": "Recent work shows membership inference attacks (MIAs) on large language\nmodels (LLMs) produce inconclusive results, partly due to difficulties in\ncreating non-member datasets without temporal shifts. While researchers have\nturned to synthetic data as an alternative, we show this approach can be\nfundamentally misleading. Our experiments indicate that MIAs function as\nmachine-generated text detectors, incorrectly identifying synthetic data as\ntraining samples regardless of the data source. This behavior persists across\ndifferent model architectures and sizes, from open-source models to commercial\nones such as GPT-3.5. Even synthetic text generated by different, potentially\nlarger models is classified as training data by the target model. Our findings\nhighlight a serious concern: using synthetic data in membership evaluations may\nlead to false conclusions about model memorization and data leakage. We caution\nthat this issue could affect other evaluations using model signals such as loss\nwhere synthetic or machine-generated translated data substitutes for real-world\nsamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work shows membership inference attacks (MIAs) on large language\nmodels (LLMs) produce inconclusive results, partly due to difficulties in\ncreating non-member datasets without temporal shifts. While researchers have\nturned to synthetic data as an alternative, we show this approach can be\nfundamentally misleading. Our experiments indicate that MIAs function as\nmachine-generated text detectors, incorrectly identifying synthetic data as\ntraining samples regardless of the data source. This behavior persists across\ndifferent model architectures and sizes, from open-source models to commercial\nones such as GPT-3.5. Even synthetic text generated by different, potentially\nlarger models is classified as training data by the target model. Our findings\nhighlight a serious concern: using synthetic data in membership evaluations may\nlead to false conclusions about model memorization and data leakage. We caution\nthat this issue could affect other evaluations using model signals such as loss\nwhere synthetic or machine-generated translated data substitutes for real-world\nsamples."
                },
                "authors": [
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Mireshghallah"
                },
                "author": "Niloofar Mireshghallah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11776v1",
                "updated": "2025-01-20T22:44:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    22,
                    44,
                    53,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T22:44:53Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    22,
                    44,
                    53,
                    0,
                    20,
                    0
                ],
                "title": "EfficientVITON: An Efficient Virtual Try-On Model using Optimized\n  Diffusion Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVITON: An Efficient Virtual Try-On Model using Optimized\n  Diffusion Process"
                },
                "summary": "Would not it be much more convenient for everybody to try on clothes by only\nlooking into a mirror ? The answer to that problem is virtual try-on, enabling\nusers to digitally experiment with outfits. The core challenge lies in\nrealistic image-to-image translation, where clothing must fit diverse human\nforms, poses, and figures. Early methods, which used 2D transformations,\noffered speed, but image quality was often disappointing and lacked the nuance\nof deep learning. Though GAN-based techniques enhanced realism, their\ndependence on paired data proved limiting. More adaptable methods offered great\nvisuals but demanded significant computing power and time. Recent advances in\ndiffusion models have shown promise for high-fidelity translation, yet the\ncurrent crop of virtual try-on tools still struggle with detail loss and\nwarping issues. To tackle these challenges, this paper proposes EfficientVITON,\na new virtual try-on system leveraging the impressive pre-trained Stable\nDiffusion model for better images and deployment feasibility. The system\nincludes a spatial encoder to maintain clothings finer details and zero\ncross-attention blocks to capture the subtleties of how clothes fit a human\nbody. Input images are carefully prepared, and the diffusion process has been\ntweaked to significantly cut generation time without image quality loss. The\ntraining process involves two distinct stages of fine-tuning, carefully\nincorporating a balance of loss functions to ensure both accurate try-on\nresults and high-quality visuals. Rigorous testing on the VITON-HD dataset,\nsupplemented with real-world examples, has demonstrated that EfficientVITON\nachieves state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would not it be much more convenient for everybody to try on clothes by only\nlooking into a mirror ? The answer to that problem is virtual try-on, enabling\nusers to digitally experiment with outfits. The core challenge lies in\nrealistic image-to-image translation, where clothing must fit diverse human\nforms, poses, and figures. Early methods, which used 2D transformations,\noffered speed, but image quality was often disappointing and lacked the nuance\nof deep learning. Though GAN-based techniques enhanced realism, their\ndependence on paired data proved limiting. More adaptable methods offered great\nvisuals but demanded significant computing power and time. Recent advances in\ndiffusion models have shown promise for high-fidelity translation, yet the\ncurrent crop of virtual try-on tools still struggle with detail loss and\nwarping issues. To tackle these challenges, this paper proposes EfficientVITON,\na new virtual try-on system leveraging the impressive pre-trained Stable\nDiffusion model for better images and deployment feasibility. The system\nincludes a spatial encoder to maintain clothings finer details and zero\ncross-attention blocks to capture the subtleties of how clothes fit a human\nbody. Input images are carefully prepared, and the diffusion process has been\ntweaked to significantly cut generation time without image quality loss. The\ntraining process involves two distinct stages of fine-tuning, carefully\nincorporating a balance of loss functions to ensure both accurate try-on\nresults and high-quality visuals. Rigorous testing on the VITON-HD dataset,\nsupplemented with real-world examples, has demonstrated that EfficientVITON\nachieves state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Mostafa Atef"
                    },
                    {
                        "name": "Mariam Ayman"
                    },
                    {
                        "name": "Ahmed Rashed"
                    },
                    {
                        "name": "Ashrakat Saeed"
                    },
                    {
                        "name": "Abdelrahman Saeed"
                    },
                    {
                        "name": "Ahmed Fares"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Fares"
                },
                "author": "Ahmed Fares",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02038v2",
                "updated": "2025-01-20T22:24:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    22,
                    24,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2024-09-03T16:37:45Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    37,
                    45,
                    1,
                    247,
                    0
                ],
                "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BEAVER: An Enterprise Benchmark for Text-to-SQL"
                },
                "summary": "Existing text-to-SQL benchmarks have largely been constructed from web tables\nwith human-generated question-SQL pairs. LLMs typically show strong results on\nthese benchmarks, leading to a belief that LLMs are effective at text-to-SQL\ntasks. However, how these results transfer to enterprise settings is unclear\nbecause tables in enterprise databases might differ substantially from web\ntables in structure and content. To contend with this problem, we introduce a\nnew dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from\nreal private enterprise data warehouses. This dataset includes natural language\nqueries and their correct SQL statements, which we collected from actual query\nlogs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform\npoorly, even when augmented with standard prompt engineering and RAG\ntechniques. We identify three main reasons for the poor performance: (1)\nschemas of enterprise tables are more complex than the schemas in public data,\nresulting in SQL-generation tasks intrinsically harder; (2) business-oriented\nquestions are often more complex, requiring joins over multiple tables,\naggregations, and nested queries; (3) public LLMs cannot train on private\nenterprise data warehouses that are not publicly accessible, and therefore it\nis difficult for the model to learn to solve (1) and (2). We believe BEAVER\nwill facilitate future research in building text-to-SQL systems that perform\nbetter in enterprise settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing text-to-SQL benchmarks have largely been constructed from web tables\nwith human-generated question-SQL pairs. LLMs typically show strong results on\nthese benchmarks, leading to a belief that LLMs are effective at text-to-SQL\ntasks. However, how these results transfer to enterprise settings is unclear\nbecause tables in enterprise databases might differ substantially from web\ntables in structure and content. To contend with this problem, we introduce a\nnew dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from\nreal private enterprise data warehouses. This dataset includes natural language\nqueries and their correct SQL statements, which we collected from actual query\nlogs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform\npoorly, even when augmented with standard prompt engineering and RAG\ntechniques. We identify three main reasons for the poor performance: (1)\nschemas of enterprise tables are more complex than the schemas in public data,\nresulting in SQL-generation tasks intrinsically harder; (2) business-oriented\nquestions are often more complex, requiring joins over multiple tables,\naggregations, and nested queries; (3) public LLMs cannot train on private\nenterprise data warehouses that are not publicly accessible, and therefore it\nis difficult for the model to learn to solve (1) and (2). We believe BEAVER\nwill facilitate future research in building text-to-SQL systems that perform\nbetter in enterprise settings."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Fabian Wenz"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Devin Yang"
                    },
                    {
                        "name": "Justin Choi"
                    },
                    {
                        "name": "Nesime Tatbul"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Çağatay Demiralp"
                    },
                    {
                        "name": "Michael Stonebraker"
                    }
                ],
                "author_detail": {
                    "name": "Michael Stonebraker"
                },
                "author": "Michael Stonebraker",
                "arxiv_comment": "Dataset and code are available at\n  https://peterbaile.github.io/beaver/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05970v2",
                "updated": "2025-01-20T21:45:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    45,
                    19,
                    0,
                    20,
                    0
                ],
                "published": "2024-10-08T12:17:42Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    12,
                    17,
                    42,
                    1,
                    282,
                    0
                ],
                "title": "PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with\n  End-to-End Sparse Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with\n  End-to-End Sparse Sampling"
                },
                "summary": "Multimodal document understanding is a challenging task to process and\ncomprehend large amounts of textual and visual information. Recent advances in\nLarge Language Models (LLMs) have significantly improved the performance of\nthis task. However, existing methods typically focus on either plain text or a\nlimited number of document images, struggling to handle long PDF documents with\ninterleaved text and images, especially for academic papers. In this paper, we\nintroduce PDF-WuKong, a multimodal large language model (MLLM) which is\ndesigned to enhance multimodal question-answering (QA) for long PDF documents.\nPDF-WuKong incorporates a sparse sampler that operates on both text and image\nrepresentations, significantly improving the efficiency and capability of the\nMLLM. The sparse sampler is integrated with the MLLM's image encoder and\nselects the paragraphs or diagrams most pertinent to user queries for\nprocessing by the language model. To effectively train and evaluate our model,\nwe construct PaperPDF, a dataset consisting of a broad collection of English\nand Chinese academic papers. Multiple strategies are proposed to automatically\ngenerate 1.1 million QA pairs along with their corresponding evidence sources.\nExperimental results demonstrate the superiority and high efficiency of our\napproach over other models on the task of long multimodal document\nunderstanding, surpassing proprietary products by an average of 8.6% on F1. Our\ncode and dataset will be released at https://github.com/yh-hust/PDF-Wukong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal document understanding is a challenging task to process and\ncomprehend large amounts of textual and visual information. Recent advances in\nLarge Language Models (LLMs) have significantly improved the performance of\nthis task. However, existing methods typically focus on either plain text or a\nlimited number of document images, struggling to handle long PDF documents with\ninterleaved text and images, especially for academic papers. In this paper, we\nintroduce PDF-WuKong, a multimodal large language model (MLLM) which is\ndesigned to enhance multimodal question-answering (QA) for long PDF documents.\nPDF-WuKong incorporates a sparse sampler that operates on both text and image\nrepresentations, significantly improving the efficiency and capability of the\nMLLM. The sparse sampler is integrated with the MLLM's image encoder and\nselects the paragraphs or diagrams most pertinent to user queries for\nprocessing by the language model. To effectively train and evaluate our model,\nwe construct PaperPDF, a dataset consisting of a broad collection of English\nand Chinese academic papers. Multiple strategies are proposed to automatically\ngenerate 1.1 million QA pairs along with their corresponding evidence sources.\nExperimental results demonstrate the superiority and high efficiency of our\napproach over other models on the task of long multimodal document\nunderstanding, surpassing proprietary products by an average of 8.6% on F1. Our\ncode and dataset will be released at https://github.com/yh-hust/PDF-Wukong."
                },
                "authors": [
                    {
                        "name": "Xudong Xie"
                    },
                    {
                        "name": "Hao Yan"
                    },
                    {
                        "name": "Liang Yin"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jing Ding"
                    },
                    {
                        "name": "Minghui Liao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11759v1",
                "updated": "2025-01-20T21:38:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    38,
                    36,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T21:38:36Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    38,
                    36,
                    0,
                    20,
                    0
                ],
                "title": "Poison-RAG: Adversarial Data Poisoning Attacks on Retrieval-Augmented\n  Generation in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Poison-RAG: Adversarial Data Poisoning Attacks on Retrieval-Augmented\n  Generation in Recommender Systems"
                },
                "summary": "This study presents Poison-RAG, a framework for adversarial data poisoning\nattacks targeting retrieval-augmented generation (RAG)-based recommender\nsystems. Poison-RAG manipulates item metadata, such as tags and descriptions,\nto influence recommendation outcomes. Using item metadata generated through a\nlarge language model (LLM) and embeddings derived via the OpenAI API, we\nexplore the impact of adversarial poisoning attacks on provider-side, where\nattacks are designed to promote long-tail items and demote popular ones. Two\nattack strategies are proposed: local modifications, which personalize tags for\neach item using BERT embeddings, and global modifications, applying uniform\ntags across the dataset. Experiments conducted on the MovieLens dataset in a\nblack-box setting reveal that local strategies improve manipulation\neffectiveness by up to 50\\%, while global strategies risk boosting already\npopular items. Results indicate that popular items are more susceptible to\nattacks, whereas long-tail items are harder to manipulate. Approximately 70\\%\nof items lack tags, presenting a cold-start challenge; data augmentation and\nsynthesis are proposed as potential defense mechanisms to enhance RAG-based\nsystems' resilience. The findings emphasize the need for robust metadata\nmanagement to safeguard recommendation frameworks. Code and data are available\nat https://github.com/atenanaz/Poison-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents Poison-RAG, a framework for adversarial data poisoning\nattacks targeting retrieval-augmented generation (RAG)-based recommender\nsystems. Poison-RAG manipulates item metadata, such as tags and descriptions,\nto influence recommendation outcomes. Using item metadata generated through a\nlarge language model (LLM) and embeddings derived via the OpenAI API, we\nexplore the impact of adversarial poisoning attacks on provider-side, where\nattacks are designed to promote long-tail items and demote popular ones. Two\nattack strategies are proposed: local modifications, which personalize tags for\neach item using BERT embeddings, and global modifications, applying uniform\ntags across the dataset. Experiments conducted on the MovieLens dataset in a\nblack-box setting reveal that local strategies improve manipulation\neffectiveness by up to 50\\%, while global strategies risk boosting already\npopular items. Results indicate that popular items are more susceptible to\nattacks, whereas long-tail items are harder to manipulate. Approximately 70\\%\nof items lack tags, presenting a cold-start challenge; data augmentation and\nsynthesis are proposed as potential defense mechanisms to enhance RAG-based\nsystems' resilience. The findings emphasize the need for robust metadata\nmanagement to safeguard recommendation frameworks. Code and data are available\nat https://github.com/atenanaz/Poison-RAG."
                },
                "authors": [
                    {
                        "name": "Fatemeh Nazary"
                    },
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Tommaso di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso di Noia"
                },
                "author": "Tommaso di Noia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11747v1",
                "updated": "2025-01-20T21:10:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    10,
                    22,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T21:10:22Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    10,
                    22,
                    0,
                    20,
                    0
                ],
                "title": "Optimizing Pretraining Data Mixtures with LLM-Estimated Utility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Pretraining Data Mixtures with LLM-Estimated Utility"
                },
                "summary": "Large Language Models improve with increasing amounts of high-quality\ntraining data. However, leveraging larger datasets requires balancing quality,\nquantity, and diversity across sources. After evaluating nine baseline methods\nunder both compute- and data-constrained scenarios, we find token-count\nheuristics outperform manual and learned mixes, indicating that simple\napproaches accounting for dataset size and diversity are surprisingly\neffective. Building on this insight, we propose two complementary approaches:\nUtiliMax, which extends token-based heuristics by incorporating utility\nestimates from reduced-scale ablations, achieving up to a 10.6x speedup over\nmanual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs\nto estimate data utility from small samples, matching ablation-based\nperformance while reducing computational requirements by $\\sim$200x. Together,\nthese approaches establish a new framework for automated, compute-efficient\ndata mixing that is robust across training regimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models improve with increasing amounts of high-quality\ntraining data. However, leveraging larger datasets requires balancing quality,\nquantity, and diversity across sources. After evaluating nine baseline methods\nunder both compute- and data-constrained scenarios, we find token-count\nheuristics outperform manual and learned mixes, indicating that simple\napproaches accounting for dataset size and diversity are surprisingly\neffective. Building on this insight, we propose two complementary approaches:\nUtiliMax, which extends token-based heuristics by incorporating utility\nestimates from reduced-scale ablations, achieving up to a 10.6x speedup over\nmanual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs\nto estimate data utility from small samples, matching ablation-based\nperformance while reducing computational requirements by $\\sim$200x. Together,\nthese approaches establish a new framework for automated, compute-efficient\ndata mixing that is robust across training regimes."
                },
                "authors": [
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Bhargavi Paranjape"
                    },
                    {
                        "name": "Punit Singh Koura"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Frank Zhang"
                    },
                    {
                        "name": "Todor Mihaylov"
                    }
                ],
                "author_detail": {
                    "name": "Todor Mihaylov"
                },
                "author": "Todor Mihaylov",
                "arxiv_comment": "10 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11721v1",
                "updated": "2025-01-20T20:07:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    20,
                    7,
                    18,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T20:07:18Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    20,
                    7,
                    18,
                    0,
                    20,
                    0
                ],
                "title": "Explain-Query-Test: Self-Evaluating LLMs Via Explanation and\n  Comprehension Discrepancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain-Query-Test: Self-Evaluating LLMs Via Explanation and\n  Comprehension Discrepancy"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in\ngenerating detailed and coherent explanations of complex concepts. However, the\nextent to which these models truly comprehend the concepts they articulate\nremains unclear. To assess the level of comprehension of a model relative to\nthe content it generates, we implemented a self-evaluation pipeline where\nmodels: (i) given a topic generate an excerpt with information about the topic,\n(ii) given an excerpt generate question-answer pairs, and finally (iii) given a\nquestion generate an answer. We refer to this self-evaluation approach as\nExplain-Query-Test (EQT). Interestingly, the accuracy on generated questions\nresulting from running the EQT pipeline correlates strongly with the model\nperformance as verified by typical benchmarks such as MMLU-Pro. In other words,\nEQT's performance is predictive of MMLU-Pro's, and EQT can be used to rank\nmodels without the need for any external source of evaluation data other than\nlists of topics of interest. Moreover, our results reveal a disparity between\nthe models' ability to produce detailed explanations and their performance on\nquestions related to those explanations. This gap highlights fundamental\nlimitations in the internal knowledge representation and reasoning abilities of\ncurrent LLMs. We release the code at https://github.com/asgsaeid/EQT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in\ngenerating detailed and coherent explanations of complex concepts. However, the\nextent to which these models truly comprehend the concepts they articulate\nremains unclear. To assess the level of comprehension of a model relative to\nthe content it generates, we implemented a self-evaluation pipeline where\nmodels: (i) given a topic generate an excerpt with information about the topic,\n(ii) given an excerpt generate question-answer pairs, and finally (iii) given a\nquestion generate an answer. We refer to this self-evaluation approach as\nExplain-Query-Test (EQT). Interestingly, the accuracy on generated questions\nresulting from running the EQT pipeline correlates strongly with the model\nperformance as verified by typical benchmarks such as MMLU-Pro. In other words,\nEQT's performance is predictive of MMLU-Pro's, and EQT can be used to rank\nmodels without the need for any external source of evaluation data other than\nlists of topics of interest. Moreover, our results reveal a disparity between\nthe models' ability to produce detailed explanations and their performance on\nquestions related to those explanations. This gap highlights fundamental\nlimitations in the internal knowledge representation and reasoning abilities of\ncurrent LLMs. We release the code at https://github.com/asgsaeid/EQT."
                },
                "authors": [
                    {
                        "name": "Saeid Asgari Taghanaki"
                    },
                    {
                        "name": "Joao Monteiro"
                    }
                ],
                "author_detail": {
                    "name": "Joao Monteiro"
                },
                "author": "Joao Monteiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07676v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07676v2",
                "updated": "2025-01-20T20:05:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    20,
                    5,
                    35,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-13T20:24:10Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    20,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "title": "Smells-sus: Sustainability Smells in IaC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smells-sus: Sustainability Smells in IaC"
                },
                "summary": "Practitioners use Infrastructure as Code (IaC) scripts to efficiently\nconfigure IT infrastructures through machine-readable definition files.\nHowever, during the development of these scripts, some code patterns or\ndeployment choices may lead to sustainability issues like inefficient resource\nutilization or redundant provisioning for example. We call this type of\npatterns sustainability smells. These inefficiencies pose significant\nenvironmental and financial challenges, given the growing scale of cloud\ncomputing. This research focuses on Terraform, a widely adopted IaC tool. Our\nstudy involves defining seven sustainability smells and validating them through\na survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform\nscripts from 395 open-source repositories. We performed a detailed qualitative\nanalysis of a randomly sampled 1,860 Terraform scripts from the original\ndataset to identify code patterns that correspond to the sustainability smells\nand used the other 26,467 Terraform scripts to study the prevalence of the\ndefined sustainability smells. Our results indicate varying prevalence rates of\nthese smells across the dataset. The most prevalent smell is Monolithic\nInfrastructure, which appears in 9.67\\% of the scripts. Additionally, our\nfindings highlight the complexity of conducting root cause analysis for\nsustainability issues, as these smells often arise from a confluence of script\nstructures, configuration choices, and deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners use Infrastructure as Code (IaC) scripts to efficiently\nconfigure IT infrastructures through machine-readable definition files.\nHowever, during the development of these scripts, some code patterns or\ndeployment choices may lead to sustainability issues like inefficient resource\nutilization or redundant provisioning for example. We call this type of\npatterns sustainability smells. These inefficiencies pose significant\nenvironmental and financial challenges, given the growing scale of cloud\ncomputing. This research focuses on Terraform, a widely adopted IaC tool. Our\nstudy involves defining seven sustainability smells and validating them through\na survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform\nscripts from 395 open-source repositories. We performed a detailed qualitative\nanalysis of a randomly sampled 1,860 Terraform scripts from the original\ndataset to identify code patterns that correspond to the sustainability smells\nand used the other 26,467 Terraform scripts to study the prevalence of the\ndefined sustainability smells. Our results indicate varying prevalence rates of\nthese smells across the dataset. The most prevalent smell is Monolithic\nInfrastructure, which appears in 9.67\\% of the scripts. Additionally, our\nfindings highlight the complexity of conducting root cause analysis for\nsustainability issues, as these smells often arise from a confluence of script\nstructures, configuration choices, and deployment contexts."
                },
                "authors": [
                    {
                        "name": "Seif Kosbar"
                    },
                    {
                        "name": "Mohammad Hamdaqa"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Hamdaqa"
                },
                "author": "Mohammad Hamdaqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07676v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07676v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11709v1",
                "updated": "2025-01-20T19:41:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    41,
                    42,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T19:41:42Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    41,
                    42,
                    0,
                    20,
                    0
                ],
                "title": "Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue\n  Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue\n  Resolution"
                },
                "summary": "Large language models (LLMs) have become essential in software development,\nespecially for issue resolution. However, despite their widespread use,\nsignificant challenges persist in the quality of LLM responses to issue\nresolution queries. LLM interactions often yield incorrect, incomplete, or\nambiguous information, largely due to knowledge gaps in prompt design, which\ncan lead to unproductive exchanges and reduced developer productivity. In this\npaper, we analyze 433 developer-ChatGPT conversations within GitHub issue\nthreads to examine the impact of prompt knowledge gaps and conversation styles\non issue resolution. We identify four main knowledge gaps in developer prompts:\nMissing Context, Missing Specifications, Multiple Context, and Unclear\nInstructions. Assuming that conversations within closed issues contributed to\nsuccessful resolutions while those in open issues did not, we find that\nineffective conversations contain knowledge gaps in 54.7% of prompts, compared\nto only 13.2% in effective ones. Additionally, we observe seven distinct\nconversational styles, with Directive Prompting, Chain of Thought, and\nResponsive Feedback being the most prevalent. We find that knowledge gaps are\npresent in all styles of conversations, with Missing Context being the most\nrepeated challenge developers face in issue-resolution conversations. Based on\nour analysis, we identify key textual and code related heuristics-Specificity,\nContextual Richness, and Clarity-that are associated with successful issue\nclosure and help assess prompt quality. These heuristics lay the foundation for\nan automated tool that can dynamically flag unclear prompts and suggest\nstructured improvements. To test feasibility, we developed a lightweight\nbrowser extension prototype for detecting prompt gaps, that can be easily\nadapted to other tools within developer workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become essential in software development,\nespecially for issue resolution. However, despite their widespread use,\nsignificant challenges persist in the quality of LLM responses to issue\nresolution queries. LLM interactions often yield incorrect, incomplete, or\nambiguous information, largely due to knowledge gaps in prompt design, which\ncan lead to unproductive exchanges and reduced developer productivity. In this\npaper, we analyze 433 developer-ChatGPT conversations within GitHub issue\nthreads to examine the impact of prompt knowledge gaps and conversation styles\non issue resolution. We identify four main knowledge gaps in developer prompts:\nMissing Context, Missing Specifications, Multiple Context, and Unclear\nInstructions. Assuming that conversations within closed issues contributed to\nsuccessful resolutions while those in open issues did not, we find that\nineffective conversations contain knowledge gaps in 54.7% of prompts, compared\nto only 13.2% in effective ones. Additionally, we observe seven distinct\nconversational styles, with Directive Prompting, Chain of Thought, and\nResponsive Feedback being the most prevalent. We find that knowledge gaps are\npresent in all styles of conversations, with Missing Context being the most\nrepeated challenge developers face in issue-resolution conversations. Based on\nour analysis, we identify key textual and code related heuristics-Specificity,\nContextual Richness, and Clarity-that are associated with successful issue\nclosure and help assess prompt quality. These heuristics lay the foundation for\nan automated tool that can dynamically flag unclear prompts and suggest\nstructured improvements. To test feasibility, we developed a lightweight\nbrowser extension prototype for detecting prompt gaps, that can be easily\nadapted to other tools within developer workflows."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Sakshi Pathak"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11708v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11708v1",
                "updated": "2025-01-20T19:40:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    40,
                    45,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T19:40:45Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    40,
                    45,
                    0,
                    20,
                    0
                ],
                "title": "Estimating Rural Path Loss with ITU-R P.1812-7 : Impact of Geospatial\n  Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Rural Path Loss with ITU-R P.1812-7 : Impact of Geospatial\n  Inputs"
                },
                "summary": "Accurate radio wave propagation modeling is essential for effective spectrum\nmanagement by regulators and network deployment by operators. This paper\ninvestigates the ITU-R P.1812-7 (P.1812) propagation model's reliance on\ngeospatial inputs, particularly clutter information, to improve path loss\nestimation, with an emphasis on rural geographic regions. The research\nevaluates the impact of geospatial elevation and land cover datasets, including\nGlobal Forest Canopy Height (GFCH), European Space Agency WorldCover, and\nNatural Resources Canada LandCover, on P.1812 propagation model prediction\naccuracy. Results highlight the trade-offs between dataset resolution,\ngeospatial data availability, and representative clutter height assignments.\nSimulations reveal that high-resolution data do not always yield better results\nand that global datasets such as the GFCH provide a robust alternative when\nhigh-resolution data are unavailable or out-of-date. This study provides a set\nof guidelines for geospatial dataset integration to enhance P.1812's rural path\nloss predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate radio wave propagation modeling is essential for effective spectrum\nmanagement by regulators and network deployment by operators. This paper\ninvestigates the ITU-R P.1812-7 (P.1812) propagation model's reliance on\ngeospatial inputs, particularly clutter information, to improve path loss\nestimation, with an emphasis on rural geographic regions. The research\nevaluates the impact of geospatial elevation and land cover datasets, including\nGlobal Forest Canopy Height (GFCH), European Space Agency WorldCover, and\nNatural Resources Canada LandCover, on P.1812 propagation model prediction\naccuracy. Results highlight the trade-offs between dataset resolution,\ngeospatial data availability, and representative clutter height assignments.\nSimulations reveal that high-resolution data do not always yield better results\nand that global datasets such as the GFCH provide a robust alternative when\nhigh-resolution data are unavailable or out-of-date. This study provides a set\nof guidelines for geospatial dataset integration to enhance P.1812's rural path\nloss predictions."
                },
                "authors": [
                    {
                        "name": "Mathieu Chateauvert"
                    },
                    {
                        "name": "Jonathan Ethier"
                    },
                    {
                        "name": "Adrian Florea"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Florea"
                },
                "author": "Adrian Florea",
                "arxiv_comment": "4 pages, 1 Figure, Submitted to APS Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11708v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11708v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11706v1",
                "updated": "2025-01-20T19:38:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    38,
                    50,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T19:38:50Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    38,
                    50,
                    0,
                    20,
                    0
                ],
                "title": "Trustformer: A Trusted Federated Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustformer: A Trusted Federated Transformer"
                },
                "summary": "Transformers, a cornerstone of deep-learning architectures for sequential\ndata, have achieved state-of-the-art results in tasks like Natural Language\nProcessing (NLP). Models such as BERT and GPT-3 exemplify their success and\nhave driven the rise of large language models (LLMs). However, a critical\nchallenge persists: safeguarding the privacy of data used in LLM training.\nPrivacy-preserving techniques like Federated Learning (FL) offer potential\nsolutions, but practical limitations hinder their effectiveness for Transformer\ntraining. Two primary issues are (I) the risk of sensitive information leakage\ndue to aggregation methods like FedAvg or FedSGD, and (II) the high\ncommunication overhead caused by the large size of Transformer models.\n  This paper introduces a novel FL method that reduces communication overhead\nwhile maintaining competitive utility. Our approach avoids sharing full model\nweights by simulating a global model locally. We apply k-means clustering to\neach Transformer layer, compute centroids locally, and transmit only these\ncentroids to the server instead of full weights or gradients. To enhance\nsecurity, we leverage Intel SGX for secure transmission of centroids. Evaluated\non a translation task, our method achieves utility comparable to\nstate-of-the-art baselines while significantly reducing communication costs.\nThis provides a more efficient and privacy-preserving FL solution for\nTransformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, a cornerstone of deep-learning architectures for sequential\ndata, have achieved state-of-the-art results in tasks like Natural Language\nProcessing (NLP). Models such as BERT and GPT-3 exemplify their success and\nhave driven the rise of large language models (LLMs). However, a critical\nchallenge persists: safeguarding the privacy of data used in LLM training.\nPrivacy-preserving techniques like Federated Learning (FL) offer potential\nsolutions, but practical limitations hinder their effectiveness for Transformer\ntraining. Two primary issues are (I) the risk of sensitive information leakage\ndue to aggregation methods like FedAvg or FedSGD, and (II) the high\ncommunication overhead caused by the large size of Transformer models.\n  This paper introduces a novel FL method that reduces communication overhead\nwhile maintaining competitive utility. Our approach avoids sharing full model\nweights by simulating a global model locally. We apply k-means clustering to\neach Transformer layer, compute centroids locally, and transmit only these\ncentroids to the server instead of full weights or gradients. To enhance\nsecurity, we leverage Intel SGX for secure transmission of centroids. Evaluated\non a translation task, our method achieves utility comparable to\nstate-of-the-art baselines while significantly reducing communication costs.\nThis provides a more efficient and privacy-preserving FL solution for\nTransformer models."
                },
                "authors": [
                    {
                        "name": "Ali Abbasi Tadi"
                    },
                    {
                        "name": "Dima Alhadidi"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11705v1",
                "updated": "2025-01-20T19:38:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    38,
                    21,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T19:38:21Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    19,
                    38,
                    21,
                    0,
                    20,
                    0
                ],
                "title": "Human services organizations and the responsible integration of AI:\n  Considering ethics and contextualizing risk(s)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human services organizations and the responsible integration of AI:\n  Considering ethics and contextualizing risk(s)"
                },
                "summary": "This paper examines the responsible integration of artificial intelligence\n(AI) in human services organizations (HSOs), proposing a nuanced framework for\nevaluating AI applications across multiple dimensions of risk. The authors\nargue that ethical concerns about AI deployment -- including professional\njudgment displacement, environmental impact, model bias, and data laborer\nexploitation -- vary significantly based on implementation context and specific\nuse cases. They challenge the binary view of AI adoption, demonstrating how\ndifferent applications present varying levels of risk that can often be\neffectively managed through careful implementation strategies. The paper\nhighlights promising solutions, such as local large language models, that can\nfacilitate responsible AI integration while addressing common ethical concerns.\nThe authors propose a dimensional risk assessment approach that considers\nfactors like data sensitivity, professional oversight requirements, and\npotential impact on client wellbeing. They conclude by outlining a path forward\nthat emphasizes empirical evaluation, starting with lower-risk applications and\nbuilding evidence-based understanding through careful experimentation. This\napproach enables organizations to maintain high ethical standards while\nthoughtfully exploring how AI might enhance their capacity to serve clients and\ncommunities effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the responsible integration of artificial intelligence\n(AI) in human services organizations (HSOs), proposing a nuanced framework for\nevaluating AI applications across multiple dimensions of risk. The authors\nargue that ethical concerns about AI deployment -- including professional\njudgment displacement, environmental impact, model bias, and data laborer\nexploitation -- vary significantly based on implementation context and specific\nuse cases. They challenge the binary view of AI adoption, demonstrating how\ndifferent applications present varying levels of risk that can often be\neffectively managed through careful implementation strategies. The paper\nhighlights promising solutions, such as local large language models, that can\nfacilitate responsible AI integration while addressing common ethical concerns.\nThe authors propose a dimensional risk assessment approach that considers\nfactors like data sensitivity, professional oversight requirements, and\npotential impact on client wellbeing. They conclude by outlining a path forward\nthat emphasizes empirical evaluation, starting with lower-risk applications and\nbuilding evidence-based understanding through careful experimentation. This\napproach enables organizations to maintain high ethical standards while\nthoughtfully exploring how AI might enhance their capacity to serve clients and\ncommunities effectively."
                },
                "authors": [
                    {
                        "name": "Brian E. Perron"
                    },
                    {
                        "name": "Lauri Goldkind"
                    },
                    {
                        "name": "Zia Qi"
                    },
                    {
                        "name": "Bryan G. Victor"
                    }
                ],
                "author_detail": {
                    "name": "Bryan G. Victor"
                },
                "author": "Bryan G. Victor",
                "arxiv_doi": "10.1080/15228835.2025.2457045",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1080/15228835.2025.2457045",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.11705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "1 figure. Journal of Technology in Human Services (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11651v1",
                "updated": "2025-01-20T18:33:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    18,
                    33,
                    33,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T18:33:33Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    18,
                    33,
                    33,
                    0,
                    20,
                    0
                ],
                "title": "Advancing Language Model Reasoning through Reinforcement Learning and\n  Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Language Model Reasoning through Reinforcement Learning and\n  Inference Scaling"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration and\nlearning from feedback, recent attempts yield only modest improvements in\ncomplex reasoning. In this paper, we present T1 to scale RL by encouraging\nexploration and understand inference scaling. We first initialize the LLM using\nsynthesized chain-of-thought data that integrates trial-and-error and\nself-verification. To scale RL training, we promote increased sampling\ndiversity through oversampling. We further employ an entropy bonus as an\nauxiliary loss, alongside a dynamic anchor for regularization to facilitate\nreward optimization. We demonstrate that T1 with open LLMs as its base exhibits\ninference scaling behavior and achieves superior performance on challenging\nmath reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model\noutperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and\nOmni-math-500. More importantly, we present a simple strategy to examine\ninference scaling, where increased inference budgets directly lead to T1's\nbetter performance without any additional verification. We will open-source the\nT1 models and the data used to train them at \\url{https://github.com/THUDM/T1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration and\nlearning from feedback, recent attempts yield only modest improvements in\ncomplex reasoning. In this paper, we present T1 to scale RL by encouraging\nexploration and understand inference scaling. We first initialize the LLM using\nsynthesized chain-of-thought data that integrates trial-and-error and\nself-verification. To scale RL training, we promote increased sampling\ndiversity through oversampling. We further employ an entropy bonus as an\nauxiliary loss, alongside a dynamic anchor for regularization to facilitate\nreward optimization. We demonstrate that T1 with open LLMs as its base exhibits\ninference scaling behavior and achieves superior performance on challenging\nmath reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model\noutperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and\nOmni-math-500. More importantly, we present a simple strategy to examine\ninference scaling, where increased inference budgets directly lead to T1's\nbetter performance without any additional verification. We will open-source the\nT1 models and the data used to train them at \\url{https://github.com/THUDM/T1}."
                },
                "authors": [
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05870v3",
                "updated": "2025-01-20T18:01:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    18,
                    1,
                    6,
                    0,
                    20,
                    0
                ],
                "published": "2024-06-09T17:55:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    17,
                    55,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents"
                },
                "summary": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query - ostensibly because it lacks the relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not rely on an auxiliary LLM.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database and applying an LLM to the\nretrieved documents. We demonstrate that RAG systems that operate on databases\nwith untrusted content are vulnerable to denial-of-service attacks we call\njamming. An adversary can add a single ``blocker'' document to the database\nthat will be retrieved in response to a specific query and result in the RAG\nsystem not answering this query - ostensibly because it lacks the relevant\ninformation or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not rely on an auxiliary LLM.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11623v1",
                "updated": "2025-01-20T17:46:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    46,
                    12,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T17:46:12Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    46,
                    12,
                    0,
                    20,
                    0
                ],
                "title": "Early evidence of how LLMs outperform traditional systems on OCR/HTR\n  tasks for historical records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early evidence of how LLMs outperform traditional systems on OCR/HTR\n  tasks for historical records"
                },
                "summary": "We explore the ability of two LLMs -- GPT-4o and Claude Sonnet 3.5 -- to\ntranscribe historical handwritten documents in a tabular format and compare\ntheir performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract,\nand TrOCR. Considering the tabular form of the data, two types of experiments\nare executed: one where the images are split line by line and the other where\nthe entire scan is used as input. Based on CER and BLEU, we demonstrate that\nLLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the\nevaluated CER and BLEU scores to human evaluations to better judge the outputs\nof whole-scan experiments and understand influential factors for CER and BLEU.\nCombining judgments from all the evaluation metrics, we conclude that two-shot\nGPT-4o for line-by-line images and two-shot Claude Sonnet 3.5 for whole-scan\nimages yield the transcriptions of the historical records most similar to the\nground truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the ability of two LLMs -- GPT-4o and Claude Sonnet 3.5 -- to\ntranscribe historical handwritten documents in a tabular format and compare\ntheir performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract,\nand TrOCR. Considering the tabular form of the data, two types of experiments\nare executed: one where the images are split line by line and the other where\nthe entire scan is used as input. Based on CER and BLEU, we demonstrate that\nLLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the\nevaluated CER and BLEU scores to human evaluations to better judge the outputs\nof whole-scan experiments and understand influential factors for CER and BLEU.\nCombining judgments from all the evaluation metrics, we conclude that two-shot\nGPT-4o for line-by-line images and two-shot Claude Sonnet 3.5 for whole-scan\nimages yield the transcriptions of the historical records most similar to the\nground truth."
                },
                "authors": [
                    {
                        "name": "Seorin Kim"
                    },
                    {
                        "name": "Julien Baudru"
                    },
                    {
                        "name": "Wouter Ryckbosch"
                    },
                    {
                        "name": "Hugues Bersini"
                    },
                    {
                        "name": "Vincent Ginis"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Ginis"
                },
                "author": "Vincent Ginis",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04323v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04323v3",
                "updated": "2025-01-20T17:41:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    41,
                    59,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-08T07:47:43Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    7,
                    47,
                    43,
                    2,
                    8,
                    0
                ],
                "title": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Navigating the Designs of Privacy-Preserving Fine-tuning for Large\n  Language Models"
                },
                "summary": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has proven effective in enhancing Large Language Models'\n(LLMs) performance on downstream tasks. However, real-world fine-tuning faces\ninherent conflicts between model providers' intellectual property protection,\nclients' data privacy requirements, and tuning costs. While recent approaches\nlike split learning and offsite tuning demonstrate promising architectures for\nprivacy-preserving fine-tuning, there is a gap in systematically addressing the\nmultidimensional trade-offs required for diverse real-world deployments. We\npropose several indicative evaluation metrics to guide design trade-offs for\nprivacy-preserving fine-tuning and a series of example designs, collectively\nnamed GuardedTuning; they result from novel combinations of system\narchitectures with adapted privacy-enhancement methods and emerging computation\ntechniques. Each design represents distinct trade-offs across model utility,\nprivacy guarantees, and costs. Experimental results demonstrate that these\ndesigns protect against data reconstruction attacks while maintaining\ncompetitive fine-tuning performance."
                },
                "authors": [
                    {
                        "name": "Haonan Shi"
                    },
                    {
                        "name": "Tu Ouyang"
                    },
                    {
                        "name": "An Wang"
                    }
                ],
                "author_detail": {
                    "name": "An Wang"
                },
                "author": "An Wang",
                "arxiv_comment": "Accepted to WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04323v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04323v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08167v2",
                "updated": "2025-01-20T17:34:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    34,
                    20,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-14T14:49:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    49,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Perils of Large Language Models as Judges of Unstructured\n  Textual Data"
                },
                "summary": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLM-as-judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\njudges. This LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLM-as-judge offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nOur research contributes to the growing body of knowledge on AI assisted text\nanalysis. Further, we provide recommendations for future research, emphasizing\nthe need for careful consideration when generalizing LLM-as-judge models across\nvarious contexts and use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models have unlocked remarkable\ncapabilities when it comes to processing and summarizing unstructured text\ndata. This has implications for the analysis of rich, open-ended datasets, such\nas survey responses, where LLMs hold the promise of efficiently distilling key\nthemes and sentiments. However, as organizations increasingly turn to these\npowerful AI systems to make sense of textual feedback, a critical question\narises, can we trust LLMs to accurately represent the perspectives contained\nwithin these text based datasets? While LLMs excel at generating human-like\nsummaries, there is a risk that their outputs may inadvertently diverge from\nthe true substance of the original responses. Discrepancies between the\nLLM-generated outputs and the actual themes present in the data could lead to\nflawed decision-making, with far-reaching consequences for organizations. This\nresearch investigates the effectiveness of LLM-as-judge models to evaluate the\nthematic alignment of summaries generated by other LLMs. We utilized an\nAnthropic Claude model to generate thematic summaries from open-ended survey\nresponses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as\njudges. This LLM-as-judge approach was compared to human evaluations using\nCohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable\nalternative to traditional human centric evaluation methods. Our findings\nreveal that while LLM-as-judge offer a scalable solution comparable to human\nraters, humans may still excel at detecting subtle, context-specific nuances.\nOur research contributes to the growing body of knowledge on AI assisted text\nanalysis. Further, we provide recommendations for future research, emphasizing\nthe need for careful consideration when generalizing LLM-as-judge models across\nvarious contexts and use cases."
                },
                "authors": [
                    {
                        "name": "Rewina Bedemariam"
                    },
                    {
                        "name": "Natalie Perez"
                    },
                    {
                        "name": "Sreyoshi Bhaduri"
                    },
                    {
                        "name": "Satya Kapoor"
                    },
                    {
                        "name": "Alex Gil"
                    },
                    {
                        "name": "Elizabeth Conjar"
                    },
                    {
                        "name": "Ikkei Itoku"
                    },
                    {
                        "name": "David Theil"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Naumaan Nayyar"
                    }
                ],
                "author_detail": {
                    "name": "Naumaan Nayyar"
                },
                "author": "Naumaan Nayyar",
                "arxiv_comment": "11 pages, 1 appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11613v1",
                "updated": "2025-01-20T17:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T17:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    19,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversation Routines: A Prompt Engineering Framework for Task-Oriented\n  Dialog Systems"
                },
                "summary": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof of concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nenterprise functionalities (tools) developed by software engineers, creating an\nefficient division of responsibilities where developers focus on core API\nimplementation and domain experts handle conversation design. While the\nframework shows promise in accessibility and adaptability, we identify key\nchallenges including computational overhead, non-deterministic behavior, and\ndomain-specific logic optimization. Future research directions include\nenhancing system robustness, improving scalability for complex multi-agent\ninteractions, and addressing the identified limitations across diverse business\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces Conversation Routines (CR), a structured prompt\nengineering framework for developing task-oriented dialog systems using Large\nLanguage Models (LLMs). While LLMs demonstrate remarkable natural language\nunderstanding capabilities, engineering them to reliably execute complex\nbusiness workflows remains challenging. The proposed CR framework enables the\ndevelopment of Conversation Agentic Systems (CAS) through natural language\nspecifications, embedding task-oriented logic within LLM prompts. This approach\nprovides a systematic methodology for designing and implementing complex\nconversational workflows while maintaining behavioral consistency. We\ndemonstrate the framework's effectiveness through two proof of concept\nimplementations: a Train Ticket Booking System and an Interactive\nTroubleshooting Copilot. These case studies validate CR's capability to encode\nsophisticated behavioral patterns and decision logic while preserving natural\nconversational flexibility. Results show that CR enables domain experts to\ndesign conversational workflows in natural language while leveraging custom\nenterprise functionalities (tools) developed by software engineers, creating an\nefficient division of responsibilities where developers focus on core API\nimplementation and domain experts handle conversation design. While the\nframework shows promise in accessibility and adaptability, we identify key\nchallenges including computational overhead, non-deterministic behavior, and\ndomain-specific logic optimization. Future research directions include\nenhancing system robustness, improving scalability for complex multi-agent\ninteractions, and addressing the identified limitations across diverse business\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Robino"
                    }
                ],
                "author_detail": {
                    "name": "Giorgio Robino"
                },
                "author": "Giorgio Robino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11599v1",
                "updated": "2025-01-20T17:00:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    0,
                    41,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T17:00:41Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    17,
                    0,
                    41,
                    0,
                    20,
                    0
                ],
                "title": "SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language\n  Models Tackling Knowledge-based Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language\n  Models Tackling Knowledge-based Reasoning Tasks"
                },
                "summary": "Deductive reasoning is a crucial logical capability that assists us in\nsolving complex problems based on existing knowledge. Although augmented by\nChain-of-Thought prompts, Large Language Models (LLMs) might not follow the\ncorrect reasoning paths. Enhancing the deductive reasoning abilities of LLMs,\nand leveraging their extensive built-in knowledge for various reasoning tasks,\nremains an open question. Attempting to mimic the human deductive reasoning\nparadigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought\n(SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle\ncomplex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the\nquestion and then uses the interpretation and the original question to propose\na suitable major premise. It proceeds by generating and answering minor premise\nquestions in two stages to match the minor premises. Finally, it guides LLMs to\nuse the previously generated major and minor premises to perform syllogistic\ndeductive reasoning to derive the answer to the original question. Extensive\nand thorough experiments on knowledge-based reasoning tasks have demonstrated\nthe effectiveness and advantages of our SR-FoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deductive reasoning is a crucial logical capability that assists us in\nsolving complex problems based on existing knowledge. Although augmented by\nChain-of-Thought prompts, Large Language Models (LLMs) might not follow the\ncorrect reasoning paths. Enhancing the deductive reasoning abilities of LLMs,\nand leveraging their extensive built-in knowledge for various reasoning tasks,\nremains an open question. Attempting to mimic the human deductive reasoning\nparadigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought\n(SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle\ncomplex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the\nquestion and then uses the interpretation and the original question to propose\na suitable major premise. It proceeds by generating and answering minor premise\nquestions in two stages to match the minor premises. Finally, it guides LLMs to\nuse the previously generated major and minor premises to perform syllogistic\ndeductive reasoning to derive the answer to the original question. Extensive\nand thorough experiments on knowledge-based reasoning tasks have demonstrated\nthe effectiveness and advantages of our SR-FoT."
                },
                "authors": [
                    {
                        "name": "Wentao Wan"
                    },
                    {
                        "name": "Zhuojie Yang"
                    },
                    {
                        "name": "Yongcan Chen"
                    },
                    {
                        "name": "Chenglin Luo"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Kehao Cai"
                    },
                    {
                        "name": "Nan Kang"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "arxiv_comment": "This paper has been accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]