[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v1",
                "updated": "2025-05-22T17:33:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v1",
                "updated": "2025-05-22T16:07:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16582v1",
                "updated": "2025-05-22T12:17:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T12:17:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    12,
                    17,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended\n  Question Answering"
                },
                "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones."
                },
                "authors": [
                    {
                        "name": "Jianbiao Mei"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Licheng Wen"
                    },
                    {
                        "name": "Xuemeng Yang"
                    },
                    {
                        "name": "Rong Wu"
                    },
                    {
                        "name": "Pinlong Cai"
                    },
                    {
                        "name": "Xing Gao"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Chengjun Xie"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Qiao"
                },
                "author": "Yu Qiao",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v2",
                "updated": "2025-05-22T06:44:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    44,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Yang Zhen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16271v1",
                "updated": "2025-05-22T06:04:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T06:04:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    4,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations of the Near-Surface Electric field measured at Aragats during\n  Geomagnetic Storms"
                },
                "summary": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At least two mechanisms effectively transfer interplanetary magnetic field\n(IMF) disturbances into the atmosphere. First, the inflow of solar wind into\nthe ionosphere at low latitudes significantly enhances the total vertical\nelectron content, increasing atmospheric conductivity. Second, Forbush\ndecreases (FD) reduce the cosmic ray flux by a few percent, lowering ionization\nlevels at middle latitudes and decreasing conductivity. Changes in atmospheric\nconductivity affect the global electric circuit and atmospheric electric field\n(AEF). However, to study the response of AEF to geomagnetic storms (GMS), it is\nnecessary to carefully monitor atmospheric conditions before and during storms,\nas meteorological influences can be much stronger than those of GMS. Charged\nclouds above detectors, lightning flashes, and abrupt weather changes\nsignificantly impact near-surface electric field (NSEF) variations, which serve\nas a proxy for AEF measured at the Earth's surface. The facilities at Aragats\nstation monitor all environmental parameters on a one-minute timescale. We\nanalyze four GMS events described in previous studies, detailing the\ncorresponding weather conditions to isolate the genuine influence of GMS on\nNSEF. The GMS of June 22, 2015, and September 8, 2017, occurred under\nfair-weather conditions, providing clear evidence of GMS influence on NSEF.\nThese events were long-lasting, positive, and modest, ranging from 0.2 to 0.3\nkV/m, and coincided with the depletion phase of FD. The sky was clear, no rain\nwas detected, and lightning flashes from previous thunderstorms were more than\n20 km from the station. The other two events did not meet favorable weather\ncriteria, and their occurrence during GMS seemed incidental. We identify a\nfeature that may indicate the solar (FD) origin of NSEF enhancement: a dip in\nthe enhanced NSEF during the daytime."
                },
                "authors": [
                    {
                        "name": "A. Chilingarian"
                    }
                ],
                "author_detail": {
                    "name": "A. Chilingarian"
                },
                "author": "A. Chilingarian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15793v2",
                "updated": "2025-05-22T04:48:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    48,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T17:47:24Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    47,
                    24,
                    2,
                    141,
                    0
                ],
                "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving"
                },
                "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations. Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios."
                },
                "authors": [
                    {
                        "name": "Zhiwen Chen"
                    },
                    {
                        "name": "Bo Leng"
                    },
                    {
                        "name": "Zhuoren Li"
                    },
                    {
                        "name": "Hanming Deng"
                    },
                    {
                        "name": "Guizhe Jin"
                    },
                    {
                        "name": "Ran Yu"
                    },
                    {
                        "name": "Huanxi Wen"
                    }
                ],
                "author_detail": {
                    "name": "Huanxi Wen"
                },
                "author": "Huanxi Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16210v1",
                "updated": "2025-05-22T04:23:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T04:23:19Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    23,
                    19,
                    3,
                    142,
                    0
                ],
                "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution\n  Characteristics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."
                },
                "authors": [
                    {
                        "name": "Zhihang Cai"
                    },
                    {
                        "name": "Xingjun Zhang"
                    },
                    {
                        "name": "Zhendong Tan"
                    },
                    {
                        "name": "Zheng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wei"
                },
                "author": "Zheng Wei",
                "arxiv_comment": "11 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16175v1",
                "updated": "2025-05-22T03:26:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T03:26:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    26,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickVideo: Real-Time Long Video Understanding with System Algorithm\n  Co-Design"
                },
                "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice."
                },
                "authors": [
                    {
                        "name": "Benjamin Schneider"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "arxiv_comment": "19 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16076v1",
                "updated": "2025-05-21T23:23:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T23:23:37Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    23,
                    23,
                    37,
                    2,
                    141,
                    0
                ],
                "title": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMorphix: Training-free audio editing with diffusion probabilistic\n  models"
                },
                "summary": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Editing sound with precision is a crucial yet underexplored challenge in\naudio content creation. While existing works can manipulate sounds by text\ninstructions or audio exemplar pairs, they often struggled to modify audio\ncontent precisely while preserving fidelity to the original recording. In this\nwork, we introduce a novel editing approach that enables localized\nmodifications to specific time-frequency regions while keeping the remaining of\nthe audio intact by operating on spectrograms directly. To achieve this, we\npropose AudioMorphix, a training-free audio editor that manipulates a target\nregion on the spectrogram by referring to another recording. Inspired by\nmorphing theory, we conceptualize audio mixing as a process where different\nsounds blend seamlessly through morphing and can be decomposed back into\nindividual components via demorphing. Our AudioMorphix optimizes the noised\nlatent conditioned on raw input and reference audio while rectifying the guided\ndiffusion process through a series of energy functions. Additionally, we\nenhance self-attention layers with a cache mechanism to preserve detailed\ncharacteristics from the original recordings. To advance audio editing\nresearch, we devise a new evaluation benchmark, which includes a curated\ndataset with a variety of editing instructions. Extensive experiments\ndemonstrate that AudioMorphix yields promising performance on various audio\nediting tasks, including addition, removal, time shifting and stretching, and\npitch shifting, achieving high fidelity and precision. Demo and code are\navailable at this url."
                },
                "authors": [
                    {
                        "name": "Jinhua Liang"
                    },
                    {
                        "name": "Yuanzhe Chen"
                    },
                    {
                        "name": "Yi Yuan"
                    },
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16056v1",
                "updated": "2025-05-21T22:13:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T22:13:09Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    22,
                    13,
                    9,
                    2,
                    141,
                    0
                ],
                "title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Models Suit Expert Offloading: On Local Routing Consistency of\n  Mixture-of-Expert Models"
                },
                "summary": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) enables efficient scaling of large language models\n(LLMs) with sparsely activated experts during inference. To effectively deploy\nlarge MoE models on memory-constrained devices, many systems introduce *expert\noffloading* that caches a subset of experts in fast memory, leaving others on\nslow memory to run on CPU or load on demand. While some research has exploited\nthe locality of expert activations, where consecutive tokens activate similar\nexperts, the degree of this **local routing consistency** varies across models\nand remains understudied. In this paper, we propose two metrics to measure\nlocal routing consistency of MoE models: (1) **Segment Routing Best Performance\n(SRP)**, which evaluates how well a fixed group of experts can cover the needs\nof a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which\nmeasures the optimal segment-level cache hit rate under a given cache size\nlimit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found\nthat models that apply MoE on every layer and do not use shared experts exhibit\nthe highest local routing consistency. We further showed that\ndomain-specialized experts contribute more to routing consistency than\nvocabulary-specialized ones, and that most models can balance between cache\neffectiveness and efficiency with cache sizes approximately 2x the active\nexperts. These findings pave the way for memory-efficient MoE design and\ndeployment without compromising inference speed. We publish the code for\nreplicating experiments at https://github.com/ljcleo/moe-lrc ."
                },
                "authors": [
                    {
                        "name": "Jingcong Liang"
                    },
                    {
                        "name": "Siyuan Wang"
                    },
                    {
                        "name": "Miren Tian"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Zhongyu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhongyu Wei"
                },
                "author": "Zhongyu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v2",
                "updated": "2025-05-21T20:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    52,
                    59,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache. Accepted\n  at CVPR eLVM workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v2",
                "updated": "2025-05-21T20:42:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    20,
                    42,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15781v1",
                "updated": "2025-05-21T17:32:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T17:32:10Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    17,
                    32,
                    10,
                    2,
                    141,
                    0
                ],
                "title": "dKV-Cache: The Cache for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dKV-Cache: The Cache for Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "The code is available at https://github.com/horseee/dKV-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15684v1",
                "updated": "2025-05-21T15:58:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkLess: A Training-Free Inference-Efficient Method for Reducing\n  Reasoning Redundancy"
                },
                "summary": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain-of-Thought (CoT) prompting improves reasoning in large language\nmodels (LLMs), the excessive length of reasoning tokens increases latency and\nKV cache memory usage, and may even truncate final answers under context\nlimits. We propose ThinkLess, an inference-efficient framework that terminates\nreasoning generation early and maintains output quality without modifying the\nmodel. Atttention analysis reveals that answer tokens focus minimally on\nearlier reasoning steps and primarily attend to the reasoning terminator token,\ndue to information migration under causal masking. Building on this insight,\nThinkLess inserts the terminator token at earlier positions to skip redundant\nreasoning while preserving the underlying knowledge transfer. To prevent format\ndiscruption casued by early termination, ThinkLess employs a lightweight\npost-regulation mechanism, relying on the model's natural instruction-following\nability to produce well-structured answers. Without fine-tuning or auxiliary\ndata, ThinkLess achieves comparable accuracy to full-length CoT decoding while\ngreatly reducing decoding time and memory consumption."
                },
                "authors": [
                    {
                        "name": "Gengyang Li"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v1",
                "updated": "2025-05-21T15:58:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Federated Splitting Framework for LLMs: Security, Efficiency, and\n  Adaptability"
                },
                "summary": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data is typically larger and of higher quality than public data,\noffering great potential to improve LLM. However, its scattered distribution\nacross data silos and the high computational demands of LLMs limit their\ndeployment in federated environments. To address this, the transformer-based\nsplit learning model has emerged, offloading most model parameters to the\nserver while retaining only the embedding and output layers on clients to\nensure privacy. However, it still faces significant challenges in security,\nefficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,\nleading to reverse engineering of private data; 2) the autoregressive nature of\nLLMs means that federated split learning can only train and infer sequentially,\ncausing high communication overhead; 3) fixed partition points lack\nadaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a\nsecure, efficient, and adaptive federated split framework based on LLaMA2.\nFirst, we place some input and output blocks on the local client and inject\nGaussian noise into forward-pass hidden states, enabling secure end-to-end\npropagation. Second, we employ client-batch and server-hierarchical strategies\nto achieve parallel training, along with attention-mask compression and KV\ncache mechanisms to accelerate inference, reducing communication costs\neffectively. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements and hardware\nlimitations. Experiments on NLU, summarization and conversational QA tasks show\nthat FL-LLaMA maintains performance comparable to centralized LLaMA2, and\nachieves up to 2x train speedups and 8x inference speedups. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FL-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Jiaying Zheng"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Jin Dong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15558v1",
                "updated": "2025-05-21T14:17:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T14:17:06Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    17,
                    6,
                    2,
                    141,
                    0
                ],
                "title": "Robo-DM: Data Management For Large Robot Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robo-DM: Data Management For Large Robot Datasets"
                },
                "summary": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results suggest that very large datasets of teleoperated robot\ndemonstrations can be used to train transformer-based models that have the\npotential to generalize to new scenes, robots, and tasks. However, curating,\ndistributing, and loading large datasets of robot trajectories, which typically\nconsist of video, textual, and numerical modalities - including streams from\nmultiple cameras - remains challenging. We propose Robo-DM, an efficient\nopen-source cloud-based data management toolkit for collecting, sharing, and\nlearning with robot data. With Robo-DM, robot datasets are stored in a\nself-contained format with Extensible Binary Meta Language (EBML). Robo-DM can\nsignificantly reduce the size of robot trajectory data, transfer costs, and\ndata load time during training. Compared to the RLDS format used in OXE\ndatasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x\n(lossless). Robo-DM also accelerates data retrieval by load-balancing video\ndecoding with memory-mapped decoding caches. Compared to LeRobot, a framework\nthat also uses lossy video compression, Robo-DM is up to 50x faster when\ndecoding sequentially. We physically evaluate a model trained by Robo-DM with\nlossy compression, a pick-and-place task, and In-Context Robot Transformer.\nRobo-DM uses 75x compression of the original dataset and does not suffer\nreduction in downstream task accuracy."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Letian Fu"
                    },
                    {
                        "name": "David Huang"
                    },
                    {
                        "name": "Yanxiang Zhang"
                    },
                    {
                        "name": "Lawrence Yunliang Chen"
                    },
                    {
                        "name": "Huang Huang"
                    },
                    {
                        "name": "Kush Hari"
                    },
                    {
                        "name": "Ashwin Balakrishna"
                    },
                    {
                        "name": "Ted Xiao"
                    },
                    {
                        "name": "Pannag R Sanketi"
                    },
                    {
                        "name": "John Kubiatowicz"
                    },
                    {
                        "name": "Ken Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Ken Goldberg"
                },
                "author": "Ken Goldberg",
                "arxiv_comment": "Best paper finalist of IEEE ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.17644v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.17644v4",
                "updated": "2025-05-21T14:05:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    14,
                    5,
                    56,
                    2,
                    141,
                    0
                ],
                "published": "2024-01-31T07:52:48Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    7,
                    52,
                    48,
                    2,
                    31,
                    0
                ],
                "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems"
                },
                "summary": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xueze Kang"
                    },
                    {
                        "name": "Yuchu Fang"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Rui Guo"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.17644v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.17644v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15535v1",
                "updated": "2025-05-21T13:56:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:56:16Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    56,
                    16,
                    2,
                    141,
                    0
                ],
                "title": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix-Free Methods for Finite-Strain Elasticity: Automatic Code\n  Generation with No Performance Overhead"
                },
                "summary": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores matrix-free tangent evaluations in finite-strain\nelasticity with the use of automatically-generated code for the\nquadrature-point level calculations. The code generation is done via automatic\ndifferentiation (AD) with AceGen. We compare hand-written and AD-generated\ncodes under two computing strategies: on-the-fly evaluation and caching\nintermediate results. The comparison reveals that the AD-generated code\nachieves superior performance in matrix-free computations."
                },
                "authors": [
                    {
                        "name": "Michał Wichrowski"
                    },
                    {
                        "name": "Mohsen Rezaee-Hajidehi"
                    },
                    {
                        "name": "Jože Korelc"
                    },
                    {
                        "name": "Martin Kronbichler"
                    },
                    {
                        "name": "Stanisław Stupkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Stanisław Stupkiewicz"
                },
                "author": "Stanisław Stupkiewicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65M60, 74B20, 74S05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15531v1",
                "updated": "2025-05-21T13:52:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T13:52:45Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    13,
                    52,
                    45,
                    2,
                    141,
                    0
                ],
                "title": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Optimizing Latency for Delayed Hit Caching with Stochastic\n  Miss Latency"
                },
                "summary": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial for system performance, but the delayed hit phenomenon,\nwhere requests queue during lengthy fetches after a cache miss, significantly\ndegrades user-perceived latency in modern high-throughput systems. While prior\nworks address delayed hits by estimating aggregate delay, they universally\nassume deterministic fetch latencies. This paper tackles the more realistic,\nyet unexplored, scenario where fetch latencies are stochastic. We present, to\nour knowledge, the first theoretical analysis of delayed hits under this\ncondition, deriving analytical expressions for both the mean and variance of\nthe aggregate delay assuming exponentially distributed fetch latency.\nLeveraging these insights, we develop a novel variance-aware ranking function\ntailored for this stochastic setting to guide cache eviction decisions more\neffectively. The simulations on synthetic and real-world datasets demonstrate\nthat our proposed algorithm significantly reduces overall latency compared to\nstate-of-the-art delayed-hit strategies, achieving a $3\\%-30\\%$ reduction on\nsynthetic datasets and approximately $1\\%-7\\%$ reduction on real-world traces."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chaofan Ma"
                },
                "author": "Chaofan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v2",
                "updated": "2025-05-21T10:38:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    37,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v2",
                "updated": "2025-05-21T10:38:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    38,
                    1,
                    2,
                    141,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available:\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Accepted by ICML 2025. Code is available:\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v2",
                "updated": "2025-05-21T10:37:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    37,
                    50,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an underexplored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. Although existing methods achieve impressive compression ratios\non long-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive benchmark KVFundaBench to\nsystematically evaluate the effects of KV cache compression across diverse\nfundamental LLM capabilities, spanning world knowledge, commonsense reasoning,\narithmetic reasoning, code generation, safety, and long-context understanding\nand generation.Our analysis reveals serval key findings: (1)\n\\textit{Task-Dependent Degradation}; (2) \\textit{Model-Type Robustness} (3)\n\\textit{Prompt Length Vulnerability}; (4) \\textit{Chunk-Level Superiority}; (5)\n\\textit{Prompt-Gain Sensitivity}; (6) \\textit{Long-Context Generation\nSensitivity}. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v1",
                "updated": "2025-05-21T10:20:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15269v1",
                "updated": "2025-05-21T08:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T08:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    8,
                    47,
                    15,
                    2,
                    141,
                    0
                ],
                "title": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV\n  Cache and Retrieval"
                },
                "summary": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Video Large Language Models (Video LLMs) have enabled\nmodels to process long video sequences and demonstrate remarkable performance.\nNonetheless, studies predominantly focus on offline video question answering,\nneglecting memory usage and response speed that are essential in various\nreal-world applications, such as Deepseek services, autonomous driving, and\nrobotics. To mitigate these challenges, we propose $\\textbf{LiveVLM}$, a\ntraining-free framework specifically designed for streaming, online video\nunderstanding and real-time interaction. Unlike existing works that process\nvideos only after one question is posed, LiveVLM constructs an innovative\nstreaming-oriented KV cache to process video streams in real-time, retain\nlong-term video details and eliminate redundant KVs, ensuring prompt responses\nto user queries. For continuous video streams, LiveVLM generates and compresses\nvideo key-value tensors (video KVs) to reserve visual information while\nimproving memory efficiency. Furthermore, when a new question is proposed,\nLiveVLM incorporates an online question-answering process that efficiently\nfetches both short-term and long-term visual information, while minimizing\ninterference from redundant context. Extensive experiments demonstrate that\nLiveVLM enables the foundation LLaVA-OneVision model to process 44$\\times$\nnumber of frames on the same device, and achieves up to 5$\\times$ speedup in\nresponse speed compared with SoTA online methods at an input of 256 frames,\nwhile maintaining the same or better model performance."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v2",
                "updated": "2025-05-21T06:45:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    6,
                    45,
                    58,
                    2,
                    141,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to reduce latency for\nlong-context inference. FastKV improves processing speed while preserving\naccuracy by adopting Token-Selective Propagation (TSP). This approach preserves\nfull-context information in early layers of LLMs and selectively propagates\nonly a portion of this information in later layers. This design enables FastKV\nto minimize redundant computation without sacrificing contextual fidelity. Our\nexperimental results show that FastKV achieves up to 1.97$\\times$ and\n4.82$\\times$ improvements in time-to-first-token (TTFT) and throughput,\nrespectively, compared to baseline without KV cache compression. Moreover,\nFastKV successfully maintains accuracy within 1\\% of the baseline on\nlong-context benchmarks. Our code is available at\nhttps://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15859v1",
                "updated": "2025-05-21T04:32:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T04:32:35Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    32,
                    35,
                    2,
                    141,
                    0
                ],
                "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoData: A Multi-Agent System for Open Web Data Collection"
                },
                "summary": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-driven systems and AI technologies has\nintensified the demand for high-quality web-sourced datasets. While existing\ndatasets have proven valuable, conventional web data collection approaches face\nsignificant limitations in terms of human effort and scalability. Current\ndata-collecting solutions fall into two categories: wrapper-based methods that\nstruggle with adaptability and reproducibility, and large language model\n(LLM)-based approaches that incur substantial computational and financial\ncosts. To address these challenges, we propose AutoData, a novel multi-agent\nsystem for Automated web Data collection, that requires minimal human\nintervention, i.e., only necessitating a natural language instruction\nspecifying the desired dataset. In addition, AutoData is designed with a robust\nmulti-agent architecture, featuring a novel oriented message hypergraph\ncoordinated by a central task manager, to efficiently organize agents across\nresearch and development squads. Besides, we introduce a novel hypergraph cache\nsystem to advance the multi-agent collaboration process that enables efficient\nautomated data collection and mitigates the token cost issues prevalent in\nexisting LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark\ndataset supporting live data collection from web sources across three domains:\nacademic, finance, and sports. Comprehensive evaluations over Instruct2DS and\nthree existing benchmark datasets demonstrate AutoData's superior performance\ncompared to baseline methods. Case studies on challenging tasks such as picture\nbook collection and paper extraction from surveys further validate its\napplicability. Our source code and dataset are available at\nhttps://github.com/GraphResearcher/AutoData."
                },
                "authors": [
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Yiyue Qian"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Xiaoye Qian"
                    },
                    {
                        "name": "Feifan Bai"
                    },
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Xuwei Luo"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v2",
                "updated": "2025-05-21T01:34:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    1,
                    34,
                    19,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14992v1",
                "updated": "2025-05-21T00:40:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "published": "2025-05-21T00:40:05Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    0,
                    40,
                    5,
                    2,
                    141,
                    0
                ],
                "title": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective and Efficient Schema-aware Information Extraction Using\n  On-Device Large Language Models"
                },
                "summary": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information extraction (IE) plays a crucial role in natural language\nprocessing (NLP) by converting unstructured text into structured knowledge.\nDeploying computationally intensive large language models (LLMs) on\nresource-constrained devices for information extraction is challenging,\nparticularly due to issues like hallucinations, limited context length, and\nhigh latency-especially when handling diverse extraction schemas. To address\nthese challenges, we propose a two-stage information extraction approach\nadapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching\n(DLISC), which enhances both schema identification and schema-aware extraction\nin terms of effectiveness and efficiency. In particular, DLISC adopts an\nIdentification LoRA module for retrieving the most relevant schemas to a given\nquery, and an Extraction LoRA module for performing information extraction\nbased on the previously selected schemas. To accelerate extraction inference,\nIncremental Schema Caching is incorporated to reduce redundant computation,\nsubstantially improving efficiency. Extensive experiments across multiple\ninformation extraction datasets demonstrate notable improvements in both\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Zhihao Wen"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v1",
                "updated": "2025-05-20T23:12:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency to\nimprove the efficiency of large-scale autoregressive (AR) Transformer models by\nenabling multiple steps of token generation in a single forward pass.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead to\ncurrent SSM state update implementations. With the algorithm, we describe a\nhardware-aware implementation that improves naive application of AR Transformer\ntree-based speculative decoding methods to SSMs. Furthermore, we outperform\nvanilla speculative decoding with SSMs even with a baseline drafting model and\ntree structure on three different benchmarks, opening up opportunities for\nfurther speed up with SSM and hybrid model inference. Code will be released\nupon paper acceptance."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v3",
                "updated": "2025-05-20T18:49:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    18,
                    49,
                    30,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15364v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15364v3",
                "updated": "2025-05-20T17:50:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    50,
                    11,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-21T18:12:46Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    12,
                    46,
                    0,
                    111,
                    0
                ],
                "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM\n  Inference in Resource-Constrained Environments"
                },
                "summary": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
                },
                "authors": [
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew J Morse"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15364v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15364v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v4",
                "updated": "2025-05-20T16:29:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    29,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose a novel batching and scheduling algorithm\nthat minimizes inference latency while effectively managing the KV cache's\nmemory.\n  More specifically, we make the following contributions. First, to evaluate\nthe performance of online algorithms for scheduling in LLM inference, we\nintroduce a hindsight optimal benchmark, formulated as an integer program that\ncomputes the minimum total inference latency under full future information.\nSecond, we prove that no deterministic online algorithm can achieve a constant\ncompetitive ratio when the arrival process is arbitrary. Third, motivated by\nthe computational intractability of solving the integer program at scale, we\npropose a polynomial-time online scheduling algorithm and show that under\ncertain conditions it can achieve a constant competitive ratio. We also\ndemonstrate our algorithm's strong empirical performance by comparing it to the\nhindsight optimal in a synthetic dataset. Finally, we conduct empirical\nevaluations on a real-world public LLM inference dataset, simulating the\nLlama2-70B model on A100 GPUs, and show that our algorithm significantly\noutperforms the benchmark algorithms. Overall, our results offer a path toward\nmore sustainable and cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Konstantina Mellou"
                    },
                    {
                        "name": "Marco Molinaro"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14427v1",
                "updated": "2025-05-20T14:38:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:38:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    38,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyMemory: A LEO Edge Cache for Transformer Inference Optimization and\n  Scale Out"
                },
                "summary": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We expand the scope of cache memory to include LEO constellations, which are\nhighly distributed systems with thousands of satellites connected with\nfree-space optics inter-satellite links (ISL) always only one hop from any\npoint on earth. We show how to increase the number of cache hits and improve\nthe speed of inference for the important use case of LLMs. These benefits apply\nnot only to LLMs, both terrestrially hosted and on satellites, but also\ngeneralize to any cache distributed over multiple locations that needs to be\naccessed in a timely manner. We show the benefit of our key value cache (KVC)\nprotocol in simulations and present a proof-of-concept implementation of the\nprotocol for KVCs on a testbed comprising 5 Intel NUC Linux mini PCs hosting a\n19x5 constellation, with an NVIDIA Jetson Nano 8GB GPU hosting the LLM."
                },
                "authors": [
                    {
                        "name": "Thomas Sandholm"
                    },
                    {
                        "name": "Sayandev Mukherjee"
                    },
                    {
                        "name": "Lin Cheng"
                    },
                    {
                        "name": "Bernardo A. Huberman"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo A. Huberman"
                },
                "author": "Bernardo A. Huberman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14398v1",
                "updated": "2025-05-20T14:14:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T14:14:38Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    14,
                    14,
                    38,
                    1,
                    140,
                    0
                ],
                "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable\n  Computation"
                },
                "summary": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Samuel Madden"
                    },
                    {
                        "name": "Jacob Andreas"
                    },
                    {
                        "name": "Michael Cafarella"
                    }
                ],
                "author_detail": {
                    "name": "Michael Cafarella"
                },
                "author": "Michael Cafarella",
                "arxiv_comment": "Data and code are available at https://peterbaile.github.io/lag/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14085v1",
                "updated": "2025-05-20T08:46:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T08:46:23Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    46,
                    23,
                    1,
                    140,
                    0
                ],
                "title": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-LSLM: Efficient Large-Small Language Model Inference and\n  Communication via Cloud-Edge Collaboration"
                },
                "summary": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging intelligent service scenarios in 6G communication impose stringent\nrequirements for low latency, high reliability, and privacy preservation.\nGenerative large language models (LLMs) are gradually becoming key enablers for\nthe integration of semantic communication and computation. However, due to the\nlimited computational resources of edge devices and the increasing complexity\nof heterogeneous terminal access, existing centralized inference approaches\nfail to meet the dual demands of response efficiency and data privacy in\nedge-side inference tasks. To address these challenges, this paper proposes a\nnovel collaborative inference architecture that integrates cloud-based LLMs\nwith edge-deployed small language models (SLMs), enabling dynamic scheduling\nand sharing of semantic-level intermediate states, and establishing a unified\ncomputation-communication paradigm tailored for 6G networks. Specifically, a\nkey-value (KV) cache reuse mechanism is introduced to enhance the semantic\nunderstanding of edge models through contextual guidance from the cloud, while\nsignificantly reducing edge-side computational and storage overhead.\nFurthermore, a cross-node parallel scheduling mechanism is proposed to achieve\nasynchronous coordination between model state loading and decoding computation,\nthereby improving edge responsiveness. In addition, we investigate layer\nalignment and representation compression strategies between heterogeneous\nmodels to alleviate the communication burden on the edge. Experimental results\ndemonstrate that the proposed architecture exhibits superior adaptability and\nscalability in terms of inference latency, system stability, and concurrent\nprocessing capacity."
                },
                "authors": [
                    {
                        "name": "Pengyan Zhu"
                    },
                    {
                        "name": "Tingting Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yang"
                },
                "author": "Tingting Yang",
                "arxiv_comment": "14 pages, 7 figures including subplots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13002v2",
                "updated": "2025-05-20T07:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    34,
                    45,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-19T11:41:21Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    11,
                    41,
                    21,
                    0,
                    139,
                    0
                ],
                "title": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM-malloc: A Fast and Scalable Dynamic Memory Allocator for\n  Processing-In-Memory (PIM) Architectures"
                },
                "summary": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic memory allocation is essential in modern programming but remains\nunder-supported in current PIM devices. In this work, we first conduct a design\nspace exploration of PIM memory allocators, examining optimal metadata\nplacement and management strategies. Building on these insights, we propose\nPIM-malloc, a fast and scalable allocator for real PIM hardware, improving\nallocation performance by $66\\times$. We further enhance this design with a\nlightweight, per-PIM core hardware cache for dynamic allocation, achieving an\nadditional $31\\%$ performance gain. Finally, we demonstrate the effectiveness\nof PIM-malloc using a dynamic graph update workload, achieving a $28\\times$\nthroughput increase."
                },
                "authors": [
                    {
                        "name": "Dongjae Lee"
                    },
                    {
                        "name": "Bongjoon Hyun"
                    },
                    {
                        "name": "Youngjin Kwon"
                    },
                    {
                        "name": "Minsoo Rhu"
                    }
                ],
                "author_detail": {
                    "name": "Minsoo Rhu"
                },
                "author": "Minsoo Rhu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14010v1",
                "updated": "2025-05-20T07:04:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T07:04:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    7,
                    4,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache"
                },
                "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md."
                },
                "authors": [
                    {
                        "name": "Pu Wang"
                    },
                    {
                        "name": "Pengwen Dai"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Dianjie Lu"
                    },
                    {
                        "name": "Guijuan Zhang"
                    },
                    {
                        "name": "Youshan Zhang"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v3",
                "updated": "2025-05-20T04:52:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    52,
                    21,
                    1,
                    140,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Akash Das"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v1",
                "updated": "2025-05-20T03:21:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and throughput of token\ngeneration, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining KV cache that receive high\nimportance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v2",
                "updated": "2025-05-19T20:37:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    20,
                    37,
                    41,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12463v2",
                "updated": "2025-05-19T19:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    19,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2024-05-21T02:39:45Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    2,
                    39,
                    45,
                    1,
                    142,
                    0
                ],
                "title": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Learning of Computational Resource Usage as Graph Structured\n  Multimarginal Schrödinger Bridge"
                },
                "summary": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose to learn the time-varying stochastic computational resource usage\nof software as a graph structured Schr\\\"odinger bridge problem. In general,\nlearning the computational resource usage from data is challenging because\nresources such as the number of CPU instructions and the number of last level\ncache requests are both time-varying and statistically correlated. Our proposed\nmethod enables learning the joint time-varying stochasticity in computational\nresource usage from the measured profile snapshots in a nonparametric manner.\nThe method can be used to predict the most-likely time-varying distribution of\ncomputational resource availability at a desired time. We provide detailed\nalgorithms for stochastic learning in both single and multi-core cases, discuss\nthe convergence guarantees, computational complexities, and demonstrate their\npractical use in two case studies: a single-core nonlinear model predictive\ncontroller, and a synthetic multi-core software."
                },
                "authors": [
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10951v2",
                "updated": "2025-05-19T17:51:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    17,
                    51,
                    26,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-16T07:39:41Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    39,
                    41,
                    4,
                    136,
                    0
                ],
                "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
                },
                "authors": [
                    {
                        "name": "Qiuyu Zhu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qianxiong Xu"
                    },
                    {
                        "name": "Cheng Long"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13140v1",
                "updated": "2025-05-19T14:09:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T14:09:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    9,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow"
                },
                "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."
                },
                "authors": [
                    {
                        "name": "Takahiro Maeda"
                    },
                    {
                        "name": "Jinkun Cao"
                    },
                    {
                        "name": "Norimichi Ukita"
                    },
                    {
                        "name": "Kris Kitani"
                    }
                ],
                "author_detail": {
                    "name": "Kris Kitani"
                },
                "author": "Kris Kitani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v1",
                "updated": "2025-05-19T13:36:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13094v1",
                "updated": "2025-05-19T13:25:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T13:25:51Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    25,
                    51,
                    0,
                    139,
                    0
                ],
                "title": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Frequency-Based Attention Cache Memory Model for Real-Time Speech\n  Separation"
                },
                "summary": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing causal speech separation models often underperform compared to\nnon-causal models due to difficulties in retaining historical information. To\naddress this, we propose the Time-Frequency Attention Cache Memory (TFACM)\nmodel, which effectively captures spatio-temporal relationships through an\nattention mechanism and cache memory (CM) for historical information storage.\nIn TFACM, an LSTM layer captures frequency-relative positions, while causal\nmodeling is applied to the time dimension using local and global\nrepresentations. The CM module stores past information, and the causal\nattention refinement (CAR) module further enhances time-based feature\nrepresentations for finer granularity. Experimental results showed that TFACM\nachieveed comparable performance to the SOTA TF-GridNet-Causal model, with\nsignificantly lower complexity and fewer trainable parameters. For more\ndetails, visit the project page: https://cslikai.cn/TFACM/."
                },
                "authors": [
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Runxuan Yang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolin Hu"
                },
                "author": "Xiaolin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12946v1",
                "updated": "2025-05-19T10:34:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:34:54Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    34,
                    54,
                    0,
                    139,
                    0
                ],
                "title": "6G-Enabled Smart Railways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G-Enabled Smart Railways"
                },
                "summary": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart railways integrate advanced information technologies into railway\noperating systems to improve efficiency and reliability. Although the\ndevelopment of 5G has enhanced railway services, future smart railways require\nultra-high speeds, ultra-low latency, ultra-high security, full coverage, and\nultra-high positioning accuracy, which 5G cannot fully meet. Therefore, 6G is\nenvisioned to provide green and efficient all-day operations, strong\ninformation security, fully automatic driving, and low-cost intelligent\nmaintenance. To achieve these requirements, we propose an integrated network\narchitecture leveraging communications, computing, edge intelligence, and\ncaching in railway systems. We have conducted in-depth investigations on key\nenabling technologies for reliable transmissions and wireless coverage. For\nhigh-speed mobile scenarios, we propose an AI-enabled cross-domain channel\nmodeling and orthogonal time-frequency space-time spread multiple access\nmechanism to alleviate the conflict between limited spectrum availability and\nmassive user access. The roles of blockchain, edge intelligence, and privacy\ntechnologies in endogenously secure rail communications are also evaluated. We\nfurther explore the application of emerging paradigms such as integrated\nsensing and communications, AI-assisted Internet of Things, semantic\ncommunications, and digital twin networks for railway maintenance, monitoring,\nprediction, and accident warning. Finally, possible future research and\ndevelopment directions are discussed."
                },
                "authors": [
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Yunlong Lu"
                    },
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Guoyu Ma"
                    },
                    {
                        "name": "Yong Niu"
                    },
                    {
                        "name": "Zhangdui Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhangdui Zhong"
                },
                "author": "Zhangdui Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v1",
                "updated": "2025-05-19T10:29:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09928v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09928v2",
                "updated": "2025-05-19T10:13:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    13,
                    31,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-15T03:25:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    25,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles"
                },
                "summary": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
                },
                "authors": [
                    {
                        "name": "Xingchen Sun"
                    },
                    {
                        "name": "Runhua Xu"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09928v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09928v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12742v1",
                "updated": "2025-05-19T05:56:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:56:44Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    56,
                    44,
                    0,
                    139,
                    0
                ],
                "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian\n  Conditioning"
                },
                "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."
                },
                "authors": [
                    {
                        "name": "Jinhua Zhang"
                    },
                    {
                        "name": "Wei Long"
                    },
                    {
                        "name": "Minghao Han"
                    },
                    {
                        "name": "Weiyi You"
                    },
                    {
                        "name": "Shuhang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shuhang Gu"
                },
                "author": "Shuhang Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12731v1",
                "updated": "2025-05-19T05:39:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T05:39:38Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    5,
                    39,
                    38,
                    0,
                    139,
                    0
                ],
                "title": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Adaptive Retrieval Augmented Generation via\n  Instruction-Driven Representation Reduction of Retrieval Overlaps"
                },
                "summary": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has emerged as a pivotal method for\nexpanding the knowledge of large language models. To handle complex queries\nmore effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the\ngenerated quality through multiple interactions with external knowledge bases.\nDespite its effectiveness, A-RAG exacerbates the pre-existing efficiency\nchallenges inherent in RAG, which are attributable to its reliance on multiple\niterations of generation. Existing A-RAG approaches process all retrieved\ncontents from scratch. However, they ignore the situation where there is a\nsignificant overlap in the content of the retrieval results across rounds. The\noverlapping content is redundantly represented, which leads to a large\nproportion of repeated computations, thus affecting the overall efficiency. To\naddress this issue, this paper introduces a model-agnostic approach that can be\ngenerally applied to A-RAG methods, which is dedicated to reducing the\nredundant representation process caused by the overlapping of retrieval\nresults. Specifically, we use cache access and parallel generation to speed up\nthe prefilling and decoding stages respectively. Additionally, we also propose\nan instruction-driven module to further guide the model to more effectively\nattend to each part of the content in a more suitable way for LLMs. Experiments\nshow that our approach achieves 2.79 and 2.33 times significant acceleration on\naverage for prefilling and decoding respectively while maintaining equal\ngeneration quality."
                },
                "authors": [
                    {
                        "name": "Jie Ou"
                    },
                    {
                        "name": "Jinyu Guo"
                    },
                    {
                        "name": "Shuaihong Jiang"
                    },
                    {
                        "name": "Zhaokun Wang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Wenhong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Wenhong Tian"
                },
                "author": "Wenhong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v2",
                "updated": "2025-05-19T02:21:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    21,
                    16,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-domain compression has proven effective in reducing redundancies\nfor spatial signals. In this work, we propose FreqKV, a novel frequency domain\nkey-value (KV) compression technique that enables efficient context window\nextension for decoder-only large language models (LLMs). Our approach is\nmotivated by a key observation that, in the frequency domain, the energy\ndistribution of the KV cache is predominantly concentrated in low-frequency\ncomponents. By discarding high-frequency components, we achieve efficient\ncompression of the KV cache with minimal information loss. FreqKV iteratively\ncompresses the increasing KV cache to a fixed size in the frequency domain,\nallowing models to process lengthy contexts efficiently. Introducing no\nadditional parameters or architectural modifications, FreqKV is applicable to\nboth fine-tuning and inference. With minimal fine-tuning, LLMs can learn to\nleverage the limited cache that is compressed in the frequency domain and\nextend the context window. Experiments on a range of long context language\nmodeling and understanding tasks demonstrate the efficiency and effectiveness\nof the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12594v1",
                "updated": "2025-05-19T01:14:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "published": "2025-05-19T01:14:57Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    1,
                    14,
                    57,
                    0,
                    139,
                    0
                ],
                "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection"
                },
                "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD."
                },
                "authors": [
                    {
                        "name": "Tiankai Yang"
                    },
                    {
                        "name": "Junjun Liu"
                    },
                    {
                        "name": "Wingchun Siu"
                    },
                    {
                        "name": "Jiahang Wang"
                    },
                    {
                        "name": "Zhuangzhuang Qian"
                    },
                    {
                        "name": "Chanjuan Song"
                    },
                    {
                        "name": "Cheng Cheng"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12392v1",
                "updated": "2025-05-18T12:37:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "published": "2025-05-18T12:37:56Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    12,
                    37,
                    56,
                    6,
                    138,
                    0
                ],
                "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLOT: Sample-specific Language Model Optimization at Test-time"
                },
                "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT."
                },
                "authors": [
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Xueji Fang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Huatian Zhang"
                    },
                    {
                        "name": "Guojun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Guojun Qi"
                },
                "author": "Guojun Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18394v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18394v7",
                "updated": "2025-05-18T03:12:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    18,
                    3,
                    12,
                    25,
                    6,
                    138,
                    0
                ],
                "published": "2025-02-25T17:43:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    17,
                    43,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPECTRE: An FFT-Based Efficient Drop-In Replacement to Self-Attention\n  for Long Contexts"
                },
                "summary": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context transformers face significant efficiency challenges due to the\nquadratic cost of self-attention. However, many modern applications-from\nmulti-turn dialogue to high-resolution vision-require contexts spanning tens of\nthousands of tokens. We introduce SPECTRE, a method that replaces each\nattention head with a fast real FFT, a content-adaptive spectral gate, and an\ninverse FFT, reducing per-layer complexity from $\\mathcal{O}(L^{2})$ to\n$O(L\\log L)$ while preserving the surrounding architecture. We extend this\nefficiency to autoregressive generation through our Prefix-FFT cache and\nenhance local feature representation with an optional wavelet module that adds\nnegligible computational overhead. Our experiments demonstrate that SPECTRE\noperates up to 7$\\times$ faster than FlashAttention-2 on 128k-token contexts\nwhile matching or exceeding baseline performance on PG-19 language modeling and\nImageNet-1k classification tasks. SPECTRE achieves these improvements by adding\nfewer than 6\\% parameters to the base model, making hundred-kilotoken context\nprocessing feasible on commodity GPUs without specialized hardware."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18394v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18394v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v3",
                "updated": "2025-05-17T23:26:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    23,
                    26,
                    8,
                    5,
                    137,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v3",
                "updated": "2025-05-17T21:15:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    21,
                    15,
                    2,
                    5,
                    137,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T. Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15804v2",
                "updated": "2025-05-17T12:22:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    12,
                    22,
                    59,
                    5,
                    137,
                    0
                ],
                "published": "2025-02-19T06:14:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    6,
                    14,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference"
                },
                "summary": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache techniques in Transformer models aim to reduce redundant\ncomputations at the expense of substantially increased memory usage, making KV\ncache compression an important and popular research topic. Recently,\nstate-of-the-art KV cache compression methods implement imbalanced, per-head\nallocation algorithms that dynamically adjust the KV cache budget for each\nattention head, achieving excellent performance in single-GPU scenarios.\nHowever, we observe that such imbalanced compression leads to significant load\nimbalance when deploying multi-GPU inference, as some GPUs become overburdened\nwhile others remain underutilized. In this paper, we propose FairKV, a method\ndesigned to ensure fair memory usage among attention heads in systems employing\nimbalanced KV cache compression. The core technique of FairKV is Fair-Copying,\nwhich replicates a small subset of memory-intensive attention heads across GPUs\nusing data parallelism to mitigate load imbalance. Our experiments on popular\nmodels, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV\nincreases throughput by 1.66x compared to standard tensor parallelism\ninference. Our code will be released as open source upon acceptance."
                },
                "authors": [
                    {
                        "name": "Bingzhe Zhao"
                    },
                    {
                        "name": "Ke Cheng"
                    },
                    {
                        "name": "Aomufei Yuan"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Lian Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lian Yu"
                },
                "author": "Lian Yu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14709v1",
                "updated": "2025-05-17T05:00:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    5,
                    0,
                    39,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T05:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    5,
                    0,
                    39,
                    5,
                    137,
                    0
                ],
                "title": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video\n  Generation on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video\n  Generation on the Edge"
                },
                "summary": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car"
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Weize Ma"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Enhao Tang"
                    },
                    {
                        "name": "Yanyue Xie"
                    },
                    {
                        "name": "Zhengang Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Quanyi Wang"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Jun Lin"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Preprint Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11820v1",
                "updated": "2025-05-17T04:06:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T04:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    4,
                    6,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "Chain-of-Model Learning for Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Model Learning for Language Model"
                },
                "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM."
                },
                "authors": [
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Xiaohua Wang"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Cen LU"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Zifan Song"
                    },
                    {
                        "name": "Caihua Shan"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Kan Ren"
                    },
                    {
                        "name": "Xiaoqing Zheng"
                    },
                    {
                        "name": "Tao Qin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11783v1",
                "updated": "2025-05-17T01:31:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "published": "2025-05-17T01:31:21Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    1,
                    31,
                    21,
                    5,
                    137,
                    0
                ],
                "title": "Efficient Vector Search on Disaggregated Memory with d-HNSW",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Vector Search on Disaggregated Memory with d-HNSW"
                },
                "summary": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Fei Fang"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_comment": "To appear in HotStorage 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v1",
                "updated": "2025-05-16T21:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11625v1",
                "updated": "2025-05-16T18:41:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T18:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    18,
                    41,
                    33,
                    4,
                    136,
                    0
                ],
                "title": "Nearest Neighbor Multivariate Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nearest Neighbor Multivariate Time Series Forecasting"
                },
                "summary": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate time series (MTS) forecasting has a wide range of applications\nin both industry and academia. Recently, spatial-temporal graph neural networks\n(STGNNs) have gained popularity as MTS forecasting methods. However, current\nSTGNNs can only use the finite length of MTS input data due to the\ncomputational complexity. Moreover, they lack the ability to identify similar\npatterns throughout the entire dataset and struggle with data that exhibit\nsparsely and discontinuously distributed correlations among variables over an\nextensive historical period, resulting in only marginal improvements. In this\narticle, we introduce a simple yet effective k-nearest neighbor MTS forecasting\n( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval\nmechanism over a large datastore of cached series, using representations from\nthe MTS model for similarity search. This approach requires no additional\ntraining and scales to give the MTS model direct access to the whole dataset at\ntest time, resulting in a highly expressive model that consistently improves\nperformance, and has the ability to extract sparse distributed but similar\npatterns spanning over multivariables from the entire dataset. Furthermore, a\nhybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can\ncapture both long-term temporal and short-term spatial-temporal dependencies\nand is shown to provide accurate representation for kNN-MTSfor better\nforecasting. Experimental results on several real-world datasets show a\nsignificant improvement in the forecasting performance of kNN-MTS. The\nquantitative analysis also illustrates the interpretability and efficiency of\nkNN-MTS, showing better application prospects and opening up a new path for\nefficiently using the large dataset in MTS models."
                },
                "authors": [
                    {
                        "name": "Huiliang Zhang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Lijun Sun"
                    },
                    {
                        "name": "Benoit Boulet"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Boulet"
                },
                "author": "Benoit Boulet",
                "arxiv_doi": "10.1109/TNNLS.2024.3490603",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNNLS.2024.3490603",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Trans. Neural Netw. Learn. Syst., early access, 14 Nov. 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11302v1",
                "updated": "2025-05-16T14:30:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    30,
                    46,
                    4,
                    136,
                    0
                ],
                "title": "Depth first representations of $k^2$-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth first representations of $k^2$-trees"
                },
                "summary": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k^2$-tree is a compact data structure designed to efficiently store\nsparse binary matrices by leveraging both sparsity and clustering of nonzero\nelements. This representation supports efficiently navigational operations and\ncomplex binary operations, such as matrix-matrix multiplication, while\nmaintaining space efficiency. The standard $k^2$-tree follows a level-by-level\nrepresentation, which, while effective, prevents further compression of\nidentical subtrees and it si not cache friendly when accessing individual\nsubtrees. In this work, we introduce some novel depth-first representations of\nthe $k^2$-tree and propose an efficient linear-time algorithm to identify and\ncompress identical subtrees within these structures. Our experimental results\nshow that the use of a depth-first representations is a strategy worth\npursuing: for the adjacency matrix of web graphs exploiting the presence of\nidentical subtrees does improve the compression ratio, and for some matrices\ndepth-first representations turns out to be faster than the standard $k^2$-tree\nin computing the matrix-matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Gabriel Carmona"
                    },
                    {
                        "name": "Giovanni Manzini"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Manzini"
                },
                "author": "Giovanni Manzini",
                "arxiv_comment": "extended submission for SPIRE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11271v1",
                "updated": "2025-05-16T14:04:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T14:04:31Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    14,
                    4,
                    31,
                    4,
                    136,
                    0
                ],
                "title": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching of Contextual Summaries for Efficient\n  Question-Answering with Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants."
                },
                "authors": [
                    {
                        "name": "Camille Couturier"
                    },
                    {
                        "name": "Spyros Mastorakis"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Victor Rühle"
                    }
                ],
                "author_detail": {
                    "name": "Victor Rühle"
                },
                "author": "Victor Rühle",
                "arxiv_comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.10272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.10272v2",
                "updated": "2025-05-16T13:56:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    13,
                    56,
                    7,
                    4,
                    136,
                    0
                ],
                "published": "2022-09-21T11:24:10Z",
                "published_parsed": [
                    2022,
                    9,
                    21,
                    11,
                    24,
                    10,
                    2,
                    264,
                    0
                ],
                "title": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Continuous Basic Graph Patterns over Dynamic Link Data Graphs"
                },
                "summary": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate the problem of evaluating Basic Graph Patterns\n(BGP, for short, a subclass of SPARQL queries) over dynamic Linked Data graphs;\ni.e., Linked Data graphs that are continuously updated. We consider a setting\nwhere the updates are continuously received through a stream of messages and\nsupport both insertions and deletions of triples (updates are straightforwardly\nhandled as a combination of deletions and insertions). In this context, we\npropose a set of in-memory algorithms minimizing the cached data to efficiently\nand continuously answer BGP queries. The queries are typically submitted into a\nsystem and continuously result in the delta answers while the update messages\nare processed.\n  To efficiently and continuously evaluate the submitted query over the\nstreaming data, as well as to minimize the amount of cached data, we propose an\napproach where the submitted query is decomposed into simpler subqueries and\nthe query evaluation is achieved by combining the intermediate answers of the\nsubqueries. Using this approach, the proposed algorithms compute the delta\nanswers of a BGP query in polynomial time and space. Note that for certain\nsubclasses of BGP queries, we show that the evaluation can be achieved in\nconstant or linear time and space. Consolidating all the historical delta\nanswers, the algorithms ensure that the answer to each query is constructed at\nany given time."
                },
                "authors": [
                    {
                        "name": "Manolis Gergatsoulis"
                    },
                    {
                        "name": "Matthew Damigos"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Damigos"
                },
                "author": "Matthew Damigos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.10272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.10272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16525v2",
                "updated": "2025-05-16T12:42:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    42,
                    48,
                    4,
                    136,
                    0
                ],
                "published": "2025-03-17T16:43:35Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    16,
                    43,
                    35,
                    0,
                    76,
                    0
                ],
                "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant\n  KV Cache Reuse"
                },
                "summary": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Renji Zhang"
                    },
                    {
                        "name": "Mingzhe Huang"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Yin Tang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Deyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhang"
                },
                "author": "Deyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v3",
                "updated": "2025-05-16T12:32:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    12,
                    32,
                    36,
                    4,
                    136,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v2",
                "updated": "2025-05-16T09:40:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    9,
                    40,
                    1,
                    4,
                    136,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10938v1",
                "updated": "2025-05-16T07:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T07:23:12Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    12,
                    4,
                    136,
                    0
                ],
                "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate KV Cache Quantization with Outlier Tokens Tracing"
                },
                "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v3",
                "updated": "2025-05-16T03:34:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    34,
                    33,
                    4,
                    136,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. Code is available at:\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Accepted by TMLR, 23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10806v1",
                "updated": "2025-05-16T03:01:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "published": "2025-05-16T03:01:47Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    3,
                    1,
                    47,
                    4,
                    136,
                    0
                ],
                "title": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Communication Efficient Large-Scale Distributed Training of\n  Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have achieved state-of-the-art (SOTA)\nperformance in diverse domains. However, training GNNs on large-scale graphs\nposes significant challenges due to high memory demands and significant\ncommunication overhead in distributed settings. Traditional sampling-based\napproaches mitigate computation load to some extent but often fail to address\ncommunication inefficiencies inherent in distributed environments. This paper\npresents RapidGNN that introduces a deterministic sampling strategy to\nprecompute mini-batches. By leveraging the sampling strategy, RapidGNN\naccurately anticipates feature access patterns, enabling optimal cache\nconstruction and timely prefetching of remote features. This reduces the\nfrequency and latency of remote data transfers without compromising the\nstochastic nature of training. Evaluations on Reddit and OGBN-Products datasets\ndemonstrate that RapidGNN achieves significant reductions in training time and\nremote feature fetches, outperforming existing models in both communication\nefficiency and throughput. Our findings highlight RapidGNN's potential for\nscalable, high-performance GNN training across large, real-world graph datasets\nalong with improving energy efficiency. Our model improves end-to-end training\nthroughput by 2.10x on average over SOTA model GraphSAGE-METIS (up to 2.45x in\nsome settings), while cutting remote feature fetches by over 4x. It also\nreduces energy consumption up to 23%."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v2",
                "updated": "2025-05-16T00:56:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    16,
                    0,
                    56,
                    30,
                    4,
                    136,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Accepted to MLSys 2025,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10560v1",
                "updated": "2025-05-15T17:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "title": "Approximation-First Timeseries Monitoring Query At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximation-First Timeseries Monitoring Query At Scale"
                },
                "summary": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch."
                },
                "authors": [
                    {
                        "name": "Zeying Zhu"
                    },
                    {
                        "name": "Jonathan Chamberlain"
                    },
                    {
                        "name": "Kenny Wu"
                    },
                    {
                        "name": "David Starobinski"
                    },
                    {
                        "name": "Zaoxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zaoxing Liu"
                },
                "author": "Zaoxing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11554v1",
                "updated": "2025-05-15T16:40:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:40:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for\n  Multicore Real-Time Systems"
                },
                "summary": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory bandwidth regulation and cache partitioning are widely used techniques\nfor achieving predictable timing in real-time computing systems. Combined with\npartitioned scheduling, these methods require careful co-allocation of tasks\nand resources to cores, as task execution times strongly depend on available\nallocated resources. To address this challenge, this paper presents a 0-1\nlinear program for task-resource co-allocation, along with a multi-objective\nheuristic designed to minimize resource usage while guaranteeing schedulability\nunder a preemptive EDF scheduling policy. Our heuristic employs a multi-layer\nframework, where an outer layer explores resource allocations using\nPareto-pruned search, and an inner layer optimizes task allocation by solving a\nknapsack problem using dynamic programming. To evaluate the performance of the\nproposed optimization algorithm, we profile real-world benchmarks on an\nembedded AMD UltraScale+ ZCU102 platform, with fine-grained resource\npartitioning enabled by the Jailhouse hypervisor, leveraging cache set\npartitioning and MemGuard for memory bandwidth regulation. Experiments based on\nthe benchmarking results show that the proposed 0-1 linear program outperforms\nexisting mixed-integer programs by finding more optimal solutions within the\nsame time limit. Moreover, the proposed multi-objective multi-layer heuristic\nperforms consistently better than the state-of-the-art multi-resource-task\nco-allocation algorithm in terms of schedulability, resource usage, number of\nnon-dominated solutions, and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Binqi Sun"
                    },
                    {
                        "name": "Zhihang Wei"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Debayan Roy"
                    },
                    {
                        "name": "Mirco Theile"
                    },
                    {
                        "name": "Tomasz Kloda"
                    },
                    {
                        "name": "Rodolfo Pellizzoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_doi": "10.4230/LIPIcs.ECRTS.2025.7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ECRTS.2025.7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.11554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in the 37th Euromicro Conference on Real-Time Systems (ECRTS\n  2025)",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v7",
                "updated": "2025-05-15T13:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v3",
                "updated": "2025-05-15T03:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    29,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v2",
                "updated": "2025-05-15T03:27:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    27,
                    28,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Zongming Guo"
                    },
                    {
                        "name": "Xinggong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggong Zhang"
                },
                "author": "Xinggong Zhang",
                "arxiv_doi": "10.1145/3735358.3735383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3735358.3735383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages (excluding references), 10 figures, to appear in APNET 2025",
                "arxiv_journal_ref": "Proc. 9th Asia-Pacific Workshop on Networking (APNET), Aug 2025,\n  Paper No. 24",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10584v1",
                "updated": "2025-05-14T13:39:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T13:39:53Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    39,
                    53,
                    2,
                    134,
                    0
                ],
                "title": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aquarius: A Family of Industry-Level Video Generation Models for\n  Marketing Scenarios"
                },
                "summary": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates."
                },
                "authors": [
                    {
                        "name": "Huafeng Shi"
                    },
                    {
                        "name": "Jianzhong Liang"
                    },
                    {
                        "name": "Rongchang Xie"
                    },
                    {
                        "name": "Xian Wu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08958v1",
                "updated": "2025-05-13T20:51:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:51:59Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Entanglement Generation for Quantum Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Entanglement Generation for Quantum Routing"
                },
                "summary": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%."
                },
                "authors": [
                    {
                        "name": "Tasdiqul Islam"
                    },
                    {
                        "name": "Md Arifuzzaman"
                    },
                    {
                        "name": "Engin Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Arslan"
                },
                "author": "Engin Arslan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicolás A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.17020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17020v1",
                "updated": "2025-05-22T17:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual\n  Cross-Attention Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual\n  Cross-Attention Mechanisms"
                },
                "summary": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources."
                },
                "authors": [
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Joey Tsai"
                    },
                    {
                        "name": "Hongwei Xue"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Lingyi Hong"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Ray Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ray Zhang"
                },
                "author": "Ray Zhang",
                "arxiv_comment": "Project page: https://github.com/shilinyan99/CrossLMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17017v1",
                "updated": "2025-05-22T17:59:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:59:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO"
                },
                "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"
                },
                "authors": [
                    {
                        "name": "Chengzhuo Tong"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Wenyu Shan"
                    },
                    {
                        "name": "Xinyu Wei"
                    },
                    {
                        "name": "Zhenghao Xing"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "arxiv_comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06776v2",
                "updated": "2025-05-22T17:59:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    11,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-10T18:54:05Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    54,
                    5,
                    0,
                    41,
                    0
                ],
                "title": "InSTA: Towards Internet-Scale Training For Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InSTA: Towards Internet-Scale Training For Agents"
                },
                "summary": "The predominant approach for training web navigation agents is to gather\nhuman demonstrations for a set of popular websites and hand-written tasks, but\nit is becoming clear that human data is an inefficient resource. We develop a\npipeline to facilitate internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM annotates 150k sites with agentic\ntasks. In the next stage, LLM agents complete tasks and produce trajectories.\nIn the final stage, an LLM filters trajectories by judging their success.\nLanguage models are powerful data curation tools, identifying harmful content\nwith an accuracy of 97%, judging successful trajectories with an accuracy of\n82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that\nare competitive with frontier LLMs as web agents, while being smaller and\nfaster. Our top agent reaches a success rate of 56.9%, outperforming the data\ncollection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and\nreaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code,\nmodels and data at: https://data-for-agents.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The predominant approach for training web navigation agents is to gather\nhuman demonstrations for a set of popular websites and hand-written tasks, but\nit is becoming clear that human data is an inefficient resource. We develop a\npipeline to facilitate internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM annotates 150k sites with agentic\ntasks. In the next stage, LLM agents complete tasks and produce trajectories.\nIn the final stage, an LLM filters trajectories by judging their success.\nLanguage models are powerful data curation tools, identifying harmful content\nwith an accuracy of 97%, judging successful trajectories with an accuracy of\n82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that\nare competitive with frontier LLMs as web agents, while being smaller and\nfaster. Our top agent reaches a success rate of 56.9%, outperforming the data\ncollection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and\nreaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code,\nmodels and data at: https://data-for-agents.github.io."
                },
                "authors": [
                    {
                        "name": "Brandon Trabucco"
                    },
                    {
                        "name": "Gunnar Sigurdsson"
                    },
                    {
                        "name": "Robinson Piramuthu"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    }
                ],
                "author_detail": {
                    "name": "Ruslan Salakhutdinov"
                },
                "author": "Ruslan Salakhutdinov",
                "arxiv_comment": "Improved results, zero-shot transfer to Web Voyager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17011v1",
                "updated": "2025-05-22T17:59:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:59:02Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    2,
                    3,
                    142,
                    0
                ],
                "title": "Learning Adaptive and Temporally Causal Video Tokenization in a 1D\n  Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Adaptive and Temporally Causal Video Tokenization in a 1D\n  Latent Space"
                },
                "summary": "We propose AdapTok, an adaptive temporal causal video tokenizer that can\nflexibly allocate tokens for different frames based on video content. AdapTok\nis equipped with a block-wise masking strategy that randomly drops tail tokens\nof each block during training, and a block causal scorer to predict the\nreconstruction quality of video frames using different numbers of tokens.\nDuring inference, an adaptive token allocation strategy based on integer linear\nprogramming is further proposed to adjust token usage given predicted scores.\nSuch design allows for sample-wise, content-aware, and temporally dynamic token\nallocation under a controllable overall budget. Extensive experiments for video\nreconstruction and generation on UCF-101 and Kinetics-600 demonstrate the\neffectiveness of our approach. Without additional image data, AdapTok\nconsistently improves reconstruction quality and generation performance under\ndifferent token budgets, allowing for more scalable and token-efficient\ngenerative video modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose AdapTok, an adaptive temporal causal video tokenizer that can\nflexibly allocate tokens for different frames based on video content. AdapTok\nis equipped with a block-wise masking strategy that randomly drops tail tokens\nof each block during training, and a block causal scorer to predict the\nreconstruction quality of video frames using different numbers of tokens.\nDuring inference, an adaptive token allocation strategy based on integer linear\nprogramming is further proposed to adjust token usage given predicted scores.\nSuch design allows for sample-wise, content-aware, and temporally dynamic token\nallocation under a controllable overall budget. Extensive experiments for video\nreconstruction and generation on UCF-101 and Kinetics-600 demonstrate the\neffectiveness of our approach. Without additional image data, AdapTok\nconsistently improves reconstruction quality and generation performance under\ndifferent token budgets, allowing for more scalable and token-efficient\ngenerative video modeling."
                },
                "authors": [
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Ning Liao"
                    },
                    {
                        "name": "Weiwei Guo"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Xue Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xue Yang"
                },
                "author": "Xue Yang",
                "arxiv_comment": "Code: https://github.com/VisionXLab/AdapTok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17005v1",
                "updated": "2025-05-22T17:58:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    58,
                    26,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:58:26Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    58,
                    26,
                    3,
                    142,
                    0
                ],
                "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus."
                },
                "authors": [
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Wenqing Tian"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yuhuan Wu"
                    },
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17004v1",
                "updated": "2025-05-22T17:58:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    58,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:58:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    58,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs"
                },
                "summary": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general framework for conditional sampling in PDE-based inverse\nproblems, targeting the recovery of whole solutions from extremely sparse or\nnoisy measurements. This is accomplished by a function-space diffusion model\nand plug-and-play guidance for conditioning. Our method first trains an\nunconditional discretization-agnostic denoising model using neural operator\narchitectures. At inference, we refine the samples to satisfy sparse\nobservation data via a gradient-based guidance mechanism. Through rigorous\nmathematical analysis, we extend Tweedie's formula to infinite-dimensional\nHilbert spaces, providing the theoretical foundation for our posterior sampling\napproach. Our method (FunDPS) accurately captures posterior distributions in\nfunction spaces under minimal supervision and severe data scarcity. Across five\nPDE tasks with only 3% observation, our method achieves an average 32% accuracy\nimprovement over state-of-the-art fixed-resolution diffusion baselines while\nreducing sampling steps by 4x. Furthermore, multi-resolution fine-tuning\nensures strong cross-resolution generalizability. To the best of our knowledge,\nthis is the first diffusion-based framework to operate independently of\ndiscretization, offering a practical and flexible solution for forward and\ninverse problems in the context of PDEs. Code is available at\nhttps://github.com/neuraloperator/FunDPS"
                },
                "authors": [
                    {
                        "name": "Jiachen Yao"
                    },
                    {
                        "name": "Abbas Mammadov"
                    },
                    {
                        "name": "Julius Berner"
                    },
                    {
                        "name": "Gavin Kerrigan"
                    },
                    {
                        "name": "Jong Chul Ye"
                    },
                    {
                        "name": "Kamyar Azizzadenesheli"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16998v1",
                "updated": "2025-05-22T17:57:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    57,
                    23,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:57:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    57,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?"
                },
                "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval."
                },
                "authors": [
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Liangcai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Liangcai Gao"
                },
                "author": "Liangcai Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16997v1",
                "updated": "2025-05-22T17:56:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:56:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs"
                },
                "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qimin Wu"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16995v1",
                "updated": "2025-05-22T17:56:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:56:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "DecoupledESC: Enhancing Emotional Support Generation via\n  Strategy-Response Decoupled Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecoupledESC: Enhancing Emotional Support Generation via\n  Strategy-Response Decoupled Preference Optimization"
                },
                "summary": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality."
                },
                "authors": [
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Xueqiao Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yawei Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yawei Luo"
                },
                "author": "Yawei Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05066v2",
                "updated": "2025-05-22T17:55:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-07T01:11:39Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    1,
                    11,
                    39,
                    4,
                    66,
                    0
                ],
                "title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of\n  Experts"
                },
                "summary": "The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct."
                },
                "authors": [
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Weilin Cai"
                    },
                    {
                        "name": "Jiayi Huang"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16994v1",
                "updated": "2025-05-22T17:55:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    43,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:55:43Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    43,
                    3,
                    142,
                    0
                ],
                "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning"
                },
                "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec."
                },
                "authors": [
                    {
                        "name": "Runyang You"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16990v1",
                "updated": "2025-05-22T17:55:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    4,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:55:04Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    4,
                    3,
                    142,
                    0
                ],
                "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel\n  Decoding"
                },
                "summary": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only $\\frac{\\text{response\nlength}}{3}$. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only $\\frac{\\text{response\nlength}}{3}$. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple."
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16988v1",
                "updated": "2025-05-22T17:54:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent\n  Systems"
                },
                "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Keduan Huang"
                    },
                    {
                        "name": "Qimin Wu"
                    },
                    {
                        "name": "Yuzhu Cai"
                    },
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Jiaqi Su"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Kaiqu Liang"
                    },
                    {
                        "name": "Jiaao Chen"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Rongye Shi"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Wenjun Wu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16984v1",
                "updated": "2025-05-22T17:53:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:53:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UFT: Unifying Supervised and Reinforcement Fine-Tuning"
                },
                "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Mingyang Liu"
                    },
                    {
                        "name": "Gabriele Farina"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    }
                ],
                "author_detail": {
                    "name": "Asuman Ozdaglar"
                },
                "author": "Asuman Ozdaglar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16983v1",
                "updated": "2025-05-22T17:53:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    28,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:53:28Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    28,
                    3,
                    142,
                    0
                ],
                "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch\n  Mismatches with Group Position Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as Effective Streaming Processor: Bridging Streaming-Batch\n  Mismatches with Group Position Encoding"
                },
                "summary": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM."
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Zixuan Lin"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16982v1",
                "updated": "2025-05-22T17:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    59,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:52:59Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    59,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Correlation: Towards Causal Large Language Model Agents in\n  Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Correlation: Towards Causal Large Language Model Agents in\n  Biomedicine"
                },
                "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress."
                },
                "authors": [
                    {
                        "name": "Adib Bazgir"
                    },
                    {
                        "name": "Amir Habibdoust Lafmajani"
                    },
                    {
                        "name": "Yuwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwen Zhang"
                },
                "author": "Yuwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16979v1",
                "updated": "2025-05-22T17:52:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    33,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:52:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\n  Design"
                },
                "summary": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired."
                },
                "authors": [
                    {
                        "name": "Zhenkun Li"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16978v1",
                "updated": "2025-05-22T17:52:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    31,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:52:31Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    31,
                    3,
                    142,
                    0
                ],
                "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation"
                },
                "summary": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs."
                },
                "authors": [
                    {
                        "name": "Weizhi Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Chris Sypherd"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    },
                    {
                        "name": "Vaishak Belle"
                    }
                ],
                "author_detail": {
                    "name": "Vaishak Belle"
                },
                "author": "Vaishak Belle",
                "arxiv_comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/RutaTang/HyGenar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16976v1",
                "updated": "2025-05-22T17:51:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "Creatively Upscaling Images with Global-Regional Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creatively Upscaling Images with Global-Regional Priors"
                },
                "summary": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details."
                },
                "authors": [
                    {
                        "name": "Yurui Qian"
                    },
                    {
                        "name": "Qi Cai"
                    },
                    {
                        "name": "Yingwei Pan"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Tao Mei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Mei"
                },
                "author": "Tao Mei",
                "arxiv_comment": "International Journal of Computer Vision (IJCV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16975v1",
                "updated": "2025-05-22T17:51:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software\n  Development"
                },
                "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Yuzhu Cai"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yu Qian"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16974v1",
                "updated": "2025-05-22T17:51:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    48,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:48Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    48,
                    3,
                    142,
                    0
                ],
                "title": "OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step\n  Visual Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step\n  Visual Reasoning"
                },
                "summary": "Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its\ncapacity to generalize segmentation beyond predefined categories. However,\nexisting methods typically predict segmentation masks with simple forward\ninference, lacking explicit reasoning and interpretability. This makes it\nchallenging for OVS model to distinguish similar categories in open-world\nsettings due to the lack of contextual understanding and discriminative visual\ncues. To address this limitation, we propose a step-by-step visual reasoning\nframework for open-vocabulary segmentation, named OpenSeg-R. The proposed\nOpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical\nvisual reasoning before segmentation. Specifically, we generate both generic\nand image-specific reasoning for each image, forming structured triplets that\nexplain the visual reason for objects in a coarse-to-fine manner. Based on\nthese reasoning steps, we can compose detailed description prompts, and feed\nthem to the segmentor to produce more accurate segmentation masks. To the best\nof our knowledge, OpenSeg-R is the first framework to introduce explicit\nstep-by-step visual reasoning into OVS. Experimental results demonstrate that\nOpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary\nsemantic segmentation across five benchmark datasets. Moreover, it achieves\nconsistent gains across all metrics on open-vocabulary panoptic segmentation.\nQualitative results further highlight the effectiveness of our reasoning-guided\nframework in improving both segmentation precision and interpretability. Our\ncode is publicly available at https://github.com/Hanzy1996/OpenSeg-R.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its\ncapacity to generalize segmentation beyond predefined categories. However,\nexisting methods typically predict segmentation masks with simple forward\ninference, lacking explicit reasoning and interpretability. This makes it\nchallenging for OVS model to distinguish similar categories in open-world\nsettings due to the lack of contextual understanding and discriminative visual\ncues. To address this limitation, we propose a step-by-step visual reasoning\nframework for open-vocabulary segmentation, named OpenSeg-R. The proposed\nOpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical\nvisual reasoning before segmentation. Specifically, we generate both generic\nand image-specific reasoning for each image, forming structured triplets that\nexplain the visual reason for objects in a coarse-to-fine manner. Based on\nthese reasoning steps, we can compose detailed description prompts, and feed\nthem to the segmentor to produce more accurate segmentation masks. To the best\nof our knowledge, OpenSeg-R is the first framework to introduce explicit\nstep-by-step visual reasoning into OVS. Experimental results demonstrate that\nOpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary\nsemantic segmentation across five benchmark datasets. Moreover, it achieves\nconsistent gains across all metrics on open-vocabulary panoptic segmentation.\nQualitative results further highlight the effectiveness of our reasoning-guided\nframework in improving both segmentation precision and interpretability. Our\ncode is publicly available at https://github.com/Hanzy1996/OpenSeg-R."
                },
                "authors": [
                    {
                        "name": "Zongyan Han"
                    },
                    {
                        "name": "Jiale Cao"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Tong Wang"
                    },
                    {
                        "name": "Jorma Laaksonen"
                    },
                    {
                        "name": "Rao Muhammad Anwer"
                    }
                ],
                "author_detail": {
                    "name": "Rao Muhammad Anwer"
                },
                "author": "Rao Muhammad Anwer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16973v1",
                "updated": "2025-05-22T17:51:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "VeriFastScore: Speeding up long-form factuality evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriFastScore: Speeding up long-form factuality evaluation"
                },
                "summary": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets."
                },
                "authors": [
                    {
                        "name": "Rishanth Rajendhran"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Matthew Sarte"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16971v1",
                "updated": "2025-05-22T17:50:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    50,
                    52,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:50:52Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    50,
                    52,
                    3,
                    142,
                    0
                ],
                "title": "UniPhy: Learning a Unified Constitutive Model for Inverse Physics\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniPhy: Learning a Unified Constitutive Model for Inverse Physics\n  Simulation"
                },
                "summary": "We propose UniPhy, a common latent-conditioned neural constitutive model that\ncan encode the physical properties of diverse materials. At inference UniPhy\nallows `inverse simulation' i.e. inferring material properties by optimizing\nthe scene-specific latent to match the available observations via\ndifferentiable simulation. In contrast to existing methods that treat such\ninference as system identification, UniPhy does not rely on user-specified\nmaterial type information. Compared to prior neural constitutive modeling\napproaches which learn instance specific networks, the shared training across\nmaterials improves both, robustness and accuracy of the estimates. We train\nUniPhy using simulated trajectories across diverse geometries and materials --\nelastic, plasticine, sand, and fluids (Newtonian & non-Newtonian). At\ninference, given an object with unknown material properties, UniPhy can infer\nthe material properties via latent optimization to match the motion\nobservations, and can then allow re-simulating the object under diverse\nscenarios. We compare UniPhy against prior inverse simulation methods, and show\nthat the inference from UniPhy enables more accurate replay and re-simulation\nunder novel conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose UniPhy, a common latent-conditioned neural constitutive model that\ncan encode the physical properties of diverse materials. At inference UniPhy\nallows `inverse simulation' i.e. inferring material properties by optimizing\nthe scene-specific latent to match the available observations via\ndifferentiable simulation. In contrast to existing methods that treat such\ninference as system identification, UniPhy does not rely on user-specified\nmaterial type information. Compared to prior neural constitutive modeling\napproaches which learn instance specific networks, the shared training across\nmaterials improves both, robustness and accuracy of the estimates. We train\nUniPhy using simulated trajectories across diverse geometries and materials --\nelastic, plasticine, sand, and fluids (Newtonian & non-Newtonian). At\ninference, given an object with unknown material properties, UniPhy can infer\nthe material properties via latent optimization to match the motion\nobservations, and can then allow re-simulating the object under diverse\nscenarios. We compare UniPhy against prior inverse simulation methods, and show\nthat the inference from UniPhy enables more accurate replay and re-simulation\nunder novel conditions."
                },
                "authors": [
                    {
                        "name": "Himangi Mittal"
                    },
                    {
                        "name": "Peiye Zhuang"
                    },
                    {
                        "name": "Hsin-Ying Lee"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    }
                ],
                "author_detail": {
                    "name": "Shubham Tulsiani"
                },
                "author": "Shubham Tulsiani",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14625v2",
                "updated": "2025-05-22T17:49:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    49,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-20T17:16:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning"
                },
                "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Bhaskar Ramasubramanian"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16968v1",
                "updated": "2025-05-22T17:48:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    48,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:48:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    48,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark"
                },
                "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."
                },
                "authors": [
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Sarim Hashmi"
                    },
                    {
                        "name": "Gustavo Bertolo Stahl"
                    },
                    {
                        "name": "Seung Hun Eddie Han"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Abdulrahman Mahmoud"
                    }
                ],
                "author_detail": {
                    "name": "Abdulrahman Mahmoud"
                },
                "author": "Abdulrahman Mahmoud",
                "arxiv_comment": "20 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16967v1",
                "updated": "2025-05-22T17:47:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    47,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:47:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    47,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval"
                },
                "summary": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16964v1",
                "updated": "2025-05-22T17:46:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    46,
                    11,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:46:11Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    46,
                    11,
                    3,
                    142,
                    0
                ],
                "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning"
                },
                "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Suhao Yu"
                    },
                    {
                        "name": "Haojin Wang"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Yuyin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuyin Zhou"
                },
                "author": "Yuyin Zhou",
                "arxiv_comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16957v1",
                "updated": "2025-05-22T17:36:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    36,
                    33,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:36:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    36,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External\n  Resources for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Prompts, Visible Threats: Malicious Font Injection in External\n  Resources for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content."
                },
                "authors": [
                    {
                        "name": "Junjie Xiong"
                    },
                    {
                        "name": "Changjia Zhu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Lingyao Li"
                    }
                ],
                "author_detail": {
                    "name": "Lingyao Li"
                },
                "author": "Lingyao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00278v2",
                "updated": "2025-05-22T17:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    36,
                    3,
                    3,
                    142,
                    0
                ],
                "published": "2024-11-01T00:24:15Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    0,
                    24,
                    15,
                    4,
                    306,
                    0
                ],
                "title": "KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAN-AD: Time Series Anomaly Detection with Kolmogorov-Arnold Networks"
                },
                "summary": "Time series anomaly detection (TSAD) underpins real-time monitoring in cloud\nservices and web systems, allowing rapid identification of anomalies to prevent\ncostly failures. Most TSAD methods driven by forecasting models tend to overfit\nby emphasizing minor fluctuations. Our analysis reveals that effective TSAD\nshould focus on modeling \"normal\" behavior through smooth local patterns. To\nachieve this, we reformulate time series modeling as approximating the series\nwith smooth univariate functions. The local smoothness of each univariate\nfunction ensures that the fitted time series remains resilient against local\ndisturbances. However, a direct KAN implementation proves susceptible to these\ndisturbances due to the inherently localized characteristics of B-spline\nfunctions. We thus propose KAN-AD, replacing B-splines with truncated Fourier\nexpansions and introducing a novel lightweight learning mechanism that\nemphasizes global patterns while staying robust to local disturbances. On four\npopular TSAD benchmarks, KAN-AD achieves an average 15% improvement in\ndetection accuracy (with peaks exceeding 27%) over state-of-the-art baselines.\nRemarkably, it requires fewer than 1,000 trainable parameters, resulting in a\n50% faster inference speed compared to the original KAN, demonstrating the\napproach's efficiency and practical viability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly detection (TSAD) underpins real-time monitoring in cloud\nservices and web systems, allowing rapid identification of anomalies to prevent\ncostly failures. Most TSAD methods driven by forecasting models tend to overfit\nby emphasizing minor fluctuations. Our analysis reveals that effective TSAD\nshould focus on modeling \"normal\" behavior through smooth local patterns. To\nachieve this, we reformulate time series modeling as approximating the series\nwith smooth univariate functions. The local smoothness of each univariate\nfunction ensures that the fitted time series remains resilient against local\ndisturbances. However, a direct KAN implementation proves susceptible to these\ndisturbances due to the inherently localized characteristics of B-spline\nfunctions. We thus propose KAN-AD, replacing B-splines with truncated Fourier\nexpansions and introducing a novel lightweight learning mechanism that\nemphasizes global patterns while staying robust to local disturbances. On four\npopular TSAD benchmarks, KAN-AD achieves an average 15% improvement in\ndetection accuracy (with peaks exceeding 27%) over state-of-the-art baselines.\nRemarkably, it requires fewer than 1,000 trainable parameters, resulting in a\n50% faster inference speed compared to the original KAN, demonstrating the\napproach's efficiency and practical viability."
                },
                "authors": [
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Changhua Pei"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Jing Han"
                    },
                    {
                        "name": "Zhengwei Gao"
                    },
                    {
                        "name": "Dan Pei"
                    },
                    {
                        "name": "Haiming Zhang"
                    },
                    {
                        "name": "Gaogang Xie"
                    },
                    {
                        "name": "Jianhui Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianhui Li"
                },
                "author": "Jianhui Li",
                "arxiv_comment": "11 pages, ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16954v1",
                "updated": "2025-05-22T17:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    45,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:34:45Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    45,
                    3,
                    142,
                    0
                ],
                "title": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of\n  Vulnerabilities in Privacy Protection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of\n  Vulnerabilities in Privacy Protection"
                },
                "summary": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection."
                },
                "authors": [
                    {
                        "name": "Jiaying Fu"
                    },
                    {
                        "name": "Yiyang Lu"
                    },
                    {
                        "name": "Zehua Yang"
                    },
                    {
                        "name": "Fiona Nah"
                    },
                    {
                        "name": "RAY LC"
                    }
                ],
                "author_detail": {
                    "name": "RAY LC"
                },
                "author": "RAY LC",
                "arxiv_doi": "10.1145/3715336.3735812",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715336.3735812",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.16954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, In Designing Interactive Systems Conference (DIS 25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16952v1",
                "updated": "2025-05-22T17:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "A Comprehensive Evaluation of Contemporary ML-Based Solvers for\n  Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Contemporary ML-Based Solvers for\n  Combinatorial Optimization"
                },
                "summary": "Machine learning (ML) has demonstrated considerable potential in supporting\nmodel design and optimization for combinatorial optimization (CO) problems.\nHowever, much of the progress to date has been evaluated on small-scale,\nsynthetic datasets, raising concerns about the practical effectiveness of\nML-based solvers in real-world, large-scale CO scenarios. Additionally, many\nexisting CO benchmarks lack sufficient training data, limiting their utility\nfor evaluating data-driven approaches. To address these limitations, we\nintroduce FrontierCO, a comprehensive benchmark that covers eight canonical CO\nproblem types and evaluates 16 representative ML-based solvers--including graph\nneural networks and large language model (LLM) agents. FrontierCO features\nchallenging instances drawn from industrial applications and frontier CO\nresearch, offering both realistic problem difficulty and abundant training\ndata. Our empirical results provide critical insights into the strengths and\nlimitations of current ML methods, helping to guide more robust and practically\nrelevant advances at the intersection of machine learning and combinatorial\noptimization. Our data is available at\nhttps://huggingface.co/datasets/CO-Bench/FrontierCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has demonstrated considerable potential in supporting\nmodel design and optimization for combinatorial optimization (CO) problems.\nHowever, much of the progress to date has been evaluated on small-scale,\nsynthetic datasets, raising concerns about the practical effectiveness of\nML-based solvers in real-world, large-scale CO scenarios. Additionally, many\nexisting CO benchmarks lack sufficient training data, limiting their utility\nfor evaluating data-driven approaches. To address these limitations, we\nintroduce FrontierCO, a comprehensive benchmark that covers eight canonical CO\nproblem types and evaluates 16 representative ML-based solvers--including graph\nneural networks and large language model (LLM) agents. FrontierCO features\nchallenging instances drawn from industrial applications and frontier CO\nresearch, offering both realistic problem difficulty and abundant training\ndata. Our empirical results provide critical insights into the strengths and\nlimitations of current ML methods, helping to guide more robust and practically\nrelevant advances at the intersection of machine learning and combinatorial\noptimization. Our data is available at\nhttps://huggingface.co/datasets/CO-Bench/FrontierCO."
                },
                "authors": [
                    {
                        "name": "Shengyu Feng"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Yiming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Yang"
                },
                "author": "Yiming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16947v1",
                "updated": "2025-05-22T17:32:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    32,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:32:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    32,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs"
                },
                "summary": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."
                },
                "authors": [
                    {
                        "name": "Csaba Dékány"
                    },
                    {
                        "name": "Stefan Balauca"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Dimitar I. Dimitrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16944v1",
                "updated": "2025-05-22T17:31:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    31,
                    10,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:31:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    31,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios"
                },
                "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Youfeng Liu"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16943v1",
                "updated": "2025-05-22T17:30:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    30,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:30:58Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    30,
                    58,
                    3,
                    142,
                    0
                ],
                "title": "Sliding Friction of Hard Sliders on Rubber: Theory and Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sliding Friction of Hard Sliders on Rubber: Theory and Experiment"
                },
                "summary": "We present a study of sliding friction for rigid triangular steel sliders on\nsoft rubber substrates under both lubricated and dry conditions. For rubber\nsurfaces lubricated with a thin film of silicone oil, the measured sliding\nfriction at room temperature agrees well with theoretical predictions obtained\nfrom a viscoelastic model originally developed for rolling friction. On the\nlubricated surface, the sliding friction is primarily due to bulk viscoelastic\nenergy dissipation in the rubber. The model, which includes strain-dependent\nsoftening of the rubber modulus, accurately predicts the experimental friction\ncurves. At lower temperatures ($T = -20^\\circ {\\rm C}$ and $-40^\\circ {\\rm\nC}$), the measured friction exceeds the theoretical prediction. We attribute\nthis increase to penetration of the lubricant film by surface asperities,\nleading to a larger adhesive contribution. For dry surfaces, the adhesive\ncontribution becomes dominant. By subtracting the viscoelastic component\ninferred from the lubricated case, we estimate the interfacial frictional shear\nstress. This shear stress increases approximately linearly with the logarithm\nof the sliding speed, consistent with stress-augmented thermal activation\nmechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a study of sliding friction for rigid triangular steel sliders on\nsoft rubber substrates under both lubricated and dry conditions. For rubber\nsurfaces lubricated with a thin film of silicone oil, the measured sliding\nfriction at room temperature agrees well with theoretical predictions obtained\nfrom a viscoelastic model originally developed for rolling friction. On the\nlubricated surface, the sliding friction is primarily due to bulk viscoelastic\nenergy dissipation in the rubber. The model, which includes strain-dependent\nsoftening of the rubber modulus, accurately predicts the experimental friction\ncurves. At lower temperatures ($T = -20^\\circ {\\rm C}$ and $-40^\\circ {\\rm\nC}$), the measured friction exceeds the theoretical prediction. We attribute\nthis increase to penetration of the lubricant film by surface asperities,\nleading to a larger adhesive contribution. For dry surfaces, the adhesive\ncontribution becomes dominant. By subtracting the viscoelastic component\ninferred from the lubricated case, we estimate the interfacial frictional shear\nstress. This shear stress increases approximately linearly with the logarithm\nof the sliding speed, consistent with stress-augmented thermal activation\nmechanisms."
                },
                "authors": [
                    {
                        "name": "R. Xu"
                    },
                    {
                        "name": "B. N. J Persson"
                    }
                ],
                "author_detail": {
                    "name": "B. N. J Persson"
                },
                "author": "B. N. J Persson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16942v1",
                "updated": "2025-05-22T17:30:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    30,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:30:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    30,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical\n  Flow Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical\n  Flow Estimation"
                },
                "summary": "Recent optical flow estimation methods often employ local cost sampling from\na dense all-pairs correlation volume. This results in quadratic computational\nand memory complexity in the number of pixels. Although an alternative\nmemory-efficient implementation with on-demand cost computation exists, this is\nslower in practice and therefore prior methods typically process images at\nreduced resolutions, missing fine-grained details.\n  To address this, we propose a more efficient implementation of the all-pairs\ncorrelation volume sampling, still matching the exact mathematical operator as\ndefined by RAFT. Our approach outperforms on-demand sampling by up to 90% while\nmaintaining low memory usage, and performs on par with the default\nimplementation with up to 95% lower memory usage. As cost sampling makes up a\nsignificant portion of the overall runtime, this can translate to up to 50%\nsavings for the total end-to-end model inference in memory-constrained\nenvironments. Our evaluation of existing methods includes an 8K\nultra-high-resolution dataset and an additional inference-time modification of\nthe recent SEA-RAFT method. With this, we achieve state-of-the-art results at\nhigh resolutions both in accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent optical flow estimation methods often employ local cost sampling from\na dense all-pairs correlation volume. This results in quadratic computational\nand memory complexity in the number of pixels. Although an alternative\nmemory-efficient implementation with on-demand cost computation exists, this is\nslower in practice and therefore prior methods typically process images at\nreduced resolutions, missing fine-grained details.\n  To address this, we propose a more efficient implementation of the all-pairs\ncorrelation volume sampling, still matching the exact mathematical operator as\ndefined by RAFT. Our approach outperforms on-demand sampling by up to 90% while\nmaintaining low memory usage, and performs on par with the default\nimplementation with up to 95% lower memory usage. As cost sampling makes up a\nsignificant portion of the overall runtime, this can translate to up to 50%\nsavings for the total end-to-end model inference in memory-constrained\nenvironments. Our evaluation of existing methods includes an 8K\nultra-high-resolution dataset and an additional inference-time modification of\nthe recent SEA-RAFT method. With this, we achieve state-of-the-art results at\nhigh resolutions both in accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Karlis Martins Briedis"
                    },
                    {
                        "name": "Markus Gross"
                    },
                    {
                        "name": "Christopher Schroers"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Schroers"
                },
                "author": "Christopher Schroers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17956v2",
                "updated": "2025-05-22T17:29:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    29,
                    6,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-25T08:27:28Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    27,
                    28,
                    1,
                    56,
                    0
                ],
                "title": "Towards Better Understanding of Program-of-Thought Reasoning in\n  Cross-Lingual and Multilingual Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Understanding of Program-of-Thought Reasoning in\n  Cross-Lingual and Multilingual Environments"
                },
                "summary": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic."
                },
                "authors": [
                    {
                        "name": "Patomporn Payoungkhamdee"
                    },
                    {
                        "name": "Pume Tuchinda"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Can Udomcharoenchaikit"
                    },
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Ekapol Chuangsuwanich"
                    },
                    {
                        "name": "Sarana Nutanong"
                    }
                ],
                "author_detail": {
                    "name": "Sarana Nutanong"
                },
                "author": "Sarana Nutanong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16936v1",
                "updated": "2025-05-22T17:26:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    26,
                    23,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:26:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    26,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "SPAR: Self-supervised Placement-Aware Representation Learning for\n  Multi-Node IoT Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAR: Self-supervised Placement-Aware Representation Learning for\n  Multi-Node IoT Systems"
                },
                "summary": "This work develops the underpinnings of self-supervised placement-aware\nrepresentation learning given spatially-distributed (multi-view and multimodal)\nsensor observations, motivated by the need to represent external environmental\nstate in multi-sensor IoT systems in a manner that correctly distills spatial\nphenomena from the distributed multi-vantage observations. The objective of\nsensing in IoT systems is, in general, to collectively represent an externally\nobserved environment given multiple vantage points from which sensory\nobservations occur. Pretraining of models that help interpret sensor data must\ntherefore encode the relation between signals observed by sensors and the\nobservers' vantage points in order to attain a representation that encodes the\nobserved spatial phenomena in a manner informed by the specific placement of\nthe measuring instruments, while allowing arbitrary placement. The work\nsignificantly advances self-supervised model pretraining from IoT signals\nbeyond current solutions that often overlook the distinctive spatial nature of\nIoT data. Our framework explicitly learns the dependencies between measurements\nand geometric observer layouts and structural characteristics, guided by a core\ndesign principle: the duality between signals and observer positions. We\nfurther provide theoretical analyses from the perspectives of information\ntheory and occlusion-invariant representation learning to offer insight into\nthe rationale behind our design. Experiments on three real-world\ndatasets--covering vehicle monitoring, human activity recognition, and\nearthquake localization--demonstrate the superior generalizability and\nrobustness of our method across diverse modalities, sensor placements,\napplication-level inference tasks, and spatial scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work develops the underpinnings of self-supervised placement-aware\nrepresentation learning given spatially-distributed (multi-view and multimodal)\nsensor observations, motivated by the need to represent external environmental\nstate in multi-sensor IoT systems in a manner that correctly distills spatial\nphenomena from the distributed multi-vantage observations. The objective of\nsensing in IoT systems is, in general, to collectively represent an externally\nobserved environment given multiple vantage points from which sensory\nobservations occur. Pretraining of models that help interpret sensor data must\ntherefore encode the relation between signals observed by sensors and the\nobservers' vantage points in order to attain a representation that encodes the\nobserved spatial phenomena in a manner informed by the specific placement of\nthe measuring instruments, while allowing arbitrary placement. The work\nsignificantly advances self-supervised model pretraining from IoT signals\nbeyond current solutions that often overlook the distinctive spatial nature of\nIoT data. Our framework explicitly learns the dependencies between measurements\nand geometric observer layouts and structural characteristics, guided by a core\ndesign principle: the duality between signals and observer positions. We\nfurther provide theoretical analyses from the perspectives of information\ntheory and occlusion-invariant representation learning to offer insight into\nthe rationale behind our design. Experiments on three real-world\ndatasets--covering vehicle monitoring, human activity recognition, and\nearthquake localization--demonstrate the superior generalizability and\nrobustness of our method across diverse modalities, sensor placements,\napplication-level inference tasks, and spatial scales."
                },
                "authors": [
                    {
                        "name": "Yizhuo Chen"
                    },
                    {
                        "name": "Tianchen Wang"
                    },
                    {
                        "name": "You Lyu"
                    },
                    {
                        "name": "Yanlan Hu"
                    },
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "Tomoyoshi Kimura"
                    },
                    {
                        "name": "Hongjue Zhao"
                    },
                    {
                        "name": "Yigong Hu"
                    },
                    {
                        "name": "Denizhan Kara"
                    },
                    {
                        "name": "Tarek Abdelzaher"
                    }
                ],
                "author_detail": {
                    "name": "Tarek Abdelzaher"
                },
                "author": "Tarek Abdelzaher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16934v1",
                "updated": "2025-05-22T17:24:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:24:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "In-Context Watermarks for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Watermarks for Large Language Models"
                },
                "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution."
                },
                "authors": [
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Christopher Kruegel"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Yuheng Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Bu"
                },
                "author": "Yuheng Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16929v1",
                "updated": "2025-05-22T17:22:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    22,
                    16,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:22:16Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    22,
                    16,
                    3,
                    142,
                    0
                ],
                "title": "Neutron Star crust informed by nuclear structure data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron Star crust informed by nuclear structure data"
                },
                "summary": "We perform a Bayesian analysis of the neutron star (NS) equation of state\n(EoS) based on a wide set of Skyrme functionals, derived from previous nuclear\nphysics inferences. The novelty of this approach lies in starting from the full\nmultidimensional posterior distribution of nuclear matter parameters,\nconsistent with a comprehensive set of static and dynamic nuclear structure\nobservables. We construct unified EoSs for $npe\\mu$ matter, where the inner\ncrust of the NS is treated using an extended Thomas-Fermi method, providing for\nthe first time a fully consistent Bayesian treatment of the correlation of bulk\nwith surface as well as with spin-orbit and effective mass parameters. We then\nemploy a standard Bayesian framework to identify those EoSs that satisfy\nastrophysical constraints from NS mass measurements, the tidal deformability\nfrom GW170817, and NICER mass-radius observations. We also examine NS\nobservables, such as the crustal moment of inertia, which is crucial in\nunderstanding pulsar glitches. Compared to previous works, we observe an\nincrease in both the NS surface thickness and the crustal moment of inertia.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We perform a Bayesian analysis of the neutron star (NS) equation of state\n(EoS) based on a wide set of Skyrme functionals, derived from previous nuclear\nphysics inferences. The novelty of this approach lies in starting from the full\nmultidimensional posterior distribution of nuclear matter parameters,\nconsistent with a comprehensive set of static and dynamic nuclear structure\nobservables. We construct unified EoSs for $npe\\mu$ matter, where the inner\ncrust of the NS is treated using an extended Thomas-Fermi method, providing for\nthe first time a fully consistent Bayesian treatment of the correlation of bulk\nwith surface as well as with spin-orbit and effective mass parameters. We then\nemploy a standard Bayesian framework to identify those EoSs that satisfy\nastrophysical constraints from NS mass measurements, the tidal deformability\nfrom GW170817, and NICER mass-radius observations. We also examine NS\nobservables, such as the crustal moment of inertia, which is crucial in\nunderstanding pulsar glitches. Compared to previous works, we observe an\nincrease in both the NS surface thickness and the crustal moment of inertia."
                },
                "authors": [
                    {
                        "name": "Pietro Klausner"
                    },
                    {
                        "name": "Marco Antonelli"
                    },
                    {
                        "name": "Francesca Gulminelli"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Gulminelli"
                },
                "author": "Francesca Gulminelli",
                "arxiv_comment": "18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16928v1",
                "updated": "2025-05-22T17:20:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning"
                },
                "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning."
                },
                "authors": [
                    {
                        "name": "Bosung Kim"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    }
                ],
                "author_detail": {
                    "name": "Prithviraj Ammanabrolu"
                },
                "author": "Prithviraj Ammanabrolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16922v1",
                "updated": "2025-05-22T17:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    16,
                    8,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    16,
                    8,
                    3,
                    142,
                    0
                ],
                "title": "UNCLE: Uncertainty Expressions in Long-Form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNCLE: Uncertainty Expressions in Long-Form Generation"
                },
                "summary": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16918v1",
                "updated": "2025-05-22T17:13:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    13,
                    1,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:13:01Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    13,
                    1,
                    3,
                    142,
                    0
                ],
                "title": "Scalable and Interpretable Contextual Bandits: A Literature Review and\n  Retail Offer Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Interpretable Contextual Bandits: A Literature Review and\n  Retail Offer Prototype"
                },
                "summary": "This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)\nmethods and introduces an experimental framework for scalable, interpretable\noffer selection, addressing the challenge of fast-changing offers. The approach\nmodels context at the product category level, allowing offers to span multiple\ncategories and enabling knowledge transfer across similar offers. This improves\nlearning efficiency and generalization in dynamic environments. The framework\nextends standard CMAB methodology to support multi-category contexts, and\nachieves scalability through efficient feature engineering and modular design.\nAdvanced features such as MPG (Member Purchase Gap) and MF (Matrix\nFactorization) capture nuanced user-offer interactions, with implementation in\nPython for practical deployment.\n  A key contribution is interpretability at scale: logistic regression models\nyield transparent weight vectors, accessible via a large language model (LLM)\ninterface for real-time, user-level tracking and explanation of evolving\npreferences. This enables the generation of detailed member profiles and\nidentification of behavioral patterns, supporting personalized offer\noptimization and enhancing trust in automated decisions. By situating our\nprototype alongside established paradigms like Generalized Linear Models and\nThompson Sampling, we demonstrate its value for both research and real-world\nCMAB applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)\nmethods and introduces an experimental framework for scalable, interpretable\noffer selection, addressing the challenge of fast-changing offers. The approach\nmodels context at the product category level, allowing offers to span multiple\ncategories and enabling knowledge transfer across similar offers. This improves\nlearning efficiency and generalization in dynamic environments. The framework\nextends standard CMAB methodology to support multi-category contexts, and\nachieves scalability through efficient feature engineering and modular design.\nAdvanced features such as MPG (Member Purchase Gap) and MF (Matrix\nFactorization) capture nuanced user-offer interactions, with implementation in\nPython for practical deployment.\n  A key contribution is interpretability at scale: logistic regression models\nyield transparent weight vectors, accessible via a large language model (LLM)\ninterface for real-time, user-level tracking and explanation of evolving\npreferences. This enables the generation of detailed member profiles and\nidentification of behavioral patterns, supporting personalized offer\noptimization and enhancing trust in automated decisions. By situating our\nprototype alongside established paradigms like Generalized Linear Models and\nThompson Sampling, we demonstrate its value for both research and real-world\nCMAB applications."
                },
                "authors": [
                    {
                        "name": "Nikola Tankovic"
                    },
                    {
                        "name": "Robert Sajina"
                    }
                ],
                "author_detail": {
                    "name": "Robert Sajina"
                },
                "author": "Robert Sajina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14652v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14652v3",
                "updated": "2025-05-22T17:05:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    5,
                    9,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-20T17:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Reasoner: Advancing LLM Reasoning Across All Domains"
                },
                "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14652v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14652v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16903v1",
                "updated": "2025-05-22T17:03:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    3,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:03:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    3,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Unsupervised Prompting for Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Prompting for Graph Neural Networks"
                },
                "summary": "Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to\naddress the semantic gap between pre-training and fine-tuning steps. However,\nexisting GNN prompting methods rely on labeled data and involve lightweight\nfine-tuning for downstream tasks. Meanwhile, in-context learning methods for\nLarge Language Models (LLMs) have shown promising performance with no parameter\nupdating and no or minimal labeled data. Inspired by these approaches, in this\nwork, we first introduce a challenging problem setup to evaluate GNN prompting\nmethods. This setup encourages a prompting function to enhance a pre-trained\nGNN's generalization to a target dataset under covariate shift without updating\nthe GNN's parameters and with no labeled data. Next, we propose a fully\nunsupervised prompting method based on consistency regularization through\npseudo-labeling. We use two regularization techniques to align the prompted\ngraphs' distribution with the original data and reduce biased predictions.\nThrough extensive experiments under our problem setting, we demonstrate that\nour unsupervised approach outperforms the state-of-the-art prompting methods\nthat have access to labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to\naddress the semantic gap between pre-training and fine-tuning steps. However,\nexisting GNN prompting methods rely on labeled data and involve lightweight\nfine-tuning for downstream tasks. Meanwhile, in-context learning methods for\nLarge Language Models (LLMs) have shown promising performance with no parameter\nupdating and no or minimal labeled data. Inspired by these approaches, in this\nwork, we first introduce a challenging problem setup to evaluate GNN prompting\nmethods. This setup encourages a prompting function to enhance a pre-trained\nGNN's generalization to a target dataset under covariate shift without updating\nthe GNN's parameters and with no labeled data. Next, we propose a fully\nunsupervised prompting method based on consistency regularization through\npseudo-labeling. We use two regularization techniques to align the prompted\ngraphs' distribution with the original data and reduce biased predictions.\nThrough extensive experiments under our problem setting, we demonstrate that\nour unsupervised approach outperforms the state-of-the-art prompting methods\nthat have access to labels."
                },
                "authors": [
                    {
                        "name": "Peyman Baghershahi"
                    },
                    {
                        "name": "Sourav Medya"
                    }
                ],
                "author_detail": {
                    "name": "Sourav Medya"
                },
                "author": "Sourav Medya",
                "arxiv_comment": "25 pages, 5 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16901v1",
                "updated": "2025-05-22T17:00:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    0,
                    55,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:00:55Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    0,
                    55,
                    3,
                    142,
                    0
                ],
                "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%."
                },
                "authors": [
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Zhenhao Tang"
                    },
                    {
                        "name": "Hongen Peng"
                    },
                    {
                        "name": "Xukun Zhu"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Yingguang Yang"
                    },
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Zhaogui Xu"
                    },
                    {
                        "name": "Haipeng Zhang"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Peng Di"
                    }
                ],
                "author_detail": {
                    "name": "Peng Di"
                },
                "author": "Peng Di",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13113v2",
                "updated": "2025-05-22T16:59:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    59,
                    54,
                    3,
                    142,
                    0
                ],
                "published": "2024-10-17T00:50:44Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    0,
                    50,
                    44,
                    3,
                    291,
                    0
                ],
                "title": "A new statistical approach for joint modeling of longitudinal outcomes\n  measured in electronic health records with clinically informative presence\n  and observation processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new statistical approach for joint modeling of longitudinal outcomes\n  measured in electronic health records with clinically informative presence\n  and observation processes"
                },
                "summary": "Biobanks with genetics-linked electronic health records (EHR) have opened up\nopportunities to study associations between genetic, social, or environmental\nfactors and longitudinal lab biomarkers. However, in EHRs, the timing of\npatient visits and the recording of lab tests often depend on patient health\nstatus, referred to as informative presence (IP) and informative observation\n(IO), which can bias exposure-biomarker associations. Two gaps remain in\nEHR-based research: (1) the performance of existing IP-aware methods is unclear\nin real-world EHR settings, and (2) no existing methods handle IP and IO\nsimultaneously. To address these challenges, we first conduct extensive\nsimulation studies tailored to EHR-specific IP patterns to assess existing\nmethods. We then propose a joint modeling framework, EHRJoint, that\nsimultaneously models the visiting, observation, and longitudinal biomarker\nprocesses to address both IP and IO. We develop a computationally efficient\nestimation procedure based on estimating equations and provide asymptotically\nvalid inference. Simulations show that EHRJoint yields unbiased exposure effect\nestimates under both IP and IO, while existing methods fail. We apply EHRJoint\nto the Michigan Genomics Initiative data to examine associations between\nrepeated glucose measurements and two exposures: genetic variants and\neducational disadvantage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biobanks with genetics-linked electronic health records (EHR) have opened up\nopportunities to study associations between genetic, social, or environmental\nfactors and longitudinal lab biomarkers. However, in EHRs, the timing of\npatient visits and the recording of lab tests often depend on patient health\nstatus, referred to as informative presence (IP) and informative observation\n(IO), which can bias exposure-biomarker associations. Two gaps remain in\nEHR-based research: (1) the performance of existing IP-aware methods is unclear\nin real-world EHR settings, and (2) no existing methods handle IP and IO\nsimultaneously. To address these challenges, we first conduct extensive\nsimulation studies tailored to EHR-specific IP patterns to assess existing\nmethods. We then propose a joint modeling framework, EHRJoint, that\nsimultaneously models the visiting, observation, and longitudinal biomarker\nprocesses to address both IP and IO. We develop a computationally efficient\nestimation procedure based on estimating equations and provide asymptotically\nvalid inference. Simulations show that EHRJoint yields unbiased exposure effect\nestimates under both IP and IO, while existing methods fail. We apply EHRJoint\nto the Michigan Genomics Initiative data to examine associations between\nrepeated glucose measurements and two exposures: genetic variants and\neducational disadvantage."
                },
                "authors": [
                    {
                        "name": "Jiacong Du"
                    },
                    {
                        "name": "Xu Shi"
                    },
                    {
                        "name": "Bhramar Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Bhramar Mukherjee"
                },
                "author": "Bhramar Mukherjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16894v1",
                "updated": "2025-05-22T16:50:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:50:58Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    58,
                    3,
                    142,
                    0
                ],
                "title": "Shadows in the Attention: Contextual Perturbation and Representation\n  Drift in the Dynamics of Hallucination in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadows in the Attention: Contextual Perturbation and Representation\n  Drift in the Dynamics of Hallucination in LLMs"
                },
                "summary": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms."
                },
                "authors": [
                    {
                        "name": "Zeyu Wei"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xiaohui Rong"
                    },
                    {
                        "name": "Xuemin Liu"
                    },
                    {
                        "name": "He Li"
                    }
                ],
                "author_detail": {
                    "name": "He Li"
                },
                "author": "He Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16893v1",
                "updated": "2025-05-22T16:50:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    55,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:50:55Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    55,
                    3,
                    142,
                    0
                ],
                "title": "Statistical Test for Saliency Maps of Graph Neural Networks via\n  Selective Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Test for Saliency Maps of Graph Neural Networks via\n  Selective Inference"
                },
                "summary": "Graph Neural Networks (GNNs) have gained prominence for their ability to\nprocess graph-structured data across various domains. However, interpreting GNN\ndecisions remains a significant challenge, leading to the adoption of saliency\nmaps for identifying influential nodes and edges. Despite their utility, the\nreliability of GNN saliency maps has been questioned, particularly in terms of\ntheir robustness to noise. In this study, we propose a statistical testing\nframework to rigorously evaluate the significance of saliency maps. Our main\ncontribution lies in addressing the inflation of the Type I error rate caused\nby double-dipping of data, leveraging the framework of Selective Inference. Our\nmethod provides statistically valid $p$-values while controlling the Type I\nerror rate, ensuring that identified salient subgraphs contain meaningful\ninformation rather than random artifacts. To demonstrate the effectiveness of\nour method, we conduct experiments on both synthetic and real-world datasets,\nshowing its effectiveness in assessing the reliability of GNN interpretations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have gained prominence for their ability to\nprocess graph-structured data across various domains. However, interpreting GNN\ndecisions remains a significant challenge, leading to the adoption of saliency\nmaps for identifying influential nodes and edges. Despite their utility, the\nreliability of GNN saliency maps has been questioned, particularly in terms of\ntheir robustness to noise. In this study, we propose a statistical testing\nframework to rigorously evaluate the significance of saliency maps. Our main\ncontribution lies in addressing the inflation of the Type I error rate caused\nby double-dipping of data, leveraging the framework of Selective Inference. Our\nmethod provides statistically valid $p$-values while controlling the Type I\nerror rate, ensuring that identified salient subgraphs contain meaningful\ninformation rather than random artifacts. To demonstrate the effectiveness of\nour method, we conduct experiments on both synthetic and real-world datasets,\nshowing its effectiveness in assessing the reliability of GNN interpretations."
                },
                "authors": [
                    {
                        "name": "Shuichi Nishino"
                    },
                    {
                        "name": "Tomohiro Shiraishi"
                    },
                    {
                        "name": "Teruyuki Katsuoka"
                    },
                    {
                        "name": "Ichiro Takeuchi"
                    }
                ],
                "author_detail": {
                    "name": "Ichiro Takeuchi"
                },
                "author": "Ichiro Takeuchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16892v1",
                "updated": "2025-05-22T16:50:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:50:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "FlashBack: Consistency Model-Accelerated Shared Autonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack: Consistency Model-Accelerated Shared Autonomy"
                },
                "summary": "Shared autonomy is an enabling technology that provides users with control\nauthority over robots that would otherwise be difficult if not impossible to\ndirectly control. Yet, standard methods make assumptions that limit their\nadoption in practice-for example, prior knowledge of the user's goals or the\nobjective (i.e., reward) function that they wish to optimize, knowledge of the\nuser's policy, or query-level access to the user during training.\nDiffusion-based approaches to shared autonomy do not make such assumptions and\ninstead only require access to demonstrations of desired behaviors, while\nallowing the user to maintain control authority. However, these advantages have\ncome at the expense of high computational complexity, which has made real-time\nshared autonomy all but impossible. To overcome this limitation, we propose\nConsistency Shared Autonomy (CSA), a shared autonomy framework that employs a\nconsistency model-based formulation of diffusion. Key to CSA is that it employs\nthe distilled probability flow of ordinary differential equations (PF ODE) to\ngenerate high-fidelity samples in a single step. This results in inference\nspeeds significantly than what is possible with previous diffusion-based\napproaches to shared autonomy, enabling real-time assistance in complex domains\nwith only a single function evaluation. Further, by intervening on flawed\nactions at intermediate states of the PF ODE, CSA enables varying levels of\nassistance. We evaluate CSA on a variety of challenging simulated and\nreal-world robot control problems, demonstrating significant improvements over\nstate-of-the-art methods both in terms of task performance and computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared autonomy is an enabling technology that provides users with control\nauthority over robots that would otherwise be difficult if not impossible to\ndirectly control. Yet, standard methods make assumptions that limit their\nadoption in practice-for example, prior knowledge of the user's goals or the\nobjective (i.e., reward) function that they wish to optimize, knowledge of the\nuser's policy, or query-level access to the user during training.\nDiffusion-based approaches to shared autonomy do not make such assumptions and\ninstead only require access to demonstrations of desired behaviors, while\nallowing the user to maintain control authority. However, these advantages have\ncome at the expense of high computational complexity, which has made real-time\nshared autonomy all but impossible. To overcome this limitation, we propose\nConsistency Shared Autonomy (CSA), a shared autonomy framework that employs a\nconsistency model-based formulation of diffusion. Key to CSA is that it employs\nthe distilled probability flow of ordinary differential equations (PF ODE) to\ngenerate high-fidelity samples in a single step. This results in inference\nspeeds significantly than what is possible with previous diffusion-based\napproaches to shared autonomy, enabling real-time assistance in complex domains\nwith only a single function evaluation. Further, by intervening on flawed\nactions at intermediate states of the PF ODE, CSA enables varying levels of\nassistance. We evaluate CSA on a variety of challenging simulated and\nreal-world robot control problems, demonstrating significant improvements over\nstate-of-the-art methods both in terms of task performance and computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Luzhe Sun"
                    },
                    {
                        "name": "Jingtian Ji"
                    },
                    {
                        "name": "Xiangshan Tan"
                    },
                    {
                        "name": "Matthew R. Walter"
                    }
                ],
                "author_detail": {
                    "name": "Matthew R. Walter"
                },
                "author": "Matthew R. Walter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16888v1",
                "updated": "2025-05-22T16:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    47,
                    15,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    47,
                    15,
                    3,
                    142,
                    0
                ],
                "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious\n  System Prompt Generation and Refining Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious\n  System Prompt Generation and Refining Framework"
                },
                "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Viet Pham"
                    },
                    {
                        "name": "Thai Le"
                    }
                ],
                "author_detail": {
                    "name": "Thai Le"
                },
                "author": "Thai Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16886v1",
                "updated": "2025-05-22T16:41:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    41,
                    37,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:41:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    41,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?"
                },
                "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers."
                },
                "authors": [
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16881v1",
                "updated": "2025-05-22T16:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    35,
                    33,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:35:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    35,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "CASTILLO: Characterizing Response Length Distributions of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTILLO: Characterizing Response Length Distributions of Large Language\n  Models"
                },
                "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems."
                },
                "authors": [
                    {
                        "name": "Daniel F. Perez-Ramirez"
                    },
                    {
                        "name": "Dejan Kostic"
                    },
                    {
                        "name": "Magnus Boman"
                    }
                ],
                "author_detail": {
                    "name": "Magnus Boman"
                },
                "author": "Magnus Boman",
                "arxiv_comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04380v2",
                "updated": "2025-05-22T16:34:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    34,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-05T17:21:01Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    21,
                    1,
                    2,
                    36,
                    0
                ],
                "title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data"
                },
                "summary": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs."
                },
                "authors": [
                    {
                        "name": "Zhenqing Ling"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Qianli Shen"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Ying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shen"
                },
                "author": "Ying Shen",
                "arxiv_comment": "33 pages, 20 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16876v1",
                "updated": "2025-05-22T16:32:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    32,
                    13,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:32:13Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    32,
                    13,
                    3,
                    142,
                    0
                ],
                "title": "Inferring neutron star merger ejecta morphologies with kilonovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring neutron star merger ejecta morphologies with kilonovae"
                },
                "summary": "In this study we incorporate a new grid of kilonova simulations produced by\nthe Monte Carlo radiative transfer code SuperNu in an inference pipeline for\nastrophysical transients, and evaluate their performance. These simulations\ncontain four different two-component ejecta morphology classes. We analyze\nfollow-up observational strategies by Vera Rubin Observatory in optical, and\nJames Webb Space Telescope (JWST) in mid-infrared (MIR). Our analysis suggests\nthat, within these strategies, it is possible to discriminate between different\nmorphologies only when late-time JWST observations in MIR are available. We\nconclude that follow-ups by the new Vera Rubin Observatory alone are not\nsufficient to determine ejecta morphology. Additionally, we make comparisons\nbetween surrogate models based on radiative transfer simulation grids by\nSuperNu and POSSIS, by analyzing the historic kilonova AT2017gfo that\naccompanied the gravitational wave event GW170817. We show that both SuperNu\nand POSSIS models provide similar fits to photometric observations. Our results\nshow a slight preference for SuperNu models, since the wind ejecta parameters\nrecovered with these models are in better agreement with expectations from\nnumerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study we incorporate a new grid of kilonova simulations produced by\nthe Monte Carlo radiative transfer code SuperNu in an inference pipeline for\nastrophysical transients, and evaluate their performance. These simulations\ncontain four different two-component ejecta morphology classes. We analyze\nfollow-up observational strategies by Vera Rubin Observatory in optical, and\nJames Webb Space Telescope (JWST) in mid-infrared (MIR). Our analysis suggests\nthat, within these strategies, it is possible to discriminate between different\nmorphologies only when late-time JWST observations in MIR are available. We\nconclude that follow-ups by the new Vera Rubin Observatory alone are not\nsufficient to determine ejecta morphology. Additionally, we make comparisons\nbetween surrogate models based on radiative transfer simulation grids by\nSuperNu and POSSIS, by analyzing the historic kilonova AT2017gfo that\naccompanied the gravitational wave event GW170817. We show that both SuperNu\nand POSSIS models provide similar fits to photometric observations. Our results\nshow a slight preference for SuperNu models, since the wind ejecta parameters\nrecovered with these models are in better agreement with expectations from\nnumerical simulations."
                },
                "authors": [
                    {
                        "name": "Brendan L. King"
                    },
                    {
                        "name": "Soumi De"
                    },
                    {
                        "name": "Oleg Korobkin"
                    },
                    {
                        "name": "Michael W. Coughlin"
                    },
                    {
                        "name": "Peter T. H. Pang"
                    }
                ],
                "author_detail": {
                    "name": "Peter T. H. Pang"
                },
                "author": "Peter T. H. Pang",
                "arxiv_comment": "Submitted to PASP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17775v2",
                "updated": "2025-05-22T16:26:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    26,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-25T02:10:30Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    10,
                    30,
                    1,
                    56,
                    0
                ],
                "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks"
                },
                "summary": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Tanawan Premsri"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v2",
                "updated": "2025-05-22T16:26:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    26,
                    55,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16869v1",
                "updated": "2025-05-22T16:24:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:24:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: Multilingual Safety Alignment via Reward Gap Optimization"
                },
                "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "To Appear at ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16864v1",
                "updated": "2025-05-22T16:21:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    21,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:21:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    21,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Efficient Video Generation via Dynamic Token Carving"
                },
                "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga"
                },
                "authors": [
                    {
                        "name": "Yuechen Zhang"
                    },
                    {
                        "name": "Jinbo Xing"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Shaoteng Liu"
                    },
                    {
                        "name": "Bohao Peng"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Eric Lo"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Project Page: https://julianjuaner.github.io/projects/jenga/ , 24\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15463v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15463v3",
                "updated": "2025-05-22T16:17:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    17,
                    34,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-19T17:41:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment"
                },
                "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15463v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15463v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16849v1",
                "updated": "2025-05-22T16:11:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    11,
                    35,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:11:35Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    11,
                    35,
                    3,
                    142,
                    0
                ],
                "title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented\n  Generation via Knowledge Graph Walks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented\n  Generation via Knowledge Graph Walks"
                },
                "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research."
                },
                "authors": [
                    {
                        "name": "Martin Böckling"
                    },
                    {
                        "name": "Heiko Paulheim"
                    },
                    {
                        "name": "Andreea Iana"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Iana"
                },
                "author": "Andreea Iana",
                "arxiv_comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16846v1",
                "updated": "2025-05-22T16:10:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    10,
                    3,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:10:03Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    10,
                    3,
                    3,
                    142,
                    0
                ],
                "title": "Ray-tracing GR-MHD-generated Outflows from AGNs Hosting Thin Accretion\n  Disks: An Analysis Approaching Horizon Scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ray-tracing GR-MHD-generated Outflows from AGNs Hosting Thin Accretion\n  Disks: An Analysis Approaching Horizon Scales"
                },
                "summary": "AGNs exhibit a wide range of black hole masses and inflow/outflow properties.\nIt is now possible to probe regions close to the event horizons of nearby SMBHs\nusing VLBI with earth-sized baselines, as performed by the EHT. This study\nexplores the emission properties of accretion and outflows near the event\nhorizon of both low-mass and high-mass SMBHs. Using resistive GR-MHD\nsimulations, we model AGNs with thin Keplerian disks. This contrasts with\nwidely studied models featuring thick disks, such as magnetically arrested\ndisks (MADs) or the standard and normal evolution (SANE) scenario. Our models\nserve as simplified representations to study disk-jet-wind structures. These\nsimulations are postprocessed and ray-traced, using constraints of black hole\nmass and observed SEDs. Thermal synchrotron emission generated near the event\nhorizon is used to create emission maps, which are analysed by separating\naccretion and outflow components to determine their contributions to the total\nintensity. Whether the emission appears optically thick or thin at a given\nfrequency depends on its position relative to the synchrotron SED peak. At 230\nGHz, low-mass SMBHs appear optically thicker than high-mass ones, even at lower\naccretion rates. Doppler beaming affects the brightness of emission from\noutflows with changing viewing angles in low-mass systems. Eddington ratios\nfrom our models align with those inferred by the EHTC for M87 and SgrA* using\nthicker MAD/SANE models. Although thin disks are optically thicker, their\nspectral properties make high-mass systems appear optically thinner at 230 GHz;\nideal for probing GR effects like photon rings. In contrast, low-mass systems\nremain optically thicker at these frequencies because of synchrotron\nself-absorption, making outflow emissions near the horizon more pronounced.\nHowever, distinguishing these features remains challenging with current EHT\nresolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGNs exhibit a wide range of black hole masses and inflow/outflow properties.\nIt is now possible to probe regions close to the event horizons of nearby SMBHs\nusing VLBI with earth-sized baselines, as performed by the EHT. This study\nexplores the emission properties of accretion and outflows near the event\nhorizon of both low-mass and high-mass SMBHs. Using resistive GR-MHD\nsimulations, we model AGNs with thin Keplerian disks. This contrasts with\nwidely studied models featuring thick disks, such as magnetically arrested\ndisks (MADs) or the standard and normal evolution (SANE) scenario. Our models\nserve as simplified representations to study disk-jet-wind structures. These\nsimulations are postprocessed and ray-traced, using constraints of black hole\nmass and observed SEDs. Thermal synchrotron emission generated near the event\nhorizon is used to create emission maps, which are analysed by separating\naccretion and outflow components to determine their contributions to the total\nintensity. Whether the emission appears optically thick or thin at a given\nfrequency depends on its position relative to the synchrotron SED peak. At 230\nGHz, low-mass SMBHs appear optically thicker than high-mass ones, even at lower\naccretion rates. Doppler beaming affects the brightness of emission from\noutflows with changing viewing angles in low-mass systems. Eddington ratios\nfrom our models align with those inferred by the EHTC for M87 and SgrA* using\nthicker MAD/SANE models. Although thin disks are optically thicker, their\nspectral properties make high-mass systems appear optically thinner at 230 GHz;\nideal for probing GR effects like photon rings. In contrast, low-mass systems\nremain optically thicker at these frequencies because of synchrotron\nself-absorption, making outflow emissions near the horizon more pronounced.\nHowever, distinguishing these features remains challenging with current EHT\nresolution."
                },
                "authors": [
                    {
                        "name": "Bidisha Bandyopadhyay"
                    },
                    {
                        "name": "Christian Fendt"
                    },
                    {
                        "name": "Dominik R. G. Schleicher"
                    },
                    {
                        "name": "Neil M. Nagar"
                    },
                    {
                        "name": "Felipe Agurto-Sepulveda"
                    },
                    {
                        "name": "Javier Pedreros"
                    }
                ],
                "author_detail": {
                    "name": "Javier Pedreros"
                },
                "author": "Javier Pedreros",
                "arxiv_comment": "13 pages, 8 figures, 1 table, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v3",
                "updated": "2025-05-22T16:09:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    9,
                    52,
                    3,
                    142,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v1",
                "updated": "2025-05-22T16:07:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16838v1",
                "updated": "2025-05-22T16:06:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    6,
                    59,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:06:59Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    6,
                    59,
                    3,
                    142,
                    0
                ],
                "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and\n  Search"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress"
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Huanjin Yao"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16834v1",
                "updated": "2025-05-22T16:05:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis"
                },
                "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."
                },
                "authors": [
                    {
                        "name": "Shuang Sun"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Fei Bai"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16831v1",
                "updated": "2025-05-22T16:02:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    2,
                    10,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:02:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    2,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine\n  Unlearning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Isn't Deletion: Investigating Reversibility of Machine\n  Unlearning in LLMs"
                },
                "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Minxin Du"
                    }
                ],
                "author_detail": {
                    "name": "Minxin Du"
                },
                "author": "Minxin Du",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16821v1",
                "updated": "2025-05-22T15:55:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    56,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:55:56Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    56,
                    3,
                    142,
                    0
                ],
                "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols"
                },
                "summary": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards."
                },
                "authors": [
                    {
                        "name": "Ziming liu"
                    },
                    {
                        "name": "Bryan Liu"
                    },
                    {
                        "name": "Alvaro Valcarce"
                    },
                    {
                        "name": "Xiaoli Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Chu"
                },
                "author": "Xiaoli Chu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Focuses on applying LLMs to 5G RRC protocol generation; primary: cs.NI;\n  cross-list: eess.SP, cs.LG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01222v2",
                "updated": "2025-05-22T15:55:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    37,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-03T06:40:21Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    40,
                    21,
                    0,
                    62,
                    0
                ],
                "title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG"
                },
                "summary": "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench."
                },
                "authors": [
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19110v2",
                "updated": "2025-05-22T15:53:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    53,
                    14,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-27T05:04:02Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    5,
                    4,
                    2,
                    6,
                    117,
                    0
                ],
                "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal\n  Math Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal\n  Math Libraries"
                },
                "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."
                },
                "authors": [
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Luming Li"
                    },
                    {
                        "name": "Xiaoran Jin"
                    },
                    {
                        "name": "Jacques Fleuriot"
                    },
                    {
                        "name": "Wenda Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenda Li"
                },
                "author": "Wenda Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16810v1",
                "updated": "2025-05-22T15:49:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    49,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:49:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    49,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "DeepRec: Towards a Deep Dive Into the Item Space with Large Language\n  Model Based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepRec: Towards a Deep Dive Into the Item Space with Large Language\n  Model Based Recommendation"
                },
                "summary": "Recently, large language models (LLMs) have been introduced into recommender\nsystems (RSs), either to enhance traditional recommendation models (TRMs) or\nserve as recommendation backbones. However, existing LLM-based RSs often do not\nfully exploit the complementary advantages of LLMs (e.g., world knowledge and\nreasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to\nfully explore the item space. To address this, we propose DeepRec, a novel\nLLM-based RS that enables autonomous multi-turn interactions between LLMs and\nTRMs for deep exploration of the item space. In each interaction turn, LLMs\nreason over user preferences and interact with TRMs to retrieve candidate\nitems. After multi-turn interactions, LLMs rank the retrieved items to generate\nthe final recommendations. We adopt reinforcement learning(RL) based\noptimization and propose novel designs from three aspects: recommendation model\nbased data rollout, recommendation-oriented hierarchical rewards, and a\ntwo-stage RL training strategy. For data rollout, we introduce a\npreference-aware TRM, with which LLMs interact to construct trajectory data.\nFor rewards, we design a hierarchical reward function that involves both\nprocess-level and outcome-level rewards to optimize the interaction process and\nrecommendation performance, respectively. For RL training, we develop a\ntwo-stage training strategy, where the first stage aims to guide LLMs to\ninteract with TRMs and the second stage focuses on performance improvement.\nExperiments on public datasets demonstrate that DeepRec significantly\noutperforms both traditional and LLM-based baselines, offering a new paradigm\nfor deep exploration in recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been introduced into recommender\nsystems (RSs), either to enhance traditional recommendation models (TRMs) or\nserve as recommendation backbones. However, existing LLM-based RSs often do not\nfully exploit the complementary advantages of LLMs (e.g., world knowledge and\nreasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to\nfully explore the item space. To address this, we propose DeepRec, a novel\nLLM-based RS that enables autonomous multi-turn interactions between LLMs and\nTRMs for deep exploration of the item space. In each interaction turn, LLMs\nreason over user preferences and interact with TRMs to retrieve candidate\nitems. After multi-turn interactions, LLMs rank the retrieved items to generate\nthe final recommendations. We adopt reinforcement learning(RL) based\noptimization and propose novel designs from three aspects: recommendation model\nbased data rollout, recommendation-oriented hierarchical rewards, and a\ntwo-stage RL training strategy. For data rollout, we introduce a\npreference-aware TRM, with which LLMs interact to construct trajectory data.\nFor rewards, we design a hierarchical reward function that involves both\nprocess-level and outcome-level rewards to optimize the interaction process and\nrecommendation performance, respectively. For RL training, we develop a\ntwo-stage training strategy, where the first stage aims to guide LLMs to\ninteract with TRMs and the second stage focuses on performance improvement.\nExperiments on public datasets demonstrate that DeepRec significantly\noutperforms both traditional and LLM-based baselines, offering a new paradigm\nfor deep exploration in recommendation systems."
                },
                "authors": [
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xiaolei Wang"
                    },
                    {
                        "name": "Enze Liu"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Lu Hongyu"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16806v1",
                "updated": "2025-05-22T15:45:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    45,
                    29,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:45:29Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    45,
                    29,
                    3,
                    142,
                    0
                ],
                "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement"
                },
                "summary": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8."
                },
                "authors": [
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Bowen Deng"
                    },
                    {
                        "name": "Weixu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weixu Chen"
                },
                "author": "Weixu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02190v2",
                "updated": "2025-05-22T15:43:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    43,
                    4,
                    3,
                    142,
                    0
                ],
                "published": "2024-04-02T18:00:00Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    18,
                    0,
                    0,
                    1,
                    93,
                    0
                ],
                "title": "Matching Cosmic Shear Analysis in Harmonic and Real Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching Cosmic Shear Analysis in Harmonic and Real Space"
                },
                "summary": "Recent cosmic shear analyses have exhibited discrepancies of up to $1\\sigma$\nbetween the inferred cosmological parameters when analyzing summary statistics\nin real space versus harmonic space. In this paper, we demonstrate the\nconsistent measurement and analysis of cosmic shear two-point functions in\nharmonic and real space using the $i${\\sc Master} algorithm. This algorithm\nprovides a unified prescription to model the survey window effects and scale\ncuts in both real space (due to observational systematics) and harmonic space\n(due to model limitations), resulting in a matching estimation of the cosmic\nshear power spectrum from both harmonic and real space estimators. We show that\nthe $i$\\textsc{Master} algorithm gives matching results using measurements from\nthe HSC Y1 mock shape catalogs in both real and harmonic space, resulting in\nmatching inferences of $S_8=\\sigma_8(\\Omega_m/0.3)^{0.5}$. This method provides\nan unbiased estimate of the cosmic shear power spectrum, and $S_8$ inference\nthat has a correlation coefficient of 0.997 between analyses using measurements\nin real space and harmonic space when $S_8$ is the only free parameter. We\nobserve the mean difference between the two inferred $S_8$ values to be 0.0004\nacross noise-free mock realizations, far below the observed difference of 0.042\nfor the published HSC Y1 analyses and well below the statistical uncertainties.\nWhile the notation employed in this paper is specific to photometric galaxy\nsurveys, the methods are equally applicable and can be extended to\nspectroscopic galaxy surveys, intensity mapping, and CMB surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent cosmic shear analyses have exhibited discrepancies of up to $1\\sigma$\nbetween the inferred cosmological parameters when analyzing summary statistics\nin real space versus harmonic space. In this paper, we demonstrate the\nconsistent measurement and analysis of cosmic shear two-point functions in\nharmonic and real space using the $i${\\sc Master} algorithm. This algorithm\nprovides a unified prescription to model the survey window effects and scale\ncuts in both real space (due to observational systematics) and harmonic space\n(due to model limitations), resulting in a matching estimation of the cosmic\nshear power spectrum from both harmonic and real space estimators. We show that\nthe $i$\\textsc{Master} algorithm gives matching results using measurements from\nthe HSC Y1 mock shape catalogs in both real and harmonic space, resulting in\nmatching inferences of $S_8=\\sigma_8(\\Omega_m/0.3)^{0.5}$. This method provides\nan unbiased estimate of the cosmic shear power spectrum, and $S_8$ inference\nthat has a correlation coefficient of 0.997 between analyses using measurements\nin real space and harmonic space when $S_8$ is the only free parameter. We\nobserve the mean difference between the two inferred $S_8$ values to be 0.0004\nacross noise-free mock realizations, far below the observed difference of 0.042\nfor the published HSC Y1 analyses and well below the statistical uncertainties.\nWhile the notation employed in this paper is specific to photometric galaxy\nsurveys, the methods are equally applicable and can be extended to\nspectroscopic galaxy surveys, intensity mapping, and CMB surveys."
                },
                "authors": [
                    {
                        "name": "Andy Park"
                    },
                    {
                        "name": "Sukhdeep Singh"
                    },
                    {
                        "name": "Xiangchong Li"
                    },
                    {
                        "name": "Rachel Mandelbaum"
                    },
                    {
                        "name": "Tianqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianqing Zhang"
                },
                "author": "Tianqing Zhang",
                "arxiv_comment": "17 pages, 6 figures. Accepted in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16800v1",
                "updated": "2025-05-22T15:40:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    40,
                    9,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    40,
                    9,
                    3,
                    142,
                    0
                ],
                "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for\n  Low-Resource Canonical Morpheme Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Beyond Limits: Multitask Learning and Synthetic Data for\n  Low-Resource Canonical Morpheme Segmentation"
                },
                "summary": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages."
                },
                "authors": [
                    {
                        "name": "Changbing Yang"
                    },
                    {
                        "name": "Garrett Nicolai"
                    }
                ],
                "author_detail": {
                    "name": "Garrett Nicolai"
                },
                "author": "Garrett Nicolai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16795v1",
                "updated": "2025-05-22T15:36:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    36,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:36:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    36,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Sequential simulation-based inference for extreme mass ratio inspirals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential simulation-based inference for extreme mass ratio inspirals"
                },
                "summary": "Extreme mass-ratio inspirals pose a difficult challenge in terms of both\nsearch and parameter estimation for upcoming space-based gravitational-wave\ndetectors such as LISA. Their signals are long and of complex morphology,\nmeaning they carry a large amount of information about their source, but are\nalso difficult to search for and analyse. We explore how sequential\nsimulation-based inference methods, specifically truncated marginal neural\nratio estimation, could offer solutions to some of the challenges surrounding\nextreme-mass-ratio inspiral data analysis. We show that this method can\nefficiently narrow down the volume of the complex 11-dimensional search\nparameter space by a factor of $10^6-10^7$ and provide 1-dimensional marginal\nproposal distributions for non-spinning extreme-mass-ratio inspirals. We\ndiscuss the current limitations of this approach and place it in the broader\ncontext of a global strategy for future space-based gravitational-wave data\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme mass-ratio inspirals pose a difficult challenge in terms of both\nsearch and parameter estimation for upcoming space-based gravitational-wave\ndetectors such as LISA. Their signals are long and of complex morphology,\nmeaning they carry a large amount of information about their source, but are\nalso difficult to search for and analyse. We explore how sequential\nsimulation-based inference methods, specifically truncated marginal neural\nratio estimation, could offer solutions to some of the challenges surrounding\nextreme-mass-ratio inspiral data analysis. We show that this method can\nefficiently narrow down the volume of the complex 11-dimensional search\nparameter space by a factor of $10^6-10^7$ and provide 1-dimensional marginal\nproposal distributions for non-spinning extreme-mass-ratio inspirals. We\ndiscuss the current limitations of this approach and place it in the broader\ncontext of a global strategy for future space-based gravitational-wave data\nanalysis."
                },
                "authors": [
                    {
                        "name": "Philippa S. Cole"
                    },
                    {
                        "name": "James Alvey"
                    },
                    {
                        "name": "Lorenzo Speri"
                    },
                    {
                        "name": "Christoph Weniger"
                    },
                    {
                        "name": "Uddipta Bhardwaj"
                    },
                    {
                        "name": "Davide Gerosa"
                    },
                    {
                        "name": "Gianfranco Bertone"
                    }
                ],
                "author_detail": {
                    "name": "Gianfranco Bertone"
                },
                "author": "Gianfranco Bertone",
                "arxiv_comment": "11 pages, 9 figures plus appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16785v1",
                "updated": "2025-05-22T15:28:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    28,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:28:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    28,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of\n  Large Language Models"
                },
                "summary": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification."
                },
                "authors": [
                    {
                        "name": "Zhenzhen Ren"
                    },
                    {
                        "name": "GuoBiao Li"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Zhenxing Qian"
                    },
                    {
                        "name": "Xinpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinpeng Zhang"
                },
                "author": "Xinpeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16782v1",
                "updated": "2025-05-22T15:26:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    26,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:26:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    26,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Beyond Language: A Comprehensive Survey on Latent\n  Chain-of-Thought Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT."
                },
                "authors": [
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16774v1",
                "updated": "2025-05-22T15:15:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    15,
                    29,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:15:29Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    15,
                    29,
                    3,
                    142,
                    0
                ],
                "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in\n  Audio-based Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFEval-Audio: Benchmarking Instruction-Following Capability in\n  Audio-based Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area."
                },
                "authors": [
                    {
                        "name": "Yiming Gao"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Chengwei Wei"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "AiTi Aw"
                    }
                ],
                "author_detail": {
                    "name": "AiTi Aw"
                },
                "author": "AiTi Aw",
                "arxiv_comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16765v1",
                "updated": "2025-05-22T15:07:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    7,
                    34,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:07:34Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    7,
                    34,
                    3,
                    142,
                    0
                ],
                "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak\n  Attack on LLMs via Steganographic Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak\n  Attack on LLMs via Steganographic Techniques"
                },
                "summary": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66"
                },
                "authors": [
                    {
                        "name": "Jianing Geng"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Zekun Fei"
                    },
                    {
                        "name": "Tongxi Wu"
                    },
                    {
                        "name": "Lihai Nie"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11741v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11741v3",
                "updated": "2025-05-22T15:06:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    6,
                    30,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-17T12:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    28,
                    11,
                    0,
                    48,
                    0
                ],
                "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL"
                },
                "summary": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1."
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Ripeng Li"
                    },
                    {
                        "name": "Zhonghong Ou"
                    },
                    {
                        "name": "Jiangfeng Sun"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Xiaoran Shang"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Yifan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhu"
                },
                "author": "Yifan Zhu",
                "arxiv_comment": "28 pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11741v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11741v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16674v2",
                "updated": "2025-05-22T15:05:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-20T19:40:40Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    40,
                    40,
                    3,
                    79,
                    0
                ],
                "title": "Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants,\n  and Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants,\n  and Markets"
                },
                "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\nseven widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via Socratic probing. By using our self-contained\nSocratic approach, the study aims to directly measure the models' biases rather\nthan relying on external interpretations, thereby minimizing subjective\njudgments about media bias. Our results reveal a consistent preference of\nDemocratic over Republican positions across all models. Conversely, in economic\ntopics, biases vary among Western LLMs, while those developed in China lean\nmore strongly toward socialism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\nseven widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via Socratic probing. By using our self-contained\nSocratic approach, the study aims to directly measure the models' biases rather\nthan relying on external interpretations, thereby minimizing subjective\njudgments about media bias. Our results reveal a consistent preference of\nDemocratic over Republican positions across all models. Conversely, in economic\ntopics, biases vary among Western LLMs, while those developed in China lean\nmore strongly toward socialism."
                },
                "authors": [
                    {
                        "name": "Molly Kennedy"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Timo Spinde"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16755v1",
                "updated": "2025-05-22T14:59:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    59,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:59:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    59,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "Multi-Output Gaussian Processes for Graph-Structured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Output Gaussian Processes for Graph-Structured Data"
                },
                "summary": "Graph-structured data is a type of data to be obtained associated with a\ngraph structure where vertices and edges describe some kind of data\ncorrelation. This paper proposes a regression method on graph-structured data,\nwhich is based on multi-output Gaussian processes (MOGP), to capture both the\ncorrelation between vertices and the correlation between associated data. The\nproposed formulation is built on the definition of MOGP. This allows it to be\napplied to a wide range of data configurations and scenarios. Moreover, it has\nhigh expressive capability due to its flexibility in kernel design. It includes\nexisting methods of Gaussian processes for graph-structured data as special\ncases and is possible to remove restrictions on data configurations, model\nselection, and inference scenarios in the existing methods. The performance of\nextensions achievable by the proposed formulation is evaluated through computer\nexperiments with synthetic and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-structured data is a type of data to be obtained associated with a\ngraph structure where vertices and edges describe some kind of data\ncorrelation. This paper proposes a regression method on graph-structured data,\nwhich is based on multi-output Gaussian processes (MOGP), to capture both the\ncorrelation between vertices and the correlation between associated data. The\nproposed formulation is built on the definition of MOGP. This allows it to be\napplied to a wide range of data configurations and scenarios. Moreover, it has\nhigh expressive capability due to its flexibility in kernel design. It includes\nexisting methods of Gaussian processes for graph-structured data as special\ncases and is possible to remove restrictions on data configurations, model\nselection, and inference scenarios in the existing methods. The performance of\nextensions achievable by the proposed formulation is evaluated through computer\nexperiments with synthetic and real data."
                },
                "authors": [
                    {
                        "name": "Ayano Nakai-Kasai"
                    },
                    {
                        "name": "Tadashi Wadayama"
                    }
                ],
                "author_detail": {
                    "name": "Tadashi Wadayama"
                },
                "author": "Tadashi Wadayama",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16752v1",
                "updated": "2025-05-22T14:58:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    58,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:58:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    58,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "Action is All You Need: Dual-Flow Generative Ranking Network for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action is All You Need: Dual-Flow Generative Ranking Network for\n  Recommendation"
                },
                "summary": "We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Dual-Flow Generative Ranking Network (DFGR), a two-stream\narchitecture designed for recommendation systems. DFGR integrates innovative\ninteraction patterns between real and fake flows within the QKV modules of the\nself-attention mechanism, enhancing both training and inference efficiency.\nThis approach effectively addresses a key limitation observed in Meta's\nproposed HSTU generative recommendation approach, where heterogeneous\ninformation volumes are mapped into identical vector spaces, leading to\ntraining instability. Unlike traditional recommendation models, DFGR only\nrelies on user history behavior sequences and minimal attribute information,\neliminating the need for extensive manual feature engineering. Comprehensive\nevaluations on open-source and industrial datasets reveal DFGR's superior\nperformance compared to established baselines such as DIN, DCN, DIEN, and\nDeepFM. We also investigate optimal parameter allocation strategies under\ncomputational constraints, establishing DFGR as an efficient and effective\nnext-generation generate ranking paradigm."
                },
                "authors": [
                    {
                        "name": "Hao Guo"
                    },
                    {
                        "name": "Erpeng Xue"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Shichao Wang"
                    },
                    {
                        "name": "Xiaolei Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Chen"
                },
                "author": "Sheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16743v1",
                "updated": "2025-05-22T14:53:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative\n  Metric-driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative\n  Metric-driven Pruning"
                },
                "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM"
                },
                "authors": [
                    {
                        "name": "Florentin Beck"
                    },
                    {
                        "name": "William Rudman"
                    },
                    {
                        "name": "Carsten Eickhoff"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Eickhoff"
                },
                "author": "Carsten Eickhoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04598v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04598v3",
                "updated": "2025-05-22T14:53:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    31,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-06T16:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization"
                },
                "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04598v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04598v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15091v2",
                "updated": "2025-05-22T14:53:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    0,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T04:25:18Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    25,
                    18,
                    2,
                    141,
                    0
                ],
                "title": "ThinkRec: Thinking-based recommendation via LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkRec: Thinking-based recommendation via LLM"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec."
                },
                "authors": [
                    {
                        "name": "Qihang Yu"
                    },
                    {
                        "name": "Kairui Fu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16737v1",
                "updated": "2025-05-22T14:52:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    52,
                    10,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:52:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    52,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing\n  Optimization"
                },
                "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP."
                },
                "authors": [
                    {
                        "name": "Chengcan Wu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07053v2",
                "updated": "2025-05-22T14:49:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    49,
                    3,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-09T17:14:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    14,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling"
                },
                "summary": "Recent efforts target spoken language models (SLMs) that not only listen but\nalso speak for more natural human-LLM interaction. Joint speech-text modeling\nis a promising direction to achieve this. However, the effectiveness of recent\nspeech tokens for joint modeling remains underexplored. To address this, we\nintroduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that\ndirectly addresses the modality gap by aligning speech token with the\ncorresponding text transcription during the tokenization stage. We propose a\nmethod that can achieve this through a attention-based aggregation mechanism\nand with speech reconstruction as the training objective. We conduct extensive\nexperiments and show that TASTE can preserve essential paralinguistic\ninformation while dramatically reducing the token sequence length. With TASTE,\nwe perform straightforward joint spoken language modeling by using Low-Rank\nAdaptation on the pre-trained text LLM. Experimental results show that\nTASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;\nwhile significantly outperform other pre-trained SLMs on speech continuation\nacross subjective and objective evaluations. To our knowledge, TASTE is the\nfirst end-to-end approach that utilizes a reconstruction objective to\nautomatically learn a text-aligned speech tokenization and embedding suitable\nfor spoken language modeling. Our demo, code, and model are available at\nhttps://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts target spoken language models (SLMs) that not only listen but\nalso speak for more natural human-LLM interaction. Joint speech-text modeling\nis a promising direction to achieve this. However, the effectiveness of recent\nspeech tokens for joint modeling remains underexplored. To address this, we\nintroduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that\ndirectly addresses the modality gap by aligning speech token with the\ncorresponding text transcription during the tokenization stage. We propose a\nmethod that can achieve this through a attention-based aggregation mechanism\nand with speech reconstruction as the training objective. We conduct extensive\nexperiments and show that TASTE can preserve essential paralinguistic\ninformation while dramatically reducing the token sequence length. With TASTE,\nwe perform straightforward joint spoken language modeling by using Low-Rank\nAdaptation on the pre-trained text LLM. Experimental results show that\nTASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;\nwhile significantly outperform other pre-trained SLMs on speech continuation\nacross subjective and objective evaluations. To our knowledge, TASTE is the\nfirst end-to-end approach that utilizes a reconstruction objective to\nautomatically learn a text-aligned speech tokenization and embedding suitable\nfor spoken language modeling. Our demo, code, and model are available at\nhttps://mtkresearch.github.io/TASTE-SpokenLM.github.io."
                },
                "authors": [
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Kuan-Yi Lee"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16733v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16733v1",
                "updated": "2025-05-22T14:47:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    47,
                    7,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:47:07Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    47,
                    7,
                    3,
                    142,
                    0
                ],
                "title": "Forward-only Diffusion Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward-only Diffusion Probabilistic Models"
                },
                "summary": "This work presents a forward-only diffusion (FoD) approach for generative\nmodelling. In contrast to traditional diffusion models that rely on a coupled\nforward-backward diffusion scheme, FoD directly learns data generation through\na single forward diffusion process, yielding a simple yet efficient generative\nframework. The core of FoD is a state-dependent linear stochastic differential\nequation that involves a mean-reverting term in both the drift and diffusion\nfunctions. This mean-reversion property guarantees the convergence to clean\ndata, naturally simulating a stochastic interpolation between source and target\ndistributions. More importantly, FoD is analytically tractable and is trained\nusing a simple stochastic flow matching objective, enabling a few-step\nnon-Markov chain sampling during inference. The proposed FoD model, despite its\nsimplicity, achieves competitive performance on various image-conditioned\n(e.g., image restoration) and unconditional generation tasks, demonstrating its\neffectiveness in generative modelling. Our code is available at\nhttps://github.com/Algolzw/FoD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a forward-only diffusion (FoD) approach for generative\nmodelling. In contrast to traditional diffusion models that rely on a coupled\nforward-backward diffusion scheme, FoD directly learns data generation through\na single forward diffusion process, yielding a simple yet efficient generative\nframework. The core of FoD is a state-dependent linear stochastic differential\nequation that involves a mean-reverting term in both the drift and diffusion\nfunctions. This mean-reversion property guarantees the convergence to clean\ndata, naturally simulating a stochastic interpolation between source and target\ndistributions. More importantly, FoD is analytically tractable and is trained\nusing a simple stochastic flow matching objective, enabling a few-step\nnon-Markov chain sampling during inference. The proposed FoD model, despite its\nsimplicity, achieves competitive performance on various image-conditioned\n(e.g., image restoration) and unconditional generation tasks, demonstrating its\neffectiveness in generative modelling. Our code is available at\nhttps://github.com/Algolzw/FoD."
                },
                "authors": [
                    {
                        "name": "Ziwei Luo"
                    },
                    {
                        "name": "Fredrik K. Gustafsson"
                    },
                    {
                        "name": "Jens Sjölund"
                    },
                    {
                        "name": "Thomas B. Schön"
                    }
                ],
                "author_detail": {
                    "name": "Thomas B. Schön"
                },
                "author": "Thomas B. Schön",
                "arxiv_comment": "Project page: https://algolzw.github.io/fod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16733v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16733v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01106v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01106v3",
                "updated": "2025-05-22T14:47:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    47,
                    0,
                    3,
                    142,
                    0
                ],
                "published": "2024-10-01T22:28:39Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    22,
                    28,
                    39,
                    1,
                    275,
                    0
                ],
                "title": "Statistical inference on black-box generative models in the data kernel\n  perspective space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference on black-box generative models in the data kernel\n  perspective space"
                },
                "summary": "Generative models are capable of producing human-expert level content across\na variety of topics and domains. As the impact of generative models grows, it\nis necessary to develop statistical methods to understand collections of\navailable models. These methods are particularly important in settings where\nthe user may not have access to information related to a model's pre-training\ndata, weights, or other relevant model-level covariates. In this paper we\nextend recent results on representations of black-box generative models to\nmodel-level statistical inference tasks. We demonstrate that the model-level\nrepresentations are effective for multiple inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models are capable of producing human-expert level content across\na variety of topics and domains. As the impact of generative models grows, it\nis necessary to develop statistical methods to understand collections of\navailable models. These methods are particularly important in settings where\nthe user may not have access to information related to a model's pre-training\ndata, weights, or other relevant model-level covariates. In this paper we\nextend recent results on representations of black-box generative models to\nmodel-level statistical inference tasks. We demonstrate that the model-level\nrepresentations are effective for multiple inference tasks."
                },
                "authors": [
                    {
                        "name": "Hayden Helm"
                    },
                    {
                        "name": "Aranyak Acharyya"
                    },
                    {
                        "name": "Brandon Duderstadt"
                    },
                    {
                        "name": "Youngser Park"
                    },
                    {
                        "name": "Carey E. Priebe"
                    }
                ],
                "author_detail": {
                    "name": "Carey E. Priebe"
                },
                "author": "Carey E. Priebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01106v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01106v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01449v2",
                "updated": "2025-05-22T14:45:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    45,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2024-05-02T16:32:41Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    32,
                    41,
                    3,
                    123,
                    0
                ],
                "title": "Convection and the Core $g$-mode in Proto-Compact Stars -- A detailed\n  analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convection and the Core $g$-mode in Proto-Compact Stars -- A detailed\n  analysis"
                },
                "summary": "We present a detailed analysis of the dynamics of proto-compact star (PCS)\nconvection and the core ${}^2\\!g_1$-mode in core-collapse supernovae based on\ngeneral relativistic 2D and 3D neutrino hydrodynamics simulations. Based on 2D\nsimulations, we derive a mode relation for the core $g$-mode frequency in terms\nof PCS and equation of state parameters, and discuss its limits of accuracy.\nThis relation may prove useful for parameter inference from future supernova\ngravitational wave (GW) signals if the core $g$-mode or an emission gap at the\navoided crossing with the fundamental mode can be detected. The current 3D\nsimulation does not show GW emission from the core $g$-mode due to less power\nin high-frequency convective motions to excite the mode, however. Analysing the\ndynamics of PCS convection in 3D, we find that simple scaling laws for\nconvective velocity from mixing-length theory (MLT) do not apply. Energy and\nlepton number transport is instead governed by a more complex balance between\nneutrino fluxes and turbulent fluxes that results in roughly uniform rates of\nchange of entropy and lepton number in the PCS convection zone. Electron\nfraction and enthalpy contrasts in PCS convection are not well captured by the\nMLT gradient approximation. We find distinctly different spectra for the\nturbulent kinetic energy and turbulent fluctuations in the electron fraction,\nwhich scale approximately as $l^{-1}$ without a downturn at low $l$. We suggest\nthat the different turbulence spectrum of the electron fraction is naturally\nexpected for a passive scalar quantity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a detailed analysis of the dynamics of proto-compact star (PCS)\nconvection and the core ${}^2\\!g_1$-mode in core-collapse supernovae based on\ngeneral relativistic 2D and 3D neutrino hydrodynamics simulations. Based on 2D\nsimulations, we derive a mode relation for the core $g$-mode frequency in terms\nof PCS and equation of state parameters, and discuss its limits of accuracy.\nThis relation may prove useful for parameter inference from future supernova\ngravitational wave (GW) signals if the core $g$-mode or an emission gap at the\navoided crossing with the fundamental mode can be detected. The current 3D\nsimulation does not show GW emission from the core $g$-mode due to less power\nin high-frequency convective motions to excite the mode, however. Analysing the\ndynamics of PCS convection in 3D, we find that simple scaling laws for\nconvective velocity from mixing-length theory (MLT) do not apply. Energy and\nlepton number transport is instead governed by a more complex balance between\nneutrino fluxes and turbulent fluxes that results in roughly uniform rates of\nchange of entropy and lepton number in the PCS convection zone. Electron\nfraction and enthalpy contrasts in PCS convection are not well captured by the\nMLT gradient approximation. We find distinctly different spectra for the\nturbulent kinetic energy and turbulent fluctuations in the electron fraction,\nwhich scale approximately as $l^{-1}$ without a downturn at low $l$. We suggest\nthat the different turbulence spectrum of the electron fraction is naturally\nexpected for a passive scalar quantity."
                },
                "authors": [
                    {
                        "name": "Pia Jakobus"
                    },
                    {
                        "name": "Bernhard Mueller"
                    },
                    {
                        "name": "Alexander Heger"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Heger"
                },
                "author": "Alexander Heger",
                "arxiv_comment": "Accepted 2025 May 22 (MNRAS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16732v1",
                "updated": "2025-05-22T14:45:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    45,
                    46,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:45:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    45,
                    46,
                    3,
                    142,
                    0
                ],
                "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs"
                },
                "summary": "Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal decision-making under partial observability requires agents to\nbalance reducing uncertainty (exploration) against pursuing immediate\nobjectives (exploitation). In this paper, we introduce a novel policy\noptimization framework for continuous partially observable Markov decision\nprocesses (POMDPs) that explicitly addresses this challenge. Our method casts\npolicy learning as probabilistic inference in a non-Markovian Feynman--Kac\nmodel that inherently captures the value of information gathering by\nanticipating future observations, without requiring extrinsic exploration\nbonuses or handcrafted heuristics. To optimize policies under this model, we\ndevelop a nested sequential Monte Carlo~(SMC) algorithm that efficiently\nestimates a history-dependent policy gradient under samples from the optimal\ntrajectory distribution induced by the POMDP. We demonstrate the effectiveness\nof our algorithm across standard continuous POMDP benchmarks, where existing\nmethods struggle to act under uncertainty."
                },
                "authors": [
                    {
                        "name": "Hany Abdulsamad"
                    },
                    {
                        "name": "Sahel Iqbal"
                    },
                    {
                        "name": "Simo Särkkä"
                    }
                ],
                "author_detail": {
                    "name": "Simo Särkkä"
                },
                "author": "Simo Särkkä",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01822v4",
                "updated": "2025-05-22T14:33:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    33,
                    36,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-03T21:00:14Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    0,
                    14,
                    0,
                    34,
                    0
                ],
                "title": "Firewalls to Secure Dynamic LLM Agentic Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firewalls to Secure Dynamic LLM Agentic Networks"
                },
                "summary": "LLM agents will likely communicate on behalf of users with other\nentity-representing agents on tasks involving long-horizon plans with\ninterdependent goals. Current work neglects these agentic networks and their\nchallenges. We identify required properties for agent communication:\nproactivity, adaptability, privacy (sharing only task-necessary information),\nand security (preserving integrity and utility against selfish entities). After\ndemonstrating communication vulnerabilities, we propose a practical design and\nprotocol inspired by network security principles. Our framework automatically\nderives task-specific rules from prior conversations to build firewalls. These\nfirewalls construct a closed language that is completely controlled by the\ndeveloper. They transform any personal data to the allowed degree of\npermissibility entailed by the task. Both operations are completely quarantined\nfrom external attackers, disabling the potential for prompt injections,\njailbreaks, or manipulation. By incorporating rules learned from their previous\nmistakes, agents rewrite their instructions and self-correct during\ncommunication. Evaluations on diverse attacks demonstrate our framework\nsignificantly reduces privacy and security vulnerabilities while allowing\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents will likely communicate on behalf of users with other\nentity-representing agents on tasks involving long-horizon plans with\ninterdependent goals. Current work neglects these agentic networks and their\nchallenges. We identify required properties for agent communication:\nproactivity, adaptability, privacy (sharing only task-necessary information),\nand security (preserving integrity and utility against selfish entities). After\ndemonstrating communication vulnerabilities, we propose a practical design and\nprotocol inspired by network security principles. Our framework automatically\nderives task-specific rules from prior conversations to build firewalls. These\nfirewalls construct a closed language that is completely controlled by the\ndeveloper. They transform any personal data to the allowed degree of\npermissibility entailed by the task. Both operations are completely quarantined\nfrom external attackers, disabling the potential for prompt injections,\njailbreaks, or manipulation. By incorporating rules learned from their previous\nmistakes, agents rewrite their instructions and self-correct during\ncommunication. Evaluations on diverse attacks demonstrate our framework\nsignificantly reduces privacy and security vulnerabilities while allowing\nadaptability."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Amr Gomaa"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    },
                    {
                        "name": "Per Ola Kristensson"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16725v1",
                "updated": "2025-05-22T14:33:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    33,
                    3,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:33:03Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    33,
                    3,
                    3,
                    142,
                    0
                ],
                "title": "Masked Conditioning for Deep Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Conditioning for Deep Generative Models"
                },
                "summary": "Datasets in engineering domains are often small, sparsely labeled, and\ncontain numerical as well as categorical conditions. Additionally.\ncomputational resources are typically limited in practical applications which\nhinders the adoption of generative models for engineering tasks. We introduce a\nnovel masked-conditioning approach, that enables generative models to work with\nsparse, mixed-type data. We mask conditions during training to simulate sparse\nconditions at inference time. For this purpose, we explore the use of various\nsparsity schedules that show different strengths and weaknesses. In addition,\nwe introduce a flexible embedding that deals with categorical as well as\nnumerical conditions. We integrate our method into an efficient variational\nautoencoder as well as a latent diffusion model and demonstrate the\napplicability of our approach on two engineering-related datasets of 2D point\nclouds and images. Finally, we show that small models trained on limited data\ncan be coupled with large pretrained foundation models to improve generation\nquality while retaining the controllability induced by our conditioning scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datasets in engineering domains are often small, sparsely labeled, and\ncontain numerical as well as categorical conditions. Additionally.\ncomputational resources are typically limited in practical applications which\nhinders the adoption of generative models for engineering tasks. We introduce a\nnovel masked-conditioning approach, that enables generative models to work with\nsparse, mixed-type data. We mask conditions during training to simulate sparse\nconditions at inference time. For this purpose, we explore the use of various\nsparsity schedules that show different strengths and weaknesses. In addition,\nwe introduce a flexible embedding that deals with categorical as well as\nnumerical conditions. We integrate our method into an efficient variational\nautoencoder as well as a latent diffusion model and demonstrate the\napplicability of our approach on two engineering-related datasets of 2D point\nclouds and images. Finally, we show that small models trained on limited data\ncan be coupled with large pretrained foundation models to improve generation\nquality while retaining the controllability induced by our conditioning scheme."
                },
                "authors": [
                    {
                        "name": "Phillip Mueller"
                    },
                    {
                        "name": "Jannik Wiese"
                    },
                    {
                        "name": "Sebastian Mueller"
                    },
                    {
                        "name": "Lars Mikelsons"
                    }
                ],
                "author_detail": {
                    "name": "Lars Mikelsons"
                },
                "author": "Lars Mikelsons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16723v1",
                "updated": "2025-05-22T14:32:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    32,
                    23,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:32:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    32,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "Robust LLM Fingerprinting via Domain-Specific Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Fingerprinting via Domain-Specific Watermarks"
                },
                "summary": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06738v3",
                "updated": "2025-05-22T14:30:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    47,
                    3,
                    142,
                    0
                ],
                "published": "2024-02-09T19:16:27Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    19,
                    16,
                    27,
                    4,
                    40,
                    0
                ],
                "title": "EntGPT: Entity Linking with Generative Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntGPT: Entity Linking with Generative Large Language Models"
                },
                "summary": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL."
                },
                "authors": [
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Amrit Poudel"
                    },
                    {
                        "name": "Qingkai Zeng"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Sanmitra Bhattacharya"
                },
                "author": "Sanmitra Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16722v1",
                "updated": "2025-05-22T14:30:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    14,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:30:14Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    14,
                    3,
                    142,
                    0
                ],
                "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."
                },
                "authors": [
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13386v2",
                "updated": "2025-05-22T14:30:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    7,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-18T00:24:52Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    0,
                    24,
                    52,
                    4,
                    108,
                    0
                ],
                "title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis"
                },
                "summary": "In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations. The code and models will be\navailable at https://thunder.is.tue.mpg.de/"
                },
                "authors": [
                    {
                        "name": "Radek Daněček"
                    },
                    {
                        "name": "Carolin Schmitt"
                    },
                    {
                        "name": "Senya Polikovsky"
                    },
                    {
                        "name": "Michael J. Black"
                    }
                ],
                "author_detail": {
                    "name": "Michael J. Black"
                },
                "author": "Michael J. Black",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16710v1",
                "updated": "2025-05-22T14:11:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    11,
                    34,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:11:34Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    11,
                    34,
                    3,
                    142,
                    0
                ],
                "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization"
                },
                "summary": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02693v2",
                "updated": "2025-05-22T14:09:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    9,
                    1,
                    3,
                    142,
                    0
                ],
                "published": "2024-10-03T17:18:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    18,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Discovering Spoofing Attempts on Language Model Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Spoofing Attempts on Language Model Watermarks"
                },
                "summary": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. Despite recent work\ndemonstrating that state-of-the-art schemes are, in fact, vulnerable to\nspoofing, no prior work has focused on post-hoc methods to discover spoofing\nattempts. In this work, we for the first time propose a reliable statistical\nmethod to distinguish spoofed from genuinely watermarked text, suggesting that\ncurrent spoofing attacks are less effective than previously thought. In\nparticular, we show that regardless of their underlying approach, all current\nlearning-based spoofing methods consistently leave observable artifacts in\nspoofed texts, indicative of watermark forgery. We build upon these findings to\npropose rigorous statistical tests that reliably reveal the presence of such\nartifacts and thus demonstrate that a watermark has been spoofed. Our\nexperimental evaluation shows high test power across all learning-based\nspoofing methods, providing insights into their fundamental limitations and\nsuggesting a way to mitigate this threat. We make all our code available at\nhttps://github.com/eth-sri/watermark-spoofing-detection .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. Despite recent work\ndemonstrating that state-of-the-art schemes are, in fact, vulnerable to\nspoofing, no prior work has focused on post-hoc methods to discover spoofing\nattempts. In this work, we for the first time propose a reliable statistical\nmethod to distinguish spoofed from genuinely watermarked text, suggesting that\ncurrent spoofing attacks are less effective than previously thought. In\nparticular, we show that regardless of their underlying approach, all current\nlearning-based spoofing methods consistently leave observable artifacts in\nspoofed texts, indicative of watermark forgery. We build upon these findings to\npropose rigorous statistical tests that reliably reveal the presence of such\nartifacts and thus demonstrate that a watermark has been spoofed. Our\nexperimental evaluation shows high test power across all learning-based\nspoofing methods, providing insights into their fundamental limitations and\nsuggesting a way to mitigate this threat. We make all our code available at\nhttps://github.com/eth-sri/watermark-spoofing-detection ."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.17020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17020v1",
                "updated": "2025-05-22T17:59:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:59:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual\n  Cross-Attention Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual\n  Cross-Attention Mechanisms"
                },
                "summary": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources."
                },
                "authors": [
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Joey Tsai"
                    },
                    {
                        "name": "Hongwei Xue"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Lingyi Hong"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Ray Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ray Zhang"
                },
                "author": "Ray Zhang",
                "arxiv_comment": "Project page: https://github.com/shilinyan99/CrossLMM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17017v1",
                "updated": "2025-05-22T17:59:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:59:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO"
                },
                "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT"
                },
                "authors": [
                    {
                        "name": "Chengzhuo Tong"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Wenyu Shan"
                    },
                    {
                        "name": "Xinyu Wei"
                    },
                    {
                        "name": "Zhenghao Xing"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    }
                ],
                "author_detail": {
                    "name": "Pheng-Ann Heng"
                },
                "author": "Pheng-Ann Heng",
                "arxiv_comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06776v2",
                "updated": "2025-05-22T17:59:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    59,
                    11,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-10T18:54:05Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    54,
                    5,
                    0,
                    41,
                    0
                ],
                "title": "InSTA: Towards Internet-Scale Training For Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InSTA: Towards Internet-Scale Training For Agents"
                },
                "summary": "The predominant approach for training web navigation agents is to gather\nhuman demonstrations for a set of popular websites and hand-written tasks, but\nit is becoming clear that human data is an inefficient resource. We develop a\npipeline to facilitate internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM annotates 150k sites with agentic\ntasks. In the next stage, LLM agents complete tasks and produce trajectories.\nIn the final stage, an LLM filters trajectories by judging their success.\nLanguage models are powerful data curation tools, identifying harmful content\nwith an accuracy of 97%, judging successful trajectories with an accuracy of\n82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that\nare competitive with frontier LLMs as web agents, while being smaller and\nfaster. Our top agent reaches a success rate of 56.9%, outperforming the data\ncollection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and\nreaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code,\nmodels and data at: https://data-for-agents.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The predominant approach for training web navigation agents is to gather\nhuman demonstrations for a set of popular websites and hand-written tasks, but\nit is becoming clear that human data is an inefficient resource. We develop a\npipeline to facilitate internet-scale training for agents without laborious\nhuman annotations. In the first stage, an LLM annotates 150k sites with agentic\ntasks. In the next stage, LLM agents complete tasks and produce trajectories.\nIn the final stage, an LLM filters trajectories by judging their success.\nLanguage models are powerful data curation tools, identifying harmful content\nwith an accuracy of 97%, judging successful trajectories with an accuracy of\n82.6%, and producing effective data. We train agents based on Qwen 3 1.7B that\nare competitive with frontier LLMs as web agents, while being smaller and\nfaster. Our top agent reaches a success rate of 56.9%, outperforming the data\ncollection policy Qwen 3 235B, a 235 times larger Llama 4 Maverick, and\nreaching 94.7% of the performance of Gemini 2.5 Flash. We are releasing code,\nmodels and data at: https://data-for-agents.github.io."
                },
                "authors": [
                    {
                        "name": "Brandon Trabucco"
                    },
                    {
                        "name": "Gunnar Sigurdsson"
                    },
                    {
                        "name": "Robinson Piramuthu"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    }
                ],
                "author_detail": {
                    "name": "Ruslan Salakhutdinov"
                },
                "author": "Ruslan Salakhutdinov",
                "arxiv_comment": "Improved results, zero-shot transfer to Web Voyager",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17005v1",
                "updated": "2025-05-22T17:58:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    58,
                    26,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:58:26Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    58,
                    26,
                    3,
                    142,
                    0
                ],
                "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus."
                },
                "authors": [
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Wenqing Tian"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Yuhuan Wu"
                    },
                    {
                        "name": "Jiahao Zhao"
                    },
                    {
                        "name": "Yingqian Min"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16998v1",
                "updated": "2025-05-22T17:57:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    57,
                    23,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:57:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    57,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Excel in Complex Logical Reasoning with Formal\n  Language?"
                },
                "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval."
                },
                "authors": [
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Jianing Wang"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Liangcai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Liangcai Gao"
                },
                "author": "Liangcai Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16997v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16997v1",
                "updated": "2025-05-22T17:56:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:56:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs"
                },
                "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Qimin Wu"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16997v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16997v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16995v1",
                "updated": "2025-05-22T17:56:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    21,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:56:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    56,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "DecoupledESC: Enhancing Emotional Support Generation via\n  Strategy-Response Decoupled Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecoupledESC: Enhancing Emotional Support Generation via\n  Strategy-Response Decoupled Preference Optimization"
                },
                "summary": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality."
                },
                "authors": [
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Xin Shi"
                    },
                    {
                        "name": "Xueqiao Zhang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yawei Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yawei Luo"
                },
                "author": "Yawei Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16994v1",
                "updated": "2025-05-22T17:55:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    43,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:55:43Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    55,
                    43,
                    3,
                    142,
                    0
                ],
                "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning"
                },
                "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec."
                },
                "authors": [
                    {
                        "name": "Runyang You"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16988v1",
                "updated": "2025-05-22T17:54:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent\n  Systems"
                },
                "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community."
                },
                "authors": [
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Keduan Huang"
                    },
                    {
                        "name": "Qimin Wu"
                    },
                    {
                        "name": "Yuzhu Cai"
                    },
                    {
                        "name": "Tian Jin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Jiaqi Su"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Kaiqu Liang"
                    },
                    {
                        "name": "Jiaao Chen"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Rongye Shi"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Wenjun Wu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v1",
                "updated": "2025-05-22T17:54:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16984v1",
                "updated": "2025-05-22T17:53:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:53:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UFT: Unifying Supervised and Reinforcement Fine-Tuning"
                },
                "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Mingyang Liu"
                    },
                    {
                        "name": "Gabriele Farina"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    }
                ],
                "author_detail": {
                    "name": "Asuman Ozdaglar"
                },
                "author": "Asuman Ozdaglar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16983v1",
                "updated": "2025-05-22T17:53:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    28,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:53:28Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    53,
                    28,
                    3,
                    142,
                    0
                ],
                "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch\n  Mismatches with Group Position Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as Effective Streaming Processor: Bridging Streaming-Batch\n  Mismatches with Group Position Encoding"
                },
                "summary": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM."
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Jinlan Fu"
                    },
                    {
                        "name": "Zixuan Lin"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16982v1",
                "updated": "2025-05-22T17:52:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    59,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:52:59Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    59,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Correlation: Towards Causal Large Language Model Agents in\n  Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Correlation: Towards Causal Large Language Model Agents in\n  Biomedicine"
                },
                "summary": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promise in biomedicine but lack true causal\nunderstanding, relying instead on correlations. This paper envisions causal LLM\nagents that integrate multimodal data (text, images, genomics, etc.) and\nperform intervention-based reasoning to infer cause-and-effect. Addressing this\nrequires overcoming key challenges: designing safe, controllable agentic\nframeworks; developing rigorous benchmarks for causal evaluation; integrating\nheterogeneous data sources; and synergistically combining LLMs with structured\nknowledge (KGs) and formal causal inference tools. Such agents could unlock\ntransformative opportunities, including accelerating drug discovery through\nautomated hypothesis generation and simulation, enabling personalized medicine\nthrough patient-specific causal models. This research agenda aims to foster\ninterdisciplinary efforts, bridging causal concepts and foundation models to\ndevelop reliable AI partners for biomedical progress."
                },
                "authors": [
                    {
                        "name": "Adib Bazgir"
                    },
                    {
                        "name": "Amir Habibdoust Lafmajani"
                    },
                    {
                        "name": "Yuwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuwen Zhang"
                },
                "author": "Yuwen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16979v1",
                "updated": "2025-05-22T17:52:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    33,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:52:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know the Ropes: A Heuristic Strategy for LLM-based Multi-Agent System\n  Design"
                },
                "summary": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-agent LLMs hit hard limits--finite context, role overload, and brittle\ndomain transfer. Conventional multi-agent fixes soften those edges yet expose\nfresh pains: ill-posed decompositions, fuzzy contracts, and verification\noverhead that blunts the gains. We therefore present Know-The-Ropes (KtR), a\nframework that converts domain priors into an algorithmic blueprint hierarchy,\nin which tasks are recursively split into typed, controller-mediated subtasks,\neach solved zero-shot or with the lightest viable boost (e.g.,\nchain-of-thought, micro-tune, self-check). Grounded in the No-Free-Lunch\ntheorem, KtR trades the chase for a universal prompt for disciplined\ndecomposition. On the Knapsack problem (3-8 items), three GPT-4o-mini agents\nraise accuracy from 3% zero-shot to 95% on size-5 instances after patching a\nsingle bottleneck agent. On the tougher Task-Assignment problem (6-15 jobs), a\nsix-agent o3-mini blueprint hits 100% up to size 10 and 84% on sizes 13-15,\nversus 11% zero-shot. Algorithm-aware decomposition plus targeted augmentation\nthus turns modest models into reliable collaborators--no ever-larger monoliths\nrequired."
                },
                "authors": [
                    {
                        "name": "Zhenkun Li"
                    },
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16978v1",
                "updated": "2025-05-22T17:52:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    31,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:52:31Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    52,
                    31,
                    3,
                    142,
                    0
                ],
                "title": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar\n  Generation"
                },
                "summary": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammar plays a critical role in natural language processing and text/code\ngeneration by enabling the definition of syntax, the creation of parsers, and\nguiding structured outputs. Although large language models (LLMs) demonstrate\nimpressive capabilities across domains, their ability to infer and generate\ngrammars has not yet been thoroughly explored. In this paper, we aim to study\nand improve the ability of LLMs for few-shot grammar generation, where grammars\nare inferred from sets of a small number of positive and negative examples and\ngenerated in Backus-Naur Form. To explore this, we introduced a novel dataset\ncomprising 540 structured grammar generation challenges, devised 6 metrics, and\nevaluated 8 various LLMs against it. Our findings reveal that existing LLMs\nperform sub-optimally in grammar generation. To address this, we propose an\nLLM-driven hybrid genetic algorithm, namely HyGenar, to optimize grammar\ngeneration. HyGenar achieves substantial improvements in both the syntactic and\nsemantic correctness of generated grammars across LLMs."
                },
                "authors": [
                    {
                        "name": "Weizhi Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Chris Sypherd"
                    },
                    {
                        "name": "Elizabeth Polgreen"
                    },
                    {
                        "name": "Vaishak Belle"
                    }
                ],
                "author_detail": {
                    "name": "Vaishak Belle"
                },
                "author": "Vaishak Belle",
                "arxiv_comment": "Accepted to ACL 2025 Findings. Code available at\n  https://github.com/RutaTang/HyGenar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16976v1",
                "updated": "2025-05-22T17:51:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "Creatively Upscaling Images with Global-Regional Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creatively Upscaling Images with Global-Regional Priors"
                },
                "summary": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details."
                },
                "authors": [
                    {
                        "name": "Yurui Qian"
                    },
                    {
                        "name": "Qi Cai"
                    },
                    {
                        "name": "Yingwei Pan"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Tao Mei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Mei"
                },
                "author": "Tao Mei",
                "arxiv_comment": "International Journal of Computer Vision (IJCV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16975v1",
                "updated": "2025-05-22T17:51:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software\n  Development"
                },
                "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Yuzhu Cai"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Yu Qian"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16973v1",
                "updated": "2025-05-22T17:51:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:51:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    51,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "VeriFastScore: Speeding up long-form factuality evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriFastScore: Speeding up long-form factuality evaluation"
                },
                "summary": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets."
                },
                "authors": [
                    {
                        "name": "Rishanth Rajendhran"
                    },
                    {
                        "name": "Amir Zadeh"
                    },
                    {
                        "name": "Matthew Sarte"
                    },
                    {
                        "name": "Chuan Li"
                    },
                    {
                        "name": "Mohit Iyyer"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Iyyer"
                },
                "author": "Mohit Iyyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14625v2",
                "updated": "2025-05-22T17:49:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    49,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-20T17:16:44Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    16,
                    44,
                    1,
                    140,
                    0
                ],
                "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning"
                },
                "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Yuetai Li"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Bhaskar Ramasubramanian"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16968v1",
                "updated": "2025-05-22T17:48:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    48,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:48:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    48,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark"
                },
                "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."
                },
                "authors": [
                    {
                        "name": "Ahmed Heakl"
                    },
                    {
                        "name": "Sarim Hashmi"
                    },
                    {
                        "name": "Gustavo Bertolo Stahl"
                    },
                    {
                        "name": "Seung Hun Eddie Han"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Abdulrahman Mahmoud"
                    }
                ],
                "author_detail": {
                    "name": "Abdulrahman Mahmoud"
                },
                "author": "Abdulrahman Mahmoud",
                "arxiv_comment": "20 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16967v1",
                "updated": "2025-05-22T17:47:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    47,
                    57,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:47:57Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    47,
                    57,
                    3,
                    142,
                    0
                ],
                "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard\n  Negatives for Robust Information Retrieval"
                },
                "summary": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16964v1",
                "updated": "2025-05-22T17:46:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    46,
                    11,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:46:11Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    46,
                    11,
                    3,
                    142,
                    0
                ],
                "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning"
                },
                "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Suhao Yu"
                    },
                    {
                        "name": "Haojin Wang"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Cihang Xie"
                    },
                    {
                        "name": "Yuyin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuyin Zhou"
                },
                "author": "Yuyin Zhou",
                "arxiv_comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16957v1",
                "updated": "2025-05-22T17:36:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    36,
                    33,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:36:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    36,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "Invisible Prompts, Visible Threats: Malicious Font Injection in External\n  Resources for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Prompts, Visible Threats: Malicious Font Injection in External\n  Resources for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly equipped with capabilities of\nreal-time web search and integrated with protocols like Model Context Protocol\n(MCP). This extension could introduce new security vulnerabilities. We present\na systematic investigation of LLM vulnerabilities to hidden adversarial prompts\nthrough malicious font injection in external resources like webpages, where\nattackers manipulate code-to-glyph mapping to inject deceptive content which\nare invisible to users. We evaluate two critical attack scenarios: (1)\n\"malicious content relay\" and (2) \"sensitive data leakage\" through MCP-enabled\ntools. Our experiments reveal that indirect prompts with injected malicious\nfont can bypass LLM safety mechanisms through external resources, achieving\nvarying success rates based on data sensitivity and prompt design. Our research\nunderscores the urgent need for enhanced security measures in LLM deployments\nwhen processing external content."
                },
                "authors": [
                    {
                        "name": "Junjie Xiong"
                    },
                    {
                        "name": "Changjia Zhu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Lingyao Li"
                    }
                ],
                "author_detail": {
                    "name": "Lingyao Li"
                },
                "author": "Lingyao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16954v1",
                "updated": "2025-05-22T17:34:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    45,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:34:45Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    45,
                    3,
                    142,
                    0
                ],
                "title": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of\n  Vulnerabilities in Privacy Protection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking Aegis: An Adversarial LLM-based Game for Raising Awareness of\n  Vulnerabilities in Privacy Protection"
                },
                "summary": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for raising awareness of privacy protection often fail to\nengage users or provide hands-on insights into how privacy vulnerabilities are\nexploited. To address this, we incorporate an adversarial mechanic in the\ndesign of the dialogue-based serious game Cracking Aegis. Leveraging LLMs to\nsimulate natural interactions, the game challenges players to impersonate\ncharacters and extract sensitive information from an AI agent, Aegis. A user\nstudy (n=22) revealed that players employed diverse deceptive linguistic\nstrategies, including storytelling and emotional rapport, to manipulate Aegis.\nAfter playing, players reported connecting in-game scenarios with real-world\nprivacy vulnerabilities, such as phishing and impersonation, and expressed\nintentions to strengthen privacy control, such as avoiding oversharing personal\ninformation with AI systems. This work highlights the potential of LLMs to\nsimulate complex relational interactions in serious games, while demonstrating\nhow an adversarial game strategy provides unique insights for designs for\nsocial good, particularly privacy protection."
                },
                "authors": [
                    {
                        "name": "Jiaying Fu"
                    },
                    {
                        "name": "Yiyang Lu"
                    },
                    {
                        "name": "Zehua Yang"
                    },
                    {
                        "name": "Fiona Nah"
                    },
                    {
                        "name": "RAY LC"
                    }
                ],
                "author_detail": {
                    "name": "RAY LC"
                },
                "author": "RAY LC",
                "arxiv_doi": "10.1145/3715336.3735812",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715336.3735812",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.16954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, In Designing Interactive Systems Conference (DIS 25)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16952v1",
                "updated": "2025-05-22T17:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "A Comprehensive Evaluation of Contemporary ML-Based Solvers for\n  Combinatorial Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Contemporary ML-Based Solvers for\n  Combinatorial Optimization"
                },
                "summary": "Machine learning (ML) has demonstrated considerable potential in supporting\nmodel design and optimization for combinatorial optimization (CO) problems.\nHowever, much of the progress to date has been evaluated on small-scale,\nsynthetic datasets, raising concerns about the practical effectiveness of\nML-based solvers in real-world, large-scale CO scenarios. Additionally, many\nexisting CO benchmarks lack sufficient training data, limiting their utility\nfor evaluating data-driven approaches. To address these limitations, we\nintroduce FrontierCO, a comprehensive benchmark that covers eight canonical CO\nproblem types and evaluates 16 representative ML-based solvers--including graph\nneural networks and large language model (LLM) agents. FrontierCO features\nchallenging instances drawn from industrial applications and frontier CO\nresearch, offering both realistic problem difficulty and abundant training\ndata. Our empirical results provide critical insights into the strengths and\nlimitations of current ML methods, helping to guide more robust and practically\nrelevant advances at the intersection of machine learning and combinatorial\noptimization. Our data is available at\nhttps://huggingface.co/datasets/CO-Bench/FrontierCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) has demonstrated considerable potential in supporting\nmodel design and optimization for combinatorial optimization (CO) problems.\nHowever, much of the progress to date has been evaluated on small-scale,\nsynthetic datasets, raising concerns about the practical effectiveness of\nML-based solvers in real-world, large-scale CO scenarios. Additionally, many\nexisting CO benchmarks lack sufficient training data, limiting their utility\nfor evaluating data-driven approaches. To address these limitations, we\nintroduce FrontierCO, a comprehensive benchmark that covers eight canonical CO\nproblem types and evaluates 16 representative ML-based solvers--including graph\nneural networks and large language model (LLM) agents. FrontierCO features\nchallenging instances drawn from industrial applications and frontier CO\nresearch, offering both realistic problem difficulty and abundant training\ndata. Our empirical results provide critical insights into the strengths and\nlimitations of current ML methods, helping to guide more robust and practically\nrelevant advances at the intersection of machine learning and combinatorial\noptimization. Our data is available at\nhttps://huggingface.co/datasets/CO-Bench/FrontierCO."
                },
                "authors": [
                    {
                        "name": "Shengyu Feng"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Shanda Li"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Yiming Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Yang"
                },
                "author": "Yiming Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16953v1",
                "updated": "2025-05-22T17:34:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:34:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    34,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "ICYM2I: The illusion of multimodal informativeness under missingness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICYM2I: The illusion of multimodal informativeness under missingness"
                },
                "summary": "Multimodal learning is of continued interest in artificial intelligence-based\napplications, motivated by the potential information gain from combining\ndifferent types of data. However, modalities collected and curated during\ndevelopment may differ from the modalities available at deployment due to\nmultiple factors including cost, hardware failure, or -- as we argue in this\nwork -- the perceived informativeness of a given modality. Na{\\\"i}ve estimation\nof the information gain associated with including an additional modality\nwithout accounting for missingness may result in improper estimates of that\nmodality's value in downstream tasks. Our work formalizes the problem of\nmissingness in multimodal learning and demonstrates the biases resulting from\nignoring this process. To address this issue, we introduce ICYM2I (In Case You\nMultimodal Missed It), a framework for the evaluation of predictive performance\nand information gain under missingness through inverse probability\nweighting-based correction. We demonstrate the importance of the proposed\nadjustment to estimate information gain under missingness on synthetic,\nsemi-synthetic, and real-world medical datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal learning is of continued interest in artificial intelligence-based\napplications, motivated by the potential information gain from combining\ndifferent types of data. However, modalities collected and curated during\ndevelopment may differ from the modalities available at deployment due to\nmultiple factors including cost, hardware failure, or -- as we argue in this\nwork -- the perceived informativeness of a given modality. Na{\\\"i}ve estimation\nof the information gain associated with including an additional modality\nwithout accounting for missingness may result in improper estimates of that\nmodality's value in downstream tasks. Our work formalizes the problem of\nmissingness in multimodal learning and demonstrates the biases resulting from\nignoring this process. To address this issue, we introduce ICYM2I (In Case You\nMultimodal Missed It), a framework for the evaluation of predictive performance\nand information gain under missingness through inverse probability\nweighting-based correction. We demonstrate the importance of the proposed\nadjustment to estimate information gain under missingness on synthetic,\nsemi-synthetic, and real-world medical datasets."
                },
                "authors": [
                    {
                        "name": "Young Sang Choi"
                    },
                    {
                        "name": "Vincent Jeanselme"
                    },
                    {
                        "name": "Pierre Elias"
                    },
                    {
                        "name": "Shalmali Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Shalmali Joshi"
                },
                "author": "Shalmali Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06059v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06059v2",
                "updated": "2025-05-22T17:33:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-09T22:41:24Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    22,
                    41,
                    24,
                    6,
                    40,
                    0
                ],
                "title": "Position: We Need An Adaptive Interpretation of Helpful, Honest, and\n  Harmless Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: We Need An Adaptive Interpretation of Helpful, Honest, and\n  Harmless Principles"
                },
                "summary": "The Helpful, Honest, and Harmless (HHH) principle is a foundational framework\nfor aligning AI systems with human values. However, existing interpretations of\nthe HHH principle often overlook contextual variability and conflicting\nrequirements across applications. In this paper, we argue for an adaptive\ninterpretation of the HHH principle and propose a reference framework for its\nadaptation to diverse scenarios. We first examine the principle's foundational\nsignificance and identify ambiguities and conflicts through case studies of its\ndimensions. To address these challenges, we introduce the concept of priority\norder, which provides a structured approach for balancing trade-offs among\nhelpfulness, honesty, and harmlessness. Further, we explore the\ninterrelationships between these dimensions, demonstrating how harmlessness and\nhelpfulness can be jointly enhanced and analyzing their interdependencies in\nhigh-risk evaluations. Building on these insights, we propose a reference\nframework that integrates context definition, value prioritization, risk\nassessment, and benchmarking standards to guide the adaptive application of the\nHHH principle. This work offers practical insights for improving AI alignment,\nensuring that HHH principles remain both ethically grounded and operationally\neffective in real-world AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Helpful, Honest, and Harmless (HHH) principle is a foundational framework\nfor aligning AI systems with human values. However, existing interpretations of\nthe HHH principle often overlook contextual variability and conflicting\nrequirements across applications. In this paper, we argue for an adaptive\ninterpretation of the HHH principle and propose a reference framework for its\nadaptation to diverse scenarios. We first examine the principle's foundational\nsignificance and identify ambiguities and conflicts through case studies of its\ndimensions. To address these challenges, we introduce the concept of priority\norder, which provides a structured approach for balancing trade-offs among\nhelpfulness, honesty, and harmlessness. Further, we explore the\ninterrelationships between these dimensions, demonstrating how harmlessness and\nhelpfulness can be jointly enhanced and analyzing their interdependencies in\nhigh-risk evaluations. Building on these insights, we propose a reference\nframework that integrates context definition, value prioritization, risk\nassessment, and benchmarking standards to guide the adaptive application of the\nHHH principle. This work offers practical insights for improving AI alignment,\nensuring that HHH principles remain both ethically grounded and operationally\neffective in real-world AI deployment."
                },
                "authors": [
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Kehan Guo"
                    },
                    {
                        "name": "Xiangqi Wang"
                    },
                    {
                        "name": "Or Cohen-Sasson"
                    },
                    {
                        "name": "Max Lamparth"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06059v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16947v1",
                "updated": "2025-05-22T17:32:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    32,
                    50,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:32:50Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    32,
                    50,
                    3,
                    142,
                    0
                ],
                "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs"
                },
                "summary": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in Large Language Models (LLMs) safety and alignment,\ncurrent adversarial attacks on frontier LLMs are still able to force harmful\ngenerations consistently. Although adversarial training has been widely studied\nand shown to significantly improve the robustness of traditional machine\nlearning models, its strengths and weaknesses in the context of LLMs are less\nunderstood. Specifically, while existing discrete adversarial attacks are\neffective at producing harmful content, training LLMs with concrete adversarial\nprompts is often computationally expensive, leading to reliance on continuous\nrelaxations. As these relaxations do not correspond to discrete input tokens,\nsuch latent training methods often leave models vulnerable to a diverse set of\ndiscrete attacks. In this work, we aim to bridge this gap by introducing MixAT,\na novel method that combines stronger discrete and faster continuous attacks\nduring training. We rigorously evaluate MixAT across a wide spectrum of\nstate-of-the-art attacks, proposing the At Least One Attack Success Rate\n(ALO-ASR) metric to capture the worst-case vulnerability of models. We show\nMixAT achieves substantially better robustness (ALO-ASR < 20%) compared to\nprior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to\nmethods based on continuous relaxations. We further analyze MixAT in realistic\ndeployment settings, exploring how chat templates, quantization, low-rank\nadapters, and temperature affect both adversarial training and evaluation,\nrevealing additional blind spots in current methodologies. Our results\ndemonstrate that MixAT's discrete-continuous defense offers a principled and\nsuperior robustness-accuracy tradeoff with minimal computational overhead,\nhighlighting its promise for building safer LLMs. We provide our code and\nmodels at https://github.com/insait-institute/MixAT."
                },
                "authors": [
                    {
                        "name": "Csaba Dékány"
                    },
                    {
                        "name": "Stefan Balauca"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Dimitar I. Dimitrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16944v1",
                "updated": "2025-05-22T17:31:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    31,
                    10,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:31:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    31,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTIF: Benchmarking Instruction Following of Large Language Models in\n  Agentic Scenarios"
                },
                "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Amy Xin"
                    },
                    {
                        "name": "Youfeng Liu"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17956v2",
                "updated": "2025-05-22T17:29:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    29,
                    6,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-25T08:27:28Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    8,
                    27,
                    28,
                    1,
                    56,
                    0
                ],
                "title": "Towards Better Understanding of Program-of-Thought Reasoning in\n  Cross-Lingual and Multilingual Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Better Understanding of Program-of-Thought Reasoning in\n  Cross-Lingual and Multilingual Environments"
                },
                "summary": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-step reasoning is essential for large language models (LLMs), yet\nmultilingual performance remains challenging. While Chain-of-Thought (CoT)\nprompting improves reasoning, it struggles with non-English languages due to\nthe entanglement of reasoning and execution. Program-of-Thought (PoT) prompting\nseparates reasoning from execution, offering a promising alternative but\nshifting the challenge to generating programs from non-English questions. We\npropose a framework to evaluate PoT by separating multilingual reasoning from\ncode execution to examine (i) the impact of fine-tuning on question-reasoning\nalignment and (ii) how reasoning quality affects answer correctness. Our\nfindings demonstrate that PoT fine-tuning substantially enhances multilingual\nreasoning, outperforming CoT fine-tuned models. We further demonstrate a strong\ncorrelation between reasoning quality (measured through code quality) and\nanswer accuracy, highlighting its potential as a test-time performance\nimprovement heuristic."
                },
                "authors": [
                    {
                        "name": "Patomporn Payoungkhamdee"
                    },
                    {
                        "name": "Pume Tuchinda"
                    },
                    {
                        "name": "Jinheon Baek"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Can Udomcharoenchaikit"
                    },
                    {
                        "name": "Potsawee Manakul"
                    },
                    {
                        "name": "Peerat Limkonchotiwat"
                    },
                    {
                        "name": "Ekapol Chuangsuwanich"
                    },
                    {
                        "name": "Sarana Nutanong"
                    }
                ],
                "author_detail": {
                    "name": "Sarana Nutanong"
                },
                "author": "Sarana Nutanong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16934v1",
                "updated": "2025-05-22T17:24:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:24:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "In-Context Watermarks for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Watermarks for Large Language Models"
                },
                "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution."
                },
                "authors": [
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Christopher Kruegel"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Yuheng Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Bu"
                },
                "author": "Yuheng Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16928v1",
                "updated": "2025-05-22T17:20:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:20:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Needle(s) in the Embodied Haystack: Environment, Architecture,\n  and Training Considerations for Long Context Reasoning"
                },
                "summary": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce $\\infty$-THOR, a new framework for long-horizon embodied tasks\nthat advances long-context understanding in embodied AI. $\\infty$-THOR\nprovides: (1) a generation framework for synthesizing scalable, reproducible,\nand unlimited long-horizon trajectories; (2) a novel embodied QA task,\nNeedle(s) in the Embodied Haystack, where multiple scattered clues across\nextended trajectories test agents' long-context reasoning ability; and (3) a\nlong-horizon dataset and benchmark suite featuring complex tasks that span\nhundreds of environment steps, each paired with ground-truth action sequences.\nTo enable this capability, we explore architectural adaptations, including\ninterleaved Goal-State-Action modeling, context extension techniques, and\nContext Parallelism, to equip LLM-based agents for extreme long-context\nreasoning and interaction. Experimental results and analyses highlight the\nchallenges posed by our benchmark and provide insights into training strategies\nand model behaviors under long-horizon conditions. Our work provides a\nfoundation for the next generation of embodied AI systems capable of robust,\nlong-term reasoning and planning."
                },
                "authors": [
                    {
                        "name": "Bosung Kim"
                    },
                    {
                        "name": "Prithviraj Ammanabrolu"
                    }
                ],
                "author_detail": {
                    "name": "Prithviraj Ammanabrolu"
                },
                "author": "Prithviraj Ammanabrolu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16923v1",
                "updated": "2025-05-22T17:16:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    16,
                    41,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    16,
                    41,
                    3,
                    142,
                    0
                ],
                "title": "TULiP: Test-time Uncertainty Estimation via Linearization and Weight\n  Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TULiP: Test-time Uncertainty Estimation via Linearization and Weight\n  Perturbation"
                },
                "summary": "A reliable uncertainty estimation method is the foundation of many modern\nout-of-distribution (OOD) detectors, which are critical for safe deployments of\ndeep learning models in the open world. In this work, we propose TULiP, a\ntheoretically-driven post-hoc uncertainty estimator for OOD detection. Our\napproach considers a hypothetical perturbation applied to the network before\nconvergence. Based on linearized training dynamics, we bound the effect of such\nperturbation, resulting in an uncertainty score computable by perturbing model\nparameters. Ultimately, our approach computes uncertainty from a set of sampled\npredictions. We visualize our bound on synthetic regression and classification\ndatasets. Furthermore, we demonstrate the effectiveness of TULiP using\nlarge-scale OOD detection benchmarks for image classification. Our method\nexhibits state-of-the-art performance, particularly for near-distribution\nsamples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A reliable uncertainty estimation method is the foundation of many modern\nout-of-distribution (OOD) detectors, which are critical for safe deployments of\ndeep learning models in the open world. In this work, we propose TULiP, a\ntheoretically-driven post-hoc uncertainty estimator for OOD detection. Our\napproach considers a hypothetical perturbation applied to the network before\nconvergence. Based on linearized training dynamics, we bound the effect of such\nperturbation, resulting in an uncertainty score computable by perturbing model\nparameters. Ultimately, our approach computes uncertainty from a set of sampled\npredictions. We visualize our bound on synthetic regression and classification\ndatasets. Furthermore, we demonstrate the effectiveness of TULiP using\nlarge-scale OOD detection benchmarks for image classification. Our method\nexhibits state-of-the-art performance, particularly for near-distribution\nsamples."
                },
                "authors": [
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Dongshen Wu"
                    },
                    {
                        "name": "Yuichiro Wada"
                    },
                    {
                        "name": "Takafumi Kanamori"
                    }
                ],
                "author_detail": {
                    "name": "Takafumi Kanamori"
                },
                "author": "Takafumi Kanamori",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16922v1",
                "updated": "2025-05-22T17:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    16,
                    8,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    16,
                    8,
                    3,
                    142,
                    0
                ],
                "title": "UNCLE: Uncertainty Expressions in Long-Form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNCLE: Uncertainty Expressions in Long-Form Generation"
                },
                "summary": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16918v1",
                "updated": "2025-05-22T17:13:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    13,
                    1,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:13:01Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    13,
                    1,
                    3,
                    142,
                    0
                ],
                "title": "Scalable and Interpretable Contextual Bandits: A Literature Review and\n  Retail Offer Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable and Interpretable Contextual Bandits: A Literature Review and\n  Retail Offer Prototype"
                },
                "summary": "This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)\nmethods and introduces an experimental framework for scalable, interpretable\noffer selection, addressing the challenge of fast-changing offers. The approach\nmodels context at the product category level, allowing offers to span multiple\ncategories and enabling knowledge transfer across similar offers. This improves\nlearning efficiency and generalization in dynamic environments. The framework\nextends standard CMAB methodology to support multi-category contexts, and\nachieves scalability through efficient feature engineering and modular design.\nAdvanced features such as MPG (Member Purchase Gap) and MF (Matrix\nFactorization) capture nuanced user-offer interactions, with implementation in\nPython for practical deployment.\n  A key contribution is interpretability at scale: logistic regression models\nyield transparent weight vectors, accessible via a large language model (LLM)\ninterface for real-time, user-level tracking and explanation of evolving\npreferences. This enables the generation of detailed member profiles and\nidentification of behavioral patterns, supporting personalized offer\noptimization and enhancing trust in automated decisions. By situating our\nprototype alongside established paradigms like Generalized Linear Models and\nThompson Sampling, we demonstrate its value for both research and real-world\nCMAB applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB)\nmethods and introduces an experimental framework for scalable, interpretable\noffer selection, addressing the challenge of fast-changing offers. The approach\nmodels context at the product category level, allowing offers to span multiple\ncategories and enabling knowledge transfer across similar offers. This improves\nlearning efficiency and generalization in dynamic environments. The framework\nextends standard CMAB methodology to support multi-category contexts, and\nachieves scalability through efficient feature engineering and modular design.\nAdvanced features such as MPG (Member Purchase Gap) and MF (Matrix\nFactorization) capture nuanced user-offer interactions, with implementation in\nPython for practical deployment.\n  A key contribution is interpretability at scale: logistic regression models\nyield transparent weight vectors, accessible via a large language model (LLM)\ninterface for real-time, user-level tracking and explanation of evolving\npreferences. This enables the generation of detailed member profiles and\nidentification of behavioral patterns, supporting personalized offer\noptimization and enhancing trust in automated decisions. By situating our\nprototype alongside established paradigms like Generalized Linear Models and\nThompson Sampling, we demonstrate its value for both research and real-world\nCMAB applications."
                },
                "authors": [
                    {
                        "name": "Nikola Tankovic"
                    },
                    {
                        "name": "Robert Sajina"
                    }
                ],
                "author_detail": {
                    "name": "Robert Sajina"
                },
                "author": "Robert Sajina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03637v2",
                "updated": "2025-05-22T17:12:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    12,
                    6,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-05T16:16:46Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    16,
                    16,
                    46,
                    2,
                    64,
                    0
                ],
                "title": "L2RDaS: Synthesizing 4D Radar Tensors for Model Generalization via\n  Dataset Expansion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L2RDaS: Synthesizing 4D Radar Tensors for Model Generalization via\n  Dataset Expansion"
                },
                "summary": "4-dimensional (4D) radar is increasingly adopted in autonomous driving for\nperception tasks, owing to its robustness under adverse weather conditions. To\nbetter utilize the spatial information inherent in 4D radar data, recent deep\nlearning methods have transitioned from using sparse point cloud to 4D radar\ntensors. However, the scarcity of publicly available 4D radar tensor datasets\nlimits model generalization across diverse driving scenarios. Previous methods\naddressed this by synthesizing radar data, but the outputs did not fully\nexploit the spatial information characteristic of 4D radar. To overcome these\nlimitations, we propose LiDAR-to-4D radar data synthesis (L2RDaS), a framework\nthat synthesizes spatially informative 4D radar tensors from LiDAR data\navailable in existing autonomous driving datasets. L2RDaS integrates a modified\nU-Net architecture to effectively capture spatial information and an object\ninformation supplement (OBIS) module to enhance reflection fidelity. This\nframework enables the synthesis of radar tensors across diverse driving\nscenarios without additional sensor deployment or data collection. L2RDaS\nimproves model generalization by expanding real datasets with synthetic radar\ntensors, achieving an average increase of 4.25\\% in ${{AP}_{BEV}}$ and 2.87\\%\nin ${{AP}_{3D}}$ across three detection models. Additionally, L2RDaS supports\nground-truth augmentation (GT-Aug) by embedding annotated objects into LiDAR\ndata and synthesizing them into radar tensors, resulting in further average\nincreases of 3.75\\% in ${{AP}_{BEV}}$ and 4.03\\% in ${{AP}_{3D}}$. The\nimplementation will be available at https://github.com/kaist-avelab/K-Radar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "4-dimensional (4D) radar is increasingly adopted in autonomous driving for\nperception tasks, owing to its robustness under adverse weather conditions. To\nbetter utilize the spatial information inherent in 4D radar data, recent deep\nlearning methods have transitioned from using sparse point cloud to 4D radar\ntensors. However, the scarcity of publicly available 4D radar tensor datasets\nlimits model generalization across diverse driving scenarios. Previous methods\naddressed this by synthesizing radar data, but the outputs did not fully\nexploit the spatial information characteristic of 4D radar. To overcome these\nlimitations, we propose LiDAR-to-4D radar data synthesis (L2RDaS), a framework\nthat synthesizes spatially informative 4D radar tensors from LiDAR data\navailable in existing autonomous driving datasets. L2RDaS integrates a modified\nU-Net architecture to effectively capture spatial information and an object\ninformation supplement (OBIS) module to enhance reflection fidelity. This\nframework enables the synthesis of radar tensors across diverse driving\nscenarios without additional sensor deployment or data collection. L2RDaS\nimproves model generalization by expanding real datasets with synthetic radar\ntensors, achieving an average increase of 4.25\\% in ${{AP}_{BEV}}$ and 2.87\\%\nin ${{AP}_{3D}}$ across three detection models. Additionally, L2RDaS supports\nground-truth augmentation (GT-Aug) by embedding annotated objects into LiDAR\ndata and synthesizing them into radar tensors, resulting in further average\nincreases of 3.75\\% in ${{AP}_{BEV}}$ and 4.03\\% in ${{AP}_{3D}}$. The\nimplementation will be available at https://github.com/kaist-avelab/K-Radar."
                },
                "authors": [
                    {
                        "name": "Woo-Jin Jung"
                    },
                    {
                        "name": "Dong-Hee Paek"
                    },
                    {
                        "name": "Seung-Hyun Kong"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hyun Kong"
                },
                "author": "Seung-Hyun Kong",
                "arxiv_comment": "9 pages, 3 figures, Arxiv preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14652v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14652v3",
                "updated": "2025-05-22T17:05:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    5,
                    9,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-20T17:41:33Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    17,
                    41,
                    33,
                    1,
                    140,
                    0
                ],
                "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-Reasoner: Advancing LLM Reasoning Across All Domains"
                },
                "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xueguang Ma"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Dongfu Jiang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    },
                    {
                        "name": "Wenhu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenhu Chen"
                },
                "author": "Wenhu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14652v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14652v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16903v1",
                "updated": "2025-05-22T17:03:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    3,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:03:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    3,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "Unsupervised Prompting for Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Prompting for Graph Neural Networks"
                },
                "summary": "Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to\naddress the semantic gap between pre-training and fine-tuning steps. However,\nexisting GNN prompting methods rely on labeled data and involve lightweight\nfine-tuning for downstream tasks. Meanwhile, in-context learning methods for\nLarge Language Models (LLMs) have shown promising performance with no parameter\nupdating and no or minimal labeled data. Inspired by these approaches, in this\nwork, we first introduce a challenging problem setup to evaluate GNN prompting\nmethods. This setup encourages a prompting function to enhance a pre-trained\nGNN's generalization to a target dataset under covariate shift without updating\nthe GNN's parameters and with no labeled data. Next, we propose a fully\nunsupervised prompting method based on consistency regularization through\npseudo-labeling. We use two regularization techniques to align the prompted\ngraphs' distribution with the original data and reduce biased predictions.\nThrough extensive experiments under our problem setting, we demonstrate that\nour unsupervised approach outperforms the state-of-the-art prompting methods\nthat have access to labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt tuning methods for Graph Neural Networks (GNNs) have become popular to\naddress the semantic gap between pre-training and fine-tuning steps. However,\nexisting GNN prompting methods rely on labeled data and involve lightweight\nfine-tuning for downstream tasks. Meanwhile, in-context learning methods for\nLarge Language Models (LLMs) have shown promising performance with no parameter\nupdating and no or minimal labeled data. Inspired by these approaches, in this\nwork, we first introduce a challenging problem setup to evaluate GNN prompting\nmethods. This setup encourages a prompting function to enhance a pre-trained\nGNN's generalization to a target dataset under covariate shift without updating\nthe GNN's parameters and with no labeled data. Next, we propose a fully\nunsupervised prompting method based on consistency regularization through\npseudo-labeling. We use two regularization techniques to align the prompted\ngraphs' distribution with the original data and reduce biased predictions.\nThrough extensive experiments under our problem setting, we demonstrate that\nour unsupervised approach outperforms the state-of-the-art prompting methods\nthat have access to labels."
                },
                "authors": [
                    {
                        "name": "Peyman Baghershahi"
                    },
                    {
                        "name": "Sourav Medya"
                    }
                ],
                "author_detail": {
                    "name": "Sourav Medya"
                },
                "author": "Sourav Medya",
                "arxiv_comment": "25 pages, 5 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16901v1",
                "updated": "2025-05-22T17:00:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    0,
                    55,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T17:00:55Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    0,
                    55,
                    3,
                    142,
                    0
                ],
                "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%."
                },
                "authors": [
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Zhenhao Tang"
                    },
                    {
                        "name": "Hongen Peng"
                    },
                    {
                        "name": "Xukun Zhu"
                    },
                    {
                        "name": "Bingchang Liu"
                    },
                    {
                        "name": "Yingguang Yang"
                    },
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Zhaogui Xu"
                    },
                    {
                        "name": "Haipeng Zhang"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Peng Di"
                    }
                ],
                "author_detail": {
                    "name": "Peng Di"
                },
                "author": "Peng Di",
                "arxiv_comment": "31 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16894v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16894v1",
                "updated": "2025-05-22T16:50:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:50:58Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    50,
                    58,
                    3,
                    142,
                    0
                ],
                "title": "Shadows in the Attention: Contextual Perturbation and Representation\n  Drift in the Dynamics of Hallucination in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadows in the Attention: Contextual Perturbation and Representation\n  Drift in the Dynamics of Hallucination in LLMs"
                },
                "summary": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms."
                },
                "authors": [
                    {
                        "name": "Zeyu Wei"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xiaohui Rong"
                    },
                    {
                        "name": "Xuemin Liu"
                    },
                    {
                        "name": "He Li"
                    }
                ],
                "author_detail": {
                    "name": "He Li"
                },
                "author": "He Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16894v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16894v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16888v1",
                "updated": "2025-05-22T16:47:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    47,
                    15,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:47:15Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    47,
                    15,
                    3,
                    142,
                    0
                ],
                "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious\n  System Prompt Generation and Refining Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious\n  System Prompt Generation and Refining Framework"
                },
                "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Viet Pham"
                    },
                    {
                        "name": "Thai Le"
                    }
                ],
                "author_detail": {
                    "name": "Thai Le"
                },
                "author": "Thai Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16886v1",
                "updated": "2025-05-22T16:41:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    41,
                    37,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:41:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    41,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?"
                },
                "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers."
                },
                "authors": [
                    {
                        "name": "Nour Jedidi"
                    },
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "James Glass"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16881v1",
                "updated": "2025-05-22T16:35:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    35,
                    33,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:35:33Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    35,
                    33,
                    3,
                    142,
                    0
                ],
                "title": "CASTILLO: Characterizing Response Length Distributions of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTILLO: Characterizing Response Length Distributions of Large Language\n  Models"
                },
                "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems."
                },
                "authors": [
                    {
                        "name": "Daniel F. Perez-Ramirez"
                    },
                    {
                        "name": "Dejan Kostic"
                    },
                    {
                        "name": "Magnus Boman"
                    }
                ],
                "author_detail": {
                    "name": "Magnus Boman"
                },
                "author": "Magnus Boman",
                "arxiv_comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04380v2",
                "updated": "2025-05-22T16:34:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    34,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-05T17:21:01Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    17,
                    21,
                    1,
                    2,
                    36,
                    0
                ],
                "title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of\n  Domain-Undetermined Data"
                },
                "summary": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) using diverse datasets is crucial\nfor enhancing their overall performance across various domains. In practical\nscenarios, existing methods based on modeling the mixture proportions of data\ncomposition often struggle with data whose domain labels are missing, imprecise\nor non-normalized, while methods based on data selection usually encounter\ndifficulties in balancing multi-domain performance. To address these\nchallenges, in this work, we investigate the role of data diversity in\nenhancing the overall abilities of LLMs by empirically constructing contrastive\ndata pools and theoretically deriving explanations. Building upon the insights\ngained, we propose a new method that gives the LLM a dual identity: an output\nmodel to cognitively probe and select data based on diversity reward, as well\nas an input model to be tuned with the selected data. Extensive experiments\nshow that the proposed method notably boosts performance across\ndomain-undetermined data and a series of foundational downstream tasks when\napplied to various advanced LLMs. We release our code and hope this study can\nshed light on the understanding of data diversity and advance feedback-driven\ndata-model co-design for LLMs."
                },
                "authors": [
                    {
                        "name": "Zhenqing Ling"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Qianli Shen"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Ying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shen"
                },
                "author": "Ying Shen",
                "arxiv_comment": "33 pages, 20 figures, 21 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17775v2",
                "updated": "2025-05-22T16:26:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    26,
                    58,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-25T02:10:30Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    10,
                    30,
                    1,
                    56,
                    0
                ],
                "title": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks"
                },
                "summary": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial reasoning is a fundamental aspect of human intelligence. One key\nconcept in spatial cognition is the Frame of Reference (FoR), which identifies\nthe perspective of spatial expressions. Despite its significance, FoR has\nreceived limited attention in AI models that need spatial intelligence. There\nis a lack of dedicated benchmarks and in-depth evaluation of large language\nmodels (LLMs) in this area. To address this issue, we introduce the Frame of\nReference Evaluation in Spatial Reasoning Tasks (FoREST) benchmark, designed to\nassess FoR comprehension in LLMs. We evaluate LLMs on answering questions that\nrequire FoR comprehension and layout generation in text-to-image models using\nFoREST. Our results reveal a notable performance gap across different FoR\nclasses in various LLMs, affecting their ability to generate accurate layouts\nfor text-to-image generation. This highlights critical shortcomings in FoR\ncomprehension. To improve FoR understanding, we propose Spatial-Guided\nprompting, which improves LLMs ability to extract essential spatial concepts.\nOur proposed method improves overall performance across spatial reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Tanawan Premsri"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v2",
                "updated": "2025-05-22T16:26:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    26,
                    55,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16869v1",
                "updated": "2025-05-22T16:24:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:24:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    24,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: Multilingual Safety Alignment via Reward Gap Optimization"
                },
                "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhao"
                    },
                    {
                        "name": "Yulin Hu"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Wenxuan Zhang"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "An Zhang"
                    },
                    {
                        "name": "Yanyan Zhao"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "arxiv_comment": "To Appear at ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16864v1",
                "updated": "2025-05-22T16:21:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    21,
                    32,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:21:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    21,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Efficient Video Generation via Dynamic Token Carving"
                },
                "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga"
                },
                "authors": [
                    {
                        "name": "Yuechen Zhang"
                    },
                    {
                        "name": "Jinbo Xing"
                    },
                    {
                        "name": "Bin Xia"
                    },
                    {
                        "name": "Shaoteng Liu"
                    },
                    {
                        "name": "Bohao Peng"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Eric Lo"
                    },
                    {
                        "name": "Jiaya Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jiaya Jia"
                },
                "author": "Jiaya Jia",
                "arxiv_comment": "Project Page: https://julianjuaner.github.io/projects/jenga/ , 24\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15463v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15463v3",
                "updated": "2025-05-22T16:17:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    17,
                    34,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-19T17:41:46Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    17,
                    41,
                    46,
                    2,
                    78,
                    0
                ],
                "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference\n  for User-level Alignment"
                },
                "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our approach toward user-adaptive AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15463v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15463v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16849v1",
                "updated": "2025-05-22T16:11:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    11,
                    35,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:11:35Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    11,
                    35,
                    3,
                    142,
                    0
                ],
                "title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented\n  Generation via Knowledge Graph Walks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented\n  Generation via Knowledge Graph Walks"
                },
                "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research."
                },
                "authors": [
                    {
                        "name": "Martin Böckling"
                    },
                    {
                        "name": "Heiko Paulheim"
                    },
                    {
                        "name": "Andreea Iana"
                    }
                ],
                "author_detail": {
                    "name": "Andreea Iana"
                },
                "author": "Andreea Iana",
                "arxiv_comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15966v3",
                "updated": "2025-05-22T16:09:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    9,
                    52,
                    3,
                    142,
                    0
                ],
                "published": "2024-08-28T17:38:44Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    38,
                    44,
                    2,
                    241,
                    0
                ],
                "title": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Text, Less Point: Towards 3D Data-Efficient Point-Language\n  Understanding"
                },
                "summary": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models (LLMs) to comprehend the 3D physical world\nremains a significant challenge. Due to the lack of large-scale 3D-text pair\ndatasets, the success of LLMs has yet to be replicated in 3D understanding. In\nthis paper, we rethink this issue and propose a new task: 3D Data-Efficient\nPoint-Language Understanding. The goal is to enable LLMs to achieve robust 3D\nobject understanding with minimal 3D point cloud and text data pairs. To\naddress this task, we introduce GreenPLM, which leverages more text data to\ncompensate for the lack of 3D data. First, inspired by using CLIP to align\nimages and text, we utilize a pre-trained point cloud-text encoder to map the\n3D point cloud space to the text space. This mapping leaves us to seamlessly\nconnect the text space with LLMs. Once the point-text-LLM connection is\nestablished, we further enhance text-LLM alignment by expanding the\nintermediate text space, thereby reducing the reliance on 3D point cloud data.\nSpecifically, we generate 6M free-text descriptions of 3D objects, and design a\nthree-stage training strategy to help LLMs better explore the intrinsic\nconnections between different modalities. To achieve efficient modality\nalignment, we design a zero-parameter cross-attention module for token pooling.\nExtensive experimental results show that GreenPLM requires only 12% of the 3D\ntraining data used by existing state-of-the-art models to achieve superior 3D\nunderstanding. Remarkably, GreenPLM also achieves competitive performance using\ntext-only data. The code and weights are available at:\nhttps://github.com/TangYuan96/GreenPLM."
                },
                "authors": [
                    {
                        "name": "Yuan Tang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Xianzhi Li"
                    },
                    {
                        "name": "Qiao Yu"
                    },
                    {
                        "name": "Jinfeng Xu"
                    },
                    {
                        "name": "Yixue Hao"
                    },
                    {
                        "name": "Long Hu"
                    },
                    {
                        "name": "Min Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min Chen"
                },
                "author": "Min Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16841v1",
                "updated": "2025-05-22T16:08:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    8,
                    26,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:08:26Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    8,
                    26,
                    3,
                    142,
                    0
                ],
                "title": "On the Deployment of RIS-mounted UAV Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Deployment of RIS-mounted UAV Networks"
                },
                "summary": "Reconfigurable intelligent surfaces (RIS) enable smart wireless environments\nby dynamically controlling signal propagation to enhance communication and\nlocalization. Unmanned aerial vehicles (UAVs) can act as flying base stations\nand thus, improve system performance by avoiding signal blockages. In this\npaper, we propose a gradient ascent and coordinate search based method to\ndetermine the optimal location for a system that consists of a UAV and a RIS,\nwhere the UAV serves cellular users (CUs) and the RIS serves device-to-device\n(D2D) pairs. In particular, by optimizing the net throughput for both the D2D\npairs and the CUs, the suggested method establishes the ideal location for the\nRIS-mounted UAV. We consider both line of sight (LoS) and non-LoS paths for the\nRIS and UAV to calculate the throughput while accounting for blockages in the\nsystem. The numerical results show that the proposed method performs better\nthan the existing approaches in terms of both the net throughput and the user\nfairness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RIS) enable smart wireless environments\nby dynamically controlling signal propagation to enhance communication and\nlocalization. Unmanned aerial vehicles (UAVs) can act as flying base stations\nand thus, improve system performance by avoiding signal blockages. In this\npaper, we propose a gradient ascent and coordinate search based method to\ndetermine the optimal location for a system that consists of a UAV and a RIS,\nwhere the UAV serves cellular users (CUs) and the RIS serves device-to-device\n(D2D) pairs. In particular, by optimizing the net throughput for both the D2D\npairs and the CUs, the suggested method establishes the ideal location for the\nRIS-mounted UAV. We consider both line of sight (LoS) and non-LoS paths for the\nRIS and UAV to calculate the throughput while accounting for blockages in the\nsystem. The numerical results show that the proposed method performs better\nthan the existing approaches in terms of both the net throughput and the user\nfairness."
                },
                "authors": [
                    {
                        "name": "Anupam Mondal"
                    },
                    {
                        "name": "Priyadarshi Mukherjee"
                    },
                    {
                        "name": "Sasthi C. Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Sasthi C. Ghosh"
                },
                "author": "Sasthi C. Ghosh",
                "arxiv_comment": "Submitted for a possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16838v1",
                "updated": "2025-05-22T16:06:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    6,
                    59,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:06:59Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    6,
                    59,
                    3,
                    142,
                    0
                ],
                "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and\n  Search"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress"
                },
                "authors": [
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Huanjin Yao"
                    },
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16834v1",
                "updated": "2025-05-22T16:05:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:05:02Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning\n  Trajectory Synthesis"
                },
                "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher."
                },
                "authors": [
                    {
                        "name": "Shuang Sun"
                    },
                    {
                        "name": "Huatong Song"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Fei Bai"
                    },
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Lei Fang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16831v1",
                "updated": "2025-05-22T16:02:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    2,
                    10,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T16:02:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    2,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine\n  Unlearning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Isn't Deletion: Investigating Reversibility of Machine\n  Unlearning in LLMs"
                },
                "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Xu"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Minxin Du"
                    }
                ],
                "author_detail": {
                    "name": "Minxin Du"
                },
                "author": "Minxin Du",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16821v1",
                "updated": "2025-05-22T15:55:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    56,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:55:56Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    56,
                    3,
                    142,
                    0
                ],
                "title": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Emulation of the Radio Resource Control Layer: Towards\n  AI-Native RAN Protocols"
                },
                "summary": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large AI models (LAMs) into 6G mobile networks promises to\nredefine protocol design and control-plane intelligence by enabling autonomous,\ncognitive network operations. While industry concepts, such as ETSI's\nExperiential Networked Intelligence (ENI), envision LAM-driven agents for\nadaptive network slicing and intent-based management, practical implementations\nstill face challenges in protocol literacy and real-world deployment. This\npaper presents an end-to-end demonstration of a LAM that generates\nstandards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as\npart of control-plane procedures inside a gNB. We treat RRC messaging as a\ndomain-specific language and fine-tune a decoder-only transformer model (LLaMA\nclass) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages\nlinearized to retain their ASN.1 syntactic structure before standard byte-pair\nencoding tokenization. This enables combinatorial generalization over RRC\nprotocol states while minimizing training overhead. On 30k field-test\nrequest-response pairs, our 8 B model achieves a median cosine similarity of\n0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a\nzero-shot LLaMA-3 8B baseline -- indicating substantially improved structural\nand semantic RRC fidelity. Overall, our results show that LAMs, when augmented\nwith Radio Access Network (RAN)-specific reasoning, can directly orchestrate\ncontrol-plane procedures, representing a stepping stone toward the AI-native\nair-interface paradigm. Beyond RRC emulation, this work lays the groundwork for\nfuture AI-native wireless standards."
                },
                "authors": [
                    {
                        "name": "Ziming liu"
                    },
                    {
                        "name": "Bryan Liu"
                    },
                    {
                        "name": "Alvaro Valcarce"
                    },
                    {
                        "name": "Xiaoli Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Chu"
                },
                "author": "Xiaoli Chu",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Focuses on applying LLMs to 5G RRC protocol generation; primary: cs.NI;\n  cross-list: eess.SP, cs.LG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01222v2",
                "updated": "2025-05-22T15:55:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    55,
                    37,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-03T06:40:21Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    6,
                    40,
                    21,
                    0,
                    62,
                    0
                ],
                "title": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Perception: High-Resolution Image Perception Meets\n  Visual RAG"
                },
                "summary": "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution (HR) image perception remains a key challenge in multimodal\nlarge language models (MLLMs). To overcome the limitations of existing methods,\nthis paper shifts away from prior dedicated heuristic approaches and revisits\nthe most fundamental idea to HR perception by enhancing the long-context\ncapability of MLLMs, driven by recent advances in long-context techniques like\nretrieval-augmented generation (RAG) for general LLMs. Towards this end, this\npaper presents the first study exploring the use of RAG to address HR\nperception challenges. Specifically, we propose Retrieval-Augmented Perception\n(RAP), a training-free framework that retrieves and fuses relevant image crops\nwhile preserving spatial context using the proposed Spatial-Awareness Layout.\nTo accommodate different tasks, the proposed Retrieved-Exploration Search\n(RE-Search) dynamically selects the optimal number of crops based on model\nconfidence and retrieval scores. Experimental results on HR benchmarks\ndemonstrate the significant effectiveness of RAP, with LLaVA-v1.5-13B achieving\na 43% improvement on $V^*$ Bench and 19% on HR-Bench."
                },
                "authors": [
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Yong Luo"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19110v2",
                "updated": "2025-05-22T15:53:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    53,
                    14,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-27T05:04:02Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    5,
                    4,
                    2,
                    6,
                    117,
                    0
                ],
                "title": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal\n  Math Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE-Bench I: Towards File-level Automated Proof Engineering of Formal\n  Math Libraries"
                },
                "summary": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has shown promise in formal\ntheorem proving, yet existing benchmarks remain limited to isolated, static\nproof tasks, failing to capture the iterative, engineering-intensive workflows\nof real-world formal mathematics libraries. Motivated by analogous advances in\nsoftware engineering, we introduce the paradigm of Automated Proof Engineering\n(APE), which aims to automate proof engineering tasks such as feature addition,\nproof refactoring, and bug fixing using LLMs. To facilitate research in this\ndirection, we present APE-Bench I, the first realistic benchmark built from\nreal-world commit histories of Mathlib4, featuring diverse file-level tasks\ndescribed in natural language and verified via a hybrid approach combining the\nLean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable\nparallel verification infrastructure optimized for proof checking across\nmultiple versions of Mathlib. Empirical results on state-of-the-art LLMs\ndemonstrate strong performance on localized edits but substantial degradation\non handling complex proof engineering. This work lays the foundation for\ndeveloping agentic workflows in proof engineering, with future benchmarks\ntargeting multi-file coordination, project-scale verification, and autonomous\nagents capable of planning, editing, and repairing formal libraries."
                },
                "authors": [
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Luming Li"
                    },
                    {
                        "name": "Xiaoran Jin"
                    },
                    {
                        "name": "Jacques Fleuriot"
                    },
                    {
                        "name": "Wenda Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenda Li"
                },
                "author": "Wenda Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16810v1",
                "updated": "2025-05-22T15:49:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    49,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:49:38Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    49,
                    38,
                    3,
                    142,
                    0
                ],
                "title": "DeepRec: Towards a Deep Dive Into the Item Space with Large Language\n  Model Based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepRec: Towards a Deep Dive Into the Item Space with Large Language\n  Model Based Recommendation"
                },
                "summary": "Recently, large language models (LLMs) have been introduced into recommender\nsystems (RSs), either to enhance traditional recommendation models (TRMs) or\nserve as recommendation backbones. However, existing LLM-based RSs often do not\nfully exploit the complementary advantages of LLMs (e.g., world knowledge and\nreasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to\nfully explore the item space. To address this, we propose DeepRec, a novel\nLLM-based RS that enables autonomous multi-turn interactions between LLMs and\nTRMs for deep exploration of the item space. In each interaction turn, LLMs\nreason over user preferences and interact with TRMs to retrieve candidate\nitems. After multi-turn interactions, LLMs rank the retrieved items to generate\nthe final recommendations. We adopt reinforcement learning(RL) based\noptimization and propose novel designs from three aspects: recommendation model\nbased data rollout, recommendation-oriented hierarchical rewards, and a\ntwo-stage RL training strategy. For data rollout, we introduce a\npreference-aware TRM, with which LLMs interact to construct trajectory data.\nFor rewards, we design a hierarchical reward function that involves both\nprocess-level and outcome-level rewards to optimize the interaction process and\nrecommendation performance, respectively. For RL training, we develop a\ntwo-stage training strategy, where the first stage aims to guide LLMs to\ninteract with TRMs and the second stage focuses on performance improvement.\nExperiments on public datasets demonstrate that DeepRec significantly\noutperforms both traditional and LLM-based baselines, offering a new paradigm\nfor deep exploration in recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been introduced into recommender\nsystems (RSs), either to enhance traditional recommendation models (TRMs) or\nserve as recommendation backbones. However, existing LLM-based RSs often do not\nfully exploit the complementary advantages of LLMs (e.g., world knowledge and\nreasoning) and TRMs (e.g., recommendation-specific knowledge and efficiency) to\nfully explore the item space. To address this, we propose DeepRec, a novel\nLLM-based RS that enables autonomous multi-turn interactions between LLMs and\nTRMs for deep exploration of the item space. In each interaction turn, LLMs\nreason over user preferences and interact with TRMs to retrieve candidate\nitems. After multi-turn interactions, LLMs rank the retrieved items to generate\nthe final recommendations. We adopt reinforcement learning(RL) based\noptimization and propose novel designs from three aspects: recommendation model\nbased data rollout, recommendation-oriented hierarchical rewards, and a\ntwo-stage RL training strategy. For data rollout, we introduce a\npreference-aware TRM, with which LLMs interact to construct trajectory data.\nFor rewards, we design a hierarchical reward function that involves both\nprocess-level and outcome-level rewards to optimize the interaction process and\nrecommendation performance, respectively. For RL training, we develop a\ntwo-stage training strategy, where the first stage aims to guide LLMs to\ninteract with TRMs and the second stage focuses on performance improvement.\nExperiments on public datasets demonstrate that DeepRec significantly\noutperforms both traditional and LLM-based baselines, offering a new paradigm\nfor deep exploration in recommendation systems."
                },
                "authors": [
                    {
                        "name": "Bowen Zheng"
                    },
                    {
                        "name": "Xiaolei Wang"
                    },
                    {
                        "name": "Enze Liu"
                    },
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Lu Hongyu"
                    },
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16806v1",
                "updated": "2025-05-22T15:45:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    45,
                    29,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:45:29Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    45,
                    29,
                    3,
                    142,
                    0
                ],
                "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement"
                },
                "summary": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8."
                },
                "authors": [
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Junlan Chen"
                    },
                    {
                        "name": "Daifeng Li"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yangyang Feng"
                    },
                    {
                        "name": "Bowen Deng"
                    },
                    {
                        "name": "Weixu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weixu Chen"
                },
                "author": "Weixu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16800v1",
                "updated": "2025-05-22T15:40:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    40,
                    9,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:40:09Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    40,
                    9,
                    3,
                    142,
                    0
                ],
                "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for\n  Low-Resource Canonical Morpheme Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Beyond Limits: Multitask Learning and Synthetic Data for\n  Low-Resource Canonical Morpheme Segmentation"
                },
                "summary": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages."
                },
                "authors": [
                    {
                        "name": "Changbing Yang"
                    },
                    {
                        "name": "Garrett Nicolai"
                    }
                ],
                "author_detail": {
                    "name": "Garrett Nicolai"
                },
                "author": "Garrett Nicolai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16785v1",
                "updated": "2025-05-22T15:28:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    28,
                    25,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:28:25Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    28,
                    25,
                    3,
                    142,
                    0
                ],
                "title": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of\n  Large Language Models"
                },
                "summary": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite providing superior performance, open-source large language models\n(LLMs) are vulnerable to abusive usage. To address this issue, recent works\npropose LLM fingerprinting methods to identify the specific source LLMs behind\nsuspect applications. However, these methods fail to provide stealthy and\nrobust fingerprint verification. In this paper, we propose a novel LLM\nfingerprinting scheme, namely CoTSRF, which utilizes the Chain of Thought (CoT)\nas the fingerprint of an LLM. CoTSRF first collects the responses from the\nsource LLM by querying it with crafted CoT queries. Then, it applies\ncontrastive learning to train a CoT extractor that extracts the CoT feature\n(i.e., fingerprint) from the responses. Finally, CoTSRF conducts fingerprint\nverification by comparing the Kullback-Leibler divergence between the CoT\nfeatures of the source and suspect LLMs against an empirical threshold. Various\nexperiments have been conducted to demonstrate the advantage of our proposed\nCoTSRF for fingerprinting LLMs, particularly in stealthy and robust fingerprint\nverification."
                },
                "authors": [
                    {
                        "name": "Zhenzhen Ren"
                    },
                    {
                        "name": "GuoBiao Li"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Zhenxing Qian"
                    },
                    {
                        "name": "Xinpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinpeng Zhang"
                },
                "author": "Xinpeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16782v1",
                "updated": "2025-05-22T15:26:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    26,
                    51,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:26:51Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    26,
                    51,
                    3,
                    142,
                    0
                ],
                "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent\n  Chain-of-Thought Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Beyond Language: A Comprehensive Survey on Latent\n  Chain-of-Thought Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT."
                },
                "authors": [
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Xuan Lu"
                    },
                    {
                        "name": "Hanlin Wang"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16774v1",
                "updated": "2025-05-22T15:15:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    15,
                    29,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:15:29Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    15,
                    29,
                    3,
                    142,
                    0
                ],
                "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in\n  Audio-based Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFEval-Audio: Benchmarking Instruction-Following Capability in\n  Audio-based Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area."
                },
                "authors": [
                    {
                        "name": "Yiming Gao"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Chengwei Wei"
                    },
                    {
                        "name": "Shuo Sun"
                    },
                    {
                        "name": "AiTi Aw"
                    }
                ],
                "author_detail": {
                    "name": "AiTi Aw"
                },
                "author": "AiTi Aw",
                "arxiv_comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16765v1",
                "updated": "2025-05-22T15:07:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    7,
                    34,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T15:07:34Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    7,
                    34,
                    3,
                    142,
                    0
                ],
                "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak\n  Attack on LLMs via Steganographic Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak\n  Attack on LLMs via Steganographic Techniques"
                },
                "summary": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks pose a serious threat to large language models (LLMs) by\nbypassing built-in safety mechanisms and leading to harmful outputs. Studying\nthese attacks is crucial for identifying vulnerabilities and improving model\nsecurity. This paper presents a systematic survey of jailbreak methods from the\nnovel perspective of stealth. We find that existing attacks struggle to\nsimultaneously achieve toxic stealth (concealing toxic content) and linguistic\nstealth (maintaining linguistic naturalness). Motivated by this, we propose\nStegoAttack, a fully stealthy jailbreak attack that uses steganography to hide\nthe harmful query within benign, semantically coherent text. The attack then\nprompts the LLM to extract the hidden query and respond in an encrypted manner.\nThis approach effectively hides malicious intent while preserving naturalness,\nallowing it to evade both built-in and external safety mechanisms. We evaluate\nStegoAttack on four safety-aligned LLMs from major providers, benchmarking\nagainst eight state-of-the-art methods. StegoAttack achieves an average attack\nsuccess rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%.\nIts ASR drops by less than 1% even under external detection (e.g., Llama\nGuard). Moreover, it attains the optimal comprehensive scores on stealth\ndetection metrics, demonstrating both high efficacy and exceptional stealth\ncapabilities. The code is available at\nhttps://anonymous.4open.science/r/StegoAttack-Jail66"
                },
                "authors": [
                    {
                        "name": "Jianing Geng"
                    },
                    {
                        "name": "Biao Yi"
                    },
                    {
                        "name": "Zekun Fei"
                    },
                    {
                        "name": "Tongxi Wu"
                    },
                    {
                        "name": "Lihai Nie"
                    },
                    {
                        "name": "Zheli Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheli Liu"
                },
                "author": "Zheli Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11741v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11741v3",
                "updated": "2025-05-22T15:06:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    6,
                    30,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-17T12:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    12,
                    28,
                    11,
                    0,
                    48,
                    0
                ],
                "title": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL-o1: A Self-Reward Heuristic Dynamic Search Method for Text-to-SQL"
                },
                "summary": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL (Text2SQL) aims to map natural language questions to executable\nSQL queries. Although large language models (LLMs) have driven significant\nprogress, current approaches struggle with poor transferability to open-source\nLLMs, limited robustness against logic and function errors in complex queries,\nand inefficiencies in structured search. We introduce SQL-o1, a\nself-reward-driven heuristic search framework built on an agent-based\narchitecture to enhance model reasoning capabilities. SQL-o1 leverages Monte\nCarlo Tree Search (MCTS) for structured, multi-step exploration, and\nincorporates a dynamic pruning strategy to accelerate inference without\nsacrificing accuracy. On the Spider and Bird benchmarks, SQL-o1 achieves a\n+10.8 execution accuracy improvement on the complex Bird dataset, surpassing\neven GPT-4-based models. Notably, it exhibits strong few-shot generalization\nand robust cross-model transferability across open-source LLMs. Our code is\navailable at:https://github.com/ShuaiLyu0110/SQL-o1."
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Ripeng Li"
                    },
                    {
                        "name": "Zhonghong Ou"
                    },
                    {
                        "name": "Jiangfeng Sun"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Xiaoran Shang"
                    },
                    {
                        "name": "Meina Song"
                    },
                    {
                        "name": "Yifan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhu"
                },
                "author": "Yifan Zhu",
                "arxiv_comment": "28 pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11741v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11741v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16674v2",
                "updated": "2025-05-22T15:05:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    15,
                    5,
                    2,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-20T19:40:40Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    40,
                    40,
                    3,
                    79,
                    0
                ],
                "title": "Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants,\n  and Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants,\n  and Markets"
                },
                "summary": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\nseven widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via Socratic probing. By using our self-contained\nSocratic approach, the study aims to directly measure the models' biases rather\nthan relying on external interpretations, thereby minimizing subjective\njudgments about media bias. Our results reveal a consistent preference of\nDemocratic over Republican positions across all models. Conversely, in economic\ntopics, biases vary among Western LLMs, while those developed in China lean\nmore strongly toward socialism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While detecting and avoiding bias in LLM-generated text is becoming\nincreasingly important, media bias often remains subtle and subjective, making\nit particularly difficult to identify and mitigate. In this study, we assess\nmedia bias in LLM-generated content and LLMs' ability to detect subtle\nideological bias. We conduct this evaluation using two datasets, PoliGen and\nEconoLex, covering political and economic discourse, respectively. We evaluate\nseven widely used LLMs by prompting them to generate articles and analyze their\nideological preferences via Socratic probing. By using our self-contained\nSocratic approach, the study aims to directly measure the models' biases rather\nthan relying on external interpretations, thereby minimizing subjective\njudgments about media bias. Our results reveal a consistent preference of\nDemocratic over Republican positions across all models. Conversely, in economic\ntopics, biases vary among Western LLMs, while those developed in China lean\nmore strongly toward socialism."
                },
                "authors": [
                    {
                        "name": "Molly Kennedy"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Timo Spinde"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16754v1",
                "updated": "2025-05-22T14:59:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    59,
                    20,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:59:20Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    59,
                    20,
                    3,
                    142,
                    0
                ],
                "title": "PyTupli: A Scalable Infrastructure for Collaborative Offline\n  Reinforcement Learning Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyTupli: A Scalable Infrastructure for Collaborative Offline\n  Reinforcement Learning Projects"
                },
                "summary": "Offline reinforcement learning (RL) has gained traction as a powerful\nparadigm for learning control policies from pre-collected data, eliminating the\nneed for costly or risky online interactions. While many open-source libraries\noffer robust implementations of offline RL algorithms, they all rely on\ndatasets composed of experience tuples consisting of state, action, next state,\nand reward. Managing, curating, and distributing such datasets requires\nsuitable infrastructure. Although static datasets exist for established\nbenchmark problems, no standardized or scalable solution supports developing\nand sharing datasets for novel or user-defined benchmarks. To address this gap,\nwe introduce PyTupli, a Python-based tool to streamline the creation, storage,\nand dissemination of benchmark environments and their corresponding tuple\ndatasets. PyTupli includes a lightweight client library with defined interfaces\nfor uploading and retrieving benchmarks and data. It supports fine-grained\nfiltering at both the episode and tuple level, allowing researchers to curate\nhigh-quality, task-specific datasets. A containerized server component enables\nproduction-ready deployment with authentication, access control, and automated\ncertificate provisioning for secure use. By addressing key barriers in dataset\ninfrastructure, PyTupli facilitates more collaborative, reproducible, and\nscalable offline RL research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline reinforcement learning (RL) has gained traction as a powerful\nparadigm for learning control policies from pre-collected data, eliminating the\nneed for costly or risky online interactions. While many open-source libraries\noffer robust implementations of offline RL algorithms, they all rely on\ndatasets composed of experience tuples consisting of state, action, next state,\nand reward. Managing, curating, and distributing such datasets requires\nsuitable infrastructure. Although static datasets exist for established\nbenchmark problems, no standardized or scalable solution supports developing\nand sharing datasets for novel or user-defined benchmarks. To address this gap,\nwe introduce PyTupli, a Python-based tool to streamline the creation, storage,\nand dissemination of benchmark environments and their corresponding tuple\ndatasets. PyTupli includes a lightweight client library with defined interfaces\nfor uploading and retrieving benchmarks and data. It supports fine-grained\nfiltering at both the episode and tuple level, allowing researchers to curate\nhigh-quality, task-specific datasets. A containerized server component enables\nproduction-ready deployment with authentication, access control, and automated\ncertificate provisioning for secure use. By addressing key barriers in dataset\ninfrastructure, PyTupli facilitates more collaborative, reproducible, and\nscalable offline RL research."
                },
                "authors": [
                    {
                        "name": "Hannah Markgraf"
                    },
                    {
                        "name": "Michael Eichelbeck"
                    },
                    {
                        "name": "Daria Cappey"
                    },
                    {
                        "name": "Selin Demirtürk"
                    },
                    {
                        "name": "Yara Schattschneider"
                    },
                    {
                        "name": "Matthias Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Althoff"
                },
                "author": "Matthias Althoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16743v1",
                "updated": "2025-05-22T14:53:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:53:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative\n  Metric-driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative\n  Metric-driven Pruning"
                },
                "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM"
                },
                "authors": [
                    {
                        "name": "Florentin Beck"
                    },
                    {
                        "name": "William Rudman"
                    },
                    {
                        "name": "Carsten Eickhoff"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Eickhoff"
                },
                "author": "Carsten Eickhoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04598v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04598v3",
                "updated": "2025-05-22T14:53:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    31,
                    3,
                    142,
                    0
                ],
                "published": "2025-03-06T16:40:48Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    16,
                    40,
                    48,
                    3,
                    65,
                    0
                ],
                "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization"
                },
                "summary": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the position of layer normalization. While\nPre-Norm structures facilitate more stable training owing to their stronger\nidentity path, they often lead to suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a simple yet effective hybrid\nnormalization strategy that integrates the advantages of both Pre-Norm and\nPost-Norm. Specifically, HybridNorm employs QKV normalization within the\nattention mechanism and Post-Norm in the feed-forward network (FFN) of each\ntransformer block. We provide both theoretical insights and empirical evidence\ndemonstrating that HybridNorm improves gradient flow and model robustness.\nExtensive experiments on large-scale transformer models, including both dense\nand sparse variants, show that HybridNorm consistently outperforms both\nPre-Norm and Post-Norm approaches across multiple benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. Code is\navailable at https://github.com/BryceZhuo/HybridNorm."
                },
                "authors": [
                    {
                        "name": "Zhijian Zhuo"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Xiaoqing Li"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Jinwen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jinwen Ma"
                },
                "author": "Jinwen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04598v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04598v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15091v2",
                "updated": "2025-05-22T14:53:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    53,
                    0,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-21T04:25:18Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    4,
                    25,
                    18,
                    2,
                    141,
                    0
                ],
                "title": "ThinkRec: Thinking-based recommendation via LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkRec: Thinking-based recommendation via LLM"
                },
                "summary": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have enabled more\nsemantic-aware recommendations through natural language generation. Existing\nLLM for recommendation (LLM4Rec) methods mostly operate in a System 1-like\nmanner, relying on superficial features to match similar items based on click\nhistory, rather than reasoning through deeper behavioral logic. This often\nleads to superficial and erroneous recommendations. Motivated by this, we\npropose ThinkRec, a thinking-based framework that shifts LLM4Rec from System 1\nto System 2 (rational system). Technically, ThinkRec introduces a thinking\nactivation mechanism that augments item metadata with keyword summarization and\ninjects synthetic reasoning traces, guiding the model to form interpretable\nreasoning chains that consist of analyzing interaction histories, identifying\nuser preferences, and making decisions based on target items. On top of this,\nwe propose an instance-wise expert fusion mechanism to reduce the reasoning\ndifficulty. By dynamically assigning weights to expert models based on users'\nlatent features, ThinkRec adapts its reasoning path to individual users,\nthereby enhancing precision and personalization. Extensive experiments on\nreal-world datasets demonstrate that ThinkRec significantly improves the\naccuracy and interpretability of recommendations. Our implementations are\navailable in anonymous Github: https://github.com/Yu-Qi-hang/ThinkRec."
                },
                "authors": [
                    {
                        "name": "Qihang Yu"
                    },
                    {
                        "name": "Kairui Fu"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16737v1",
                "updated": "2025-05-22T14:52:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    52,
                    10,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:52:10Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    52,
                    10,
                    3,
                    142,
                    0
                ],
                "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing\n  Optimization"
                },
                "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP."
                },
                "authors": [
                    {
                        "name": "Chengcan Wu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Zeming Wei"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Meng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Meng Sun"
                },
                "author": "Meng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07053v2",
                "updated": "2025-05-22T14:49:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    49,
                    3,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-09T17:14:33Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    14,
                    33,
                    2,
                    99,
                    0
                ],
                "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken\n  Language Modeling"
                },
                "summary": "Recent efforts target spoken language models (SLMs) that not only listen but\nalso speak for more natural human-LLM interaction. Joint speech-text modeling\nis a promising direction to achieve this. However, the effectiveness of recent\nspeech tokens for joint modeling remains underexplored. To address this, we\nintroduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that\ndirectly addresses the modality gap by aligning speech token with the\ncorresponding text transcription during the tokenization stage. We propose a\nmethod that can achieve this through a attention-based aggregation mechanism\nand with speech reconstruction as the training objective. We conduct extensive\nexperiments and show that TASTE can preserve essential paralinguistic\ninformation while dramatically reducing the token sequence length. With TASTE,\nwe perform straightforward joint spoken language modeling by using Low-Rank\nAdaptation on the pre-trained text LLM. Experimental results show that\nTASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;\nwhile significantly outperform other pre-trained SLMs on speech continuation\nacross subjective and objective evaluations. To our knowledge, TASTE is the\nfirst end-to-end approach that utilizes a reconstruction objective to\nautomatically learn a text-aligned speech tokenization and embedding suitable\nfor spoken language modeling. Our demo, code, and model are available at\nhttps://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent efforts target spoken language models (SLMs) that not only listen but\nalso speak for more natural human-LLM interaction. Joint speech-text modeling\nis a promising direction to achieve this. However, the effectiveness of recent\nspeech tokens for joint modeling remains underexplored. To address this, we\nintroduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that\ndirectly addresses the modality gap by aligning speech token with the\ncorresponding text transcription during the tokenization stage. We propose a\nmethod that can achieve this through a attention-based aggregation mechanism\nand with speech reconstruction as the training objective. We conduct extensive\nexperiments and show that TASTE can preserve essential paralinguistic\ninformation while dramatically reducing the token sequence length. With TASTE,\nwe perform straightforward joint spoken language modeling by using Low-Rank\nAdaptation on the pre-trained text LLM. Experimental results show that\nTASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze;\nwhile significantly outperform other pre-trained SLMs on speech continuation\nacross subjective and objective evaluations. To our knowledge, TASTE is the\nfirst end-to-end approach that utilizes a reconstruction objective to\nautomatically learn a text-aligned speech tokenization and embedding suitable\nfor spoken language modeling. Our demo, code, and model are available at\nhttps://mtkresearch.github.io/TASTE-SpokenLM.github.io."
                },
                "authors": [
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "Yi-Chang Chen"
                    },
                    {
                        "name": "Kuan-Yi Lee"
                    },
                    {
                        "name": "Da-Shan Shiu"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01822v4",
                "updated": "2025-05-22T14:33:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    33,
                    36,
                    3,
                    142,
                    0
                ],
                "published": "2025-02-03T21:00:14Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    0,
                    14,
                    0,
                    34,
                    0
                ],
                "title": "Firewalls to Secure Dynamic LLM Agentic Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firewalls to Secure Dynamic LLM Agentic Networks"
                },
                "summary": "LLM agents will likely communicate on behalf of users with other\nentity-representing agents on tasks involving long-horizon plans with\ninterdependent goals. Current work neglects these agentic networks and their\nchallenges. We identify required properties for agent communication:\nproactivity, adaptability, privacy (sharing only task-necessary information),\nand security (preserving integrity and utility against selfish entities). After\ndemonstrating communication vulnerabilities, we propose a practical design and\nprotocol inspired by network security principles. Our framework automatically\nderives task-specific rules from prior conversations to build firewalls. These\nfirewalls construct a closed language that is completely controlled by the\ndeveloper. They transform any personal data to the allowed degree of\npermissibility entailed by the task. Both operations are completely quarantined\nfrom external attackers, disabling the potential for prompt injections,\njailbreaks, or manipulation. By incorporating rules learned from their previous\nmistakes, agents rewrite their instructions and self-correct during\ncommunication. Evaluations on diverse attacks demonstrate our framework\nsignificantly reduces privacy and security vulnerabilities while allowing\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM agents will likely communicate on behalf of users with other\nentity-representing agents on tasks involving long-horizon plans with\ninterdependent goals. Current work neglects these agentic networks and their\nchallenges. We identify required properties for agent communication:\nproactivity, adaptability, privacy (sharing only task-necessary information),\nand security (preserving integrity and utility against selfish entities). After\ndemonstrating communication vulnerabilities, we propose a practical design and\nprotocol inspired by network security principles. Our framework automatically\nderives task-specific rules from prior conversations to build firewalls. These\nfirewalls construct a closed language that is completely controlled by the\ndeveloper. They transform any personal data to the allowed degree of\npermissibility entailed by the task. Both operations are completely quarantined\nfrom external attackers, disabling the potential for prompt injections,\njailbreaks, or manipulation. By incorporating rules learned from their previous\nmistakes, agents rewrite their instructions and self-correct during\ncommunication. Evaluations on diverse attacks demonstrate our framework\nsignificantly reduces privacy and security vulnerabilities while allowing\nadaptability."
                },
                "authors": [
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Amr Gomaa"
                    },
                    {
                        "name": "Eugene Bagdasarian"
                    },
                    {
                        "name": "Per Ola Kristensson"
                    },
                    {
                        "name": "Reza Shokri"
                    }
                ],
                "author_detail": {
                    "name": "Reza Shokri"
                },
                "author": "Reza Shokri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16723v1",
                "updated": "2025-05-22T14:32:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    32,
                    23,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:32:23Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    32,
                    23,
                    3,
                    142,
                    0
                ],
                "title": "Robust LLM Fingerprinting via Domain-Specific Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust LLM Fingerprinting via Domain-Specific Watermarks"
                },
                "summary": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As open-source language models (OSMs) grow more capable and are widely shared\nand finetuned, ensuring model provenance, i.e., identifying the origin of a\ngiven model instance, has become an increasingly important issue. At the same\ntime, existing backdoor-based model fingerprinting techniques often fall short\nof achieving key requirements of real-world model ownership detection. In this\nwork, we build on the observation that while current open-source model\nwatermarks fail to achieve reliable content traceability, they can be\neffectively adapted to address the challenge of model provenance. To this end,\nwe introduce the concept of domain-specific watermarking for model\nfingerprinting. Rather than watermarking all generated content, we train the\nmodel to embed watermarks only within specified subdomains (e.g., particular\nlanguages or topics). This targeted approach ensures detection reliability,\nwhile improving watermark durability and quality under a range of real-world\ndeployment settings. Our evaluations show that domain-specific watermarking\nenables model fingerprinting with strong statistical guarantees, controllable\nfalse positive rates, high detection power, and preserved generation quality.\nMoreover, we find that our fingerprints are inherently stealthy and naturally\nrobust to real-world variability across deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06738v3",
                "updated": "2025-05-22T14:30:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    47,
                    3,
                    142,
                    0
                ],
                "published": "2024-02-09T19:16:27Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    19,
                    16,
                    27,
                    4,
                    40,
                    0
                ],
                "title": "EntGPT: Entity Linking with Generative Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EntGPT: Entity Linking with Generative Large Language Models"
                },
                "summary": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Linking in natural language processing seeks to match text entities to\ntheir corresponding entries in a dictionary or knowledge base. Traditional\napproaches rely on contextual models, which can be complex, hard to train, and\nhave limited transferability across different domains. Generative large\nlanguage models like GPT offer a promising alternative but often underperform\nwith naive prompts. In this study, we introduce EntGPT, employing advanced\nprompt engineering to enhance EL tasks. Our three-step hard-prompting method\n(EntGPT-P) significantly boosts the micro-F_1 score by up to 36% over vanilla\nprompts, achieving competitive performance across 10 datasets without\nsupervised fine-tuning. Additionally, our instruction tuning method (EntGPT-I)\nimproves micro-F_1 scores by 2.1% on average in supervised EL tasks and\noutperforms several baseline models in six Question Answering tasks. Our\nmethods are compatible with both open-source and proprietary LLMs. All data and\ncode are available on GitHub at https://github.com/yifding/In_Context_EL."
                },
                "authors": [
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Amrit Poudel"
                    },
                    {
                        "name": "Qingkai Zeng"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Sanmitra Bhattacharya"
                },
                "author": "Sanmitra Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16722v1",
                "updated": "2025-05-22T14:30:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    14,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:30:14Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    14,
                    3,
                    142,
                    0
                ],
                "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."
                },
                "authors": [
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17424v2",
                "updated": "2025-05-22T14:20:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    20,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2024-11-26T13:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    31,
                    23,
                    1,
                    331,
                    0
                ],
                "title": "A Primer on AP Power Save in Wi-Fi 8: Overview, Analysis, and Open\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Primer on AP Power Save in Wi-Fi 8: Overview, Analysis, and Open\n  Challenges"
                },
                "summary": "Wi-Fi facilitates the Internet connectivity of billions of devices worldwide,\nmaking it an indispensable technology for modern life. Wi-Fi networks are\nbecoming significantly denser, making energy consumption and its effects on\noperational costs and environmental sustainability crucial considerations.\nWi-Fi has already introduced several mechanisms to enhance the energy\nefficiency of non-Access Point (non-AP) stations (STAs). However, the reduction\nof energy consumption of APs has never been a priority. Always-on APs operating\nat their highest capabilities consume significant power, which affects the\nenergy costs of the infrastructure owner, aggravates the environmental impact,\nand decreases the lifetime of battery-powered APs. IEEE 802.11bn, which will be\nthe basis of Wi-Fi 8, makes a big leap forward by introducing the AP Power Save\n(PS) framework. In this article, we describe and analyze the main proposals\ndiscussed in the IEEE 802.11bn Task Group (TGbn), such as Scheduled Power Save,\n(Semi-)Dynamic Power Save, and Cross-Link Power Save. We also consider other\nproposals that are being discussed in TGbn, namely the integration of Wake-up\nRadios (WuRs) and STA offloading. We then showcase the potential benefits of AP\nPS in several scenarios, including a deployment of 470 real APs in a university\ncampus. Our numerical analysis reveals that AP power consumption can be\ndecreased on average by up to 28 percent, with further improvement potential.\nFinally, we outline the open challenges that need to be addressed to optimally\nintegrate AP PS in Wi-Fi and ensure its compatibility with legacy devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wi-Fi facilitates the Internet connectivity of billions of devices worldwide,\nmaking it an indispensable technology for modern life. Wi-Fi networks are\nbecoming significantly denser, making energy consumption and its effects on\noperational costs and environmental sustainability crucial considerations.\nWi-Fi has already introduced several mechanisms to enhance the energy\nefficiency of non-Access Point (non-AP) stations (STAs). However, the reduction\nof energy consumption of APs has never been a priority. Always-on APs operating\nat their highest capabilities consume significant power, which affects the\nenergy costs of the infrastructure owner, aggravates the environmental impact,\nand decreases the lifetime of battery-powered APs. IEEE 802.11bn, which will be\nthe basis of Wi-Fi 8, makes a big leap forward by introducing the AP Power Save\n(PS) framework. In this article, we describe and analyze the main proposals\ndiscussed in the IEEE 802.11bn Task Group (TGbn), such as Scheduled Power Save,\n(Semi-)Dynamic Power Save, and Cross-Link Power Save. We also consider other\nproposals that are being discussed in TGbn, namely the integration of Wake-up\nRadios (WuRs) and STA offloading. We then showcase the potential benefits of AP\nPS in several scenarios, including a deployment of 470 real APs in a university\ncampus. Our numerical analysis reveals that AP power consumption can be\ndecreased on average by up to 28 percent, with further improvement potential.\nFinally, we outline the open challenges that need to be addressed to optimally\nintegrate AP PS in Wi-Fi and ensure its compatibility with legacy devices."
                },
                "authors": [
                    {
                        "name": "Roger Sanchez-Vital"
                    },
                    {
                        "name": "Andrey Belogaev"
                    },
                    {
                        "name": "Carles Gomez"
                    },
                    {
                        "name": "Jeroen Famaey"
                    },
                    {
                        "name": "Eduard Garcia-Villegas"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Garcia-Villegas"
                },
                "author": "Eduard Garcia-Villegas",
                "arxiv_doi": "10.1109/MCOM.004.2400486.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MCOM.004.2400486.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.17424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in IEEE Wireless Communications (2025)",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16714v1",
                "updated": "2025-05-22T14:18:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    18,
                    14,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:18:14Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    18,
                    14,
                    3,
                    142,
                    0
                ],
                "title": "Experimental robustness benchmark of quantum neural network on a\n  superconducting quantum processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental robustness benchmark of quantum neural network on a\n  superconducting quantum processor"
                },
                "summary": "Quantum machine learning (QML) models, like their classical counterparts, are\nvulnerable to adversarial attacks, hindering their secure deployment. Here, we\nreport the first systematic experimental robustness benchmark for 20-qubit\nquantum neural network (QNN) classifiers executed on a superconducting\nprocessor. Our benchmarking framework features an efficient adversarial attack\nalgorithm designed for QNNs, enabling quantitative characterization of\nadversarial robustness and robustness bounds. From our analysis, we verify that\nadversarial training reduces sensitivity to targeted perturbations by\nregularizing input gradients, significantly enhancing QNN's robustness.\nAdditionally, our analysis reveals that QNNs exhibit superior adversarial\nrobustness compared to classical neural networks, an advantage attributed to\ninherent quantum noise. Furthermore, the empirical upper bound extracted from\nour attack experiments shows a minimal deviation ($3 \\times 10^{-3}$) from the\ntheoretical lower bound, providing strong experimental confirmation of the\nattack's effectiveness and the tightness of fidelity-based robustness bounds.\nThis work establishes a critical experimental framework for assessing and\nimproving quantum adversarial robustness, paving the way for secure and\nreliable QML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum machine learning (QML) models, like their classical counterparts, are\nvulnerable to adversarial attacks, hindering their secure deployment. Here, we\nreport the first systematic experimental robustness benchmark for 20-qubit\nquantum neural network (QNN) classifiers executed on a superconducting\nprocessor. Our benchmarking framework features an efficient adversarial attack\nalgorithm designed for QNNs, enabling quantitative characterization of\nadversarial robustness and robustness bounds. From our analysis, we verify that\nadversarial training reduces sensitivity to targeted perturbations by\nregularizing input gradients, significantly enhancing QNN's robustness.\nAdditionally, our analysis reveals that QNNs exhibit superior adversarial\nrobustness compared to classical neural networks, an advantage attributed to\ninherent quantum noise. Furthermore, the empirical upper bound extracted from\nour attack experiments shows a minimal deviation ($3 \\times 10^{-3}$) from the\ntheoretical lower bound, providing strong experimental confirmation of the\nattack's effectiveness and the tightness of fidelity-based robustness bounds.\nThis work establishes a critical experimental framework for assessing and\nimproving quantum adversarial robustness, paving the way for secure and\nreliable QML applications."
                },
                "authors": [
                    {
                        "name": "Hai-Feng Zhang"
                    },
                    {
                        "name": "Zhao-Yun Chen"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Liang-Liang Guo"
                    },
                    {
                        "name": "Tian-Le Wang"
                    },
                    {
                        "name": "Xiao-Yan Yang"
                    },
                    {
                        "name": "Ren-Ze Zhao"
                    },
                    {
                        "name": "Ze-An Zhao"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Lei Du"
                    },
                    {
                        "name": "Hao-Ran Tao"
                    },
                    {
                        "name": "Zhi-Long Jia"
                    },
                    {
                        "name": "Wei-Cheng Kong"
                    },
                    {
                        "name": "Huan-Yu Liu"
                    },
                    {
                        "name": "Athanasios V. Vasilakos"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yu-Chun Wu"
                    },
                    {
                        "name": "Ji Guan"
                    },
                    {
                        "name": "Peng Duan"
                    },
                    {
                        "name": "Guo-Ping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guo-Ping Guo"
                },
                "author": "Guo-Ping Guo",
                "arxiv_comment": "There are 8 pages with 5 figures in the main text and 15 pages with\n  14 figures in the supplementary information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16710v1",
                "updated": "2025-05-22T14:11:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    11,
                    34,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:11:34Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    11,
                    34,
                    3,
                    142,
                    0
                ],
                "title": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Long-Context LLMs Efficiently via Chunk-wise Optimization"
                },
                "summary": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long-context large language models (LLMs) exhibit remarkable document\nprocessing capabilities, their prohibitively high training costs often hinder\ncustomized applications. To mitigate this issue, we propose \\textit{Sequential\nChunk-wise Optimization} (SeCO), a memory-efficient training paradigm that\npartitions lengthy inputs into manageable chunks. Each chunk independently\nconstructs its computational graph and performs localized backpropagation,\nensuring that only one chunk's forward activations are stored in memory.\nBuilding on SeCO, we further introduce \\textit{Sparse Chunk-wise Optimization}\n(SpaCO), which reduces computational overhead by selectively propagating\ngradients to specific chunks and incorporates a carefully designed compensation\nfactor to ensure unbiased gradient estimation. SpaCO decouples the\ncomputational cost of backpropagation from the context length, enabling\ntraining time to gradually converge to inference time as sequences become\nlonger. Implemented as lightweight training wrappers, both SeCO and SpaCO offer\nsubstantial practical benefits. For example, when fine-tuning an 8B model with\nLoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to\n16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up\nto 3x faster than SeCO under the same experimental setup. These innovations\nprovide new insights into optimizing long-context models, making them more\naccessible for practical applications. We have open-sourced the code at\n\\href{https://github.com/wenhaoli-xmu/seco}{here}."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02693v2",
                "updated": "2025-05-22T14:09:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    9,
                    1,
                    3,
                    142,
                    0
                ],
                "published": "2024-10-03T17:18:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    18,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Discovering Spoofing Attempts on Language Model Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Spoofing Attempts on Language Model Watermarks"
                },
                "summary": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. Despite recent work\ndemonstrating that state-of-the-art schemes are, in fact, vulnerable to\nspoofing, no prior work has focused on post-hoc methods to discover spoofing\nattempts. In this work, we for the first time propose a reliable statistical\nmethod to distinguish spoofed from genuinely watermarked text, suggesting that\ncurrent spoofing attacks are less effective than previously thought. In\nparticular, we show that regardless of their underlying approach, all current\nlearning-based spoofing methods consistently leave observable artifacts in\nspoofed texts, indicative of watermark forgery. We build upon these findings to\npropose rigorous statistical tests that reliably reveal the presence of such\nartifacts and thus demonstrate that a watermark has been spoofed. Our\nexperimental evaluation shows high test power across all learning-based\nspoofing methods, providing insights into their fundamental limitations and\nsuggesting a way to mitigate this threat. We make all our code available at\nhttps://github.com/eth-sri/watermark-spoofing-detection .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. Despite recent work\ndemonstrating that state-of-the-art schemes are, in fact, vulnerable to\nspoofing, no prior work has focused on post-hoc methods to discover spoofing\nattempts. In this work, we for the first time propose a reliable statistical\nmethod to distinguish spoofed from genuinely watermarked text, suggesting that\ncurrent spoofing attacks are less effective than previously thought. In\nparticular, we show that regardless of their underlying approach, all current\nlearning-based spoofing methods consistently leave observable artifacts in\nspoofed texts, indicative of watermark forgery. We build upon these findings to\npropose rigorous statistical tests that reliably reveal the presence of such\nartifacts and thus demonstrate that a watermark has been spoofed. Our\nexperimental evaluation shows high test power across all learning-based\nspoofing methods, providing insights into their fundamental limitations and\nsuggesting a way to mitigate this threat. We make all our code available at\nhttps://github.com/eth-sri/watermark-spoofing-detection ."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13176v2",
                "updated": "2025-05-22T14:08:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    8,
                    7,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-19T14:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    14,
                    30,
                    46,
                    0,
                    139,
                    0
                ],
                "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language\n  Models"
                },
                "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum."
                },
                "authors": [
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Yuhang Guo"
                    },
                    {
                        "name": "Yuanfang Guo"
                    },
                    {
                        "name": "Yunhong Wang"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "Accepted by ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16703v1",
                "updated": "2025-05-22T14:04:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    4,
                    43,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:04:43Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    4,
                    43,
                    3,
                    142,
                    0
                ],
                "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating\n  Catastrophic Forgetting in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating\n  Catastrophic Forgetting in Multimodal LLMs"
                },
                "summary": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration."
                },
                "authors": [
                    {
                        "name": "Zeping Yu"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16700v1",
                "updated": "2025-05-22T14:02:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    2,
                    37,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:02:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    2,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use\n  Capabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-RADAR: A Multi-Dimensional Benchmark for Evaluating Tool Use\n  Capabilities in Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) evolve from passive text generators to active\nreasoning agents capable of tool interaction, the Model Context Protocol (MCP)\nhas emerged as a standardized framework for dynamic tool discovery and\norchestration. Despite widespread industry adoption, existing evaluation\nmethodologies fail to adequately assess tool utilization capabilities within\nthis new paradigm. This paper introduces MCP-RADAR, the first comprehensive\nbenchmark specifically designed to evaluate LLM performance in the MCP\nframework through a novel five-dimensional approach measuring: answer accuracy,\ntool selection efficiency, computational resource efficiency, parameter\nconstruction accuracy, and execution speed. Unlike conventional benchmarks that\nrely on subjective human evaluations or binary success metrics, MCP-RADAR\nemploys objective, quantifiable measurements across multiple task domains\nincluding software engineering, mathematical reasoning, and general\nproblem-solving. Our evaluations of leading commercial and open-source LLMs\nreveal distinctive capability profiles with significant trade-offs between\naccuracy, efficiency, and speed, challenging traditional single-metric\nperformance rankings. Besides, we provide valuable guidance for developers to\noptimize their tools for maximum model compatibility and effectiveness. While\nfocused on MCP due to its standardized approach, our methodology remains\napplicable across all LLM agent tool integration frameworks, providing valuable\ninsights for both LLM developers and tool creators to optimize the entire\nLLM-tool interaction ecosystem. The implementation, configurations, and\ndatasets used in our evaluation are publicly available at\nhttps://anonymous.4open.science/r/MCPRadar-B143."
                },
                "authors": [
                    {
                        "name": "Xuanqi Gao"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Shqing Ma"
                    },
                    {
                        "name": "Chao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chao Shen"
                },
                "author": "Chao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16697v1",
                "updated": "2025-05-22T14:00:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    0,
                    29,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T14:00:29Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    0,
                    29,
                    3,
                    142,
                    0
                ],
                "title": "Software Architecture Meets LLMs: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software Architecture Meets LLMs: A Systematic Literature Review"
                },
                "summary": "Large Language Models (LLMs) are used for many different software engineering\ntasks. In software architecture, they have been applied to tasks such as\nclassification of design decisions, detection of design patterns, and\ngeneration of software architecture design from requirements. However, there is\nlittle overview on how well they work, what challenges exist, and what open\nproblems remain. In this paper, we present a systematic literature review on\nthe use of LLMs in software architecture. We analyze 18 research articles to\nanswer five research questions, such as which software architecture tasks LLMs\nare used for, how much automation they provide, which models and techniques are\nused, and how these approaches are evaluated. Our findings show that while LLMs\nare increasingly applied to a variety of software architecture tasks and often\noutperform baselines, some areas, such as generating source code from\narchitectural design, cloud-native computing and architecture, and checking\nconformance remain underexplored. Although current approaches mostly use simple\nprompting techniques, we identify a growing research interest in refining\nLLM-based approaches by integrating advanced techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are used for many different software engineering\ntasks. In software architecture, they have been applied to tasks such as\nclassification of design decisions, detection of design patterns, and\ngeneration of software architecture design from requirements. However, there is\nlittle overview on how well they work, what challenges exist, and what open\nproblems remain. In this paper, we present a systematic literature review on\nthe use of LLMs in software architecture. We analyze 18 research articles to\nanswer five research questions, such as which software architecture tasks LLMs\nare used for, how much automation they provide, which models and techniques are\nused, and how these approaches are evaluated. Our findings show that while LLMs\nare increasingly applied to a variety of software architecture tasks and often\noutperform baselines, some areas, such as generating source code from\narchitectural design, cloud-native computing and architecture, and checking\nconformance remain underexplored. Although current approaches mostly use simple\nprompting techniques, we identify a growing research interest in refining\nLLM-based approaches by integrating advanced techniques."
                },
                "authors": [
                    {
                        "name": "Larissa Schmid"
                    },
                    {
                        "name": "Tobias Hey"
                    },
                    {
                        "name": "Martin Armbruster"
                    },
                    {
                        "name": "Sophie Corallo"
                    },
                    {
                        "name": "Dominik Fuchß"
                    },
                    {
                        "name": "Jan Keim"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Anne Koziolek"
                    }
                ],
                "author_detail": {
                    "name": "Anne Koziolek"
                },
                "author": "Anne Koziolek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13141v2",
                "updated": "2025-05-22T13:55:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    55,
                    49,
                    3,
                    142,
                    0
                ],
                "published": "2025-04-17T17:50:44Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    17,
                    50,
                    44,
                    3,
                    107,
                    0
                ],
                "title": "Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity at Scale: A Quantitative Analysis of an Alibaba Microservice\n  Deployment"
                },
                "summary": "Recent studies have begun to explore the characteristics of real-world\nlarge-scale microservice deployments. However, their operational complexities,\nand the degree to which these complexities are consistent across different\ndeployments, remains under explored. In this paper, we analyse a microservice\ndeployment dataset released by Alibaba to understand its scale, heterogeneity,\nand dynamicity, and compare our results to previous large-scale deployments to\nbegin to understand their commonalities. We identify tens of thousands of\nmicroservices, that support an even broader array of front-end functionality.\nMoreover, our analysis shows wide-spread long-tailed distributions of\ncharacteristics between microservices, such as share of workload and\ndependencies, highlighting inequality. This diversity is also reflected in call\ngraphs, with front-end service functionalities producing dominant and rarer,\nnon-dominant, call graphs that can involve dissimilar microservice calls. We\nfind that dependencies within the deployment at runtime can be different from\nthe static view of the system, and that the deployment undergoes daily changes.\nWe discuss the implications of our findings for state-of-the-art research in\nmicroservice management and research testbed realism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have begun to explore the characteristics of real-world\nlarge-scale microservice deployments. However, their operational complexities,\nand the degree to which these complexities are consistent across different\ndeployments, remains under explored. In this paper, we analyse a microservice\ndeployment dataset released by Alibaba to understand its scale, heterogeneity,\nand dynamicity, and compare our results to previous large-scale deployments to\nbegin to understand their commonalities. We identify tens of thousands of\nmicroservices, that support an even broader array of front-end functionality.\nMoreover, our analysis shows wide-spread long-tailed distributions of\ncharacteristics between microservices, such as share of workload and\ndependencies, highlighting inequality. This diversity is also reflected in call\ngraphs, with front-end service functionalities producing dominant and rarer,\nnon-dominant, call graphs that can involve dissimilar microservice calls. We\nfind that dependencies within the deployment at runtime can be different from\nthe static view of the system, and that the deployment undergoes daily changes.\nWe discuss the implications of our findings for state-of-the-art research in\nmicroservice management and research testbed realism."
                },
                "authors": [
                    {
                        "name": "Giles Winchester"
                    },
                    {
                        "name": "George Parisis"
                    },
                    {
                        "name": "Guoyao Xu"
                    },
                    {
                        "name": "Luc Berthouze"
                    }
                ],
                "author_detail": {
                    "name": "Luc Berthouze"
                },
                "author": "Luc Berthouze",
                "arxiv_comment": "16 pages, 13 figures, 3 tables, revised version for conference\n  submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16690v1",
                "updated": "2025-05-22T13:55:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    55,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:55:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    55,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator"
                },
                "summary": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training of large language models is essential for adapting pre-trained\nlanguage models (PLMs) to align with human preferences and downstream tasks.\nWhile PLMs typically exhibit well-calibrated confidence, post-trained language\nmodels (PoLMs) often suffer from over-confidence, assigning high confidence to\nboth correct and incorrect outputs, which can undermine reliability in critical\napplications. A major obstacle in calibrating PoLMs is the scarcity of labeled\ndata for individual downstream tasks. To address this, we propose\nDisagreement-Aware Confidence Alignment (DACA), a novel unsupervised method to\noptimize the parameters (e.g., temperature $\\tau$) in post-hoc confidence\ncalibration. Our method is motivated by the under-confidence issue caused by\nprediction disagreement between the PLM and PoLM while aligning their\nconfidence via temperature scaling. Theoretically, the PLM's confidence\nunderestimates PoLM's prediction accuracy on disagreement examples, causing a\nlarger $\\tau$ and producing under-confident predictions. DACA mitigates this by\nselectively using only agreement examples for calibration, effectively\ndecoupling the influence of disagreement. In this manner, our method avoids an\noverly large $\\tau$ in temperature scaling caused by disagreement examples,\nimproving calibration performance. Extensive experiments demonstrate the\neffectiveness of our method, improving the average ECE of open-sourced and\nAPI-based LLMs (e.g. GPT-4o) by up to 15.08$\\%$ on common benchmarks."
                },
                "authors": [
                    {
                        "name": "Beier Luo"
                    },
                    {
                        "name": "Shuoyuan Wang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16670v1",
                "updated": "2025-05-22T13:36:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    36,
                    0,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:36:00Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    36,
                    0,
                    3,
                    142,
                    0
                ],
                "title": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitHydra: Towards Bit-flip Inference Cost Attack against Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across a wide\nrange of applications, but their ever-increasing size and resource demands make\nthem vulnerable to inference cost attacks, where attackers induce victim LLMs\nto generate the longest possible output content. In this paper, we revisit\nexisting inference cost attacks and reveal that these methods can hardly\nproduce large-scale malicious effects since they are self-targeting, where\nattackers are also the users and therefore have to execute attacks solely\nthrough the inputs, whose generated content will be charged by LLMs and can\nonly directly influence themselves. Motivated by these findings, this paper\nintroduces a new type of inference cost attacks (dubbed 'bit-flip inference\ncost attack') that target the victim model itself rather than its inputs.\nSpecifically, we design a simple yet effective method (dubbed 'BitHydra') to\neffectively flip critical bits of model parameters. This process is guided by a\nloss function designed to suppress <EOS> token's probability with an efficient\ncritical bit search algorithm, thus explicitly defining the attack objective\nand enabling effective optimization. We evaluate our method on 11 LLMs ranging\nfrom 1.5B to 14B parameters under both int8 and float16 settings. Experimental\nresults demonstrate that with just 4 search samples and as few as 3 bit flips,\nBitHydra can force 100% of test prompts to reach the maximum generation length\n(e.g., 2048 tokens) on representative LLMs such as LLaMA3, highlighting its\nefficiency, scalability, and strong transferability across unseen inputs."
                },
                "authors": [
                    {
                        "name": "Xiaobei Yan"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianwei Zhang"
                },
                "author": "Tianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16667v1",
                "updated": "2025-05-22T13:32:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    32,
                    39,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:32:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    32,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELABORATION: A Comprehensive Benchmark on Human-LLM Competitive\n  Programming"
                },
                "summary": "While recent research increasingly emphasizes the value of human-LLM\ncollaboration in competitive programming and proposes numerous empirical\nmethods, a comprehensive understanding remains elusive due to the fragmented\nnature of existing studies and their use of diverse, application-specific human\nfeedback. Thus, our work serves a three-fold purpose: First, we present the\nfirst taxonomy of human feedback consolidating the entire programming process,\nwhich promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a\nnovel programming dataset specifically designed for human-LLM collaboration,\nmeticulously annotated to enable large-scale simulated human feedback and\nfacilitate costeffective real human interaction studies. Third, we introduce\nELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM\ncompetitive programming. With ELABORATION, we pinpoint strengthes and\nweaknesses of existing methods, thereby setting the foundation for future\nimprovement. Our code and dataset are available at\nhttps://github.com/SCUNLP/ELABORATION",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent research increasingly emphasizes the value of human-LLM\ncollaboration in competitive programming and proposes numerous empirical\nmethods, a comprehensive understanding remains elusive due to the fragmented\nnature of existing studies and their use of diverse, application-specific human\nfeedback. Thus, our work serves a three-fold purpose: First, we present the\nfirst taxonomy of human feedback consolidating the entire programming process,\nwhich promotes fine-grained evaluation. Second, we introduce ELABORATIONSET, a\nnovel programming dataset specifically designed for human-LLM collaboration,\nmeticulously annotated to enable large-scale simulated human feedback and\nfacilitate costeffective real human interaction studies. Third, we introduce\nELABORATION, a novel benchmark to facilitate a thorough assessment of human-LLM\ncompetitive programming. With ELABORATION, we pinpoint strengthes and\nweaknesses of existing methods, thereby setting the foundation for future\nimprovement. Our code and dataset are available at\nhttps://github.com/SCUNLP/ELABORATION"
                },
                "authors": [
                    {
                        "name": "Xinwei Yang"
                    },
                    {
                        "name": "Zhaofeng Liu"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Jiashuai Zhang"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wenqiang Lei"
                    }
                ],
                "author_detail": {
                    "name": "Wenqiang Lei"
                },
                "author": "Wenqiang Lei",
                "arxiv_comment": "ACL 2025 Main. Our code and dataset are available at\n  https://github.com/SCUNLP/ELABORATION",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16661v1",
                "updated": "2025-05-22T13:27:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    27,
                    37,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:27:37Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    27,
                    37,
                    3,
                    142,
                    0
                ],
                "title": "A Japanese Language Model and Three New Evaluation Benchmarks for\n  Pharmaceutical NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Japanese Language Model and Three New Evaluation Benchmarks for\n  Pharmaceutical NLP"
                },
                "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval."
                },
                "authors": [
                    {
                        "name": "Issey Sukeda"
                    },
                    {
                        "name": "Takuro Fujii"
                    },
                    {
                        "name": "Kosei Buma"
                    },
                    {
                        "name": "Shunsuke Sasaki"
                    },
                    {
                        "name": "Shinnosuke Ono"
                    }
                ],
                "author_detail": {
                    "name": "Shinnosuke Ono"
                },
                "author": "Shinnosuke Ono",
                "arxiv_comment": "15 pages, 9 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16555v2",
                "updated": "2025-05-22T13:27:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    27,
                    9,
                    3,
                    142,
                    0
                ],
                "published": "2024-12-21T09:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    9,
                    43,
                    51,
                    5,
                    356,
                    0
                ],
                "title": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in various fields of society\ndue to their powerful reasoning, understanding, and generation capabilities.\nHowever, the security issues associated with these models are becoming\nincreasingly severe. Jailbreaking attacks, as an important method for detecting\nvulnerabilities in LLMs, have been explored by researchers who attempt to\ninduce these models to generate harmful content through various attack methods.\nNevertheless, existing jailbreaking methods face numerous limitations, such as\nexcessive query counts, limited coverage of jailbreak modalities, low attack\nsuccess rates, and simplistic evaluation methods. To overcome these\nconstraints, this paper proposes a multimodal jailbreaking method: JMLLM. This\nmethod integrates multiple strategies to perform comprehensive jailbreak\nattacks across text, visual, and auditory modalities. Additionally, we\ncontribute a new and comprehensive dataset for multimodal jailbreaking\nresearch: TriJail, which includes jailbreak prompts for all three modalities.\nExperiments on the TriJail dataset and the benchmark dataset AdvBench,\nconducted on 13 popular LLMs, demonstrate advanced attack success rates and\nsignificant reduction in time overhead."
                },
                "authors": [
                    {
                        "name": "Yanxu Mao"
                    },
                    {
                        "name": "Peipei Liu"
                    },
                    {
                        "name": "Tiehan Cui"
                    },
                    {
                        "name": "Zhaoteng Yan"
                    },
                    {
                        "name": "Congying Liu"
                    },
                    {
                        "name": "Datao You"
                    }
                ],
                "author_detail": {
                    "name": "Datao You"
                },
                "author": "Datao You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03055v2",
                "updated": "2025-05-22T13:26:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    26,
                    41,
                    3,
                    142,
                    0
                ],
                "published": "2024-10-04T00:25:43Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    0,
                    25,
                    43,
                    4,
                    278,
                    0
                ],
                "title": "Permissive Information-Flow Analysis for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permissive Information-Flow Analysis for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are rapidly becoming commodity components of\nlarger software systems. This poses natural security and privacy problems:\npoisoned data retrieved from one component can change the model's behavior and\ncompromise the entire system, including coercing the model to spread\nconfidential data to untrusted components. One promising approach is to tackle\nthis problem at the system level via dynamic information flow (aka taint)\ntracking. Unfortunately, this approach of propagating the most restrictive\ninput label to the output is too conservative for applications where LLMs\noperate on inputs retrieved from diverse sources. In this paper, we propose a\nnovel, more permissive approach to propagate information flow labels through\nLLM queries. The key idea behind our approach is to propagate only the labels\nof the samples that were influential in generating the model output and to\neliminate the labels of unnecessary inputs. We implement and investigate the\neffectiveness of two variations of this approach, based on (i) prompt-based\nretrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We\ncompare these with a baseline that uses introspection to predict the output\nlabel. Our experimental results in an LLM agent setting show that the\npermissive label propagator improves over the baseline in more than 85% of the\ncases, which underscores the practicality of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are rapidly becoming commodity components of\nlarger software systems. This poses natural security and privacy problems:\npoisoned data retrieved from one component can change the model's behavior and\ncompromise the entire system, including coercing the model to spread\nconfidential data to untrusted components. One promising approach is to tackle\nthis problem at the system level via dynamic information flow (aka taint)\ntracking. Unfortunately, this approach of propagating the most restrictive\ninput label to the output is too conservative for applications where LLMs\noperate on inputs retrieved from diverse sources. In this paper, we propose a\nnovel, more permissive approach to propagate information flow labels through\nLLM queries. The key idea behind our approach is to propagate only the labels\nof the samples that were influential in generating the model output and to\neliminate the labels of unnecessary inputs. We implement and investigate the\neffectiveness of two variations of this approach, based on (i) prompt-based\nretrieval augmentation, and (ii) a $k$-nearest-neighbors language model. We\ncompare these with a baseline that uses introspection to predict the output\nlabel. Our experimental results in an LLM agent setting show that the\npermissive label propagator improves over the baseline in more than 85% of the\ncases, which underscores the practicality of our approach."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Radhika Gaonkar"
                    },
                    {
                        "name": "Boris Köpf"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Andrew Paverd"
                    },
                    {
                        "name": "Ahmed Salem"
                    },
                    {
                        "name": "Shruti Tople"
                    },
                    {
                        "name": "Lukas Wutschitz"
                    },
                    {
                        "name": "Menglin Xia"
                    },
                    {
                        "name": "Santiago Zanella-Béguelin"
                    }
                ],
                "author_detail": {
                    "name": "Santiago Zanella-Béguelin"
                },
                "author": "Santiago Zanella-Béguelin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16648v1",
                "updated": "2025-05-22T13:18:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    45,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:18:45Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    45,
                    3,
                    142,
                    0
                ],
                "title": "Collaboration among Multiple Large Language Models for Medical Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboration among Multiple Large Language Models for Medical Question\n  Answering"
                },
                "summary": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy."
                },
                "authors": [
                    {
                        "name": "Kexin Shang"
                    },
                    {
                        "name": "Chia-Hsuan Chang"
                    },
                    {
                        "name": "Christopher C. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Christopher C. Yang"
                },
                "author": "Christopher C. Yang",
                "arxiv_comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11764v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11764v4",
                "updated": "2025-05-22T13:18:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    40,
                    3,
                    142,
                    0
                ],
                "published": "2024-12-16T13:31:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    31,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Matters in Learning A Zero-Shot Sim-to-Real RL Policy for Quadrotor\n  Control? A Comprehensive Study"
                },
                "summary": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines. The policy derived by\nSimpleFlight consistently excels across both smooth polynominal trajectories\nand challenging infeasible zigzag trajectories on small thrust-to-weight\nquadrotors. In contrast, baseline methods struggle with high-speed or\ninfeasible trajectories. To support further research and reproducibility, we\nintegrate SimpleFlight into a GPU-based simulator Omnidrones and provide\nopen-source access to the code and model checkpoints. We hope SimpleFlight will\noffer valuable insights for advancing RL-based quadrotor control. For more\ndetails, visit our project website at\nhttps://sites.google.com/view/simpleflight/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Executing precise and agile flight maneuvers is critical for quadrotors in\nvarious applications. Traditional quadrotor control approaches are limited by\ntheir reliance on flat trajectories or time-consuming optimization, which\nrestricts their flexibility. Recently, RL-based policy has emerged as a\npromising alternative due to its ability to directly map observations to\nactions, reducing the need for detailed system knowledge and actuation\nconstraints. However, a significant challenge remains in bridging the\nsim-to-real gap, where RL-based policies often experience instability when\ndeployed in real world. In this paper, we investigate key factors for learning\nrobust RL-based control policies that are capable of zero-shot deployment in\nreal-world quadrotors. We identify five critical factors and we develop a\nPPO-based training framework named SimpleFlight, which integrates these five\ntechniques. We validate the efficacy of SimpleFlight on Crazyflie quadrotor,\ndemonstrating that it achieves more than a 50% reduction in trajectory tracking\nerror compared to state-of-the-art RL baselines. The policy derived by\nSimpleFlight consistently excels across both smooth polynominal trajectories\nand challenging infeasible zigzag trajectories on small thrust-to-weight\nquadrotors. In contrast, baseline methods struggle with high-speed or\ninfeasible trajectories. To support further research and reproducibility, we\nintegrate SimpleFlight into a GPU-based simulator Omnidrones and provide\nopen-source access to the code and model checkpoints. We hope SimpleFlight will\noffer valuable insights for advancing RL-based quadrotor control. For more\ndetails, visit our project website at\nhttps://sites.google.com/view/simpleflight/."
                },
                "authors": [
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yuqing Xie"
                    },
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Yinuo Chen"
                    },
                    {
                        "name": "Shu'ang Yu"
                    },
                    {
                        "name": "Wenhao Tang"
                    },
                    {
                        "name": "Shilong Ji"
                    },
                    {
                        "name": "Mo Mu"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "The first two authors contribute equally; Accepted by RA-L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11764v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11764v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16008v2",
                "updated": "2025-05-22T13:18:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    38,
                    3,
                    142,
                    0
                ],
                "published": "2024-12-20T15:56:09Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    56,
                    9,
                    4,
                    355,
                    0
                ],
                "title": "Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Aerial Spoofing Attacks to LEO Satellite Systems via Deep\n  Learning"
                },
                "summary": "Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a\ncornerstone to assessing the authenticity of the received information and\nguaranteeing robust service delivery in several application domains. The\nsolutions available today for spoofing detection either rely on additional\ncommunication systems, receivers, and antennas, or require mobile deployments.\nDetection systems working at the Physical (PHY) layer of the satellite\ncommunication link also require time-consuming and energy-hungry training\nprocesses on all satellites of the constellation, and rely on the availability\nof spoofed data, which are often challenging to collect. Moreover, none of such\ncontributions investigate the feasibility of aerial spoofing attacks launched\nvia drones operating at various altitudes. In this paper, we propose a new\nspoofing detection technique for LEO satellite constellation systems, applying\nanomaly detection on the received PHY signal via autoencoders. We validate our\nsolution through an extensive measurement campaign involving the deployment of\nan actual spoofer (Software-Defined Radio) installed on a drone and injecting\nrogue IRIDIUM messages while flying at different altitudes with various\nmovement patterns. Our results demonstrate that the proposed technique can\nreliably detect LEO spoofing attacks launched at different altitudes, while\nstate-of-the-art competing approaches simply fail. We also release the\ncollected data as open source, fostering further research on satellite\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting spoofing attacks to Low-Earth-Orbit (LEO) satellite systems is a\ncornerstone to assessing the authenticity of the received information and\nguaranteeing robust service delivery in several application domains. The\nsolutions available today for spoofing detection either rely on additional\ncommunication systems, receivers, and antennas, or require mobile deployments.\nDetection systems working at the Physical (PHY) layer of the satellite\ncommunication link also require time-consuming and energy-hungry training\nprocesses on all satellites of the constellation, and rely on the availability\nof spoofed data, which are often challenging to collect. Moreover, none of such\ncontributions investigate the feasibility of aerial spoofing attacks launched\nvia drones operating at various altitudes. In this paper, we propose a new\nspoofing detection technique for LEO satellite constellation systems, applying\nanomaly detection on the received PHY signal via autoencoders. We validate our\nsolution through an extensive measurement campaign involving the deployment of\nan actual spoofer (Software-Defined Radio) installed on a drone and injecting\nrogue IRIDIUM messages while flying at different altitudes with various\nmovement patterns. Our results demonstrate that the proposed technique can\nreliably detect LEO spoofing attacks launched at different altitudes, while\nstate-of-the-art competing approaches simply fail. We also release the\ncollected data as open source, fostering further research on satellite\nsecurity."
                },
                "authors": [
                    {
                        "name": "Jos Wigchert"
                    },
                    {
                        "name": "Savio Sciancalepore"
                    },
                    {
                        "name": "Gabriele Oligeri"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Oligeri"
                },
                "author": "Gabriele Oligeri",
                "arxiv_comment": "Accepted for Publication by Elsevier Computer Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16646v1",
                "updated": "2025-05-22T13:18:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    24,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:18:24Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    24,
                    3,
                    142,
                    0
                ],
                "title": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment\n  for LLMs' Mathematical Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Self-Generating and Self-Validating Multi-Dimensional Assessment\n  for LLMs' Mathematical Problem Solving"
                },
                "summary": "Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have achieved remarkable results on a variety of\nmathematical benchmarks. However, concerns remain as to whether these successes\nreflect genuine mathematical reasoning or superficial pattern recognition.\nCommon evaluation metrics, such as final answer accuracy, fail to disentangle\nthe underlying competencies involved, offering limited diagnostic value. To\naddress these limitations, we introduce SMART: a Self-Generating and\nSelf-Validating Multi-Dimensional Assessment Framework. SMART decomposes\nmathematical problem solving into four distinct dimensions: understanding,\nreasoning, arithmetic, and reflection \\& refinement. Each dimension is\nevaluated independently through tailored tasks, enabling interpretable and\nfine-grained analysis of LLM behavior. Crucially, SMART integrates an automated\nself-generating and self-validating mechanism to produce and verify benchmark\ndata, ensuring both scalability and reliability. We apply SMART to 21\nstate-of-the-art open- and closed-source LLMs, uncovering significant\ndiscrepancies in their abilities across different dimensions. Our findings\ndemonstrate the inadequacy of final answer accuracy as a sole metric and\nmotivate a new holistic metric to better capture true problem-solving\ncapabilities. Code and benchmarks will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Yujie Hou"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Mei Wang"
                    },
                    {
                        "name": "Xuetao Ma"
                    },
                    {
                        "name": "Hu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hu Huang"
                },
                "author": "Hu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13865v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13865v3",
                "updated": "2025-05-22T13:18:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    18,
                    14,
                    3,
                    142,
                    0
                ],
                "published": "2024-11-21T06:01:47Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    1,
                    47,
                    3,
                    326,
                    0
                ],
                "title": "Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for\n  Exploration and Exploitation in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for\n  Exploration and Exploitation in Recommender Systems"
                },
                "summary": "Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a semantic-enhanced\nhierarchical mechanism that aligns rich textual descriptions processed by large\nlanguage models (LLMs) with collaborative information directly in hyperbolic\nspace, allowing for more nuanced updates that respect the underlying\nhierarchical structure in user-item profiles; (2) an automatic hierarchical\nrepresentation by optimizing Dasgupta's cost, which discovers hierarchical\nstructures without requiring predefined hyperparameters, enabling\nuser-adjustable exploration-exploitation trade-offs. Extensive experiments\ndemonstrate that HERec consistently outperforms both Euclidean and hyperbolic\nbaselines, achieving up to 5.49% improvement in utility metrics and 11.39%\nincrease in diversity metrics, effectively mitigating information cocoons. We\nopen-source our model implementation at https://github.com/Martin-qyma/HERec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a semantic-enhanced\nhierarchical mechanism that aligns rich textual descriptions processed by large\nlanguage models (LLMs) with collaborative information directly in hyperbolic\nspace, allowing for more nuanced updates that respect the underlying\nhierarchical structure in user-item profiles; (2) an automatic hierarchical\nrepresentation by optimizing Dasgupta's cost, which discovers hierarchical\nstructures without requiring predefined hyperparameters, enabling\nuser-adjustable exploration-exploitation trade-offs. Extensive experiments\ndemonstrate that HERec consistently outperforms both Euclidean and hyperbolic\nbaselines, achieving up to 5.49% improvement in utility metrics and 11.39%\nincrease in diversity metrics, effectively mitigating information cocoons. We\nopen-source our model implementation at https://github.com/Martin-qyma/HERec."
                },
                "authors": [
                    {
                        "name": "Qiyao Ma"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Mingxuan Ju"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Neil Shah"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13865v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13865v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16643v1",
                "updated": "2025-05-22T13:16:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    16,
                    53,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    16,
                    53,
                    3,
                    142,
                    0
                ],
                "title": "From Evaluation to Defense: Advancing Safety in Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Evaluation to Defense: Advancing Safety in Video Large Language\n  Models"
                },
                "summary": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}"
                },
                "authors": [
                    {
                        "name": "Yiwei Sun"
                    },
                    {
                        "name": "Peiqi Jiang"
                    },
                    {
                        "name": "Chuanbin Liu"
                    },
                    {
                        "name": "Luohao Lin"
                    },
                    {
                        "name": "Zhiying Lu"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "arxiv_comment": "49 pages, 12 figures, 17 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02172v2",
                "updated": "2025-05-22T13:14:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    14,
                    15,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-04T16:24:12Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    16,
                    24,
                    12,
                    6,
                    124,
                    0
                ],
                "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance,\n  Scale, and Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance,\n  Scale, and Memorization"
                },
                "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks."
                },
                "authors": [
                    {
                        "name": "Chuck Arvin"
                    }
                ],
                "author_detail": {
                    "name": "Chuck Arvin"
                },
                "author": "Chuck Arvin",
                "arxiv_comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16640v1",
                "updated": "2025-05-22T13:12:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    12,
                    46,
                    3,
                    142,
                    0
                ],
                "published": "2025-05-22T13:12:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    13,
                    12,
                    46,
                    3,
                    142,
                    0
                ],
                "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via\n  Objective-Decoupled Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via\n  Objective-Decoupled Optimization"
                },
                "summary": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have advanced robotic control by enabling\nend-to-end decision-making directly from multimodal inputs. However, their\ntightly coupled architectures expose novel security vulnerabilities. Unlike\ntraditional adversarial perturbations, backdoor attacks represent a stealthier,\npersistent, and practically significant threat-particularly under the emerging\nTraining-as-a-Service paradigm-but remain largely unexplored in the context of\nVLA models. To address this gap, we propose BadVLA, a backdoor attack method\nbased on Objective-Decoupled Optimization, which for the first time exposes the\nbackdoor vulnerabilities of VLA models. Specifically, it consists of a\ntwo-stage process: (1) explicit feature-space separation to isolate trigger\nrepresentations from benign inputs, and (2) conditional control deviations that\nactivate only in the presence of the trigger, while preserving clean-task\nperformance. Empirical results on multiple VLA benchmarks demonstrate that\nBadVLA consistently achieves near-100% attack success rates with minimal impact\non clean task accuracy. Further analyses confirm its robustness against common\ninput perturbations, task transfers, and model fine-tuning, underscoring\ncritical security vulnerabilities in current VLA deployments. Our work offers\nthe first systematic investigation of backdoor vulnerabilities in VLA models,\nhighlighting an urgent need for secure and trustworthy embodied model design\npractices. We have released the project page at\nhttps://badvla-project.github.io/."
                },
                "authors": [
                    {
                        "name": "Xueyang Zhou"
                    },
                    {
                        "name": "Guiyao Tie"
                    },
                    {
                        "name": "Guowen Zhang"
                    },
                    {
                        "name": "Hechang Wang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "19 pages, 12 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]