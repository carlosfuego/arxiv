[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v3",
                "updated": "2025-06-22T05:23:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    5,
                    23,
                    9,
                    6,
                    173,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hoßfeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v2",
                "updated": "2025-06-20T16:59:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    59,
                    5,
                    4,
                    171,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Suraj Kothawade"
                    },
                    {
                        "name": "Pacal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pacal Poupart"
                },
                "author": "Pacal Poupart",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v3",
                "updated": "2025-06-20T12:59:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    12,
                    59,
                    40,
                    4,
                    171,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thévenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thévenet"
                },
                "author": "M. Thévenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v1",
                "updated": "2025-06-18T17:59:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rúben Adão"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "João Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v1",
                "updated": "2025-06-14T13:16:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07703v1",
                "updated": "2025-06-09T12:41:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T12:41:31Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "title": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion"
                },
                "summary": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices."
                },
                "authors": [
                    {
                        "name": "Junwen Lai"
                    },
                    {
                        "name": "Tianye Yu"
                    },
                    {
                        "name": "Peitao Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Guozhong Xing"
                    },
                    {
                        "name": "Xing-Qiu Chen"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v1",
                "updated": "2025-06-09T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v2",
                "updated": "2025-06-09T09:48:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07533v1",
                "updated": "2025-06-09T08:16:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T08:16:24Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts"
                },
                "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Haocheng Lu"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v2",
                "updated": "2025-06-09T07:58:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    58,
                    19,
                    0,
                    160,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "The paper needs major modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07334v1",
                "updated": "2025-06-09T00:30:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T00:30:08Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models"
                },
                "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07311v1",
                "updated": "2025-06-08T22:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T22:59:20Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference"
                },
                "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment."
                },
                "authors": [
                    {
                        "name": "Thomas Joshi"
                    },
                    {
                        "name": "Herman Saini"
                    },
                    {
                        "name": "Neil Dhillon"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar El Maghraoui"
                },
                "author": "Kaoutar El Maghraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v3",
                "updated": "2025-06-08T21:23:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    21,
                    23,
                    22,
                    6,
                    159,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v3",
                "updated": "2025-06-08T20:04:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    20,
                    4,
                    17,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.08508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.08508v2",
                "updated": "2025-06-08T16:07:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    7,
                    44,
                    6,
                    159,
                    0
                ],
                "published": "2022-10-16T11:21:26Z",
                "published_parsed": [
                    2022,
                    10,
                    16,
                    11,
                    21,
                    26,
                    6,
                    289,
                    0
                ],
                "title": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory"
                },
                "summary": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions."
                },
                "authors": [
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Geraldo F. Oliveira"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Rachata Ausavarungnirun"
                    },
                    {
                        "name": "Juan Gómez Luna"
                    },
                    {
                        "name": "João Ferreira"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Nandita Vijaykumar"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.08508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.08508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v5",
                "updated": "2025-06-08T16:04:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    4,
                    59,
                    6,
                    159,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.18899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18899v1",
                "updated": "2025-06-23T17:59:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:59:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    16,
                    0,
                    174,
                    0
                ],
                "title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated\n  Film Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated\n  Film Generation"
                },
                "summary": "AI-driven content creation has shown potential in film production. However,\nexisting film generation systems struggle to implement cinematic principles and\nthus fail to generate professional-quality films, particularly lacking diverse\ncamera language and cinematic rhythm. This results in templated visuals and\nunengaging narratives. To address this, we introduce FilMaster, an end-to-end\nAI system that integrates real-world cinematic principles for\nprofessional-grade film generation, yielding editable, industry-standard\noutputs. FilMaster is built on two key principles: (1) learning cinematography\nfrom extensive real-world film data and (2) emulating professional,\naudience-centric post-production workflows. Inspired by these principles,\nFilMaster incorporates two stages: a Reference-Guided Generation Stage which\ntransforms user input to video clips, and a Generative Post-Production Stage\nwhich transforms raw footage into audiovisual outputs by orchestrating visual\nand auditory elements for cinematic rhythm. Our generation stage highlights a\nMulti-shot Synergized RAG Camera Language Design module to guide the AI in\ngenerating professional camera language by retrieving reference clips from a\nvast corpus of 440,000 film clips. Our post-production stage emulates\nprofessional workflows by designing an Audience-Centric Cinematic Rhythm\nControl module, including Rough Cut and Fine Cut processes informed by\nsimulated audience feedback, for effective integration of audiovisual elements\nto achieve engaging content. The system is empowered by generative AI models\nlike (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a\ncomprehensive benchmark for evaluating AI-generated films. Extensive\nexperiments show FilMaster's superior performance in camera language design and\ncinematic rhythm control, advancing generative AI in professional filmmaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven content creation has shown potential in film production. However,\nexisting film generation systems struggle to implement cinematic principles and\nthus fail to generate professional-quality films, particularly lacking diverse\ncamera language and cinematic rhythm. This results in templated visuals and\nunengaging narratives. To address this, we introduce FilMaster, an end-to-end\nAI system that integrates real-world cinematic principles for\nprofessional-grade film generation, yielding editable, industry-standard\noutputs. FilMaster is built on two key principles: (1) learning cinematography\nfrom extensive real-world film data and (2) emulating professional,\naudience-centric post-production workflows. Inspired by these principles,\nFilMaster incorporates two stages: a Reference-Guided Generation Stage which\ntransforms user input to video clips, and a Generative Post-Production Stage\nwhich transforms raw footage into audiovisual outputs by orchestrating visual\nand auditory elements for cinematic rhythm. Our generation stage highlights a\nMulti-shot Synergized RAG Camera Language Design module to guide the AI in\ngenerating professional camera language by retrieving reference clips from a\nvast corpus of 440,000 film clips. Our post-production stage emulates\nprofessional workflows by designing an Audience-Centric Cinematic Rhythm\nControl module, including Rough Cut and Fine Cut processes informed by\nsimulated audience feedback, for effective integration of audiovisual elements\nto achieve engaging content. The system is empowered by generative AI models\nlike (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a\ncomprehensive benchmark for evaluating AI-generated films. Extensive\nexperiments show FilMaster's superior performance in camera language design and\ncinematic rhythm control, advancing generative AI in professional filmmaking."
                },
                "authors": [
                    {
                        "name": "Kaiyi Huang"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Zinan Lin"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project Page: https://filmaster-ai.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18898v1",
                "updated": "2025-06-23T17:59:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    14,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    14,
                    0,
                    174,
                    0
                ],
                "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations"
                },
                "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com"
                },
                "authors": [
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project page: https://tar.csuhan.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18896v1",
                "updated": "2025-06-23T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs"
                },
                "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux"
                },
                "authors": [
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Jingwen Gu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18887v1",
                "updated": "2025-06-23T17:56:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    56,
                    34,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:56:34Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    56,
                    34,
                    0,
                    174,
                    0
                ],
                "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Conceptual Bias via Transformer Latent-Subspace Activation"
                },
                "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems."
                },
                "authors": [
                    {
                        "name": "Vansh Sharma"
                    },
                    {
                        "name": "Venkat Raman"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Raman"
                },
                "author": "Venkat Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.1; D.3.3; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05213v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05213v3",
                "updated": "2025-06-23T17:54:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    54,
                    20,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-06T17:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    43,
                    51,
                    4,
                    341,
                    0
                ],
                "title": "Probing neutrino mass ordering with supernova neutrinos at NO$ν$A\n  including the effect of sterile neutrinos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing neutrino mass ordering with supernova neutrinos at NO$ν$A\n  including the effect of sterile neutrinos"
                },
                "summary": "In this work, we explore the possibility of probing the mass ordering\nsensitivity as a function of supernova distance in the context of the ongoing\nneutrino experiment NO$\\nu$A. We provide a detailed study of the active-active\nand active-sterile mixing frameworks, illustrating how supernova neutrinos can\nbe used to realize the existence of sterile neutrinos. Interestingly, we infer\nthat observation of the NC channel alone can differentiate between the presence\nand absence of sterile neutrinos. Our results indicate that the primary channel\nof NO$\\nu$A can distinguish normal mass ordering from inverted mass ordering at\n$5 \\sigma$ confidence level for a supernova explosion occurring at a distance\nof 5 kpc. Additionally, we examine the impact of systematic uncertainties on\nmass ordering sensitivity, showing that higher levels of systematics lead to a\nreduction in sensitivity. Similarly, the inclusion of energy smearing\nsignificantly diminishes ordering sensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the possibility of probing the mass ordering\nsensitivity as a function of supernova distance in the context of the ongoing\nneutrino experiment NO$\\nu$A. We provide a detailed study of the active-active\nand active-sterile mixing frameworks, illustrating how supernova neutrinos can\nbe used to realize the existence of sterile neutrinos. Interestingly, we infer\nthat observation of the NC channel alone can differentiate between the presence\nand absence of sterile neutrinos. Our results indicate that the primary channel\nof NO$\\nu$A can distinguish normal mass ordering from inverted mass ordering at\n$5 \\sigma$ confidence level for a supernova explosion occurring at a distance\nof 5 kpc. Additionally, we examine the impact of systematic uncertainties on\nmass ordering sensitivity, showing that higher levels of systematics lead to a\nreduction in sensitivity. Similarly, the inclusion of energy smearing\nsignificantly diminishes ordering sensitivity."
                },
                "authors": [
                    {
                        "name": "Papia Panda"
                    },
                    {
                        "name": "Rukmani Mohanta"
                    }
                ],
                "author_detail": {
                    "name": "Rukmani Mohanta"
                },
                "author": "Rukmani Mohanta",
                "arxiv_comment": "26 pages, 8 figures, 3 tables (one table in Appendix), Accepted for\n  publication in Nuclear Physics B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05213v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05213v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18881v1",
                "updated": "2025-06-23T17:52:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    52,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:52:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    52,
                    16,
                    0,
                    174,
                    0
                ],
                "title": "Let Your Video Listen to Your Music!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let Your Video Listen to Your Music!"
                },
                "summary": "Aligning the rhythm of visual motion in a video with a given music track is a\npractical need in multimedia production, yet remains an underexplored task in\nautonomous video editing. Effective alignment between motion and musical beats\nenhances viewer engagement and visual appeal, particularly in music videos,\npromotional content, and cinematic editing. Existing methods typically depend\non labor-intensive manual cutting, speed adjustments, or heuristic-based\nediting techniques to achieve synchronization. While some generative models\nhandle joint video and music generation, they often entangle the two\nmodalities, limiting flexibility in aligning video to music beats while\npreserving the full visual content. In this paper, we propose a novel and\nefficient framework, termed MVAA (Music-Video Auto-Alignment), that\nautomatically edits video to align with the rhythm of a given music track while\npreserving the original visual content. To enhance flexibility, we modularize\nthe task into a two-step process in our MVAA: aligning motion keyframes with\naudio beats, followed by rhythm-aware video inpainting. Specifically, we first\ninsert keyframes at timestamps aligned with musical beats, then use a\nframe-conditioned diffusion model to generate coherent intermediate frames,\npreserving the original video's semantic content. Since comprehensive test-time\ntraining can be time-consuming, we adopt a two-stage strategy: pretraining the\ninpainting module on a small video set to learn general motion priors, followed\nby rapid inference-time fine-tuning for video-specific adaptation. This hybrid\napproach enables adaptation within 10 minutes with one epoch on a single NVIDIA\n4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show\nthat our approach can achieve high-quality beat alignment and visual\nsmoothness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the rhythm of visual motion in a video with a given music track is a\npractical need in multimedia production, yet remains an underexplored task in\nautonomous video editing. Effective alignment between motion and musical beats\nenhances viewer engagement and visual appeal, particularly in music videos,\npromotional content, and cinematic editing. Existing methods typically depend\non labor-intensive manual cutting, speed adjustments, or heuristic-based\nediting techniques to achieve synchronization. While some generative models\nhandle joint video and music generation, they often entangle the two\nmodalities, limiting flexibility in aligning video to music beats while\npreserving the full visual content. In this paper, we propose a novel and\nefficient framework, termed MVAA (Music-Video Auto-Alignment), that\nautomatically edits video to align with the rhythm of a given music track while\npreserving the original visual content. To enhance flexibility, we modularize\nthe task into a two-step process in our MVAA: aligning motion keyframes with\naudio beats, followed by rhythm-aware video inpainting. Specifically, we first\ninsert keyframes at timestamps aligned with musical beats, then use a\nframe-conditioned diffusion model to generate coherent intermediate frames,\npreserving the original video's semantic content. Since comprehensive test-time\ntraining can be time-consuming, we adopt a two-stage strategy: pretraining the\ninpainting module on a small video set to learn general motion priors, followed\nby rapid inference-time fine-tuning for video-specific adaptation. This hybrid\napproach enables adaptation within 10 minutes with one epoch on a single NVIDIA\n4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show\nthat our approach can achieve high-quality beat alignment and visual\nsmoothness."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Dong Gong"
                    },
                    {
                        "name": "Zicheng Duan"
                    },
                    {
                        "name": "Anton van den Hengel"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "arxiv_comment": "project page: https://zhangxinyu-xyz.github.io/MVAA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18880v1",
                "updated": "2025-06-23T17:51:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    51,
                    40,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:51:40Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    51,
                    40,
                    0,
                    174,
                    0
                ],
                "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,\n  Compositional, and Transformative Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,\n  Compositional, and Transformative Generalization"
                },
                "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency."
                },
                "authors": [
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Shawn Hu"
                    },
                    {
                        "name": "Georgia Zhou"
                    },
                    {
                        "name": "Ken Zheng"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23407v2",
                "updated": "2025-06-23T17:48:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    48,
                    35,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-30T19:19:47Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    19,
                    19,
                    47,
                    2,
                    304,
                    0
                ],
                "title": "Direct nonparametric multimessenger constraints on the equation of state\n  of cold dense nuclear matter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct nonparametric multimessenger constraints on the equation of state\n  of cold dense nuclear matter"
                },
                "summary": "We utilize the now substantial amount of astrophysical observations of\nneutron stars (NSs), along with perturbative quantum chromodynamics (pQCD)\ncalculations at high density, to directly constrain the NS equation of state\n(EOS). To this end, we construct nonparametric EOS priors by using Gaussian\nprocesses trained on 75 EOSs, which include models with either hadrons,\nhyperons, or quarks at high densities. We create a prior using the full EOS\nsample (model agnostic), and one prior for each EOS family to test model\ndiscrimination. We introduce a novel inference approach, which allows the\nsimultaneous sampling of intrinsic and extrinsic parameters of binary NS\nmergers, as well as a nonparametric equation of state. We showcase this method\nin a Bayesian updating scheme by first performing a complete analysis of the\nbinary NS merger event GW170817 with minimal assumptions, and sequentially\nadding information from x-ray and radio NS observations, along with pQCD\ncalculations. Besides providing standard constraints, such as the pressure at\ntwice nuclear saturation density $p(2\\rho_\\text{sat})=4.3^{+0.6}_{-0.6}\\,\\times\n10^{34}\\text{dyne/cm}^{2}$, at $95\\%$ confidence level, for the model agnostic\nprior, our methodology shows how the choice of EOS families used in\nconditioning changes the inferred astrophysical properties of the EOS, namely\ntidal deformability and maximum supported NS mass. We find hyperonic priors\npredicting higher tidal deformabilities for a $1.4M_\\odot$ NS, and hadronic\npriors being preferred by the considered astrophysical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We utilize the now substantial amount of astrophysical observations of\nneutron stars (NSs), along with perturbative quantum chromodynamics (pQCD)\ncalculations at high density, to directly constrain the NS equation of state\n(EOS). To this end, we construct nonparametric EOS priors by using Gaussian\nprocesses trained on 75 EOSs, which include models with either hadrons,\nhyperons, or quarks at high densities. We create a prior using the full EOS\nsample (model agnostic), and one prior for each EOS family to test model\ndiscrimination. We introduce a novel inference approach, which allows the\nsimultaneous sampling of intrinsic and extrinsic parameters of binary NS\nmergers, as well as a nonparametric equation of state. We showcase this method\nin a Bayesian updating scheme by first performing a complete analysis of the\nbinary NS merger event GW170817 with minimal assumptions, and sequentially\nadding information from x-ray and radio NS observations, along with pQCD\ncalculations. Besides providing standard constraints, such as the pressure at\ntwice nuclear saturation density $p(2\\rho_\\text{sat})=4.3^{+0.6}_{-0.6}\\,\\times\n10^{34}\\text{dyne/cm}^{2}$, at $95\\%$ confidence level, for the model agnostic\nprior, our methodology shows how the choice of EOS families used in\nconditioning changes the inferred astrophysical properties of the EOS, namely\ntidal deformability and maximum supported NS mass. We find hyperonic priors\npredicting higher tidal deformabilities for a $1.4M_\\odot$ NS, and hadronic\npriors being preferred by the considered astrophysical data."
                },
                "authors": [
                    {
                        "name": "Iuliu Cuceu"
                    },
                    {
                        "name": "Sandra Robles"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Robles"
                },
                "author": "Sandra Robles",
                "arxiv_doi": "10.1103/6d9c-c4kx",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/6d9c-c4kx",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 12 figures, 4 tables, 5 appendices. Discussion extended,\n  matches published version",
                "arxiv_journal_ref": "Phys.Rev.D 111 (2025) 12, 123029",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18876v1",
                "updated": "2025-06-23T17:48:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    48,
                    23,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    48,
                    23,
                    0,
                    174,
                    0
                ],
                "title": "A field-level reaction for screened modified gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A field-level reaction for screened modified gravity"
                },
                "summary": "We present a field-level reaction framework to emulate the nonlinear effects\nof screened modified gravity on the cosmic web. This approach is designed to\nenable field-level inference with data from Stage IV cosmological surveys.\nBuilding on the reaction method, which models the nonlinear matter power\nspectrum in modified gravity as corrections to a \"pseudo\" $\\Lambda$CDM\ncosmology, we extend the method to full field-level predictions by applying it\nto the output of $N$-body simulations, including both positions and velocities.\nWe focus on modifications to gravity that are scale-independent at the linear\nlevel, allowing us to isolate and emulate nonlinear deviations, particularly\nscreening effects. Our neural network predicts the field-level correction\n(\"reaction\") to a pseudo$\\Lambda$CDM simulation whose linear clustering matches\nthat of the target. The emulator achieves sub-percent accuracy across a broad\nrange of summary statistics, including 0.4\\% agreement in the matter power\nspectrum at scales $k < 1$ Mpc$/h$, and 2\\% accuracy in redshift-space\ndistortion multipoles at $k < 0.3$ Mpc$/h$. We also validate the emulator\nagainst $N$-body simulations with increased force resolution and time steps,\nconfirming the robustness of its performance. These results demonstrate that\nour framework is a practical and reliable tool for incorporating screened\nmodified gravity models into field-level cosmological inference, enabling\nstringent tests of extra fundamental forces at cosmological scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a field-level reaction framework to emulate the nonlinear effects\nof screened modified gravity on the cosmic web. This approach is designed to\nenable field-level inference with data from Stage IV cosmological surveys.\nBuilding on the reaction method, which models the nonlinear matter power\nspectrum in modified gravity as corrections to a \"pseudo\" $\\Lambda$CDM\ncosmology, we extend the method to full field-level predictions by applying it\nto the output of $N$-body simulations, including both positions and velocities.\nWe focus on modifications to gravity that are scale-independent at the linear\nlevel, allowing us to isolate and emulate nonlinear deviations, particularly\nscreening effects. Our neural network predicts the field-level correction\n(\"reaction\") to a pseudo$\\Lambda$CDM simulation whose linear clustering matches\nthat of the target. The emulator achieves sub-percent accuracy across a broad\nrange of summary statistics, including 0.4\\% agreement in the matter power\nspectrum at scales $k < 1$ Mpc$/h$, and 2\\% accuracy in redshift-space\ndistortion multipoles at $k < 0.3$ Mpc$/h$. We also validate the emulator\nagainst $N$-body simulations with increased force resolution and time steps,\nconfirming the robustness of its performance. These results demonstrate that\nour framework is a practical and reliable tool for incorporating screened\nmodified gravity models into field-level cosmological inference, enabling\nstringent tests of extra fundamental forces at cosmological scales."
                },
                "authors": [
                    {
                        "name": "Daniela Saadeh"
                    },
                    {
                        "name": "Kazuya Koyama"
                    },
                    {
                        "name": "Xan Morice-Atkinson"
                    }
                ],
                "author_detail": {
                    "name": "Xan Morice-Atkinson"
                },
                "author": "Xan Morice-Atkinson",
                "arxiv_comment": "13 pages, 9 figures, to be submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09886v4",
                "updated": "2025-06-23T17:43:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    43,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2024-08-19T11:01:00Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    1,
                    0,
                    0,
                    232,
                    0
                ],
                "title": "Improved Baselines with Synchronized Encoding for Universal Medical\n  Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Baselines with Synchronized Encoding for Universal Medical\n  Image Segmentation"
                },
                "summary": "Large foundation models, known for their strong zero-shot generalization\ncapabilities, can be applied to a wide range of downstream tasks. However,\ndeveloping foundation models for medical image segmentation poses a significant\nchallenge due to the domain gap between natural and medical images. While\nfine-tuning techniques based on the Segment Anything Model (SAM) have been\nexplored, they primarily focus on scaling up data or refining inference\nstrategies without incorporating domain-specific architectural designs,\nlimiting their zero-shot performance. To optimize segmentation performance\nunder standard inference settings and provide a strong baseline for future\nresearch, we introduce SyncSAM, which employs a synchronized dual-branch\nencoder that integrates convolution and Transformer features in a synchronized\nmanner to enhance medical image encoding, and a multi-scale dual-branch decoder\nto preserve image details. SyncSAM is trained on two of the largest medical\nimage segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series\nof pre-trained models for universal medical image segmentation. Experimental\nresults demonstrate that SyncSAM not only achieves state-of-the-art performance\non test sets but also exhibits strong zero-shot capabilities on unseen\ndatasets. Code and checkpoints are available at\nhttps://github.com/Hhankyangg/SyncSAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models, known for their strong zero-shot generalization\ncapabilities, can be applied to a wide range of downstream tasks. However,\ndeveloping foundation models for medical image segmentation poses a significant\nchallenge due to the domain gap between natural and medical images. While\nfine-tuning techniques based on the Segment Anything Model (SAM) have been\nexplored, they primarily focus on scaling up data or refining inference\nstrategies without incorporating domain-specific architectural designs,\nlimiting their zero-shot performance. To optimize segmentation performance\nunder standard inference settings and provide a strong baseline for future\nresearch, we introduce SyncSAM, which employs a synchronized dual-branch\nencoder that integrates convolution and Transformer features in a synchronized\nmanner to enhance medical image encoding, and a multi-scale dual-branch decoder\nto preserve image details. SyncSAM is trained on two of the largest medical\nimage segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series\nof pre-trained models for universal medical image segmentation. Experimental\nresults demonstrate that SyncSAM not only achieves state-of-the-art performance\non test sets but also exhibits strong zero-shot capabilities on unseen\ndatasets. Code and checkpoints are available at\nhttps://github.com/Hhankyangg/SyncSAM."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Jiadong Feng"
                    },
                    {
                        "name": "Xuande Mi"
                    },
                    {
                        "name": "Haixia Bi"
                    },
                    {
                        "name": "Hai Zhang"
                    },
                    {
                        "name": "Jian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jian Sun"
                },
                "author": "Jian Sun",
                "arxiv_journal_ref": "MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07584v3",
                "updated": "2025-06-24T10:10:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    10,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-10T17:48:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    48,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Talking to GDELT Through Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking to GDELT Through Knowledge Graphs"
                },
                "summary": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end."
                },
                "authors": [
                    {
                        "name": "Audun Myers"
                    },
                    {
                        "name": "Max Vargas"
                    },
                    {
                        "name": "Sinan G. Aksoy"
                    },
                    {
                        "name": "Cliff Joslyn"
                    },
                    {
                        "name": "Benjamin Wilson"
                    },
                    {
                        "name": "Lee Burke"
                    },
                    {
                        "name": "Tom Grimes"
                    }
                ],
                "author_detail": {
                    "name": "Tom Grimes"
                },
                "author": "Tom Grimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18870v1",
                "updated": "2025-06-23T17:38:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    38,
                    48,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:38:48Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    38,
                    48,
                    0,
                    174,
                    0
                ],
                "title": "Amplifying Machine Learning Attacks Through Strategic Compositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Machine Learning Attacks Through Strategic Compositions"
                },
                "summary": "Machine learning (ML) models are proving to be vulnerable to a variety of\nattacks that allow the adversary to learn sensitive information, cause\nmispredictions, and more. While these attacks have been extensively studied,\ncurrent research predominantly focuses on analyzing each attack type\nindividually. In practice, however, adversaries may employ multiple attack\nstrategies simultaneously rather than relying on a single approach. This\nprompts a crucial yet underexplored question: When the adversary has multiple\nattacks at their disposal, are they able to mount or amplify the effect of one\nattack with another? In this paper, we take the first step in studying the\nstrategic interactions among different attacks, which we define as attack\ncompositions. Specifically, we focus on four well-studied attacks during the\nmodel's inference phase: adversarial examples, attribute inference, membership\ninference, and property inference. To facilitate the study of their\ninteractions, we propose a taxonomy based on three stages of the attack\npipeline: preparation, execution, and evaluation. Using this taxonomy, we\nidentify four effective attack compositions, such as property inference\nassisting attribute inference at its preparation level and adversarial examples\nassisting property inference at its execution level. We conduct extensive\nexperiments on the attack compositions using three ML model architectures and\nthree benchmark image datasets. Empirical results demonstrate the effectiveness\nof these four attack compositions. We implement and release a modular reusable\ntoolkit, COAT. Arguably, our work serves as a call for researchers and\npractitioners to consider advanced adversarial settings involving multiple\nattack strategies, aiming to strengthen the security and robustness of AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are proving to be vulnerable to a variety of\nattacks that allow the adversary to learn sensitive information, cause\nmispredictions, and more. While these attacks have been extensively studied,\ncurrent research predominantly focuses on analyzing each attack type\nindividually. In practice, however, adversaries may employ multiple attack\nstrategies simultaneously rather than relying on a single approach. This\nprompts a crucial yet underexplored question: When the adversary has multiple\nattacks at their disposal, are they able to mount or amplify the effect of one\nattack with another? In this paper, we take the first step in studying the\nstrategic interactions among different attacks, which we define as attack\ncompositions. Specifically, we focus on four well-studied attacks during the\nmodel's inference phase: adversarial examples, attribute inference, membership\ninference, and property inference. To facilitate the study of their\ninteractions, we propose a taxonomy based on three stages of the attack\npipeline: preparation, execution, and evaluation. Using this taxonomy, we\nidentify four effective attack compositions, such as property inference\nassisting attribute inference at its preparation level and adversarial examples\nassisting property inference at its execution level. We conduct extensive\nexperiments on the attack compositions using three ML model architectures and\nthree benchmark image datasets. Empirical results demonstrate the effectiveness\nof these four attack compositions. We implement and release a modular reusable\ntoolkit, COAT. Arguably, our work serves as a call for researchers and\npractitioners to consider advanced adversarial settings involving multiple\nattack strategies, aiming to strengthen the security and robustness of AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Yugeng Liu"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Hai Huang"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhang"
                },
                "author": "Yang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12858v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12858v3",
                "updated": "2025-06-23T17:31:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    31,
                    25,
                    0,
                    174,
                    0
                ],
                "published": "2024-11-19T21:02:09Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    2,
                    9,
                    1,
                    324,
                    0
                ],
                "title": "CDI: Copyrighted Data Identification in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDI: Copyrighted Data Identification in Diffusion Models"
                },
                "summary": "Diffusion Models (DMs) benefit from large and diverse datasets for their\ntraining. Since this data is often scraped from the Internet without permission\nfrom the data owners, this raises concerns about copyright and intellectual\nproperty protections. While (illicit) use of data is easily detected for\ntraining samples perfectly re-created by a DM at inference time, it is much\nharder for data owners to verify if their data was used for training when the\noutputs from the suspect DM are not close replicas. Conceptually, membership\ninference attacks (MIAs), which detect if a given data point was used during\ntraining, present themselves as a suitable tool to address this challenge.\nHowever, we demonstrate that existing MIAs are not strong enough to reliably\ndetermine the membership of individual images in large, state-of-the-art DMs.\nTo overcome this limitation, we propose CDI, a framework for data owners to\nidentify whether their dataset was used to train a given DM. CDI relies on\ndataset inference techniques, i.e., instead of using the membership signal from\na single data point, CDI leverages the fact that most data owners, such as\nproviders of stock photography, visual media companies, or even individual\nartists, own datasets with multiple publicly exposed data points which might\nall be included in the training of a given DM. By selectively aggregating\nsignals from existing MIAs and using new handcrafted methods to extract\nfeatures for these datasets, feeding them to a scoring model, and applying\nrigorous statistical testing, CDI allows data owners with as little as 70 data\npoints to identify with a confidence of more than 99% whether their data was\nused to train a given DM. Thereby, CDI represents a valuable tool for data\nowners to claim illegitimate use of their copyrighted data. We make the code\navailable at https://github.com/sprintml/copyrighted_data_identification",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models (DMs) benefit from large and diverse datasets for their\ntraining. Since this data is often scraped from the Internet without permission\nfrom the data owners, this raises concerns about copyright and intellectual\nproperty protections. While (illicit) use of data is easily detected for\ntraining samples perfectly re-created by a DM at inference time, it is much\nharder for data owners to verify if their data was used for training when the\noutputs from the suspect DM are not close replicas. Conceptually, membership\ninference attacks (MIAs), which detect if a given data point was used during\ntraining, present themselves as a suitable tool to address this challenge.\nHowever, we demonstrate that existing MIAs are not strong enough to reliably\ndetermine the membership of individual images in large, state-of-the-art DMs.\nTo overcome this limitation, we propose CDI, a framework for data owners to\nidentify whether their dataset was used to train a given DM. CDI relies on\ndataset inference techniques, i.e., instead of using the membership signal from\na single data point, CDI leverages the fact that most data owners, such as\nproviders of stock photography, visual media companies, or even individual\nartists, own datasets with multiple publicly exposed data points which might\nall be included in the training of a given DM. By selectively aggregating\nsignals from existing MIAs and using new handcrafted methods to extract\nfeatures for these datasets, feeding them to a scoring model, and applying\nrigorous statistical testing, CDI allows data owners with as little as 70 data\npoints to identify with a confidence of more than 99% whether their data was\nused to train a given DM. Thereby, CDI represents a valuable tool for data\nowners to claim illegitimate use of their copyrighted data. We make the code\navailable at https://github.com/sprintml/copyrighted_data_identification"
                },
                "authors": [
                    {
                        "name": "Jan Dubiński"
                    },
                    {
                        "name": "Antoni Kowalczuk"
                    },
                    {
                        "name": "Franziska Boenisch"
                    },
                    {
                        "name": "Adam Dziedzic"
                    }
                ],
                "author_detail": {
                    "name": "Adam Dziedzic"
                },
                "author": "Adam Dziedzic",
                "arxiv_comment": "Accepted at CVPR2025 (Conference on Computer Vision and Pattern\n  Recognition) Code available at\n  https://github.com/sprintml/copyrighted_data_identification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12858v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12858v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18863v1",
                "updated": "2025-06-23T17:26:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    26,
                    40,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:26:40Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    26,
                    40,
                    0,
                    174,
                    0
                ],
                "title": "Variational Bayesian Channel Estimation and Data Detection for Cell-Free\n  Massive MIMO with Low-Resolution Quantized Fronthaul Links",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayesian Channel Estimation and Data Detection for Cell-Free\n  Massive MIMO with Low-Resolution Quantized Fronthaul Links"
                },
                "summary": "We study the joint channel estimation and data detection (JED) problem in a\ncell-free massive multiple-input multiple-output (CF-mMIMO) network, where\naccess points (APs) communicate with a central processing unit (CPU) over\nfronthaul links. However, the bandwidth of these links is limited, and thus,\npresents challenges to the applicability of CF-mMIMO, especially with an\never-increasing number of users. To address this, we propose a method based on\nvariational Bayesian (VB) inference for performing the JED process, where the\nAPs forward low-resolution quantized versions of the signals to the CPU. We\nconsider two approaches: \\emph{quantization-and-estimation} (Q-E) and\n\\emph{estimation-and-quantization} (E-Q). In the Q-E approach, each AP uses a\nlow-bit quantizer to quantize the signal before forwarding it to the CPU, while\nin the E-Q approach, each AP first performs local channel estimation and then\nsends a low-bit quantized version of the estimated channel to the CPU. We\nevaluate the performance of our VB-based approach under perfect fronthaul link\n(PFL) with unquantized received signals, Q-E, and E-Q in terms of symbol error\nrate (SER), normalized mean square error (NMSE) of the channel estimation,\ncomputational complexity, and fronthaul signaling overhead. We also compare\nthese results with those of the linear minimum mean squared error (LMMSE)\nmethod under the PFL scenario. Our numerical results show that both the VB(Q-E)\nand VB(E-Q) approaches achieve superior performance compared to LMMSE(PFL),\nbenefiting from the nonlinear modeling inherent in VB. Furthermore, the VB(Q-E)\nmethod outperforms VB(E-Q) due to errors in the local channel estimation\nprocess at the APs within the VB(E-Q) approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the joint channel estimation and data detection (JED) problem in a\ncell-free massive multiple-input multiple-output (CF-mMIMO) network, where\naccess points (APs) communicate with a central processing unit (CPU) over\nfronthaul links. However, the bandwidth of these links is limited, and thus,\npresents challenges to the applicability of CF-mMIMO, especially with an\never-increasing number of users. To address this, we propose a method based on\nvariational Bayesian (VB) inference for performing the JED process, where the\nAPs forward low-resolution quantized versions of the signals to the CPU. We\nconsider two approaches: \\emph{quantization-and-estimation} (Q-E) and\n\\emph{estimation-and-quantization} (E-Q). In the Q-E approach, each AP uses a\nlow-bit quantizer to quantize the signal before forwarding it to the CPU, while\nin the E-Q approach, each AP first performs local channel estimation and then\nsends a low-bit quantized version of the estimated channel to the CPU. We\nevaluate the performance of our VB-based approach under perfect fronthaul link\n(PFL) with unquantized received signals, Q-E, and E-Q in terms of symbol error\nrate (SER), normalized mean square error (NMSE) of the channel estimation,\ncomputational complexity, and fronthaul signaling overhead. We also compare\nthese results with those of the linear minimum mean squared error (LMMSE)\nmethod under the PFL scenario. Our numerical results show that both the VB(Q-E)\nand VB(E-Q) approaches achieve superior performance compared to LMMSE(PFL),\nbenefiting from the nonlinear modeling inherent in VB. Furthermore, the VB(Q-E)\nmethod outperforms VB(E-Q) due to errors in the local channel estimation\nprocess at the APs within the VB(E-Q) approach."
                },
                "authors": [
                    {
                        "name": "Sajjad Nassirpour"
                    },
                    {
                        "name": "Toan-Van Nguyen"
                    },
                    {
                        "name": "Hien Q. Ngo"
                    },
                    {
                        "name": "Le-Nam Tran"
                    },
                    {
                        "name": "Tharmalingam Ratnarajah"
                    },
                    {
                        "name": "Duy H. N. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Duy H. N. Nguyen"
                },
                "author": "Duy H. N. Nguyen",
                "arxiv_comment": "14 pages, 10 figures, accepted for journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18846v1",
                "updated": "2025-06-23T17:07:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    7,
                    4,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:07:04Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    7,
                    4,
                    0,
                    174,
                    0
                ],
                "title": "Bayesian decomposition using Besov priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian decomposition using Besov priors"
                },
                "summary": "In many inverse problems, the unknown is composed of multiple components with\ndifferent regularities, for example, in imaging problems, where the unknown can\nhave both rough and smooth features. We investigate linear Bayesian inverse\nproblems, where the unknown consists of two components: one smooth and one\npiecewise constant. We model the unknown as a sum of two components and assign\nindividual priors on each component to impose the assumed behavior. We propose\nand compare two prior models: (i) a combination of a Haar wavelet-based Besov\nprior and a smoothing Besov prior, and (ii) a hierarchical Gaussian prior on\nthe gradient coupled with a smoothing Besov prior. To achieve a balanced\nreconstruction, we place hyperpriors on the prior parameters and jointly infer\nboth the components and the hyperparameters. We propose Gibbs sampling schemes\nfor posterior inference in both prior models. We demonstrate the capabilities\nof our approach on 1D and 2D deconvolution problems, where the unknown consists\nof smooth parts with jumps. The numerical results indicate that our methods\nimprove the reconstruction quality compared to single-prior approaches and that\nthe prior parameters can be successfully estimated to yield a balanced\ndecomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many inverse problems, the unknown is composed of multiple components with\ndifferent regularities, for example, in imaging problems, where the unknown can\nhave both rough and smooth features. We investigate linear Bayesian inverse\nproblems, where the unknown consists of two components: one smooth and one\npiecewise constant. We model the unknown as a sum of two components and assign\nindividual priors on each component to impose the assumed behavior. We propose\nand compare two prior models: (i) a combination of a Haar wavelet-based Besov\nprior and a smoothing Besov prior, and (ii) a hierarchical Gaussian prior on\nthe gradient coupled with a smoothing Besov prior. To achieve a balanced\nreconstruction, we place hyperpriors on the prior parameters and jointly infer\nboth the components and the hyperparameters. We propose Gibbs sampling schemes\nfor posterior inference in both prior models. We demonstrate the capabilities\nof our approach on 1D and 2D deconvolution problems, where the unknown consists\nof smooth parts with jumps. The numerical results indicate that our methods\nimprove the reconstruction quality compared to single-prior approaches and that\nthe prior parameters can be successfully estimated to yield a balanced\ndecomposition."
                },
                "authors": [
                    {
                        "name": "Andreas Horst"
                    },
                    {
                        "name": "Babak Maboudi Afkham"
                    },
                    {
                        "name": "Yiqiu Dong"
                    },
                    {
                        "name": "Jakob Lemvig"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Lemvig"
                },
                "author": "Jakob Lemvig",
                "arxiv_comment": "28 pages, 13 figures, this is a preprint of an article submitted to\n  the IOP journal on inverse problems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18842v1",
                "updated": "2025-06-23T17:00:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    0,
                    34,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:00:34Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    0,
                    34,
                    0,
                    174,
                    0
                ],
                "title": "LIGHTHOUSE: Fast and precise distance to shoreline calculations from\n  anywhere on earth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIGHTHOUSE: Fast and precise distance to shoreline calculations from\n  anywhere on earth"
                },
                "summary": "We introduce a new dataset and algorithm for fast and efficient coastal\ndistance calculations from Anywhere on Earth (AoE). Existing global coastal\ndatasets are only available at coarse resolution (e.g. 1-4 km) which limits\ntheir utility. Publicly available satellite imagery combined with computer\nvision enable much higher precision. We provide a global coastline dataset at\n10 meter resolution, a 100+ fold improvement in precision over existing data.\nTo handle the computational challenge of querying at such an increased scale,\nwe introduce a new library: Layered Iterative Geospatial Hierarchical\nTerrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both\nexceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM\nto achieve millisecond online inference, making it well suited for real-time\napplications in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new dataset and algorithm for fast and efficient coastal\ndistance calculations from Anywhere on Earth (AoE). Existing global coastal\ndatasets are only available at coarse resolution (e.g. 1-4 km) which limits\ntheir utility. Publicly available satellite imagery combined with computer\nvision enable much higher precision. We provide a global coastline dataset at\n10 meter resolution, a 100+ fold improvement in precision over existing data.\nTo handle the computational challenge of querying at such an increased scale,\nwe introduce a new library: Layered Iterative Geospatial Hierarchical\nTerrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both\nexceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM\nto achieve millisecond online inference, making it well suited for real-time\napplications in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Patrick Beukema"
                    },
                    {
                        "name": "Henry Herzog"
                    },
                    {
                        "name": "Yawen Zhang"
                    },
                    {
                        "name": "Hunter Pitelka"
                    },
                    {
                        "name": "Favyen Bastani"
                    }
                ],
                "author_detail": {
                    "name": "Favyen Bastani"
                },
                "author": "Favyen Bastani",
                "arxiv_comment": "8 pages, 7 figures, 1 table, ICML 2025 ML4RS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18841v1",
                "updated": "2025-06-23T16:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning"
                },
                "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B"
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v4",
                "updated": "2025-06-23T16:58:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    58,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18831v1",
                "updated": "2025-06-23T16:47:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    47,
                    19,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:47:19Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    47,
                    19,
                    0,
                    174,
                    0
                ],
                "title": "STU-PID: Steering Token Usage via PID Controller for Efficient Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STU-PID: Steering Token Usage via PID Controller for Efficient Large\n  Language Model Reasoning"
                },
                "summary": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency."
                },
                "authors": [
                    {
                        "name": "Aryasomayajula Ram Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "Aryasomayajula Ram Bharadwaj"
                },
                "author": "Aryasomayajula Ram Bharadwaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18824v1",
                "updated": "2025-06-23T16:34:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    34,
                    52,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:34:52Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    34,
                    52,
                    0,
                    174,
                    0
                ],
                "title": "Understanding Software Engineering Agents: A Study of\n  Thought-Action-Result Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Software Engineering Agents: A Study of\n  Thought-Action-Result Trajectories"
                },
                "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05123v2",
                "updated": "2025-06-23T16:30:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    30,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-02-04T13:32:01Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    13,
                    32,
                    1,
                    6,
                    35,
                    0
                ],
                "title": "A Survey on Data Selection for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Data Selection for LLM Instruction Tuning"
                },
                "summary": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task."
                },
                "authors": [
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Dianhui Chu"
                },
                "author": "Dianhui Chu",
                "arxiv_comment": "Accepted by JAIR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18819v1",
                "updated": "2025-06-23T16:28:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    28,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:28:03Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    28,
                    3,
                    0,
                    174,
                    0
                ],
                "title": "RWESummary: A Framework and Test for Choosing Large Language Models to\n  Summarize Real-World Evidence (RWE) Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWESummary: A Framework and Test for Choosing Large Language Models to\n  Summarize Real-World Evidence (RWE) Studies"
                },
                "summary": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization."
                },
                "authors": [
                    {
                        "name": "Arjun Mukerji"
                    },
                    {
                        "name": "Michael L. Jackson"
                    },
                    {
                        "name": "Jason Jones"
                    },
                    {
                        "name": "Neil Sanghavi"
                    }
                ],
                "author_detail": {
                    "name": "Neil Sanghavi"
                },
                "author": "Neil Sanghavi",
                "arxiv_comment": "24 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14470v3",
                "updated": "2025-06-23T16:25:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    25,
                    27,
                    0,
                    174,
                    0
                ],
                "published": "2024-08-26T17:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. Selective PEFT, a class of\nparameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these\ncomputational challenges by selectively fine-tuning only a small fraction of\nthe model parameters. Although parameter-efficient, these techniques often fail\nto match the performance of fully fine-tuned models, primarily due to inherent\nbiases introduced during parameter selection. Traditional selective PEFT\ntechniques use a fixed set of parameters selected using different importance\nheuristics, failing to capture parameter importance dynamically and often\nleading to suboptimal performance. We introduce $\\text{ID}^3$, a novel\nselective PEFT method that calculates parameter importance continually, and\ndynamically unmasks parameters by balancing exploration and exploitation in\nparameter selection. Our empirical study on 16 tasks spanning natural language\nunderstanding, mathematical reasoning and summarization demonstrates the\neffectiveness of our method compared to fixed-masking selective PEFT\ntechniques. We analytically show that $\\text{ID}^3$ reduces the number of\ngradient updates by a factor of two, enhancing computational efficiency. Since\n$\\text{ID}^3$ is robust to random initialization of neurons and operates\ndirectly on the optimization process, it is highly flexible and can be\nintegrated with existing additive and reparametrization-based PEFT techniques\nsuch as adapters and LoRA respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. Selective PEFT, a class of\nparameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these\ncomputational challenges by selectively fine-tuning only a small fraction of\nthe model parameters. Although parameter-efficient, these techniques often fail\nto match the performance of fully fine-tuned models, primarily due to inherent\nbiases introduced during parameter selection. Traditional selective PEFT\ntechniques use a fixed set of parameters selected using different importance\nheuristics, failing to capture parameter importance dynamically and often\nleading to suboptimal performance. We introduce $\\text{ID}^3$, a novel\nselective PEFT method that calculates parameter importance continually, and\ndynamically unmasks parameters by balancing exploration and exploitation in\nparameter selection. Our empirical study on 16 tasks spanning natural language\nunderstanding, mathematical reasoning and summarization demonstrates the\neffectiveness of our method compared to fixed-masking selective PEFT\ntechniques. We analytically show that $\\text{ID}^3$ reduces the number of\ngradient updates by a factor of two, enhancing computational efficiency. Since\n$\\text{ID}^3$ is robust to random initialization of neurons and operates\ndirectly on the optimization process, it is highly flexible and can be\nintegrated with existing additive and reparametrization-based PEFT techniques\nsuch as adapters and LoRA respectively."
                },
                "authors": [
                    {
                        "name": "Aradhye Agarwal"
                    },
                    {
                        "name": "Suhas K Ramesh"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "15 pages, 7 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18808v1",
                "updated": "2025-06-23T16:19:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    19,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:19:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    19,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (I): All confounders measured",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (I): All confounders measured"
                },
                "summary": "Whether a variable is the cause of another, or simply associated with it, is\noften an important scientific question. Causal Inference is the name associated\nwith the body of techniques for addressing that question in a statistical\nsetting. Although assessing causality is relatively straightforward in the\npresence of temporal information, outside of that setting - the situation\nconsidered here - it is more difficult to assess causal effects. The\ndevelopment of the field of causal inference has involved concepts from a wide\nrange of topics, thereby limiting its adoption across some fields, including\nmeteorology. However, at its core, the requisite knowledge for causal inference\ninvolves little more than basic probability theory and regression, topics\nfamiliar to most meteorologists. By focusing on these core areas, this and a\ncompanion article provide a steppingstone for the meteorology community into\nthe field of (non-temporal) causal inference. Although some theoretical\nfoundations are presented, the main goal is the application of a specific\nmethod, called matching, to a problem in meteorology. The data for the\napplication are in public domain, and R code is provided as well, forming an\neasy path for meteorology students and researchers to enter the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whether a variable is the cause of another, or simply associated with it, is\noften an important scientific question. Causal Inference is the name associated\nwith the body of techniques for addressing that question in a statistical\nsetting. Although assessing causality is relatively straightforward in the\npresence of temporal information, outside of that setting - the situation\nconsidered here - it is more difficult to assess causal effects. The\ndevelopment of the field of causal inference has involved concepts from a wide\nrange of topics, thereby limiting its adoption across some fields, including\nmeteorology. However, at its core, the requisite knowledge for causal inference\ninvolves little more than basic probability theory and regression, topics\nfamiliar to most meteorologists. By focusing on these core areas, this and a\ncompanion article provide a steppingstone for the meteorology community into\nthe field of (non-temporal) causal inference. Although some theoretical\nfoundations are presented, the main goal is the application of a specific\nmethod, called matching, to a problem in meteorology. The data for the\napplication are in public domain, and R code is provided as well, forming an\neasy path for meteorology students and researchers to enter the field."
                },
                "authors": [
                    {
                        "name": "Caren Marzban"
                    },
                    {
                        "name": "Yikun Zhang"
                    },
                    {
                        "name": "Nicholas Bond"
                    },
                    {
                        "name": "Michael Richman"
                    }
                ],
                "author_detail": {
                    "name": "Michael Richman"
                },
                "author": "Michael Richman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11019v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11019v2",
                "updated": "2025-06-23T16:14:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    14,
                    12,
                    0,
                    174,
                    0
                ],
                "published": "2024-05-17T18:00:01Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    18,
                    0,
                    1,
                    4,
                    138,
                    0
                ],
                "title": "Episodic mass loss in the very luminous red supergiant [W60] B90 in the\n  Large Magellanic Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Episodic mass loss in the very luminous red supergiant [W60] B90 in the\n  Large Magellanic Cloud"
                },
                "summary": "This study delves into [W60] B90, one of the most luminous and extreme Red\nSupergiants (RSGs) in the Large Magellanic Cloud (LMC), aiming to search for\nevidence of episodic mass loss. Our discovery of a bar-like nebular structure\nat 1 pc, reminiscent of the bar around Betelgeuse, raised the question of\nwhether [W60] B90 also has a bow shock. We collected and analyzed proper motion\ndata from Gaia, as well as new multi-epoch spectroscopic and imaging data, and\narchival time-series photometry in the optical and mid-infrared. We found [W60]\nB90 to be a walkaway star, with a supersonic peculiar velocity in the direction\nof the bar. We detected shocked emission between the bar and the star, based on\nthe [S II]/H$\\alpha$ > 0.4 criterion, providing strong evidence for a bow\nshock. The 30-year optical light curve revealed semi-regular variability,\nshowing three similar dimming events with $\\Delta V \\sim 1$ mag, a recurrence\nof $\\sim$12 yr, and a rise time of 400 d. We found the mid-IR light curve to\nvary by 0.51 mag and 0.37 mag in the WISE1 and WISE2 bands, respectively, and\nby 0.42 mag and 0.25 mag during the last dimming event. During this event,\noptical spectroscopy revealed spectral variability (M3I to M4I), a correlation\nbetween the $T_{\\rm eff}$ and the brightness, increased extinction, and, after\nthe minimum, spectral features incompatible with the models. We also found a\ndifference of >300 K between the $T_{\\rm eff}$ measured from the TiO bands in\nthe optical and the atomic lines from our $J$-band spectroscopy. We inferred\nthat [W60] B90 is a more massive analog of Betelgeuse in the LMC and the first\nextragalactic single RSG with a suspected bow shock. Its high luminosity\n$\\log(L/L_{\\odot})=5.32$, mass-loss rate, and mid-IR variability compared to\nother RSGs in the LMC, indicate that it is in an unstable evolutionary state\nundergoing episodes of mass loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study delves into [W60] B90, one of the most luminous and extreme Red\nSupergiants (RSGs) in the Large Magellanic Cloud (LMC), aiming to search for\nevidence of episodic mass loss. Our discovery of a bar-like nebular structure\nat 1 pc, reminiscent of the bar around Betelgeuse, raised the question of\nwhether [W60] B90 also has a bow shock. We collected and analyzed proper motion\ndata from Gaia, as well as new multi-epoch spectroscopic and imaging data, and\narchival time-series photometry in the optical and mid-infrared. We found [W60]\nB90 to be a walkaway star, with a supersonic peculiar velocity in the direction\nof the bar. We detected shocked emission between the bar and the star, based on\nthe [S II]/H$\\alpha$ > 0.4 criterion, providing strong evidence for a bow\nshock. The 30-year optical light curve revealed semi-regular variability,\nshowing three similar dimming events with $\\Delta V \\sim 1$ mag, a recurrence\nof $\\sim$12 yr, and a rise time of 400 d. We found the mid-IR light curve to\nvary by 0.51 mag and 0.37 mag in the WISE1 and WISE2 bands, respectively, and\nby 0.42 mag and 0.25 mag during the last dimming event. During this event,\noptical spectroscopy revealed spectral variability (M3I to M4I), a correlation\nbetween the $T_{\\rm eff}$ and the brightness, increased extinction, and, after\nthe minimum, spectral features incompatible with the models. We also found a\ndifference of >300 K between the $T_{\\rm eff}$ measured from the TiO bands in\nthe optical and the atomic lines from our $J$-band spectroscopy. We inferred\nthat [W60] B90 is a more massive analog of Betelgeuse in the LMC and the first\nextragalactic single RSG with a suspected bow shock. Its high luminosity\n$\\log(L/L_{\\odot})=5.32$, mass-loss rate, and mid-IR variability compared to\nother RSGs in the LMC, indicate that it is in an unstable evolutionary state\nundergoing episodes of mass loss."
                },
                "authors": [
                    {
                        "name": "G. Munoz-Sanchez"
                    },
                    {
                        "name": "S. de Wit"
                    },
                    {
                        "name": "A. Z. Bonanos"
                    },
                    {
                        "name": "K. Antoniadis"
                    },
                    {
                        "name": "K. Boutsia"
                    },
                    {
                        "name": "P. Boumis"
                    },
                    {
                        "name": "E. Christodoulou"
                    },
                    {
                        "name": "M. Kalitsounaki"
                    },
                    {
                        "name": "A. Udalski"
                    }
                ],
                "author_detail": {
                    "name": "A. Udalski"
                },
                "author": "A. Udalski",
                "arxiv_doi": "10.1051/0004-6361/202450737",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202450737",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.11019v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11019v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 15 figures, 4 appendices, published in A&A",
                "arxiv_journal_ref": "A&A 690, A99 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00946v2",
                "updated": "2025-06-23T16:07:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    7,
                    38,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-02T22:35:30Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    22,
                    35,
                    30,
                    6,
                    33,
                    0
                ],
                "title": "The Simons Observatory: Validation of reconstructed power spectra from\n  simulated filtered maps for the Small Aperture Telescope survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Simons Observatory: Validation of reconstructed power spectra from\n  simulated filtered maps for the Small Aperture Telescope survey"
                },
                "summary": "We present a transfer function-based method to estimate angular power spectra\nfrom filtered maps for cosmic microwave background (CMB) surveys. This is\nespecially relevant for experiments targeting the faint primordial\ngravitational wave signatures in CMB polarisation at large scales, such as the\nSimons Observatory (SO) small aperture telescopes. While timestreams can be\nfiltered to mitigate the contamination from low-frequency noise, usual methods\nthat calculate the mode coupling at individual multipoles can be challenging\nfor experiments covering large sky areas or reaching few-arcminute resolution.\nThe method we present here, although approximate, is more practical and faster\nfor larger data volumes. We validate it through the use of simulated\nobservations approximating the first year of SO data, going from half-wave\nplate-modulated timestreams to maps, and using simulations to estimate the\nmixing of polarisation modes induced by an example of time-domain filtering. We\nshow its performance through an example null test and with an end-to-end\npipeline that performs inference on cosmological parameters, including the\ntensor-to-scalar ratio $r$. The performance demonstration uses simulated\nobservations at multiple frequency bands. We find that the method can recover\nunbiased parameters for our simulated noise levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a transfer function-based method to estimate angular power spectra\nfrom filtered maps for cosmic microwave background (CMB) surveys. This is\nespecially relevant for experiments targeting the faint primordial\ngravitational wave signatures in CMB polarisation at large scales, such as the\nSimons Observatory (SO) small aperture telescopes. While timestreams can be\nfiltered to mitigate the contamination from low-frequency noise, usual methods\nthat calculate the mode coupling at individual multipoles can be challenging\nfor experiments covering large sky areas or reaching few-arcminute resolution.\nThe method we present here, although approximate, is more practical and faster\nfor larger data volumes. We validate it through the use of simulated\nobservations approximating the first year of SO data, going from half-wave\nplate-modulated timestreams to maps, and using simulations to estimate the\nmixing of polarisation modes induced by an example of time-domain filtering. We\nshow its performance through an example null test and with an end-to-end\npipeline that performs inference on cosmological parameters, including the\ntensor-to-scalar ratio $r$. The performance demonstration uses simulated\nobservations at multiple frequency bands. We find that the method can recover\nunbiased parameters for our simulated noise levels."
                },
                "authors": [
                    {
                        "name": "Carlos Hervías-Caimapo"
                    },
                    {
                        "name": "Kevin Wolz"
                    },
                    {
                        "name": "Adrien La Posta"
                    },
                    {
                        "name": "Susanna Azzoni"
                    },
                    {
                        "name": "David Alonso"
                    },
                    {
                        "name": "Kam Arnold"
                    },
                    {
                        "name": "Carlo Baccigalupi"
                    },
                    {
                        "name": "Simon Biquard"
                    },
                    {
                        "name": "Michael L. Brown"
                    },
                    {
                        "name": "Erminia Calabrese"
                    },
                    {
                        "name": "Yuji Chinone"
                    },
                    {
                        "name": "Samuel Day-Weiss"
                    },
                    {
                        "name": "Jo Dunkley"
                    },
                    {
                        "name": "Rolando Dünner"
                    },
                    {
                        "name": "Josquin Errard"
                    },
                    {
                        "name": "Giulio Fabbian"
                    },
                    {
                        "name": "Ken Ganga"
                    },
                    {
                        "name": "Serena Giardiello"
                    },
                    {
                        "name": "Emilie Hertig"
                    },
                    {
                        "name": "Kevin M. Huffenberger"
                    },
                    {
                        "name": "Bradley R. Johnson"
                    },
                    {
                        "name": "Baptiste Jost"
                    },
                    {
                        "name": "Reijo Keskitalo"
                    },
                    {
                        "name": "Theodore S. Kisner"
                    },
                    {
                        "name": "Thibaut Louis"
                    },
                    {
                        "name": "Magdy Morshed"
                    },
                    {
                        "name": "Lyman A. Page"
                    },
                    {
                        "name": "Christian L. Reichardt"
                    },
                    {
                        "name": "Erik Rosenberg"
                    },
                    {
                        "name": "Max Silva-Feaver"
                    },
                    {
                        "name": "Wuhyun Sohn"
                    },
                    {
                        "name": "Yoshinori Sueno"
                    },
                    {
                        "name": "Dan B. Thomas"
                    },
                    {
                        "name": "Ema Tsang King Sang"
                    },
                    {
                        "name": "Amalia Villarrubia-Aguilar"
                    },
                    {
                        "name": "Kyohei Yamada"
                    }
                ],
                "author_detail": {
                    "name": "Kyohei Yamada"
                },
                "author": "Kyohei Yamada",
                "arxiv_comment": "30+2 pages, 15 figures, 2 tables. Accepted for publication by JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18798v1",
                "updated": "2025-06-23T16:03:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    53,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:03:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by\n  Object-Centric Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by\n  Object-Centric Awareness"
                },
                "summary": "Autonomous driving perception faces significant challenges due to occlusions\nand incomplete scene data in the environment. To overcome these issues, the\ntask of semantic occupancy prediction (SOP) is proposed, which aims to jointly\ninfer both the geometry and semantic labels of a scene from images. However,\nconventional camera-based methods typically treat all categories equally and\nprimarily rely on local features, leading to suboptimal predictions, especially\nfor dynamic foreground objects. To address this, we propose Object-Centric SOP\n(OC-SOP), a framework that integrates high-level object-centric cues extracted\nvia a detection branch into the semantic occupancy prediction pipeline. This\nobject-centric integration significantly enhances the prediction accuracy for\nforeground objects and achieves state-of-the-art performance among all\ncategories on SemanticKITTI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving perception faces significant challenges due to occlusions\nand incomplete scene data in the environment. To overcome these issues, the\ntask of semantic occupancy prediction (SOP) is proposed, which aims to jointly\ninfer both the geometry and semantic labels of a scene from images. However,\nconventional camera-based methods typically treat all categories equally and\nprimarily rely on local features, leading to suboptimal predictions, especially\nfor dynamic foreground objects. To address this, we propose Object-Centric SOP\n(OC-SOP), a framework that integrates high-level object-centric cues extracted\nvia a detection branch into the semantic occupancy prediction pipeline. This\nobject-centric integration significantly enhances the prediction accuracy for\nforeground objects and achieves state-of-the-art performance among all\ncategories on SemanticKITTI."
                },
                "authors": [
                    {
                        "name": "Helin Cao"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18795v1",
                "updated": "2025-06-23T16:03:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:03:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    16,
                    0,
                    174,
                    0
                ],
                "title": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract\n  Vulnerability Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract\n  Vulnerability Dataset Construction"
                },
                "summary": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities..."
                },
                "authors": [
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Yiming Shen"
                    },
                    {
                        "name": "Jiashuo Zhang"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "John Grundy"
                    },
                    {
                        "name": "Zhenzhe Shao"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "Accepted for the 48th International Conference on Software\n  Engineering (ICSE 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13904v2",
                "updated": "2025-06-23T16:00:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    0,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-20T04:10:50Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    4,
                    10,
                    50,
                    1,
                    140,
                    0
                ],
                "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Insert for Constructive Neural Vehicle Routing Solver"
                },
                "summary": "Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Combinatorial Optimisation (NCO) is a promising learning-based\napproach for solving Vehicle Routing Problems (VRPs) without extensive manual\ndesign. While existing constructive NCO methods typically follow an\nappending-based paradigm that sequentially adds unvisited nodes to partial\nsolutions, this rigid approach often leads to suboptimal results. To overcome\nthis limitation, we explore the idea of insertion-based paradigm and propose\nLearning to Construct with Insertion-based Paradigm (L2C-Insert), a novel\nlearning-based method for constructive NCO. Unlike traditional approaches,\nL2C-Insert builds solutions by strategically inserting unvisited nodes at any\nvalid position in the current partial solution, which can significantly enhance\nthe flexibility and solution quality. The proposed framework introduces three\nkey components: a novel model architecture for precise insertion position\nprediction, an efficient training scheme for model optimization, and an\nadvanced inference technique that fully exploits the insertion paradigm's\nflexibility. Extensive experiments on both synthetic and real-world instances\nof the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) demonstrate that L2C-Insert consistently achieves superior\nperformance across various problem sizes."
                },
                "authors": [
                    {
                        "name": "Fu Luo"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Mengyuan Zhong"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Jianyong Sun"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18785v1",
                "updated": "2025-06-23T15:54:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    54,
                    28,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:54:28Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    54,
                    28,
                    0,
                    174,
                    0
                ],
                "title": "SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy\n  Prediction in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy\n  Prediction in Autonomous Driving"
                },
                "summary": "Perception systems in autonomous driving rely on sensors such as LiDAR and\ncameras to perceive the 3D environment. However, due to occlusions and data\nsparsity, these sensors often fail to capture complete information. Semantic\nOccupancy Prediction (SOP) addresses this challenge by inferring both occupancy\nand semantics of unobserved regions. Existing transformer-based SOP methods\nlack explicit modeling of spatial structure in attention computation, resulting\nin limited geometric awareness and poor performance in sparse or occluded\nareas. To this end, we propose Spatially-aware Window Attention (SWA), a novel\nmechanism that incorporates local spatial context into attention. SWA\nsignificantly improves scene completion and achieves state-of-the-art results\non LiDAR-based SOP benchmarks. We further validate its generality by\nintegrating SWA into a camera-based SOP pipeline, where it also yields\nconsistent gains across modalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception systems in autonomous driving rely on sensors such as LiDAR and\ncameras to perceive the 3D environment. However, due to occlusions and data\nsparsity, these sensors often fail to capture complete information. Semantic\nOccupancy Prediction (SOP) addresses this challenge by inferring both occupancy\nand semantics of unobserved regions. Existing transformer-based SOP methods\nlack explicit modeling of spatial structure in attention computation, resulting\nin limited geometric awareness and poor performance in sparse or occluded\nareas. To this end, we propose Spatially-aware Window Attention (SWA), a novel\nmechanism that incorporates local spatial context into attention. SWA\nsignificantly improves scene completion and achieves state-of-the-art results\non LiDAR-based SOP benchmarks. We further validate its generality by\nintegrating SWA into a camera-based SOP pipeline, where it also yields\nconsistent gains across modalities."
                },
                "authors": [
                    {
                        "name": "Helin Cao"
                    },
                    {
                        "name": "Rafael Materla"
                    },
                    {
                        "name": "Sven Behnke"
                    }
                ],
                "author_detail": {
                    "name": "Sven Behnke"
                },
                "author": "Sven Behnke",
                "arxiv_comment": "under reviewed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18783v1",
                "updated": "2025-06-23T15:53:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    53,
                    14,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:53:14Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    53,
                    14,
                    0,
                    174,
                    0
                ],
                "title": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation"
                },
                "summary": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks."
                },
                "authors": [
                    {
                        "name": "Kamil Szczepanik"
                    },
                    {
                        "name": "Jarosław A. Chudziak"
                    }
                ],
                "author_detail": {
                    "name": "Jarosław A. Chudziak"
                },
                "author": "Jarosław A. Chudziak",
                "arxiv_doi": "10.5220/0013321900003890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013321900003890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.18783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 10 figures, 2 tables, Accepted at the 17th International\n  Conference on Agents and Artificial Intelligence (ICAART 2025). Final version\n  published in Proceedings of ICAART 2025 (Vol. 1), pages 196-207",
                "arxiv_journal_ref": "In Proceedings of the 17th International Conference on Agents and\n  Artificial Intelligence - Volume 1: ICAART 2025; pages 196-207. SciTePress\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18750v3",
                "updated": "2025-06-23T15:51:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    51,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-25T02:48:53Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    2,
                    48,
                    53,
                    2,
                    360,
                    0
                ],
                "title": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications."
                },
                "authors": [
                    {
                        "name": "Md Nakhla Rafi"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Shaowei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Wang"
                },
                "author": "Shaowei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18781v1",
                "updated": "2025-06-23T15:50:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    50,
                    21,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:50:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    50,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLMs Are Not Self-Consistent For Simple Tasks"
                },
                "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency."
                },
                "authors": [
                    {
                        "name": "Zhenru Lin"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18778v1",
                "updated": "2025-06-23T15:47:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    47,
                    9,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:47:09Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    47,
                    9,
                    0,
                    174,
                    0
                ],
                "title": "Likelihood Ratio test for Poisson graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood Ratio test for Poisson graph"
                },
                "summary": "Directed acyclic graphs are widely used to describe the causal effects among\nrandom variables, and the inference of those causal effects has become an\npopular topic in statistics and machine learning, and has wide applications in\nneuroinformatics, bioinformatics and so on. However, most studies focus on the\nestimation or inference of the directional relations among continuous random\nvariables, those among discrete random variables have not gained much\nattentions. In this article we focus on the inference of directed linkages and\ndirected pathways in a Poisson directed graphical model. We employ likelihood\nratio tests subject to non-convex acyclicity constraints, and derive the\nasymptotic distributions of the test statistic under the null hypothesis is\ntrue in high-dimensional situations. The power analysis and simulations suggest\nthat the tests achieve the desired objectives of inference. An analysis of a\nbasketball statistics dataset of NBA players during 2016-2017 season\nillustrates the utility of the proposed method to infer directed linkages and\ndirected pathways in player's statistics network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed acyclic graphs are widely used to describe the causal effects among\nrandom variables, and the inference of those causal effects has become an\npopular topic in statistics and machine learning, and has wide applications in\nneuroinformatics, bioinformatics and so on. However, most studies focus on the\nestimation or inference of the directional relations among continuous random\nvariables, those among discrete random variables have not gained much\nattentions. In this article we focus on the inference of directed linkages and\ndirected pathways in a Poisson directed graphical model. We employ likelihood\nratio tests subject to non-convex acyclicity constraints, and derive the\nasymptotic distributions of the test statistic under the null hypothesis is\ntrue in high-dimensional situations. The power analysis and simulations suggest\nthat the tests achieve the desired objectives of inference. An analysis of a\nbasketball statistics dataset of NBA players during 2016-2017 season\nillustrates the utility of the proposed method to infer directed linkages and\ndirected pathways in player's statistics network."
                },
                "authors": [
                    {
                        "name": "Chen Shuyan"
                    },
                    {
                        "name": "Liu Xin"
                    },
                    {
                        "name": "Wang Shaoli"
                    }
                ],
                "author_detail": {
                    "name": "Wang Shaoli"
                },
                "author": "Wang Shaoli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary: 62F03, Secondary: 62F30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18777v1",
                "updated": "2025-06-23T15:45:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    45,
                    44,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:45:44Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    45,
                    44,
                    0,
                    174,
                    0
                ],
                "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions\n  During Code Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions\n  During Code Training"
                },
                "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles."
                },
                "authors": [
                    {
                        "name": "Jonathan Cook"
                    },
                    {
                        "name": "Silvia Sapora"
                    },
                    {
                        "name": "Arash Ahmadian"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Tim Rocktaschel"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Laura Ruis"
                    }
                ],
                "author_detail": {
                    "name": "Laura Ruis"
                },
                "author": "Laura Ruis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12037v2",
                "updated": "2025-06-23T15:29:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    29,
                    4,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-15T20:15:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    20,
                    15,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "Embedded Model Form Uncertainty Quantification with Measurement Noise\n  for Bayesian Model Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded Model Form Uncertainty Quantification with Measurement Noise\n  for Bayesian Model Calibration"
                },
                "summary": "A key factor in ensuring the accuracy of computer simulations that model\nphysical systems is the proper calibration of their parameters based on\nreal-world observations or experimental data. Inevitably, uncertainties arise,\nand Bayesian methods provide a robust framework for quantifying and propagating\nthese uncertainties to model predictions. Nevertheless, Bayesian methods paired\nwith inexact models usually produce predictions unable to represent the\nobserved datapoints. Additionally, the quantified uncertainties of these\noverconfident models cannot be propagated to other Quantities of Interest\n(QoIs) reliably. A promising solution involves embedding a model inadequacy\nterm in the inference parameters, allowing the quantified model form\nuncertainty to influence non-observed QoIs. This paper introduces a more\ninterpretable framework for embedding the model inadequacy compared to existing\nmethods. To overcome the limitations of current approaches, we adapt the\nexisting likelihood models to properly account for noise in the measurements\nand propose two new formulations designed to address their shortcomings.\nMoreover, we evaluate the performance of this inadequacy-embedding approach in\nthe presence of discrepancies between measurements and model predictions,\nincluding noise and outliers. Particular attention is given to how the\nuncertainty associated with the model inadequacy term propagates to the QoIs,\nenabling a more comprehensive statistical analysis of prediction's reliability.\nFinally, the proposed approach is applied to estimate the uncertainty in the\npredicted heat flux from a transient thermal simulation using temperature\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key factor in ensuring the accuracy of computer simulations that model\nphysical systems is the proper calibration of their parameters based on\nreal-world observations or experimental data. Inevitably, uncertainties arise,\nand Bayesian methods provide a robust framework for quantifying and propagating\nthese uncertainties to model predictions. Nevertheless, Bayesian methods paired\nwith inexact models usually produce predictions unable to represent the\nobserved datapoints. Additionally, the quantified uncertainties of these\noverconfident models cannot be propagated to other Quantities of Interest\n(QoIs) reliably. A promising solution involves embedding a model inadequacy\nterm in the inference parameters, allowing the quantified model form\nuncertainty to influence non-observed QoIs. This paper introduces a more\ninterpretable framework for embedding the model inadequacy compared to existing\nmethods. To overcome the limitations of current approaches, we adapt the\nexisting likelihood models to properly account for noise in the measurements\nand propose two new formulations designed to address their shortcomings.\nMoreover, we evaluate the performance of this inadequacy-embedding approach in\nthe presence of discrepancies between measurements and model predictions,\nincluding noise and outliers. Particular attention is given to how the\nuncertainty associated with the model inadequacy term propagates to the QoIs,\nenabling a more comprehensive statistical analysis of prediction's reliability.\nFinally, the proposed approach is applied to estimate the uncertainty in the\npredicted heat flux from a transient thermal simulation using temperature\nobservations."
                },
                "authors": [
                    {
                        "name": "Daniel Andrés Arcones"
                    },
                    {
                        "name": "Martin Weiser"
                    },
                    {
                        "name": "Phaedon-Stelios Koutsourelakis"
                    },
                    {
                        "name": "Jörg F. Unger"
                    }
                ],
                "author_detail": {
                    "name": "Jörg F. Unger"
                },
                "author": "Jörg F. Unger",
                "arxiv_comment": "37 pages, 23 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15357v2",
                "updated": "2025-06-23T15:29:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    29,
                    0,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-21T18:04:04Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    18,
                    4,
                    4,
                    0,
                    111,
                    0
                ],
                "title": "ALMA reveals bright circumgalactic emission and a biconical outflow in\n  z~6.4 quasar PSOJ183+05",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA reveals bright circumgalactic emission and a biconical outflow in\n  z~6.4 quasar PSOJ183+05"
                },
                "summary": "Understanding gas flows between galaxies and their surrounding\ncircum-galactic medium (CGM) is crucial to unveil the mechanisms regulating\ngalaxy evolution, especially in the early Universe. However, observations of\nthe CGM around massive galaxies at $z>6$ remain limited, particularly in the\ncold gas phase. In this work, we present multi-configuration ALMA observations\nof [CII]$\\lambda158\\mu$m and millimetre continuum emission in the $z\\sim6.4$\nquasar PSOJ183+05. We find clumpy [CII] emission, tracing gas up to a $\\sim6$\nkpc radius, consistent with the interface region between the interstellar\nmedium (ISM) and CGM. The [CII] kinematics shows a rotating disk and a\nhigh-velocity, biconical outflow extending up to 5 kpc. The inferred mass\noutflow rate is $\\dot{M}_{\\rm of}\\sim930$ M$_\\odot$ yr$^{-1}$, among the\nhighest at $z>6$, and comparable to the star-formation rate. These findings\nsuggest that quasar-driven outflows can rapidly transfer energy and momentum to\nthe CGM, without immediately quenching star formation in the host galaxy ISM.\nThis supports a delayed feedback scenario, in which outflows reshape CGM\nconditions and regulate future gas accretion over longer timescales. We find\nthat neither the high-velocity component nor the extended CGM emission in\nPSOJ183+05 are recovered when using the high-resolution dataset alone, which\nmay explain the conflicting results reported regarding [CII] sizes and the\ndetection of outflows at $z\\gtrsim6$. Combining multi-configuration ALMA data\nwith observations from JWST and MUSE will be crucial to map the CGM across its\ndifferent phases and build a comprehensive picture of the baryon cycle in the\nfirst massive galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding gas flows between galaxies and their surrounding\ncircum-galactic medium (CGM) is crucial to unveil the mechanisms regulating\ngalaxy evolution, especially in the early Universe. However, observations of\nthe CGM around massive galaxies at $z>6$ remain limited, particularly in the\ncold gas phase. In this work, we present multi-configuration ALMA observations\nof [CII]$\\lambda158\\mu$m and millimetre continuum emission in the $z\\sim6.4$\nquasar PSOJ183+05. We find clumpy [CII] emission, tracing gas up to a $\\sim6$\nkpc radius, consistent with the interface region between the interstellar\nmedium (ISM) and CGM. The [CII] kinematics shows a rotating disk and a\nhigh-velocity, biconical outflow extending up to 5 kpc. The inferred mass\noutflow rate is $\\dot{M}_{\\rm of}\\sim930$ M$_\\odot$ yr$^{-1}$, among the\nhighest at $z>6$, and comparable to the star-formation rate. These findings\nsuggest that quasar-driven outflows can rapidly transfer energy and momentum to\nthe CGM, without immediately quenching star formation in the host galaxy ISM.\nThis supports a delayed feedback scenario, in which outflows reshape CGM\nconditions and regulate future gas accretion over longer timescales. We find\nthat neither the high-velocity component nor the extended CGM emission in\nPSOJ183+05 are recovered when using the high-resolution dataset alone, which\nmay explain the conflicting results reported regarding [CII] sizes and the\ndetection of outflows at $z\\gtrsim6$. Combining multi-configuration ALMA data\nwith observations from JWST and MUSE will be crucial to map the CGM across its\ndifferent phases and build a comprehensive picture of the baryon cycle in the\nfirst massive galaxies."
                },
                "authors": [
                    {
                        "name": "Manuela Bischetti"
                    },
                    {
                        "name": "Chiara Feruglio"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Valentina D'Odorico"
                    },
                    {
                        "name": "Francesco Salvestrini"
                    },
                    {
                        "name": "Fabrizio Fiore"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Fiore"
                },
                "author": "Fabrizio Fiore",
                "arxiv_comment": "Accepted for publication in ApJL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15225v2",
                "updated": "2025-06-23T15:24:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    24,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-01-25T14:09:39Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    14,
                    9,
                    39,
                    5,
                    25,
                    0
                ],
                "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval"
                },
                "summary": "While many advanced LLMs are designed to handle long sequence data, we can\nstill observe notable quality degradation even within the sequence limit. In\nthis work, we introduce a novel approach called Scaling to Emphasize Attention\nfor Long-context retrieval (SEAL), which enhances the retrieval performance of\nlarge language models (LLMs) over long contexts. We observe that specific\nattention heads are closely tied to long-context retrieval, showing positive or\nnegative correlation with retrieval scores, and adjusting the strength of these\nheads boosts the quality of LLMs in long context by a large margin. Built on\nthis insight, we propose a learning-based mechanism that leverages generated\ndata to emphasize these heads. By applying SEAL, we achieve significant\nimprovements in long-context retrieval performance across various tasks and\nmodels. Additionally, when combined with existing training-free context\nextension techniques, SEAL extends the contextual limits of LLMs while\nmaintaining highly reliable outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many advanced LLMs are designed to handle long sequence data, we can\nstill observe notable quality degradation even within the sequence limit. In\nthis work, we introduce a novel approach called Scaling to Emphasize Attention\nfor Long-context retrieval (SEAL), which enhances the retrieval performance of\nlarge language models (LLMs) over long contexts. We observe that specific\nattention heads are closely tied to long-context retrieval, showing positive or\nnegative correlation with retrieval scores, and adjusting the strength of these\nheads boosts the quality of LLMs in long context by a large margin. Built on\nthis insight, we propose a learning-based mechanism that leverages generated\ndata to emphasize these heads. By applying SEAL, we achieve significant\nimprovements in long-context retrieval performance across various tasks and\nmodels. Additionally, when combined with existing training-free context\nextension techniques, SEAL extends the contextual limits of LLMs while\nmaintaining highly reliable outputs."
                },
                "authors": [
                    {
                        "name": "Changhun Lee"
                    },
                    {
                        "name": "Minsang Seok"
                    },
                    {
                        "name": "Jun-gyu Jin"
                    },
                    {
                        "name": "Younghyun Cho"
                    },
                    {
                        "name": "Eunhyeok Park"
                    }
                ],
                "author_detail": {
                    "name": "Eunhyeok Park"
                },
                "author": "Eunhyeok Park",
                "arxiv_comment": "Accepted at ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18748v1",
                "updated": "2025-06-23T15:20:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    20,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:20:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    20,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Fast State-Augmented Learning for Wireless Resource Allocation with Dual\n  Variable Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State-Augmented Learning for Wireless Resource Allocation with Dual\n  Variable Regression"
                },
                "summary": "We consider resource allocation problems in multi-user wireless networks,\nwhere the goal is to optimize a network-wide utility function subject to\nconstraints on the ergodic average performance of users. We demonstrate how a\nstate-augmented graph neural network (GNN) parametrization for the resource\nallocation policy circumvents the drawbacks of the ubiquitous dual subgradient\nmethods by representing the network configurations (or states) as graphs and\nviewing dual variables as dynamic inputs to the model, viewed as graph signals\nsupported over the graphs. Lagrangian maximizing state-augmented policies are\nlearned during the offline training phase, and the dual variables evolve\nthrough gradient updates while executing the learned state-augmented policies\nduring the inference phase. Our main contributions are to illustrate how\nnear-optimal initialization of dual multipliers for faster inference can be\naccomplished with dual variable regression, leveraging a secondary GNN\nparametrization, and how maximization of the Lagrangian over the multipliers\nsampled from the dual descent dynamics substantially improves the training of\nstate-augmented models. We demonstrate the superior performance of the proposed\nalgorithm with extensive numerical experiments in a case study of transmit\npower control. Finally, we prove a convergence result and an exponential\nprobability bound on the excursions of the dual function (iterate) optimality\ngaps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider resource allocation problems in multi-user wireless networks,\nwhere the goal is to optimize a network-wide utility function subject to\nconstraints on the ergodic average performance of users. We demonstrate how a\nstate-augmented graph neural network (GNN) parametrization for the resource\nallocation policy circumvents the drawbacks of the ubiquitous dual subgradient\nmethods by representing the network configurations (or states) as graphs and\nviewing dual variables as dynamic inputs to the model, viewed as graph signals\nsupported over the graphs. Lagrangian maximizing state-augmented policies are\nlearned during the offline training phase, and the dual variables evolve\nthrough gradient updates while executing the learned state-augmented policies\nduring the inference phase. Our main contributions are to illustrate how\nnear-optimal initialization of dual multipliers for faster inference can be\naccomplished with dual variable regression, leveraging a secondary GNN\nparametrization, and how maximization of the Lagrangian over the multipliers\nsampled from the dual descent dynamics substantially improves the training of\nstate-augmented models. We demonstrate the superior performance of the proposed\nalgorithm with extensive numerical experiments in a case study of transmit\npower control. Finally, we prove a convergence result and an exponential\nprobability bound on the excursions of the dual function (iterate) optimality\ngaps."
                },
                "authors": [
                    {
                        "name": "Yigit Berkay Uslu"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Mark Eisen"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ribeiro"
                },
                "author": "Alejandro Ribeiro",
                "arxiv_comment": "This work has been submitted to the IEEE TSP for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10065v2",
                "updated": "2025-06-23T15:19:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    19,
                    40,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-14T10:42:16Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    42,
                    16,
                    4,
                    45,
                    0
                ],
                "title": "Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions\n  for Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions\n  for Time Series"
                },
                "summary": "This paper proposes valid inference tools, based on self-normalization, in\ntime series expected shortfall regressions and, as a corollary, also in\nquantile regressions. Extant methods for such time series regressions, based on\na bootstrap or direct estimation of the long-run variance, are computationally\nmore involved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the (quantile, expected\nshortfall) regression parameters that are computed on an expanding window, and\nare correctly sized as we show in simulations. Two empirical applications to\nstock return predictability and to Growth-at-Risk demonstrate the practical\nusefulness of the developed inference tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes valid inference tools, based on self-normalization, in\ntime series expected shortfall regressions and, as a corollary, also in\nquantile regressions. Extant methods for such time series regressions, based on\na bootstrap or direct estimation of the long-run variance, are computationally\nmore involved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the (quantile, expected\nshortfall) regression parameters that are computed on an expanding window, and\nare correctly sized as we show in simulations. Two empirical applications to\nstock return predictability and to Growth-at-Risk demonstrate the practical\nusefulness of the developed inference tools."
                },
                "authors": [
                    {
                        "name": "Yannick Hoga"
                    },
                    {
                        "name": "Christian Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schulz"
                },
                "author": "Christian Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18732v1",
                "updated": "2025-06-23T15:09:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    9,
                    14,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:09:14Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    9,
                    14,
                    0,
                    174,
                    0
                ],
                "title": "Towards Group Fairness with Multiple Sensitive Attributes in Federated\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Group Fairness with Multiple Sensitive Attributes in Federated\n  Foundation Models"
                },
                "summary": "The deep integration of foundation models (FM) with federated learning (FL)\nenhances personalization and scalability for diverse downstream tasks, making\nit crucial in sensitive domains like healthcare. Achieving group fairness has\nbecome an increasingly prominent issue in the era of federated foundation\nmodels (FFMs), since biases in sensitive attributes might lead to inequitable\ntreatment for under-represented demographic groups. Existing studies mostly\nfocus on achieving fairness with respect to a single sensitive attribute. This\nrenders them unable to provide clear interpretability of dependencies among\nmultiple sensitive attributes which is required to achieve group fairness. Our\npaper takes the first attempt towards a causal analysis of the relationship\nbetween group fairness across various sensitive attributes in the FFM. We\nextend the FFM structure to trade off multiple sensitive attributes\nsimultaneously and quantify the causal effect behind the group fairness through\ncausal discovery and inference. Extensive experiments validate its\neffectiveness, offering insights into interpretability towards building\ntrustworthy and fair FFM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deep integration of foundation models (FM) with federated learning (FL)\nenhances personalization and scalability for diverse downstream tasks, making\nit crucial in sensitive domains like healthcare. Achieving group fairness has\nbecome an increasingly prominent issue in the era of federated foundation\nmodels (FFMs), since biases in sensitive attributes might lead to inequitable\ntreatment for under-represented demographic groups. Existing studies mostly\nfocus on achieving fairness with respect to a single sensitive attribute. This\nrenders them unable to provide clear interpretability of dependencies among\nmultiple sensitive attributes which is required to achieve group fairness. Our\npaper takes the first attempt towards a causal analysis of the relationship\nbetween group fairness across various sensitive attributes in the FFM. We\nextend the FFM structure to trade off multiple sensitive attributes\nsimultaneously and quantify the causal effect behind the group fairness through\ncausal discovery and inference. Extensive experiments validate its\neffectiveness, offering insights into interpretability towards building\ntrustworthy and fair FFM systems."
                },
                "authors": [
                    {
                        "name": "Yuning Yang"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Tianrun Gao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Guangyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Wang"
                },
                "author": "Guangyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18728v1",
                "updated": "2025-06-23T15:05:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    5,
                    54,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:05:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    5,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries"
                },
                "summary": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines."
                },
                "authors": [
                    {
                        "name": "Steven Kolawole"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Virginia Smith"
                    },
                    {
                        "name": "Pratiksha Thaker"
                    }
                ],
                "author_detail": {
                    "name": "Pratiksha Thaker"
                },
                "author": "Pratiksha Thaker",
                "arxiv_comment": "In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24616v2",
                "updated": "2025-06-23T15:01:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    1,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-30T14:08:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX"
                },
                "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."
                },
                "authors": [
                    {
                        "name": "Nikita Martynov"
                    },
                    {
                        "name": "Anastasia Mordasheva"
                    },
                    {
                        "name": "Dmitriy Gorbetskiy"
                    },
                    {
                        "name": "Danil Astafurov"
                    },
                    {
                        "name": "Ulyana Isaeva"
                    },
                    {
                        "name": "Elina Basyrova"
                    },
                    {
                        "name": "Sergey Skachkov"
                    },
                    {
                        "name": "Victoria Berestova"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Valeriia Zanina"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "arxiv_comment": "179 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18726v1",
                "updated": "2025-06-23T15:00:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    0,
                    22,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:00:22Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    0,
                    22,
                    0,
                    174,
                    0
                ],
                "title": "Tail Flexibility in the Degrees of Preferential Attachment Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail Flexibility in the Degrees of Preferential Attachment Networks"
                },
                "summary": "Devising the underlying generating mechanism of a real-life network is\ndifficult as, more often than not, only its snapshots are available, but not\nits full evolution. One candidate for the generating mechanism is preferential\nattachment which, in its simplest form, results in a degree distribution that\nfollows the power law. Consequently, the growth of real-life networks that\nroughly display such power-law behaviour is commonly modelled by preferential\nattachment. However, the validity of the power law has been challenged by the\npresence of alternatives with comparable performance, as well as the recent\nfindings that the right tail of the degree distribution is often lighter than\nimplied by the body, whilst still being heavy. In this paper, we study a\nmodified version of the model with a flexible preference function that allows\nsuper/sub-linear behaviour whilst also guaranteeing that the limiting degree\ndistribution has a heavy tail. We relate the distributions tail index directly\nto the model parameters, allowing direct inference of the parameters from the\ndegree distribution alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devising the underlying generating mechanism of a real-life network is\ndifficult as, more often than not, only its snapshots are available, but not\nits full evolution. One candidate for the generating mechanism is preferential\nattachment which, in its simplest form, results in a degree distribution that\nfollows the power law. Consequently, the growth of real-life networks that\nroughly display such power-law behaviour is commonly modelled by preferential\nattachment. However, the validity of the power law has been challenged by the\npresence of alternatives with comparable performance, as well as the recent\nfindings that the right tail of the degree distribution is often lighter than\nimplied by the body, whilst still being heavy. In this paper, we study a\nmodified version of the model with a flexible preference function that allows\nsuper/sub-linear behaviour whilst also guaranteeing that the limiting degree\ndistribution has a heavy tail. We relate the distributions tail index directly\nto the model parameters, allowing direct inference of the parameters from the\ndegree distribution alone."
                },
                "authors": [
                    {
                        "name": "Thomas Boughen"
                    },
                    {
                        "name": "Clement Lee"
                    },
                    {
                        "name": "Vianey Palacios Ramirez"
                    }
                ],
                "author_detail": {
                    "name": "Vianey Palacios Ramirez"
                },
                "author": "Vianey Palacios Ramirez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18711v1",
                "updated": "2025-06-23T14:49:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:49:03Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    3,
                    0,
                    174,
                    0
                ],
                "title": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with\n  Older Adults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with\n  Older Adults"
                },
                "summary": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults."
                },
                "authors": [
                    {
                        "name": "Marianne Bossema"
                    },
                    {
                        "name": "Somaya Ben Allouch"
                    },
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Rob Saunders"
                    }
                ],
                "author_detail": {
                    "name": "Rob Saunders"
                },
                "author": "Rob Saunders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18710v2",
                "updated": "2025-06-24T12:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    36,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T14:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Pedagogical Knowledge of Large Language Models"
                },
                "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."
                },
                "authors": [
                    {
                        "name": "Maxime Lelièvre"
                    },
                    {
                        "name": "Amy Waldock"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Natalia Valdés Aspillaga"
                    },
                    {
                        "name": "Alasdair Mackintosh"
                    },
                    {
                        "name": "María José Ogando Portela"
                    },
                    {
                        "name": "Jared Lee"
                    },
                    {
                        "name": "Paul Atherton"
                    },
                    {
                        "name": "Robin A. A. Ince"
                    },
                    {
                        "name": "Oliver G. B. Garrod"
                    }
                ],
                "author_detail": {
                    "name": "Oliver G. B. Garrod"
                },
                "author": "Oliver G. B. Garrod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09300v2",
                "updated": "2025-06-23T14:48:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    48,
                    52,
                    0,
                    174,
                    0
                ],
                "published": "2025-01-16T05:27:27Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    27,
                    27,
                    3,
                    16,
                    0
                ],
                "title": "A long-term study of Mrk 50 : Appearance and disappearance of soft\n  excess",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A long-term study of Mrk 50 : Appearance and disappearance of soft\n  excess"
                },
                "summary": "We present an extensive temporal and spectral study of the Seyfert 1 AGN Mrk\n50 using 15 years (2007-2022) of multiwavelength observations from XMM-Newton,\nSwift, and NuSTAR for the first time. From the timing analysis, we found that\nthe source exhibited variability of $\\sim$20 % during the 2007 observation,\nwhich reduced to below 10 % in the subsequent observations and became\nnon-variable in the observations from 2010 onward. From the spectral study, we\nfound that the spectra are nearly featureless. Non-detection of absorption in\nthe low-energy domain during the 15 years of observation infers the absence of\nobscuration around the central engine, rendering the nucleus a `bare' type. A\nprominent soft X-ray excess below 2 keV was detected in the source spectrum\nduring the observations between 2007 and 2010, which vanished during the later\nobservations. To describe the nature of the soft excess, we use two physical\nmodels, such as warm Comptonization and blurred reflection from the ionized\naccretion disk. Both the physical models explain the nature and origin of the\nsoft excess in this source. Our analysis found that Mrk~50 accretes at\nsub-Eddington accretion rate ($\\lambda_{Edd}=0.13-0.02$) during all the\nobservations used in this work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an extensive temporal and spectral study of the Seyfert 1 AGN Mrk\n50 using 15 years (2007-2022) of multiwavelength observations from XMM-Newton,\nSwift, and NuSTAR for the first time. From the timing analysis, we found that\nthe source exhibited variability of $\\sim$20 % during the 2007 observation,\nwhich reduced to below 10 % in the subsequent observations and became\nnon-variable in the observations from 2010 onward. From the spectral study, we\nfound that the spectra are nearly featureless. Non-detection of absorption in\nthe low-energy domain during the 15 years of observation infers the absence of\nobscuration around the central engine, rendering the nucleus a `bare' type. A\nprominent soft X-ray excess below 2 keV was detected in the source spectrum\nduring the observations between 2007 and 2010, which vanished during the later\nobservations. To describe the nature of the soft excess, we use two physical\nmodels, such as warm Comptonization and blurred reflection from the ionized\naccretion disk. Both the physical models explain the nature and origin of the\nsoft excess in this source. Our analysis found that Mrk~50 accretes at\nsub-Eddington accretion rate ($\\lambda_{Edd}=0.13-0.02$) during all the\nobservations used in this work."
                },
                "authors": [
                    {
                        "name": "Narendranath Layek"
                    },
                    {
                        "name": "Prantik Nandi"
                    },
                    {
                        "name": "Sachindra Naik"
                    },
                    {
                        "name": "Arghajit Jana"
                    }
                ],
                "author_detail": {
                    "name": "Arghajit Jana"
                },
                "author": "Arghajit Jana",
                "arxiv_comment": "27 pages,11 figures,8 tables Accepted for publication in APJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06047v2",
                "updated": "2025-06-23T14:45:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    45,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-08T19:53:18Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    19,
                    53,
                    18,
                    6,
                    343,
                    0
                ],
                "title": "Small Term Reachability and Related Problems for Terminating Term\n  Rewriting Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Term Reachability and Related Problems for Terminating Term\n  Rewriting Systems"
                },
                "summary": "Motivated by an application where we try to make proofs for Description Logic\ninferences smaller by rewriting, we consider the following decision problem,\nwhich we call the small term reachability problem: given a term rewriting\nsystem $R$, a term $s$, and a natural number $n$, decide whether there is a\nterm $t$ of size $\\leq n$ reachable from $s$ using the rules of $R$. We\ninvestigate the complexity of this problem depending on how termination of $R$\ncan be established. We show that the problem is in general NP-complete for\nlength-reducing term rewriting systems. Its complexity increases to\nN2ExpTime-complete (NExpTime-complete) if termination is proved using a\n(linear) polynomial order and to PSpace-complete for systems whose termination\ncan be shown using a restricted class of Knuth-Bendix orders. Confluence\nreduces the complexity to P for the length-reducing case, but has no effect on\nthe worst-case complexity in the other two cases. Finally, we consider the\nlarge term reachability problem, a variant of the problem where we are\ninterested in reachability of a term of size $\\geq n$. It turns out that this\nseemingly innocuous modification in some cases changes the complexity of the\nproblem, which may also become dependent on whether the number $n$ is is\nrepresented in unary or binary encoding, whereas this makes no difference for\nthe complexity of the small term reachability problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by an application where we try to make proofs for Description Logic\ninferences smaller by rewriting, we consider the following decision problem,\nwhich we call the small term reachability problem: given a term rewriting\nsystem $R$, a term $s$, and a natural number $n$, decide whether there is a\nterm $t$ of size $\\leq n$ reachable from $s$ using the rules of $R$. We\ninvestigate the complexity of this problem depending on how termination of $R$\ncan be established. We show that the problem is in general NP-complete for\nlength-reducing term rewriting systems. Its complexity increases to\nN2ExpTime-complete (NExpTime-complete) if termination is proved using a\n(linear) polynomial order and to PSpace-complete for systems whose termination\ncan be shown using a restricted class of Knuth-Bendix orders. Confluence\nreduces the complexity to P for the length-reducing case, but has no effect on\nthe worst-case complexity in the other two cases. Finally, we consider the\nlarge term reachability problem, a variant of the problem where we are\ninterested in reachability of a term of size $\\geq n$. It turns out that this\nseemingly innocuous modification in some cases changes the complexity of the\nproblem, which may also become dependent on whether the number $n$ is is\nrepresented in unary or binary encoding, whereas this makes no difference for\nthe complexity of the small term reachability problem."
                },
                "authors": [
                    {
                        "name": "Franz Baader"
                    },
                    {
                        "name": "Jürgen Giesl"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Giesl"
                },
                "author": "Jürgen Giesl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00004v2",
                "updated": "2025-06-23T14:45:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    45,
                    7,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-18T09:46:19Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    46,
                    19,
                    3,
                    200,
                    0
                ],
                "title": "Handling Numeric Expressions in Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling Numeric Expressions in Automatic Speech Recognition"
                },
                "summary": "This paper addresses the problem of correctly formatting numeric expressions\nin automatic speech recognition (ASR) transcripts. This is challenging since\nthe expected transcript format depends on the context, e.g., 1945 (year) vs.\n19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize\nand format numeric expressions such as years, timestamps, currency amounts, and\nquantities. For the end-to-end approach, we employed a data generation strategy\nusing a large language model (LLM) together with a text to speech (TTS) model\nto generate adaptation data. The results on our test data set show that while\napproaches based on LLMs perform well in recognizing formatted numeric\nexpressions, adapted end-to-end models offer competitive performance with the\nadvantage of lower latency and inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of correctly formatting numeric expressions\nin automatic speech recognition (ASR) transcripts. This is challenging since\nthe expected transcript format depends on the context, e.g., 1945 (year) vs.\n19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize\nand format numeric expressions such as years, timestamps, currency amounts, and\nquantities. For the end-to-end approach, we employed a data generation strategy\nusing a large language model (LLM) together with a text to speech (TTS) model\nto generate adaptation data. The results on our test data set show that while\napproaches based on LLMs perform well in recognizing formatted numeric\nexpressions, adapted end-to-end models offer competitive performance with the\nadvantage of lower latency and inference cost."
                },
                "authors": [
                    {
                        "name": "Christian Huber"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00140v2",
                "updated": "2025-06-23T14:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    44,
                    25,
                    0,
                    174,
                    0
                ],
                "published": "2024-09-30T18:21:26Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    21,
                    26,
                    0,
                    274,
                    0
                ],
                "title": "Probing fermionic asymmetric dark matter cores using global neutron star\n  properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing fermionic asymmetric dark matter cores using global neutron star\n  properties"
                },
                "summary": "It is possible for asymmetric dark matter (ADM) to accumulate in neutron star\ninteriors and affect their global properties. Considering the effects of this\naccumulation, neutron star mass-radius measurements can deliver new insights\ninto the cold dense matter equation of state (EoS). In this paper, we employ\nBayesian parameter estimation using real and synthetic neutron star mass-radius\ndata to infer constraints on the combined baryonic matter and fermionic ADM\nEoS, where the fermionic ADM forms a core in the neutron star interior. Using\ncurrently available mass-radius data, we find that the lower bound of the ratio\nbetween ADM effective self-repulsion strength ($g_\\chi/m_\\phi$) and particle\nmass ($m_\\chi$) can be constrained at the 68\\% (95\\%) credible level to\n$10^{-6.59}$ ($10^{-7.77}$). We also find that, if neutron star mass-radius\nmeasurement uncertainties are reduced to the 2\\% level, the constraints on the\nlower bound of the ratio of $g_\\chi/m_\\phi$ to $m_\\chi$ can be improved to\n$10^{-6.49}$ and $10^{-7.68}$ at the 68\\% and 95\\% credible levels,\nrespectively. However, all other combinations, of $m_\\chi$, $g_\\chi$, and the\nADM mass-fraction, $F_\\chi$, (i.e., the ratio of the gravitational ADM mass to\nthe gravitational mass of the neutron star) are unconstrained. Furthermore, in\nthe pressure-energy density and mass-radius planes, the inferences which\ninclude the possibility of fermionic ADM cores are nearly identical with the\ninferences that neglect fermionic ADM for $F_\\chi \\leq 1.7\\%$ and neutron star\nmass-radius uncertainties $\\geq 2\\%$. Therefore, we find that neutron star\nmass-radius measurements can constrain the ratio of $g_\\chi/m_\\phi$ to $m_\\chi$\nand that neutron stars with ADM are indistinguishable from purely baryonic\nstars. This implies that neutron stars with ADM are equally as consistent with\nthe available mass-radius data as neutron stars without ADM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is possible for asymmetric dark matter (ADM) to accumulate in neutron star\ninteriors and affect their global properties. Considering the effects of this\naccumulation, neutron star mass-radius measurements can deliver new insights\ninto the cold dense matter equation of state (EoS). In this paper, we employ\nBayesian parameter estimation using real and synthetic neutron star mass-radius\ndata to infer constraints on the combined baryonic matter and fermionic ADM\nEoS, where the fermionic ADM forms a core in the neutron star interior. Using\ncurrently available mass-radius data, we find that the lower bound of the ratio\nbetween ADM effective self-repulsion strength ($g_\\chi/m_\\phi$) and particle\nmass ($m_\\chi$) can be constrained at the 68\\% (95\\%) credible level to\n$10^{-6.59}$ ($10^{-7.77}$). We also find that, if neutron star mass-radius\nmeasurement uncertainties are reduced to the 2\\% level, the constraints on the\nlower bound of the ratio of $g_\\chi/m_\\phi$ to $m_\\chi$ can be improved to\n$10^{-6.49}$ and $10^{-7.68}$ at the 68\\% and 95\\% credible levels,\nrespectively. However, all other combinations, of $m_\\chi$, $g_\\chi$, and the\nADM mass-fraction, $F_\\chi$, (i.e., the ratio of the gravitational ADM mass to\nthe gravitational mass of the neutron star) are unconstrained. Furthermore, in\nthe pressure-energy density and mass-radius planes, the inferences which\ninclude the possibility of fermionic ADM cores are nearly identical with the\ninferences that neglect fermionic ADM for $F_\\chi \\leq 1.7\\%$ and neutron star\nmass-radius uncertainties $\\geq 2\\%$. Therefore, we find that neutron star\nmass-radius measurements can constrain the ratio of $g_\\chi/m_\\phi$ to $m_\\chi$\nand that neutron stars with ADM are indistinguishable from purely baryonic\nstars. This implies that neutron stars with ADM are equally as consistent with\nthe available mass-radius data as neutron stars without ADM."
                },
                "authors": [
                    {
                        "name": "Nathan Rutherford"
                    },
                    {
                        "name": "Chanda Prescod-Weinstein"
                    },
                    {
                        "name": "Anna Watts"
                    }
                ],
                "author_detail": {
                    "name": "Anna Watts"
                },
                "author": "Anna Watts",
                "arxiv_doi": "10.1103/PhysRevD.111.123034",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.123034",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.00140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "26 pages, 8 figures. This paper was made to be as similar to the PRD\n  version as possible",
                "arxiv_journal_ref": "Phys. Rev. D 111, 123034 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00839v2",
                "updated": "2025-06-23T14:43:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    43,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-01T14:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    28,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "Context-Aware Human Behavior Prediction Using Multimodal Large Language\n  Models: Challenges and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Human Behavior Prediction Using Multimodal Large Language\n  Models: Challenges and Insights"
                },
                "summary": "Predicting human behavior in shared environments is crucial for safe and\nefficient human-robot interaction. Traditional data-driven methods to that end\nare pre-trained on domain-specific datasets, activity types, and prediction\nhorizons. In contrast, the recent breakthroughs in Large Language Models (LLMs)\npromise open-ended cross-domain generalization to describe various human\nactivities and make predictions in any context. In particular, Multimodal LLMs\n(MLLMs) are able to integrate information from various sources, achieving more\ncontextual awareness and improved scene understanding. The difficulty in\napplying general-purpose MLLMs directly for prediction stems from their limited\ncapacity for processing large input sequences, sensitivity to prompt design,\nand expensive fine-tuning. In this paper, we present a systematic analysis of\napplying pre-trained MLLMs for context-aware human behavior prediction. To this\nend, we introduce a modular multimodal human activity prediction framework that\nallows us to benchmark various MLLMs, input variations, In-Context Learning\n(ICL), and autoregressive techniques. Our evaluation indicates that the\nbest-performing framework configuration is able to reach 92.8% semantic\nsimilarity and 66.1% exact label accuracy in predicting human behaviors in the\ntarget frame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting human behavior in shared environments is crucial for safe and\nefficient human-robot interaction. Traditional data-driven methods to that end\nare pre-trained on domain-specific datasets, activity types, and prediction\nhorizons. In contrast, the recent breakthroughs in Large Language Models (LLMs)\npromise open-ended cross-domain generalization to describe various human\nactivities and make predictions in any context. In particular, Multimodal LLMs\n(MLLMs) are able to integrate information from various sources, achieving more\ncontextual awareness and improved scene understanding. The difficulty in\napplying general-purpose MLLMs directly for prediction stems from their limited\ncapacity for processing large input sequences, sensitivity to prompt design,\nand expensive fine-tuning. In this paper, we present a systematic analysis of\napplying pre-trained MLLMs for context-aware human behavior prediction. To this\nend, we introduce a modular multimodal human activity prediction framework that\nallows us to benchmark various MLLMs, input variations, In-Context Learning\n(ICL), and autoregressive techniques. Our evaluation indicates that the\nbest-performing framework configuration is able to reach 92.8% semantic\nsimilarity and 66.1% exact label accuracy in predicting human behaviors in the\ntarget frame."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Lino Lerch"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Andrey Rudenko"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Timo Ropinski"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "arxiv_comment": "Accepted at IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18703v1",
                "updated": "2025-06-23T14:42:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    42,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:42:03Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    42,
                    3,
                    0,
                    174,
                    0
                ],
                "title": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic\n  Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic\n  Speech Recognition"
                },
                "summary": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate."
                },
                "authors": [
                    {
                        "name": "Christian Huber"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18846v2",
                "updated": "2025-06-23T14:37:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    37,
                    53,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-24T19:42:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    19,
                    42,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review\n  and Taxonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review\n  and Taxonomy"
                },
                "summary": "Sixth Generation (6G) wireless networks, which are expected to be deployed in\nthe 2030s, have already created great excitement in academia and the private\nsector with their extremely high communication speed and low latency rates.\nHowever, despite the ultra-low latency, high throughput, and AI-assisted\norchestration capabilities they promise, they are vulnerable to stealthy and\nlong-term Advanced Persistent Threats (APTs). Large Language Models (LLMs)\nstand out as an ideal candidate to fill this gap with their high success in\nsemantic reasoning and threat intelligence. In this paper, we present a\ncomprehensive systematic review and taxonomy study for LLM-assisted APT\ndetection in 6G networks. We address five research questions, namely, semantic\nmerging of fragmented logs, encrypted traffic analysis, edge distribution\nconstraints, dataset/modeling techniques, and reproducibility trends, by\nleveraging most recent studies on the intersection of LLMs, APTs, and 6G\nwireless networks. We identify open challenges such as explainability gaps,\ndata scarcity, edge hardware limitations, and the need for real-time\nslicing-aware adaptation by presenting various taxonomies such as granularity,\ndeployment models, and kill chain stages. We then conclude the paper by\nproviding several research gaps in 6G infrastructures for future researchers.\nTo the best of our knowledge, this paper is the first comprehensive systematic\nreview and classification study on LLM-based APT detection in 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sixth Generation (6G) wireless networks, which are expected to be deployed in\nthe 2030s, have already created great excitement in academia and the private\nsector with their extremely high communication speed and low latency rates.\nHowever, despite the ultra-low latency, high throughput, and AI-assisted\norchestration capabilities they promise, they are vulnerable to stealthy and\nlong-term Advanced Persistent Threats (APTs). Large Language Models (LLMs)\nstand out as an ideal candidate to fill this gap with their high success in\nsemantic reasoning and threat intelligence. In this paper, we present a\ncomprehensive systematic review and taxonomy study for LLM-assisted APT\ndetection in 6G networks. We address five research questions, namely, semantic\nmerging of fragmented logs, encrypted traffic analysis, edge distribution\nconstraints, dataset/modeling techniques, and reproducibility trends, by\nleveraging most recent studies on the intersection of LLMs, APTs, and 6G\nwireless networks. We identify open challenges such as explainability gaps,\ndata scarcity, edge hardware limitations, and the need for real-time\nslicing-aware adaptation by presenting various taxonomies such as granularity,\ndeployment models, and kill chain stages. We then conclude the paper by\nproviding several research gaps in 6G infrastructures for future researchers.\nTo the best of our knowledge, this paper is the first comprehensive systematic\nreview and classification study on LLM-based APT detection in 6G networks."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Yaser Khamayseh"
                    },
                    {
                        "name": "Suhib Bani Melhem"
                    },
                    {
                        "name": "Abdulmalik Alwarafy"
                    }
                ],
                "author_detail": {
                    "name": "Abdulmalik Alwarafy"
                },
                "author": "Abdulmalik Alwarafy",
                "arxiv_comment": "22 pages, 11 figures, 8 tables. Submitted to Computer Science Review\n  (Elsevier), May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18689v1",
                "updated": "2025-06-23T14:28:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    28,
                    30,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:28:30Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    28,
                    30,
                    0,
                    174,
                    0
                ],
                "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments"
                },
                "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions."
                },
                "authors": [
                    {
                        "name": "Alessandro Saviolo"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12557v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12557v3",
                "updated": "2025-06-23T14:26:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    26,
                    35,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-16T13:34:40Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    34,
                    40,
                    2,
                    290,
                    0
                ],
                "title": "One Step Diffusion via Shortcut Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Step Diffusion via Shortcut Models"
                },
                "summary": "Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models and flow-matching models have enabled generating diverse and\nrealistic images by learning to transfer noise to data. However, sampling from\nthese models involves iterative denoising over many neural network passes,\nmaking generation slow and expensive. Previous approaches for speeding up\nsampling require complex training regimes, such as multiple training phases,\nmultiple networks, or fragile scheduling. We introduce shortcut models, a\nfamily of generative models that use a single network and training phase to\nproduce high-quality samples in a single or multiple sampling steps. Shortcut\nmodels condition the network not only on the current noise level but also on\nthe desired step size, allowing the model to skip ahead in the generation\nprocess. Across a wide range of sampling step budgets, shortcut models\nconsistently produce higher quality samples than previous approaches, such as\nconsistency models and reflow. Compared to distillation, shortcut models reduce\ncomplexity to a single network and training phase and additionally allow\nvarying step budgets at inference time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Danijar Hafner"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12557v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12557v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18683v1",
                "updated": "2025-06-23T14:25:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    25,
                    40,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:25:40Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    25,
                    40,
                    0,
                    174,
                    0
                ],
                "title": "SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape\n  Point Clouds from RGB Images for 2D Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape\n  Point Clouds from RGB Images for 2D Classification"
                },
                "summary": "We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image\nclassification architecture that integrates 3D point cloud representations\ninferred directly from RGB images. Our key contribution lies in a\npixel-to-point transformation that converts 2D object masks into 3D point\nclouds, enabling the fusion of texture-based and geometric features for\nenhanced classification performance. SIM-Net is particularly well-suited for\nthe classification of digitized herbarium specimens (a task made challenging by\nheterogeneous backgrounds), non-plant elements, and occlusions that compromise\nconventional image-based models. To address these issues, SIM-Net employs a\nsegmentation-based preprocessing step to extract object masks prior to 3D point\ncloud generation. The architecture comprises a CNN encoder for 2D image\nfeatures and a PointNet-based encoder for geometric features, which are fused\ninto a unified latent space. Experimental evaluations on herbarium datasets\ndemonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of\nup to 9.9% in accuracy and 12.3% in F-score. It also surpasses several\ntransformer-based state-of-the-art architectures, highlighting the benefits of\nincorporating 3D structural reasoning into 2D image classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image\nclassification architecture that integrates 3D point cloud representations\ninferred directly from RGB images. Our key contribution lies in a\npixel-to-point transformation that converts 2D object masks into 3D point\nclouds, enabling the fusion of texture-based and geometric features for\nenhanced classification performance. SIM-Net is particularly well-suited for\nthe classification of digitized herbarium specimens (a task made challenging by\nheterogeneous backgrounds), non-plant elements, and occlusions that compromise\nconventional image-based models. To address these issues, SIM-Net employs a\nsegmentation-based preprocessing step to extract object masks prior to 3D point\ncloud generation. The architecture comprises a CNN encoder for 2D image\nfeatures and a PointNet-based encoder for geometric features, which are fused\ninto a unified latent space. Experimental evaluations on herbarium datasets\ndemonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of\nup to 9.9% in accuracy and 12.3% in F-score. It also surpasses several\ntransformer-based state-of-the-art architectures, highlighting the benefits of\nincorporating 3D structural reasoning into 2D image classification tasks."
                },
                "authors": [
                    {
                        "name": "Youcef Sklab"
                    },
                    {
                        "name": "Hanane Ariouat"
                    },
                    {
                        "name": "Eric Chenin"
                    },
                    {
                        "name": "Edi Prifti"
                    },
                    {
                        "name": "Jean-Daniel Zucker"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Daniel Zucker"
                },
                "author": "Jean-Daniel Zucker",
                "arxiv_comment": "25 pages, 9 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12818v2",
                "updated": "2025-06-23T14:24:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    24,
                    28,
                    0,
                    174,
                    0
                ],
                "published": "2024-06-30T15:42:18Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    15,
                    42,
                    18,
                    6,
                    182,
                    0
                ],
                "title": "\"I understand why I got this grade\": Automatic Short Answer Grading with\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I understand why I got this grade\": Automatic Short Answer Grading with\n  Feedback"
                },
                "summary": "In recent years, there has been a growing interest in using Artificial\nIntelligence (AI) to automate student assessment in education. Among different\ntypes of assessments, summative assessments play a crucial role in evaluating a\nstudent's understanding level of a course. Such examinations often involve\nshort-answer questions. However, grading these responses and providing\nmeaningful feedback manually at scale is both time-consuming and\nlabor-intensive. Feedback is particularly important, as it helps students\nrecognize their strengths and areas for improvement. Despite the importance of\nthis task, there is a significant lack of publicly available datasets that\nsupport automatic short-answer grading with feedback generation. To address\nthis gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset\ndesigned for automatic short-answer grading with feedback. The dataset covers a\ndiverse range of subjects, questions, and answer patterns from multiple\nengineering domains and contains ~5.8k data points. We incorporate feedback\ninto our dataset by leveraging the generative capabilities of state-of-the-art\nlarge language models (LLMs) using our Label-Aware Synthetic Feedback\nGeneration (LASFG) strategy. This paper underscores the importance of enhanced\nfeedback in practical educational settings, outlines dataset annotation and\nfeedback generation processes, conducts a thorough EngSAF analysis, and\nprovides different LLMs-based zero-shot and finetuned baselines for future\ncomparison. The best-performing model (Mistral-7B) achieves an overall accuracy\nof 75.4% and 58.7% on unseen answers and unseen question test sets,\nrespectively. Additionally, we demonstrate the efficiency and effectiveness of\nour ASAG system through its deployment in a real-world end-semester exam at a\nreputed institute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a growing interest in using Artificial\nIntelligence (AI) to automate student assessment in education. Among different\ntypes of assessments, summative assessments play a crucial role in evaluating a\nstudent's understanding level of a course. Such examinations often involve\nshort-answer questions. However, grading these responses and providing\nmeaningful feedback manually at scale is both time-consuming and\nlabor-intensive. Feedback is particularly important, as it helps students\nrecognize their strengths and areas for improvement. Despite the importance of\nthis task, there is a significant lack of publicly available datasets that\nsupport automatic short-answer grading with feedback generation. To address\nthis gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset\ndesigned for automatic short-answer grading with feedback. The dataset covers a\ndiverse range of subjects, questions, and answer patterns from multiple\nengineering domains and contains ~5.8k data points. We incorporate feedback\ninto our dataset by leveraging the generative capabilities of state-of-the-art\nlarge language models (LLMs) using our Label-Aware Synthetic Feedback\nGeneration (LASFG) strategy. This paper underscores the importance of enhanced\nfeedback in practical educational settings, outlines dataset annotation and\nfeedback generation processes, conducts a thorough EngSAF analysis, and\nprovides different LLMs-based zero-shot and finetuned baselines for future\ncomparison. The best-performing model (Mistral-7B) achieves an overall accuracy\nof 75.4% and 58.7% on unseen answers and unseen question test sets,\nrespectively. Additionally, we demonstrate the efficiency and effectiveness of\nour ASAG system through its deployment in a real-world end-semester exam at a\nreputed institute."
                },
                "authors": [
                    {
                        "name": "Dishank Aggarwal"
                    },
                    {
                        "name": "Pritam Sil"
                    },
                    {
                        "name": "Bhaskaran Raman"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18680v1",
                "updated": "2025-06-23T14:22:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    22,
                    50,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:22:50Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    22,
                    50,
                    0,
                    174,
                    0
                ],
                "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical\n  Masked Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical\n  Masked Modeling"
                },
                "summary": "We present DuetGen, a novel framework for generating interactive two-person\ndances from music. The key challenge of this task lies in the inherent\ncomplexities of two-person dance interactions, where the partners need to\nsynchronize both with each other and with the music. Inspired by the recent\nadvances in motion synthesis, we propose a two-stage solution: encoding\ntwo-person motions into discrete tokens and then generating these tokens from\nmusic. To effectively capture intricate interactions, we represent both\ndancers' motions as a unified whole to learn the necessary motion tokens, and\nadopt a coarse-to-fine learning strategy in both the stages. Our first stage\nutilizes a VQ-VAE that hierarchically separates high-level semantic features at\na coarse temporal resolution from low-level details at a finer resolution,\nproducing two discrete token sequences at different abstraction levels.\nSubsequently, in the second stage, two generative masked transformers learn to\nmap music signals to these dance tokens: the first producing high-level\nsemantic tokens, and the second, conditioned on music and these semantic\ntokens, producing the low-level tokens. We train both transformers to learn to\npredict randomly masked tokens within the sequence, enabling them to\niteratively generate motion tokens by filling an empty token sequence during\ninference. Through the hierarchical masked modeling and dedicated interaction\nrepresentation, DuetGen achieves the generation of synchronized and interactive\ntwo-person dances across various genres. Extensive experiments and user studies\non a benchmark duet dance dataset demonstrate state-of-the-art performance of\nDuetGen in motion realism, music-dance alignment, and partner coordination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DuetGen, a novel framework for generating interactive two-person\ndances from music. The key challenge of this task lies in the inherent\ncomplexities of two-person dance interactions, where the partners need to\nsynchronize both with each other and with the music. Inspired by the recent\nadvances in motion synthesis, we propose a two-stage solution: encoding\ntwo-person motions into discrete tokens and then generating these tokens from\nmusic. To effectively capture intricate interactions, we represent both\ndancers' motions as a unified whole to learn the necessary motion tokens, and\nadopt a coarse-to-fine learning strategy in both the stages. Our first stage\nutilizes a VQ-VAE that hierarchically separates high-level semantic features at\na coarse temporal resolution from low-level details at a finer resolution,\nproducing two discrete token sequences at different abstraction levels.\nSubsequently, in the second stage, two generative masked transformers learn to\nmap music signals to these dance tokens: the first producing high-level\nsemantic tokens, and the second, conditioned on music and these semantic\ntokens, producing the low-level tokens. We train both transformers to learn to\npredict randomly masked tokens within the sequence, enabling them to\niteratively generate motion tokens by filling an empty token sequence during\ninference. Through the hierarchical masked modeling and dedicated interaction\nrepresentation, DuetGen achieves the generation of synchronized and interactive\ntwo-person dances across various genres. Extensive experiments and user studies\non a benchmark duet dance dataset demonstrate state-of-the-art performance of\nDuetGen in motion realism, music-dance alignment, and partner coordination."
                },
                "authors": [
                    {
                        "name": "Anindita Ghosh"
                    },
                    {
                        "name": "Bing Zhou"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Vladislav Golyanik"
                    },
                    {
                        "name": "Christian Theobalt"
                    },
                    {
                        "name": "Philipp Slusallek"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "arxiv_comment": "11 pages, 7 figures, 2 tables, accepted in ACM Siggraph 2025\n  conference track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18674v1",
                "updated": "2025-06-23T14:18:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    18,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:18:46Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    18,
                    46,
                    0,
                    174,
                    0
                ],
                "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?"
                },
                "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus."
                },
                "authors": [
                    {
                        "name": "Raquel Ferrando"
                    },
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Gonzalo Martínez"
                    },
                    {
                        "name": "Pedro Reviriego"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Reviriego"
                },
                "author": "Pedro Reviriego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18670v1",
                "updated": "2025-06-23T14:14:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    14,
                    43,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:14:43Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    14,
                    43,
                    0,
                    174,
                    0
                ],
                "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation"
                },
                "summary": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever."
                },
                "authors": [
                    {
                        "name": "Jingming Liu"
                    },
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Yao-Xiang Ding"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Kun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhou"
                },
                "author": "Kun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02186v2",
                "updated": "2025-06-23T14:01:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    1,
                    7,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-04T01:49:27Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    1,
                    49,
                    27,
                    1,
                    63,
                    0
                ],
                "title": "Residual test to search for microlensing signatures in strongly lensed\n  gravitational wave signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual test to search for microlensing signatures in strongly lensed\n  gravitational wave signals"
                },
                "summary": "When a gravitational wave signal encounters a massive object, such as a\ngalaxy or galaxy cluster, it undergoes strong gravitational lensing, producing\nmultiple copies of the original signal. These strongly lensed signals exhibit\nidentical waveform morphology in the frequency domain, allowing analysis\nwithout the need for complex lens models. However, stellar fields and dark\nmatter substructures within the galactic lens introduce microlensing effects\nthat alter individual signal morphologies. Identifying these microlensing\nsignatures is computationally challenging within Bayesian frameworks. In this\nstudy, we propose a residual test to efficiently search for microlensing\nsignatures by leveraging the fact that current Bayesian inference pipelines are\noptimized solely for the strong lensing hypothesis. Using cross-correlation\ntechniques, we investigate the microlensing-induced deviations from the strong\nhypothesis, which are imprinted in the residuals. Most simulated signals from\nour realistic microlensing populations exhibit small mismatches between the\nmicrolensed and unlensed waveforms, but a fraction show significant deviations.\nWe find that 28% (52%) and 34% (66%)of microlensed events with mismatch > 0.03\nand > 0.1, respectively, can be discerned with O4 (O5) detector sensitivities,\nwhich demonstrates that high-mismatch events are more likely to be identified\nas microlensed. Including all events from a realistic population, 11% (21.5%)\nare identifiable with O4 (O5) sensitivity using our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a gravitational wave signal encounters a massive object, such as a\ngalaxy or galaxy cluster, it undergoes strong gravitational lensing, producing\nmultiple copies of the original signal. These strongly lensed signals exhibit\nidentical waveform morphology in the frequency domain, allowing analysis\nwithout the need for complex lens models. However, stellar fields and dark\nmatter substructures within the galactic lens introduce microlensing effects\nthat alter individual signal morphologies. Identifying these microlensing\nsignatures is computationally challenging within Bayesian frameworks. In this\nstudy, we propose a residual test to efficiently search for microlensing\nsignatures by leveraging the fact that current Bayesian inference pipelines are\noptimized solely for the strong lensing hypothesis. Using cross-correlation\ntechniques, we investigate the microlensing-induced deviations from the strong\nhypothesis, which are imprinted in the residuals. Most simulated signals from\nour realistic microlensing populations exhibit small mismatches between the\nmicrolensed and unlensed waveforms, but a fraction show significant deviations.\nWe find that 28% (52%) and 34% (66%)of microlensed events with mismatch > 0.03\nand > 0.1, respectively, can be discerned with O4 (O5) detector sensitivities,\nwhich demonstrates that high-mismatch events are more likely to be identified\nas microlensed. Including all events from a realistic population, 11% (21.5%)\nare identifiable with O4 (O5) sensitivity using our approach."
                },
                "authors": [
                    {
                        "name": "Eungwang Seo"
                    },
                    {
                        "name": "Xikai Shan"
                    },
                    {
                        "name": "Justin Janquart"
                    },
                    {
                        "name": "Otto A. Hannuksela"
                    },
                    {
                        "name": "Martin A. Hendry"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "arxiv_comment": "15 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18656v1",
                "updated": "2025-06-23T13:56:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    56,
                    43,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:56:43Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    56,
                    43,
                    0,
                    174,
                    0
                ],
                "title": "A Random Matrix Analysis of In-context Memorization for Nonlinear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Random Matrix Analysis of In-context Memorization for Nonlinear\n  Attention"
                },
                "summary": "Attention mechanisms have revolutionized machine learning (ML) by enabling\nefficient modeling of global dependencies across inputs. Their inherently\nparallelizable structures allow for efficient scaling with the exponentially\nincreasing size of both pretrained data and model parameters. Yet, despite\ntheir central role as the computational backbone of modern large language\nmodels (LLMs), the theoretical understanding of Attentions, especially in the\nnonlinear setting, remains limited.\n  In this paper, we provide a precise characterization of the \\emph{in-context\nmemorization error} of \\emph{nonlinear Attention}, in the high-dimensional\nproportional regime where the number of input tokens $n$ and their embedding\ndimension $p$ are both large and comparable. Leveraging recent advances in the\ntheory of large kernel random matrices, we show that nonlinear Attention\ntypically incurs higher memorization error than linear ridge regression on\nrandom inputs. However, this gap vanishes, and can even be reversed, when the\ninput exhibits statistical structure, particularly when the Attention weights\nalign with the input signal direction. Our results reveal how nonlinearity and\ninput structure interact with each other to govern the memorization performance\nof nonlinear Attention. The theoretical insights are supported by numerical\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized machine learning (ML) by enabling\nefficient modeling of global dependencies across inputs. Their inherently\nparallelizable structures allow for efficient scaling with the exponentially\nincreasing size of both pretrained data and model parameters. Yet, despite\ntheir central role as the computational backbone of modern large language\nmodels (LLMs), the theoretical understanding of Attentions, especially in the\nnonlinear setting, remains limited.\n  In this paper, we provide a precise characterization of the \\emph{in-context\nmemorization error} of \\emph{nonlinear Attention}, in the high-dimensional\nproportional regime where the number of input tokens $n$ and their embedding\ndimension $p$ are both large and comparable. Leveraging recent advances in the\ntheory of large kernel random matrices, we show that nonlinear Attention\ntypically incurs higher memorization error than linear ridge regression on\nrandom inputs. However, this gap vanishes, and can even be reversed, when the\ninput exhibits statistical structure, particularly when the Attention weights\nalign with the input signal direction. Our results reveal how nonlinearity and\ninput structure interact with each other to govern the memorization performance\nof nonlinear Attention. The theoretical insights are supported by numerical\nexperiments."
                },
                "authors": [
                    {
                        "name": "Zhenyu Liao"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "TianQi Hou"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Zenan Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zenan Ling"
                },
                "author": "Zenan Ling",
                "arxiv_comment": "40 pages, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11097v2",
                "updated": "2025-06-23T13:56:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    56,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-06T10:49:49Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    10,
                    49,
                    49,
                    4,
                    157,
                    0
                ],
                "title": "C-SEO Bench: Does Conversational SEO Work?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-SEO Bench: Does Conversational SEO Work?"
                },
                "summary": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench."
                },
                "authors": [
                    {
                        "name": "Haritz Puerto"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Tommaso Green"
                    },
                    {
                        "name": "Seong Joon Oh"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18652v1",
                "updated": "2025-06-23T13:54:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    54,
                    52,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:54:52Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    54,
                    52,
                    0,
                    174,
                    0
                ],
                "title": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (II): Unmeasured confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (II): Unmeasured confounders"
                },
                "summary": "One obstacle to ``elevating\" correlation to causation is the phenomenon of\nconfounding, i.e., when a correlation between two variables exists because both\nvariables are in fact caused by a third variable. The situation where the\nconfounders are measured is examined in an earlier, accompanying article. Here,\nit is shown that even when the confounding variables are not measured, it is\nstill possible to estimate the causal effect via a regression-based method that\nuses the notion of Instrumental Variables. Using meteorological data set,\nsimilar to that in the sister article, a number of different estimates of the\ncausal effect are compared and contrasted. It is shown that the Instrumental\nVariable results based on unmeasured confounders are consistent with those of\nthe sister article where confounders are measured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One obstacle to ``elevating\" correlation to causation is the phenomenon of\nconfounding, i.e., when a correlation between two variables exists because both\nvariables are in fact caused by a third variable. The situation where the\nconfounders are measured is examined in an earlier, accompanying article. Here,\nit is shown that even when the confounding variables are not measured, it is\nstill possible to estimate the causal effect via a regression-based method that\nuses the notion of Instrumental Variables. Using meteorological data set,\nsimilar to that in the sister article, a number of different estimates of the\ncausal effect are compared and contrasted. It is shown that the Instrumental\nVariable results based on unmeasured confounders are consistent with those of\nthe sister article where confounders are measured."
                },
                "authors": [
                    {
                        "name": "Caren Marzban"
                    },
                    {
                        "name": "Yikun Zhang"
                    },
                    {
                        "name": "Nicholas Bond"
                    },
                    {
                        "name": "Michael Richman"
                    }
                ],
                "author_detail": {
                    "name": "Michael Richman"
                },
                "author": "Michael Richman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18631v2",
                "updated": "2025-06-24T07:07:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    7,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T13:36:24Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    36,
                    24,
                    0,
                    174,
                    0
                ],
                "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
                },
                "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Jiarui Yu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "arxiv_comment": "10 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18628v1",
                "updated": "2025-06-23T13:35:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    35,
                    5,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:35:05Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    35,
                    5,
                    0,
                    174,
                    0
                ],
                "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention\n  Scores in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AggTruth: Contextual Hallucination Detection using Aggregated Attention\n  Scores in LLMs"
                },
                "summary": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results."
                },
                "authors": [
                    {
                        "name": "Piotr Matys"
                    },
                    {
                        "name": "Jan Eliasz"
                    },
                    {
                        "name": "Konrad Kiełczyński"
                    },
                    {
                        "name": "Mikołaj Langner"
                    },
                    {
                        "name": "Teddy Ferdinan"
                    },
                    {
                        "name": "Jan Kocoń"
                    },
                    {
                        "name": "Przemysław Kazienko"
                    }
                ],
                "author_detail": {
                    "name": "Przemysław Kazienko"
                },
                "author": "Przemysław Kazienko",
                "arxiv_doi": "10.1007/978-3-031-97570-7_18",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97570-7_18",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.18628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICCS 2025 Workshops",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18623v1",
                "updated": "2025-06-23T13:29:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    29,
                    51,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:29:51Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    29,
                    51,
                    0,
                    174,
                    0
                ],
                "title": "Efficient and Generalizable Speaker Diarization via Structured Pruning\n  of Self-Supervised Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Generalizable Speaker Diarization via Structured Pruning\n  of Self-Supervised Models"
                },
                "summary": "Self-supervised learning (SSL) models such as WavLM have brought substantial\nimprovements to speaker diarization by providing rich contextual\nrepresentations. However, the high computational and memory costs of these\nmodels hinder their deployment in real-time and resource-constrained scenarios.\nIn this work, we present a comprehensive study on compressing SSL-based\ndiarization models through structured pruning guided by knowledge distillation.\nBuilding upon our previous work, we extend the analysis to include pruning\nobjectives based on multiply-accumulate operations (MACs), investigate\nmodule-wise and progressive pruning strategies, and examine the impact of\ntraining data quantity. Experimental results show that our method reduces model\nsize by up to 80% without degrading performance, achieving up to 4x faster\ninference on a single GPU. We further perform large-scale evaluations on a\ndiverse compound dataset comprising eight public diarization corpora, where our\nbest pruned model achieves state-of-the-art performance across most conditions.\nAdditionally, we show strong generalization to the CHiME-6 dataset, attaining\nperformance comparable to the third-place system in the CHiME-7 challenge\nwithout any domain adaptation. All models and code are publicly released to\nsupport reproducibility and future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning (SSL) models such as WavLM have brought substantial\nimprovements to speaker diarization by providing rich contextual\nrepresentations. However, the high computational and memory costs of these\nmodels hinder their deployment in real-time and resource-constrained scenarios.\nIn this work, we present a comprehensive study on compressing SSL-based\ndiarization models through structured pruning guided by knowledge distillation.\nBuilding upon our previous work, we extend the analysis to include pruning\nobjectives based on multiply-accumulate operations (MACs), investigate\nmodule-wise and progressive pruning strategies, and examine the impact of\ntraining data quantity. Experimental results show that our method reduces model\nsize by up to 80% without degrading performance, achieving up to 4x faster\ninference on a single GPU. We further perform large-scale evaluations on a\ndiverse compound dataset comprising eight public diarization corpora, where our\nbest pruned model achieves state-of-the-art performance across most conditions.\nAdditionally, we show strong generalization to the CHiME-6 dataset, attaining\nperformance comparable to the third-place system in the CHiME-7 challenge\nwithout any domain adaptation. All models and code are publicly released to\nsupport reproducibility and future research."
                },
                "authors": [
                    {
                        "name": "Jiangyu Han"
                    },
                    {
                        "name": "Petr Pálka"
                    },
                    {
                        "name": "Marc Delcroix"
                    },
                    {
                        "name": "Federico Landini"
                    },
                    {
                        "name": "Johan Rohdin"
                    },
                    {
                        "name": "Jan Cernocký"
                    },
                    {
                        "name": "Lukáš Burget"
                    }
                ],
                "author_detail": {
                    "name": "Lukáš Burget"
                },
                "author": "Lukáš Burget",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18621v1",
                "updated": "2025-06-23T13:28:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    28,
                    33,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:28:33Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    28,
                    33,
                    0,
                    174,
                    0
                ],
                "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified\n  Speeches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified\n  Speeches"
                },
                "summary": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact."
                },
                "authors": [
                    {
                        "name": "Alisa Barkar"
                    },
                    {
                        "name": "Mathieu Chollet"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Beatrice Biancardi"
                    },
                    {
                        "name": "Chloe Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloe Clavel"
                },
                "author": "Chloe Clavel",
                "arxiv_comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13744v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13744v5",
                "updated": "2025-06-23T13:08:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    8,
                    0,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-19T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty"
                },
                "summary": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximization frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-risks and by emphasizing\n'constraints' over 'preference'. This decomposes viable economic asset-pricing\ninto that of model and non-model risks separately, leading to a unique and\nconvenient model-risk pricing formula. Its parameter, a dynamically conserved\nconstant of model-risk inference, allows an integrated representation of\nex-ante risk-pricing and bias such that their ex-post impacts are disentangled\nvia well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns\nacquire a fresh significance: peak-reward reveals ex-ante risk-premia, and\npeak-location, bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximization frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-risks and by emphasizing\n'constraints' over 'preference'. This decomposes viable economic asset-pricing\ninto that of model and non-model risks separately, leading to a unique and\nconvenient model-risk pricing formula. Its parameter, a dynamically conserved\nconstant of model-risk inference, allows an integrated representation of\nex-ante risk-pricing and bias such that their ex-post impacts are disentangled\nvia well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns\nacquire a fresh significance: peak-reward reveals ex-ante risk-premia, and\npeak-location, bias."
                },
                "authors": [
                    {
                        "name": "Ken Kangda Wren"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kangda Wren"
                },
                "author": "Ken Kangda Wren",
                "arxiv_comment": "20 pages of main text (including title and abstract), 9 pages of\n  Appendix and Bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13744v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13744v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.MF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.MF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18602v1",
                "updated": "2025-06-23T13:03:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    3,
                    59,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:03:59Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    3,
                    59,
                    0,
                    174,
                    0
                ],
                "title": "Semantic similarity estimation for domain specific data using BERT and\n  other techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic similarity estimation for domain specific data using BERT and\n  other techniques"
                },
                "summary": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data."
                },
                "authors": [
                    {
                        "name": "R. Prashanth"
                    }
                ],
                "author_detail": {
                    "name": "R. Prashanth"
                },
                "author": "R. Prashanth",
                "arxiv_comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11069v2",
                "updated": "2025-06-23T13:01:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    1,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-14T04:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    26,
                    21,
                    4,
                    73,
                    0
                ],
                "title": "API Agents vs. GUI Agents: Divergence and Convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API Agents vs. GUI Agents: Divergence and Convergence"
                },
                "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18600v1",
                "updated": "2025-06-23T12:59:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    59,
                    34,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:59:34Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    59,
                    34,
                    0,
                    174,
                    0
                ],
                "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data\n  leakage\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reply to \"Emergent LLM behaviors are observationally equivalent to data\n  leakage\""
                },
                "summary": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions."
                },
                "authors": [
                    {
                        "name": "Ariel Flint Ashery"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    },
                    {
                        "name": "Andrea Baronchelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Baronchelli"
                },
                "author": "Andrea Baronchelli",
                "arxiv_comment": "Reply to arXiv:2505.23796",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18598v1",
                "updated": "2025-06-23T12:58:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    58,
                    54,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:58:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    58,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Training Wheels: Steering Vectors for Bias Correction at Inference\n  Time"
                },
                "summary": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models."
                },
                "authors": [
                    {
                        "name": "Aviral Gupta"
                    },
                    {
                        "name": "Armaan Sethi"
                    },
                    {
                        "name": "Ameesh Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Ameesh Sethi"
                },
                "author": "Ameesh Sethi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11552v2",
                "updated": "2025-06-23T12:56:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    56,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-14T16:15:54Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    16,
                    15,
                    54,
                    4,
                    73,
                    0
                ],
                "title": "Goal-oriented Spectrum Sharing: Trading Edge Inference Power for Data\n  Streaming Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented Spectrum Sharing: Trading Edge Inference Power for Data\n  Streaming Performance"
                },
                "summary": "We study the problem of spectrum sharing between goal-oriented (GO) and\nlegacy data-oriented (DO) systems. For the former, data quality and\nrepresentation is no longer optimized based on classical communication key\nperformance indicators, but rather configured on the fly to achieve the goal of\ncommunication with the least resource overhead. This paradigm can be followed\nto flexibly adapt wireless and in-network artificial intelligence operations\nacross different nodes (e.g., access points, users, sensors or actuators) to\ndata traffic, channel conditions, energy availability and distributed computing\ncapabilities. In this paper, we argue and demonstrate that computing and\nlearning/inference operation performance strongly affect lower layers, calling\nfor a real cross-layer optimization that encompasses physical and computation\nresource orchestration, up to the application level. Focusing on a\ncommunication channel shared among a GO and a DO user, we define a\ngoal-effective achievable rate region (GEARR), to assess the maximum data rate\nattainable by the latter, subject to goal achievement guarantees for the\nformer. Finally, we propose a cross-layer dynamic resource orchestration able\nto reach the boundaries of the GEARR, under different goal-effectiveness and\ncompute resource consumption constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of spectrum sharing between goal-oriented (GO) and\nlegacy data-oriented (DO) systems. For the former, data quality and\nrepresentation is no longer optimized based on classical communication key\nperformance indicators, but rather configured on the fly to achieve the goal of\ncommunication with the least resource overhead. This paradigm can be followed\nto flexibly adapt wireless and in-network artificial intelligence operations\nacross different nodes (e.g., access points, users, sensors or actuators) to\ndata traffic, channel conditions, energy availability and distributed computing\ncapabilities. In this paper, we argue and demonstrate that computing and\nlearning/inference operation performance strongly affect lower layers, calling\nfor a real cross-layer optimization that encompasses physical and computation\nresource orchestration, up to the application level. Focusing on a\ncommunication channel shared among a GO and a DO user, we define a\ngoal-effective achievable rate region (GEARR), to assess the maximum data rate\nattainable by the latter, subject to goal achievement guarantees for the\nformer. Finally, we propose a cross-layer dynamic resource orchestration able\nto reach the boundaries of the GEARR, under different goal-effectiveness and\ncompute resource consumption constraints."
                },
                "authors": [
                    {
                        "name": "Mattia Merluzzi"
                    },
                    {
                        "name": "Miltiadis C. Filippou"
                    }
                ],
                "author_detail": {
                    "name": "Miltiadis C. Filippou"
                },
                "author": "Miltiadis C. Filippou",
                "arxiv_comment": "Accepted for presentation at EURASIP EUSIPCO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11121v2",
                "updated": "2025-06-23T12:47:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    47,
                    53,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-15T12:09:26Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    12,
                    9,
                    26,
                    1,
                    105,
                    0
                ],
                "title": "$B$ meson decays to vector charmonium(like) states and a $K$ meson: the\n  role of final-state interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$B$ meson decays to vector charmonium(like) states and a $K$ meson: the\n  role of final-state interactions"
                },
                "summary": "A series of vector charmonium(like) states, accompanied by a $K$ meson, have\nbeen observed in the decays of $B$ meson. These processes are color-suppressed\nat the quark level, as inferred from topological diagram analysis. In this\nwork, we calculate the branching fractions of the decays $B \\to \\psi K$, where\n$\\psi$ denotes the charmonium(like) states $\\psi(1S)$, $\\psi(2S)$,\n$\\psi(4040)$, $\\psi(3770)$, and $\\psi(4160)$. Our analysis incorporates both\nshort-distance (naive factorization approach) and long-distance (final-state\ninteractions) contributions. Within reasonable parameters, our results align\nwith experimental data except for the $ \\psi(4160)$, suggesting its possible\nexotic nature. Furthermore, we find that long-distance contributions dominate\nthese decay processes, highlighting the crucial role of final-state\ninteractions in the productions of charmonium(like) states in $B$ decays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A series of vector charmonium(like) states, accompanied by a $K$ meson, have\nbeen observed in the decays of $B$ meson. These processes are color-suppressed\nat the quark level, as inferred from topological diagram analysis. In this\nwork, we calculate the branching fractions of the decays $B \\to \\psi K$, where\n$\\psi$ denotes the charmonium(like) states $\\psi(1S)$, $\\psi(2S)$,\n$\\psi(4040)$, $\\psi(3770)$, and $\\psi(4160)$. Our analysis incorporates both\nshort-distance (naive factorization approach) and long-distance (final-state\ninteractions) contributions. Within reasonable parameters, our results align\nwith experimental data except for the $ \\psi(4160)$, suggesting its possible\nexotic nature. Furthermore, we find that long-distance contributions dominate\nthese decay processes, highlighting the crucial role of final-state\ninteractions in the productions of charmonium(like) states in $B$ decays."
                },
                "authors": [
                    {
                        "name": "Qi-Wei Yuan"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Ming-Zhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Zhu Liu"
                },
                "author": "Ming-Zhu Liu",
                "arxiv_comment": "To appear in Phys. Rev. D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18582v1",
                "updated": "2025-06-23T12:35:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    35,
                    41,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:35:41Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    35,
                    41,
                    0,
                    174,
                    0
                ],
                "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Continuous Chain-of-Thought with Jacobi Iteration"
                },
                "summary": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT."
                },
                "authors": [
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Zhihao Teng"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18576v1",
                "updated": "2025-06-23T12:28:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    28,
                    13,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:28:13Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    28,
                    13,
                    0,
                    174,
                    0
                ],
                "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on\n  Zero-Shot LLM Classification Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on\n  Zero-Shot LLM Classification Performance"
                },
                "summary": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures."
                },
                "authors": [
                    {
                        "name": "Matteo Melis"
                    },
                    {
                        "name": "Gabriella Lapesa"
                    },
                    {
                        "name": "Dennis Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Assenmacher"
                },
                "author": "Dennis Assenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v2",
                "updated": "2025-06-23T12:21:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    21,
                    10,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v4",
                "updated": "2025-06-23T12:11:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    11,
                    44,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18559v1",
                "updated": "2025-06-23T12:11:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    11,
                    15,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:11:15Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    11,
                    15,
                    0,
                    174,
                    0
                ],
                "title": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing\n  Logic-RAG Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing\n  Logic-RAG Agent"
                },
                "summary": "Large language models excel at generating fluent text but frequently struggle\nwith structured reasoning involving temporal constraints, causal relationships,\nand probabilistic reasoning. To address these limitations, we propose Temporal\nCausal Probabilistic Description Logic (T-CPDL), an integrated framework that\nextends traditional Description Logic with temporal interval operators,\nexplicit causal relationships, and probabilistic annotations. We present two\ndistinct variants of T-CPDL: one capturing qualitative temporal relationships\nthrough Allen's interval algebra, and another variant enriched with explicit\ntimestamped causal assertions. Both variants share a unified logical structure,\nenabling complex reasoning tasks ranging from simple temporal ordering to\nnuanced probabilistic causation. Empirical evaluations on temporal reasoning\nand causal inference benchmarks confirm that T-CPDL substantially improves\ninference accuracy, interpretability, and confidence calibration of language\nmodel outputs. By delivering transparent reasoning paths and fine-grained\ntemporal and causal semantics, T-CPDL significantly enhances the capability of\nlanguage models to support robust, explainable, and trustworthy\ndecision-making. This work also lays the groundwork for developing advanced\nLogic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially\nboosting the reasoning capabilities and efficiency of knowledge graph-enhanced\nRAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at generating fluent text but frequently struggle\nwith structured reasoning involving temporal constraints, causal relationships,\nand probabilistic reasoning. To address these limitations, we propose Temporal\nCausal Probabilistic Description Logic (T-CPDL), an integrated framework that\nextends traditional Description Logic with temporal interval operators,\nexplicit causal relationships, and probabilistic annotations. We present two\ndistinct variants of T-CPDL: one capturing qualitative temporal relationships\nthrough Allen's interval algebra, and another variant enriched with explicit\ntimestamped causal assertions. Both variants share a unified logical structure,\nenabling complex reasoning tasks ranging from simple temporal ordering to\nnuanced probabilistic causation. Empirical evaluations on temporal reasoning\nand causal inference benchmarks confirm that T-CPDL substantially improves\ninference accuracy, interpretability, and confidence calibration of language\nmodel outputs. By delivering transparent reasoning paths and fine-grained\ntemporal and causal semantics, T-CPDL significantly enhances the capability of\nlanguage models to support robust, explainable, and trustworthy\ndecision-making. This work also lays the groundwork for developing advanced\nLogic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially\nboosting the reasoning capabilities and efficiency of knowledge graph-enhanced\nRAG systems."
                },
                "authors": [
                    {
                        "name": "Hong Qing Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Qing Yu"
                },
                "author": "Hong Qing Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05872v2",
                "updated": "2025-06-23T12:02:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    2,
                    25,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-09T12:21:35Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    12,
                    21,
                    35,
                    6,
                    40,
                    0
                ],
                "title": "Flexible inference of evolutionary accumulation dynamics using uncertain\n  observational data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible inference of evolutionary accumulation dynamics using uncertain\n  observational data"
                },
                "summary": "Understanding and predicting evolutionary accumulation pathways is a key\nobjective in many fields of research, ranging from classical evolutionary\nbiology to diverse applications in medicine. In this context, we are often\nconfronted with the problem that data is sparse and uncertain. To use the\navailable data as best as possible, inference approaches that can handle this\nuncertainty are required. One way that allows us to use not only\ncross-sectional data, but also phylogenetic related and longitudinal data, is\nusing `hypercubic inference' models. In this article we introduce HyperLAU, a\nnew algorithm for hypercubic inference that makes it possible to use datasets\nincluding uncertainties for learning evolutionary pathways. Expanding the\nflexibility of accumulation modelling, HyperLAU allows us to infer dynamic\npathways and interactions between features, even when large sets of particular\nfeatures are unobserved across the source dataset. We show that HyperLAU is\nable to highlight the main pathways found by other tools, even when up to 50%\nof the features in the input data are uncertain. Additionally, we demonstrate\nhow it can help to overcome possible biases that can occur then reducing the\nused data by excluding uncertain parts. We illustrate the approach with a case\nstudy on multidrug resistance in tuberculosis, showing that HyperLAU allows\nmore flexible data and provides new information about evolutionary pathways\ncompared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting evolutionary accumulation pathways is a key\nobjective in many fields of research, ranging from classical evolutionary\nbiology to diverse applications in medicine. In this context, we are often\nconfronted with the problem that data is sparse and uncertain. To use the\navailable data as best as possible, inference approaches that can handle this\nuncertainty are required. One way that allows us to use not only\ncross-sectional data, but also phylogenetic related and longitudinal data, is\nusing `hypercubic inference' models. In this article we introduce HyperLAU, a\nnew algorithm for hypercubic inference that makes it possible to use datasets\nincluding uncertainties for learning evolutionary pathways. Expanding the\nflexibility of accumulation modelling, HyperLAU allows us to infer dynamic\npathways and interactions between features, even when large sets of particular\nfeatures are unobserved across the source dataset. We show that HyperLAU is\nable to highlight the main pathways found by other tools, even when up to 50%\nof the features in the input data are uncertain. Additionally, we demonstrate\nhow it can help to overcome possible biases that can occur then reducing the\nused data by excluding uncertain parts. We illustrate the approach with a case\nstudy on multidrug resistance in tuberculosis, showing that HyperLAU allows\nmore flexible data and provides new information about evolutionary pathways\ncompared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Jessica Renz"
                    },
                    {
                        "name": "Morten Brun"
                    },
                    {
                        "name": "Iain G. Johnston"
                    }
                ],
                "author_detail": {
                    "name": "Iain G. Johnston"
                },
                "author": "Iain G. Johnston",
                "arxiv_comment": "Added the case study in subsection 3.3 on tuberculosis data with\n  naturally included uncertainty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05348v2",
                "updated": "2025-06-23T11:59:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    59,
                    45,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-06T16:14:02Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    16,
                    14,
                    2,
                    4,
                    341,
                    0
                ],
                "title": "Accurate early detection of Parkinson's disease from SPECT imaging\n  through Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate early detection of Parkinson's disease from SPECT imaging\n  through Convolutional Neural Networks"
                },
                "summary": "Early and accurate detection of Parkinson's disease (PD) is a crucial\ndiagnostic challenge carrying immense clinical significance, for effective\ntreatment regimens and patient management. For instance, a group of subjects\ntermed SWEDD who are clinically diagnosed as PD, but show normal Single Photon\nEmission Computed Tomography (SPECT) scans, change their diagnosis as non-PD\nafter few years of follow up, and in the meantime, they are treated with PD\nmedications which do more harm than good. In this work, machine learning models\nare developed using features from SPECT images to detect early PD and SWEDD\nsubjects from normal. These models were observed to perform with high accuracy.\nIt is inferred from the study that these diagnostic models carry potential to\nhelp PD clinicians in the diagnostic process",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early and accurate detection of Parkinson's disease (PD) is a crucial\ndiagnostic challenge carrying immense clinical significance, for effective\ntreatment regimens and patient management. For instance, a group of subjects\ntermed SWEDD who are clinically diagnosed as PD, but show normal Single Photon\nEmission Computed Tomography (SPECT) scans, change their diagnosis as non-PD\nafter few years of follow up, and in the meantime, they are treated with PD\nmedications which do more harm than good. In this work, machine learning models\nare developed using features from SPECT images to detect early PD and SWEDD\nsubjects from normal. These models were observed to perform with high accuracy.\nIt is inferred from the study that these diagnostic models carry potential to\nhelp PD clinicians in the diagnostic process"
                },
                "authors": [
                    {
                        "name": "R. Prashanth"
                    }
                ],
                "author_detail": {
                    "name": "R. Prashanth"
                },
                "author": "R. Prashanth",
                "arxiv_doi": "10.36922/AIH025040005",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.36922/AIH025040005",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.05348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This article is accepted and published with revisions to the\n  Artificial Intelligence in Health journal (2025). The accepted article can be\n  accessed at https://doi.org/10.36922/AIH025040005",
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04365v2",
                "updated": "2025-06-23T11:56:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    56,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-06T05:30:10Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    5,
                    30,
                    10,
                    6,
                    96,
                    0
                ],
                "title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPDL: Automatic Prompt Optimization for LLM Agents"
                },
                "summary": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.06\\pm15.3$ percentage points),\nup to 68.9pp, and reveal that selected prompting strategies vary across models\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.06\\pm15.3$ percentage points),\nup to 68.9pp, and reveal that selected prompting strategies vary across models\nand tasks."
                },
                "authors": [
                    {
                        "name": "Claudio Spiess"
                    },
                    {
                        "name": "Mandana Vaziri"
                    },
                    {
                        "name": "Louis Mandel"
                    },
                    {
                        "name": "Martin Hirzel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Hirzel"
                },
                "author": "Martin Hirzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18543v1",
                "updated": "2025-06-23T11:53:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    53,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:53:31Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    53,
                    31,
                    0,
                    174,
                    0
                ],
                "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak\n  Attacks"
                },
                "summary": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wu"
                    },
                    {
                        "name": "Xiangman Li"
                    },
                    {
                        "name": "Jianbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Ni"
                },
                "author": "Jianbing Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21401v2",
                "updated": "2025-06-23T11:50:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    50,
                    47,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-28T18:06:19Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    18,
                    6,
                    19,
                    0,
                    302,
                    0
                ],
                "title": "The more accurately the metal-dependent star formation rate is modeled,\n  the larger the predicted excess of binary black hole mergers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The more accurately the metal-dependent star formation rate is modeled,\n  the larger the predicted excess of binary black hole mergers"
                },
                "summary": "As the number of gravitational-wave detections grows, the merger rate of\nbinary black holes (BBHs) can help us to constrain their formation, the\nproperties of their progenitors, and their birth environment. Here, we aim to\naddress the impact of the metal-dependent star formation rate (SFR) on the BBH\nmerger rate. To this end, we have developed a fully data-driven approach to\nmodel the metal-dependent SFR and coupled it to BBH evolution. We have adopted\nthe most up-to-date scaling relations, based on recent observational results,\nand we have studied how the BBH merger rate density varies over a wide grid of\ngalaxy and binary evolution parameters. Our results show that including a\nrealistic metal-dependent SFR evolution yields a value of the merger rate\ndensity which is too high compared to the one inferred from gravitational-wave\ndata. Moreover, variations in the SFR in low-mass galaxies ($M_\\ast \\lesssim\n10^8 \\mathrm{M}_{\\odot}$) do not contribute more than a factor $\\sim 2$ to the\noverall merger rate density at redshift $z=0$. These results suggest that the\ndiscrepancy between the BBH merger rate density inferred from data and\ntheoretical models is not caused by approximations in the treatment of the\nmetal-dependent SFR, but rather stems from stellar evolution models and/or BBH\nformation channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the number of gravitational-wave detections grows, the merger rate of\nbinary black holes (BBHs) can help us to constrain their formation, the\nproperties of their progenitors, and their birth environment. Here, we aim to\naddress the impact of the metal-dependent star formation rate (SFR) on the BBH\nmerger rate. To this end, we have developed a fully data-driven approach to\nmodel the metal-dependent SFR and coupled it to BBH evolution. We have adopted\nthe most up-to-date scaling relations, based on recent observational results,\nand we have studied how the BBH merger rate density varies over a wide grid of\ngalaxy and binary evolution parameters. Our results show that including a\nrealistic metal-dependent SFR evolution yields a value of the merger rate\ndensity which is too high compared to the one inferred from gravitational-wave\ndata. Moreover, variations in the SFR in low-mass galaxies ($M_\\ast \\lesssim\n10^8 \\mathrm{M}_{\\odot}$) do not contribute more than a factor $\\sim 2$ to the\noverall merger rate density at redshift $z=0$. These results suggest that the\ndiscrepancy between the BBH merger rate density inferred from data and\ntheoretical models is not caused by approximations in the treatment of the\nmetal-dependent SFR, but rather stems from stellar evolution models and/or BBH\nformation channels."
                },
                "authors": [
                    {
                        "name": "Cecilia Sgalletta"
                    },
                    {
                        "name": "Michela Mapelli"
                    },
                    {
                        "name": "Lumen Boco"
                    },
                    {
                        "name": "Filippo Santoliquido"
                    },
                    {
                        "name": "M. Celeste Artale"
                    },
                    {
                        "name": "Giuliano Iorio"
                    },
                    {
                        "name": "Andrea Lapi"
                    },
                    {
                        "name": "Mario Spera"
                    }
                ],
                "author_detail": {
                    "name": "Mario Spera"
                },
                "author": "Mario Spera",
                "arxiv_doi": "10.1051/0004-6361/202452757",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452757",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.21401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 9 figures",
                "arxiv_journal_ref": "A&A 698, A144 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15035v3",
                "updated": "2025-06-23T11:45:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    45,
                    9,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-19T16:46:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety\n  Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety\n  Inconsistencies"
                },
                "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe conduct a large-scale, comprehensive safety evaluation of the current LLM\nlandscape. For this purpose, we introduce M-ALERT, a multilingual benchmark\nthat evaluates the safety of LLMs in five languages: English, French, German,\nItalian, and Spanish. M-ALERT includes 15k high-quality prompts per language,\ntotaling 75k, with category-wise annotations. Our extensive experiments on 39\nstate-of-the-art LLMs highlight the importance of language-specific safety\nanalysis, revealing that models often exhibit significant inconsistencies in\nsafety across languages and categories. For instance, Llama3.2 shows high\nunsafety in category crime_tax for Italian but remains safe in other languages.\nSimilar inconsistencies can be observed across all models. In contrast, certain\ncategories, such as substance_cannabis and crime_propaganda, consistently\ntrigger unsafe responses across models and languages. These findings underscore\nthe need for robust multilingual safety practices in LLMs to ensure responsible\nusage across diverse communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe conduct a large-scale, comprehensive safety evaluation of the current LLM\nlandscape. For this purpose, we introduce M-ALERT, a multilingual benchmark\nthat evaluates the safety of LLMs in five languages: English, French, German,\nItalian, and Spanish. M-ALERT includes 15k high-quality prompts per language,\ntotaling 75k, with category-wise annotations. Our extensive experiments on 39\nstate-of-the-art LLMs highlight the importance of language-specific safety\nanalysis, revealing that models often exhibit significant inconsistencies in\nsafety across languages and categories. For instance, Llama3.2 shows high\nunsafety in category crime_tax for Italian but remains safe in other languages.\nSimilar inconsistencies can be observed across all models. In contrast, certain\ncategories, such as substance_cannabis and crime_propaganda, consistently\ntrigger unsafe responses across models and languages. These findings underscore\nthe need for robust multilingual safety practices in LLMs to ensure responsible\nusage across diverse communities."
                },
                "authors": [
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v4",
                "updated": "2025-06-23T11:43:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    43,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Jón Gunnar Hannesson"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Haiqiang Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peiran Ma"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18530v1",
                "updated": "2025-06-23T11:35:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    35,
                    20,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:35:20Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    35,
                    20,
                    0,
                    174,
                    0
                ],
                "title": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online\n  Learning to Scalable Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online\n  Learning to Scalable Inference"
                },
                "summary": "Edge AI applications increasingly require models that can learn and adapt\non-device with minimal energy budget. Traditional deep learning models, while\npowerful, are often overparameterized, energy-hungry, and dependent on cloud\nconnectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian\nConfidence Propagation Neural Network (BCPNN), propose a neuromorphic\nalternative by mimicking cortical architecture and biologically-constrained\nlearning. They offer sparse architectures with local learning rules and\nunsupervised/semi-supervised learning, making them well-suited for low-power\nedge intelligence. However, existing BCPNN implementations rely on GPUs or\ndatacenter FPGAs, limiting their applicability to embedded systems. This work\npresents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+\nSoC using High-Level Synthesis. We implement both online learning and\ninference-only kernels with support for variable and mixed precision. Evaluated\non MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to\n17.5x latency and 94% energy savings over ARM baselines, without sacrificing\naccuracy. This work enables practical neuromorphic computing on edge devices,\nbridging the gap between brain-like learning and real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI applications increasingly require models that can learn and adapt\non-device with minimal energy budget. Traditional deep learning models, while\npowerful, are often overparameterized, energy-hungry, and dependent on cloud\nconnectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian\nConfidence Propagation Neural Network (BCPNN), propose a neuromorphic\nalternative by mimicking cortical architecture and biologically-constrained\nlearning. They offer sparse architectures with local learning rules and\nunsupervised/semi-supervised learning, making them well-suited for low-power\nedge intelligence. However, existing BCPNN implementations rely on GPUs or\ndatacenter FPGAs, limiting their applicability to embedded systems. This work\npresents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+\nSoC using High-Level Synthesis. We implement both online learning and\ninference-only kernels with support for variable and mixed precision. Evaluated\non MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to\n17.5x latency and 94% energy savings over ARM baselines, without sacrificing\naccuracy. This work enables practical neuromorphic computing on edge devices,\nbridging the gap between brain-like learning and real-world deployment."
                },
                "authors": [
                    {
                        "name": "Muhammad Ihsan Al Hafiz"
                    },
                    {
                        "name": "Naresh Ravichandran"
                    },
                    {
                        "name": "Anders Lansner"
                    },
                    {
                        "name": "Pawel Herman"
                    },
                    {
                        "name": "Artur Podobas"
                    }
                ],
                "author_detail": {
                    "name": "Artur Podobas"
                },
                "author": "Artur Podobas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.14699v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.14699v3",
                "updated": "2025-06-23T11:31:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    31,
                    33,
                    0,
                    174,
                    0
                ],
                "published": "2022-10-26T13:28:14Z",
                "published_parsed": [
                    2022,
                    10,
                    26,
                    13,
                    28,
                    14,
                    2,
                    299,
                    0
                ],
                "title": "Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts,\n  or Black Magic?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts,\n  or Black Magic?"
                },
                "summary": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently gained attention in code\nassistants, which generate programs from a natural language task description\n(prompt). They have the potential to save time and effort but remain poorly\nunderstood, limiting their optimal use. In this article, we investigate the\nimpact of input variations on two configurations of a language model, focusing\non parameters such as task description, surrounding context, model creativity,\nand the number of generated solutions. We design specific operators to modify\nthese inputs and apply them to three LLM-based code assistants (Copilot, Codex,\nStarCoder2) and two benchmarks representing algorithmic problems (HumanEval,\nLeetCode). Our study examines whether these variations significantly affect\nprogram quality and how these effects generalize across models. Our results\nshow that varying input parameters can greatly improve performance, achieving\nup to 79.27% success in one-shot generation compared to 22.44% for Codex and\n31.1% for Copilot in default settings. Actioning this potential in practice is\nchallenging due to the complex interplay in our study - the optimal settings\nfor temperature, prompt, and number of generated solutions vary by problem.\nReproducing our study with StarCoder2 confirms these findings, indicating they\nare not model-specific. We also uncover surprising behaviors (e.g., fully\nremoving the prompt can be effective), revealing model brittleness and areas\nfor improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently gained attention in code\nassistants, which generate programs from a natural language task description\n(prompt). They have the potential to save time and effort but remain poorly\nunderstood, limiting their optimal use. In this article, we investigate the\nimpact of input variations on two configurations of a language model, focusing\non parameters such as task description, surrounding context, model creativity,\nand the number of generated solutions. We design specific operators to modify\nthese inputs and apply them to three LLM-based code assistants (Copilot, Codex,\nStarCoder2) and two benchmarks representing algorithmic problems (HumanEval,\nLeetCode). Our study examines whether these variations significantly affect\nprogram quality and how these effects generalize across models. Our results\nshow that varying input parameters can greatly improve performance, achieving\nup to 79.27% success in one-shot generation compared to 22.44% for Codex and\n31.1% for Copilot in default settings. Actioning this potential in practice is\nchallenging due to the complex interplay in our study - the optimal settings\nfor temperature, prompt, and number of generated solutions vary by problem.\nReproducing our study with StarCoder2 confirms these findings, indicating they\nare not model-specific. We also uncover surprising behaviors (e.g., fully\nremoving the prompt can be effective), revealing model brittleness and areas\nfor improvement."
                },
                "authors": [
                    {
                        "name": "Jean-Baptiste Döderlein"
                    },
                    {
                        "name": "Nguessan Hermann Kouadio"
                    },
                    {
                        "name": "Mathieu Acher"
                    },
                    {
                        "name": "Djamel Eddine Khelladi"
                    },
                    {
                        "name": "Benoit Combemale"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Combemale"
                },
                "author": "Benoit Combemale",
                "arxiv_comment": "53 pages, 3 Figures (not counted the subfigures), 16 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.14699v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.14699v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18522v1",
                "updated": "2025-06-23T11:24:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    24,
                    52,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:24:52Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    24,
                    52,
                    0,
                    174,
                    0
                ],
                "title": "DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation\n  Transformer for Dynamic System Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation\n  Transformer for Dynamic System Modeling"
                },
                "summary": "Uncovering the underlying ordinary differential equations (ODEs) that govern\ndynamic systems is crucial for advancing our understanding of complex\nphenomena. Traditional symbolic regression methods often struggle to capture\nthe temporal dynamics and intervariable correlations inherent in ODEs.\nODEFormer, a state-of-the-art method for inferring multidimensional ODEs from\nsingle trajectories, has made notable progress. However, its focus on\nsingle-trajectory evaluation is highly sensitive to initial starting points,\nwhich may not fully reflect true performance. To address this, we propose the\ndivergence difference metric (DIV-diff), which evaluates divergence over a grid\nof points within the target region, offering a comprehensive and stable\nanalysis of the variable space. Alongside, we introduce DDOT\n(Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer),\na transformer-based model designed to reconstruct multidimensional ODEs in\nsymbolic form. By incorporating an auxiliary task predicting the ODE's\nderivative, DDOT effectively captures both structure and dynamic behavior.\nExperiments on ODEBench show DDOT outperforms existing symbolic regression\nmethods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$\nfor reconstruction and generalization tasks, respectively, and an absolute\nreduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world\napplicability on an anesthesia dataset, highlighting its practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the underlying ordinary differential equations (ODEs) that govern\ndynamic systems is crucial for advancing our understanding of complex\nphenomena. Traditional symbolic regression methods often struggle to capture\nthe temporal dynamics and intervariable correlations inherent in ODEs.\nODEFormer, a state-of-the-art method for inferring multidimensional ODEs from\nsingle trajectories, has made notable progress. However, its focus on\nsingle-trajectory evaluation is highly sensitive to initial starting points,\nwhich may not fully reflect true performance. To address this, we propose the\ndivergence difference metric (DIV-diff), which evaluates divergence over a grid\nof points within the target region, offering a comprehensive and stable\nanalysis of the variable space. Alongside, we introduce DDOT\n(Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer),\na transformer-based model designed to reconstruct multidimensional ODEs in\nsymbolic form. By incorporating an auxiliary task predicting the ODE's\nderivative, DDOT effectively captures both structure and dynamic behavior.\nExperiments on ODEBench show DDOT outperforms existing symbolic regression\nmethods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$\nfor reconstruction and generalization tasks, respectively, and an absolute\nreduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world\napplicability on an anesthesia dataset, highlighting its practical impact."
                },
                "authors": [
                    {
                        "name": "Yang Chang"
                    },
                    {
                        "name": "Kuang-Da Wang"
                    },
                    {
                        "name": "Ping-Chun Hsieh"
                    },
                    {
                        "name": "Cheng-Kuan Lin"
                    },
                    {
                        "name": "Wen-Chih Peng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Chih Peng"
                },
                "author": "Wen-Chih Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18665v2",
                "updated": "2025-06-23T11:17:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    17,
                    25,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-24T13:30:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    30,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "Boosting Virtual Agent Learning and Reasoning: A Step-Wise,\n  Multi-Dimensional, and Generalist Reward Model with Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Virtual Agent Learning and Reasoning: A Step-Wise,\n  Multi-Dimensional, and Generalist Reward Model with Benchmark"
                },
                "summary": "The development of Generalist Virtual Agents (GVAs) has shown significant\npromise in autonomous task execution. However, current training paradigms face\ncritical limitations, including reliance on outcome supervision and\nlabor-intensive human annotations. To address these challenges, we propose\nSimilar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers\nfine-grained signals for agent training and can choose better action for\ninference-time scaling. Specifically, we begin by systematically defining five\ndimensions for evaluating agent actions. Building on this framework, we design\nan MCTS-P algorithm to automatically collect and annotate step-wise,\nfive-dimensional agent execution data. Using this data, we train Similar with\nthe Triple-M strategy. Furthermore, we introduce the first benchmark in the\nvirtual agent domain for step-wise, multi-dimensional reward model training and\nevaluation, named SRM. This benchmark consists of two components: SRMTrain,\nwhich serves as the training set for Similar, and SRMEval, a manually selected\ntest set for evaluating the reward model. Experimental results demonstrate that\nSimilar, through its step-wise, multi-dimensional assessment and synergistic\ngain, provides GVAs with effective intermediate signals during both training\nand inference-time scaling. The project is available at\nhttps://github.com/antgroup/Similar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Generalist Virtual Agents (GVAs) has shown significant\npromise in autonomous task execution. However, current training paradigms face\ncritical limitations, including reliance on outcome supervision and\nlabor-intensive human annotations. To address these challenges, we propose\nSimilar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers\nfine-grained signals for agent training and can choose better action for\ninference-time scaling. Specifically, we begin by systematically defining five\ndimensions for evaluating agent actions. Building on this framework, we design\nan MCTS-P algorithm to automatically collect and annotate step-wise,\nfive-dimensional agent execution data. Using this data, we train Similar with\nthe Triple-M strategy. Furthermore, we introduce the first benchmark in the\nvirtual agent domain for step-wise, multi-dimensional reward model training and\nevaluation, named SRM. This benchmark consists of two components: SRMTrain,\nwhich serves as the training set for Similar, and SRMEval, a manually selected\ntest set for evaluating the reward model. Experimental results demonstrate that\nSimilar, through its step-wise, multi-dimensional assessment and synergistic\ngain, provides GVAs with effective intermediate signals during both training\nand inference-time scaling. The project is available at\nhttps://github.com/antgroup/Similar."
                },
                "authors": [
                    {
                        "name": "Bingchen Miao"
                    },
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Wendong Bu"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Yunfei Li"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Juncheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Li"
                },
                "author": "Juncheng Li",
                "arxiv_comment": "Home page is available at https://dcd-ant-similar.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02072v2",
                "updated": "2025-06-23T11:11:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    11,
                    32,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-04T07:44:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    44,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development."
                },
                "authors": [
                    {
                        "name": "Rajiv Bahl"
                    },
                    {
                        "name": "Venkatesan N"
                    },
                    {
                        "name": "Parimal Aglawe"
                    },
                    {
                        "name": "Aastha Sarasapalli"
                    },
                    {
                        "name": "Bhavya Kancharla"
                    },
                    {
                        "name": "Chaitanya kolukuluri"
                    },
                    {
                        "name": "Harish Mohite"
                    },
                    {
                        "name": "Japneet Hora"
                    },
                    {
                        "name": "Kiran Kakollu"
                    },
                    {
                        "name": "Rahul Dhiman"
                    },
                    {
                        "name": "Shubham Kapale"
                    },
                    {
                        "name": "Sri Bhagya Kathula"
                    },
                    {
                        "name": "Vamsikrishna Motru"
                    },
                    {
                        "name": "Yogeshwar Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Yogeshwar Reddy"
                },
                "author": "Yogeshwar Reddy",
                "arxiv_comment": "17 pages, 6 Figures and this manuscript will be submitted to Q1,Q2\n  Journals",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10150v2",
                "updated": "2025-06-23T11:08:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    8,
                    0,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-13T08:22:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    22,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Kaili Ma"
                    },
                    {
                        "name": "Hongzhi Chen"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18512v1",
                "updated": "2025-06-23T11:06:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    6,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:06:31Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    6,
                    31,
                    0,
                    174,
                    0
                ],
                "title": "MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis"
                },
                "summary": "Accurate and interpretable multi-disease diagnosis remains a critical\nchallenge in medical research, particularly when leveraging heterogeneous\nmultimodal medical data. Current approaches often rely on single-modal data,\nlimiting their ability to comprehensively understand complex diseases. To\naddress this, we propose MedTVT-R1, a novel Multimodal Large Language Model\n(MLLM) framework designed to integrate clinical multimodal data for reasoning\nand diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction\ndataset that provides question-answer pairs for physiological-level\ninterpretations and disease-level diagnoses with a Chain of Evidence approach.\nMedTVT-R1 incorporates a modality perception layer to capture inter-modal\ndependencies and adaptively weight modality contributions. Additionally, we\nemploy Group Relative Policy Optimization (GRPO)-based Reinforcement\nFine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.\nExperimental results demonstrate MedTVT-R1's superiority in multimodal feature\nutilization and multi-disease diagnosis, offering significant potential for\nclinical applications such as diagnostic report generation and comorbidity\nreasoning. The dataset and code are available at\nhttps://github.com/keke-nice/MedTVT-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and interpretable multi-disease diagnosis remains a critical\nchallenge in medical research, particularly when leveraging heterogeneous\nmultimodal medical data. Current approaches often rely on single-modal data,\nlimiting their ability to comprehensively understand complex diseases. To\naddress this, we propose MedTVT-R1, a novel Multimodal Large Language Model\n(MLLM) framework designed to integrate clinical multimodal data for reasoning\nand diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction\ndataset that provides question-answer pairs for physiological-level\ninterpretations and disease-level diagnoses with a Chain of Evidence approach.\nMedTVT-R1 incorporates a modality perception layer to capture inter-modal\ndependencies and adaptively weight modality contributions. Additionally, we\nemploy Group Relative Policy Optimization (GRPO)-based Reinforcement\nFine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.\nExperimental results demonstrate MedTVT-R1's superiority in multimodal feature\nutilization and multi-disease diagnosis, offering significant potential for\nclinical applications such as diagnostic report generation and comorbidity\nreasoning. The dataset and code are available at\nhttps://github.com/keke-nice/MedTVT-R1."
                },
                "authors": [
                    {
                        "name": "Yuting Zhang"
                    },
                    {
                        "name": "Kaishen Yuan"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Yutao Yue"
                    },
                    {
                        "name": "Jintai Chen"
                    },
                    {
                        "name": "Kaishun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kaishun Wu"
                },
                "author": "Kaishun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18511v1",
                "updated": "2025-06-23T11:04:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    4,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:04:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    4,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A\n  RAG-based Framework for Medical Device Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard Applicability Judgment and Cross-jurisdictional Reasoning: A\n  RAG-based Framework for Medical Device Compliance"
                },
                "summary": "Identifying the appropriate regulatory standard applicability remains a\ncritical yet understudied challenge in medical device compliance, frequently\nnecessitating expert interpretation of fragmented and heterogeneous\ndocumentation across different jurisdictions. To address this challenge, we\nintroduce a modular AI system that leverages a retrieval-augmented generation\n(RAG) pipeline to automate standard applicability determination. Given a\nfree-text device description, our system retrieves candidate standards from a\ncurated corpus and uses large language models to infer jurisdiction-specific\napplicability, classified as Mandatory, Recommended, or Not Applicable, with\ntraceable justifications. We construct an international benchmark dataset of\nmedical device descriptions with expert-annotated standard mappings, and\nevaluate our system against retrieval-only, zero-shot, and rule-based\nbaselines. The proposed approach attains a classification accuracy of 73% and a\nTop-5 retrieval recall of 87%, demonstrating its effectiveness in identifying\nrelevant regulatory standards. We introduce the first end-to-end system for\nstandard applicability reasoning, enabling scalable and interpretable\nAI-supported regulatory science. Notably, our region-aware RAG agent performs\ncross-jurisdictional reasoning between Chinese and U.S. standards, supporting\nconflict resolution and applicability justification across regulatory\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the appropriate regulatory standard applicability remains a\ncritical yet understudied challenge in medical device compliance, frequently\nnecessitating expert interpretation of fragmented and heterogeneous\ndocumentation across different jurisdictions. To address this challenge, we\nintroduce a modular AI system that leverages a retrieval-augmented generation\n(RAG) pipeline to automate standard applicability determination. Given a\nfree-text device description, our system retrieves candidate standards from a\ncurated corpus and uses large language models to infer jurisdiction-specific\napplicability, classified as Mandatory, Recommended, or Not Applicable, with\ntraceable justifications. We construct an international benchmark dataset of\nmedical device descriptions with expert-annotated standard mappings, and\nevaluate our system against retrieval-only, zero-shot, and rule-based\nbaselines. The proposed approach attains a classification accuracy of 73% and a\nTop-5 retrieval recall of 87%, demonstrating its effectiveness in identifying\nrelevant regulatory standards. We introduce the first end-to-end system for\nstandard applicability reasoning, enabling scalable and interpretable\nAI-supported regulatory science. Notably, our region-aware RAG agent performs\ncross-jurisdictional reasoning between Chinese and U.S. standards, supporting\nconflict resolution and applicability justification across regulatory\nframeworks."
                },
                "authors": [
                    {
                        "name": "Yu Han"
                    },
                    {
                        "name": "Aaron Ceross"
                    },
                    {
                        "name": "Jeroen H. M. Bergmann"
                    }
                ],
                "author_detail": {
                    "name": "Jeroen H. M. Bergmann"
                },
                "author": "Jeroen H. M. Bergmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18510v1",
                "updated": "2025-06-23T11:04:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    4,
                    20,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:04:20Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    4,
                    20,
                    0,
                    174,
                    0
                ],
                "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich\n  Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich\n  Transcripts"
                },
                "summary": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints."
                },
                "authors": [
                    {
                        "name": "Duygu Altinok"
                    }
                ],
                "author_detail": {
                    "name": "Duygu Altinok"
                },
                "author": "Duygu Altinok",
                "arxiv_comment": "Accepted to INTERSPEECH2025 workshop DISS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.11536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.11536v2",
                "updated": "2025-06-23T10:56:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    56,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2023-03-21T01:57:40Z",
                "published_parsed": [
                    2023,
                    3,
                    21,
                    1,
                    57,
                    40,
                    1,
                    80,
                    0
                ],
                "title": "Indeterminate Probability Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indeterminate Probability Theory"
                },
                "summary": "Complex continuous or mixed joint distributions (e.g., P(Y | z_1, z_2, ...,\nz_N)) generally lack closed-form solutions, often necessitating approximations\nsuch as MCMC. This paper proposes Indeterminate Probability Theory (IPT), which\nmakes the following contributions: (1) An observer-centered framework in which\nexperimental outcomes are represented as distributions combining ground truth\nwith observation error; (2) The introduction of three independence candidate\naxioms that enable a two-phase probabilistic inference framework; (3) The\nderivation of closed-form solutions for arbitrary complex joint distributions\nunder this framework. Both the Indeterminate Probability Neural Network (IPNN)\nmodel and the non-neural multivariate time series forecasting application\ndemonstrate IPT's effectiveness in modeling high-dimensional distributions,\nwith successful validation up to 1000 dimensions. Importantly, IPT is\nconsistent with classical probability theory and subsumes the frequentist\nequation in the limit of vanishing observation error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex continuous or mixed joint distributions (e.g., P(Y | z_1, z_2, ...,\nz_N)) generally lack closed-form solutions, often necessitating approximations\nsuch as MCMC. This paper proposes Indeterminate Probability Theory (IPT), which\nmakes the following contributions: (1) An observer-centered framework in which\nexperimental outcomes are represented as distributions combining ground truth\nwith observation error; (2) The introduction of three independence candidate\naxioms that enable a two-phase probabilistic inference framework; (3) The\nderivation of closed-form solutions for arbitrary complex joint distributions\nunder this framework. Both the Indeterminate Probability Neural Network (IPNN)\nmodel and the non-neural multivariate time series forecasting application\ndemonstrate IPT's effectiveness in modeling high-dimensional distributions,\nwith successful validation up to 1000 dimensions. Importantly, IPT is\nconsistent with classical probability theory and subsumes the frequentist\nequation in the limit of vanishing observation error."
                },
                "authors": [
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Chuang Liu"
                    },
                    {
                        "name": "Xiaofeng Ma"
                    },
                    {
                        "name": "Weijia Lu"
                    },
                    {
                        "name": "Ning Wu"
                    },
                    {
                        "name": "Bingyang Li"
                    },
                    {
                        "name": "Zhifei Yang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaodong Zhang"
                    },
                    {
                        "name": "Can Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Can Zhang"
                },
                "author": "Can Zhang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.11536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.11536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18501v1",
                "updated": "2025-06-23T10:52:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    52,
                    54,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T10:52:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    52,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks:\n  Strengths, Weaknesses, and Domain-Specific Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks:\n  Strengths, Weaknesses, and Domain-Specific Performance"
                },
                "summary": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements."
                },
                "authors": [
                    {
                        "name": "Wael Etaiwi"
                    },
                    {
                        "name": "Bushra Alhijawi"
                    }
                ],
                "author_detail": {
                    "name": "Bushra Alhijawi"
                },
                "author": "Bushra Alhijawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18489v1",
                "updated": "2025-06-23T10:43:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    43,
                    4,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T10:43:04Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    43,
                    4,
                    0,
                    174,
                    0
                ],
                "title": "Infrared observations reveal the reprocessing envelope in the tidal\n  disruption event AT 2019azh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infrared observations reveal the reprocessing envelope in the tidal\n  disruption event AT 2019azh"
                },
                "summary": "Tidal disruption events (TDEs) are expected to release much of their energy\nin the far-ultraviolet (UV), which we do not observe directly. However,\ninfrared (IR) observations can observe re-radiation of the optical/UV emission\nfrom dust, and if this dust is observed in the process of sublimation, we can\ninfer the un-observed UV radiated energy. TDEs have also been predicted to show\nspectra shallower than a blackbody in the IR, but this has not yet been\nobserved. We present near/mid-IR observations of the TDE AT 2019azh spanning\nfrom -3 d before peak until >1750 d after. We evaluate these observations for\nconsistency with dust emission or direct emission from the TDE. We fit the IR\ndata with a modified blackbody associated with dust emission. The UV+optical+IR\ndata are compared with simulated spectra produced from general relativistic\nradiation magnetohydrodynamics simulations of super-Eddington accretion. We\nmodel the data at later times (> 200 d) as an IR echo. The IR data at the\nmaximum light can not be self-consistently fit with dust emission. Instead, the\ndata can be better fit with a reprocessing model, with the IR excess arising\ndue to the absorption opacity being dominated by free-free processes in the\ndense reprocessing envelope. We infer a large viewing angle of\n$\\sim$60$^{\\circ}$, consistent with previously reported X-ray observations, and\na tidally disrupted star with mass > 2 M$_{\\odot}$. The IR emission at later\ntimes is consistent with cool dust emission. We model these data as an IR echo\nand find that the dust is distant (0.65 pc), and clumpy, with a low covering\nfactor. We show that TDEs can have an IR excess not arising from dust and that\nIR observations at early times can constrain the viewing angle for the TDE in\nthe unified model. Near-IR observations are therefore essential to distinguish\nbetween hot dust and a non-thermal IR excess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tidal disruption events (TDEs) are expected to release much of their energy\nin the far-ultraviolet (UV), which we do not observe directly. However,\ninfrared (IR) observations can observe re-radiation of the optical/UV emission\nfrom dust, and if this dust is observed in the process of sublimation, we can\ninfer the un-observed UV radiated energy. TDEs have also been predicted to show\nspectra shallower than a blackbody in the IR, but this has not yet been\nobserved. We present near/mid-IR observations of the TDE AT 2019azh spanning\nfrom -3 d before peak until >1750 d after. We evaluate these observations for\nconsistency with dust emission or direct emission from the TDE. We fit the IR\ndata with a modified blackbody associated with dust emission. The UV+optical+IR\ndata are compared with simulated spectra produced from general relativistic\nradiation magnetohydrodynamics simulations of super-Eddington accretion. We\nmodel the data at later times (> 200 d) as an IR echo. The IR data at the\nmaximum light can not be self-consistently fit with dust emission. Instead, the\ndata can be better fit with a reprocessing model, with the IR excess arising\ndue to the absorption opacity being dominated by free-free processes in the\ndense reprocessing envelope. We infer a large viewing angle of\n$\\sim$60$^{\\circ}$, consistent with previously reported X-ray observations, and\na tidally disrupted star with mass > 2 M$_{\\odot}$. The IR emission at later\ntimes is consistent with cool dust emission. We model these data as an IR echo\nand find that the dust is distant (0.65 pc), and clumpy, with a low covering\nfactor. We show that TDEs can have an IR excess not arising from dust and that\nIR observations at early times can constrain the viewing angle for the TDE in\nthe unified model. Near-IR observations are therefore essential to distinguish\nbetween hot dust and a non-thermal IR excess."
                },
                "authors": [
                    {
                        "name": "Thomas M. Reynolds"
                    },
                    {
                        "name": "Lars Thomsen"
                    },
                    {
                        "name": "Seppo Mattila"
                    },
                    {
                        "name": "Takashi Nagao"
                    },
                    {
                        "name": "Joseph P. Anderson"
                    },
                    {
                        "name": "Franz E. Bauer"
                    },
                    {
                        "name": "Panos Charalampopoulos"
                    },
                    {
                        "name": "Lixin Dai"
                    },
                    {
                        "name": "Sara Faris"
                    },
                    {
                        "name": "Mariusz Gromadzki"
                    },
                    {
                        "name": "Claudia P. Gutiérrez"
                    },
                    {
                        "name": "Hanin Kuncarayakti"
                    },
                    {
                        "name": "Cosimo Inserra"
                    },
                    {
                        "name": "Erkki Kankare"
                    },
                    {
                        "name": "Timo Kravtsov"
                    },
                    {
                        "name": "Shane Moran"
                    },
                    {
                        "name": "Phil Wiseman"
                    }
                ],
                "author_detail": {
                    "name": "Phil Wiseman"
                },
                "author": "Phil Wiseman",
                "arxiv_comment": "18 pages, 14 figures. Submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.18899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18899v1",
                "updated": "2025-06-23T17:59:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:59:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    16,
                    0,
                    174,
                    0
                ],
                "title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated\n  Film Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated\n  Film Generation"
                },
                "summary": "AI-driven content creation has shown potential in film production. However,\nexisting film generation systems struggle to implement cinematic principles and\nthus fail to generate professional-quality films, particularly lacking diverse\ncamera language and cinematic rhythm. This results in templated visuals and\nunengaging narratives. To address this, we introduce FilMaster, an end-to-end\nAI system that integrates real-world cinematic principles for\nprofessional-grade film generation, yielding editable, industry-standard\noutputs. FilMaster is built on two key principles: (1) learning cinematography\nfrom extensive real-world film data and (2) emulating professional,\naudience-centric post-production workflows. Inspired by these principles,\nFilMaster incorporates two stages: a Reference-Guided Generation Stage which\ntransforms user input to video clips, and a Generative Post-Production Stage\nwhich transforms raw footage into audiovisual outputs by orchestrating visual\nand auditory elements for cinematic rhythm. Our generation stage highlights a\nMulti-shot Synergized RAG Camera Language Design module to guide the AI in\ngenerating professional camera language by retrieving reference clips from a\nvast corpus of 440,000 film clips. Our post-production stage emulates\nprofessional workflows by designing an Audience-Centric Cinematic Rhythm\nControl module, including Rough Cut and Fine Cut processes informed by\nsimulated audience feedback, for effective integration of audiovisual elements\nto achieve engaging content. The system is empowered by generative AI models\nlike (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a\ncomprehensive benchmark for evaluating AI-generated films. Extensive\nexperiments show FilMaster's superior performance in camera language design and\ncinematic rhythm control, advancing generative AI in professional filmmaking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-driven content creation has shown potential in film production. However,\nexisting film generation systems struggle to implement cinematic principles and\nthus fail to generate professional-quality films, particularly lacking diverse\ncamera language and cinematic rhythm. This results in templated visuals and\nunengaging narratives. To address this, we introduce FilMaster, an end-to-end\nAI system that integrates real-world cinematic principles for\nprofessional-grade film generation, yielding editable, industry-standard\noutputs. FilMaster is built on two key principles: (1) learning cinematography\nfrom extensive real-world film data and (2) emulating professional,\naudience-centric post-production workflows. Inspired by these principles,\nFilMaster incorporates two stages: a Reference-Guided Generation Stage which\ntransforms user input to video clips, and a Generative Post-Production Stage\nwhich transforms raw footage into audiovisual outputs by orchestrating visual\nand auditory elements for cinematic rhythm. Our generation stage highlights a\nMulti-shot Synergized RAG Camera Language Design module to guide the AI in\ngenerating professional camera language by retrieving reference clips from a\nvast corpus of 440,000 film clips. Our post-production stage emulates\nprofessional workflows by designing an Audience-Centric Cinematic Rhythm\nControl module, including Rough Cut and Fine Cut processes informed by\nsimulated audience feedback, for effective integration of audiovisual elements\nto achieve engaging content. The system is empowered by generative AI models\nlike (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a\ncomprehensive benchmark for evaluating AI-generated films. Extensive\nexperiments show FilMaster's superior performance in camera language design and\ncinematic rhythm control, advancing generative AI in professional filmmaking."
                },
                "authors": [
                    {
                        "name": "Kaiyi Huang"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Zinan Lin"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project Page: https://filmaster-ai.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18898v1",
                "updated": "2025-06-23T17:59:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    14,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    14,
                    0,
                    174,
                    0
                ],
                "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations"
                },
                "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com"
                },
                "authors": [
                    {
                        "name": "Jiaming Han"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Qi Zhao"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "arxiv_comment": "Project page: https://tar.csuhan.com",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18896v1",
                "updated": "2025-06-23T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs"
                },
                "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux"
                },
                "authors": [
                    {
                        "name": "Jiaru Zou"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Jingwen Gu"
                    },
                    {
                        "name": "Jiahao Qiu"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18887v1",
                "updated": "2025-06-23T17:56:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    56,
                    34,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:56:34Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    56,
                    34,
                    0,
                    174,
                    0
                ],
                "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Conceptual Bias via Transformer Latent-Subspace Activation"
                },
                "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems."
                },
                "authors": [
                    {
                        "name": "Vansh Sharma"
                    },
                    {
                        "name": "Venkat Raman"
                    }
                ],
                "author_detail": {
                    "name": "Venkat Raman"
                },
                "author": "Venkat Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; I.2.1; D.3.3; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18880v1",
                "updated": "2025-06-23T17:51:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    51,
                    40,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:51:40Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    51,
                    40,
                    0,
                    174,
                    0
                ],
                "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,\n  Compositional, and Transformative Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,\n  Compositional, and Transformative Generalization"
                },
                "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency."
                },
                "authors": [
                    {
                        "name": "Yiyou Sun"
                    },
                    {
                        "name": "Shawn Hu"
                    },
                    {
                        "name": "Georgia Zhou"
                    },
                    {
                        "name": "Ken Zheng"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07584v3",
                "updated": "2025-06-24T10:10:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    10,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-03-10T17:48:10Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    48,
                    10,
                    0,
                    69,
                    0
                ],
                "title": "Talking to GDELT Through Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking to GDELT Through Knowledge Graphs"
                },
                "summary": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we study various Retrieval Augmented Regeneration (RAG)\napproaches to gain an understanding of the strengths and weaknesses of each\napproach in a question-answering analysis. To gain this understanding we use a\ncase-study subset of the Global Database of Events, Language, and Tone (GDELT)\ndataset as well as a corpus of raw text scraped from the online news articles.\nTo retrieve information from the text corpus we implement a traditional vector\nstore RAG as well as state-of-the-art large language model (LLM) based\napproaches for automatically constructing KGs and retrieving the relevant\nsubgraphs. In addition to these corpus approaches, we develop a novel\nontology-based framework for constructing knowledge graphs (KGs) from GDELT\ndirectly which leverages the underlying schema of GDELT to create structured\nrepresentations of global events. For retrieving relevant information from the\nontology-based KGs we implement both direct graph queries and state-of-the-art\ngraph retrieval approaches. We compare the performance of each method in a\nquestion-answering task. We find that while our ontology-based KGs are valuable\nfor question-answering, automated extraction of the relevant subgraphs is\nchallenging. Conversely, LLM-generated KGs, while capturing event summaries,\noften lack consistency and interpretability. Our findings suggest benefits of a\nsynergistic approach between ontology and LLM-based KG construction, with\nproposed avenues toward that end."
                },
                "authors": [
                    {
                        "name": "Audun Myers"
                    },
                    {
                        "name": "Max Vargas"
                    },
                    {
                        "name": "Sinan G. Aksoy"
                    },
                    {
                        "name": "Cliff Joslyn"
                    },
                    {
                        "name": "Benjamin Wilson"
                    },
                    {
                        "name": "Lee Burke"
                    },
                    {
                        "name": "Tom Grimes"
                    }
                ],
                "author_detail": {
                    "name": "Tom Grimes"
                },
                "author": "Tom Grimes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18841v1",
                "updated": "2025-06-23T16:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    59,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning"
                },
                "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B"
                },
                "authors": [
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Zhiqiang Hu"
                    },
                    {
                        "name": "Roy Ka-Wei Lee"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v4",
                "updated": "2025-06-23T16:58:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    58,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18824v1",
                "updated": "2025-06-23T16:34:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    34,
                    52,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:34:52Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    34,
                    52,
                    0,
                    174,
                    0
                ],
                "title": "Understanding Software Engineering Agents: A Study of\n  Thought-Action-Result Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Software Engineering Agents: A Study of\n  Thought-Action-Result Trajectories"
                },
                "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05123v2",
                "updated": "2025-06-23T16:30:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    30,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-02-04T13:32:01Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    13,
                    32,
                    1,
                    6,
                    35,
                    0
                ],
                "title": "A Survey on Data Selection for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Data Selection for LLM Instruction Tuning"
                },
                "summary": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task."
                },
                "authors": [
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Dianhui Chu"
                },
                "author": "Dianhui Chu",
                "arxiv_comment": "Accepted by JAIR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18819v1",
                "updated": "2025-06-23T16:28:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    28,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:28:03Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    28,
                    3,
                    0,
                    174,
                    0
                ],
                "title": "RWESummary: A Framework and Test for Choosing Large Language Models to\n  Summarize Real-World Evidence (RWE) Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RWESummary: A Framework and Test for Choosing Large Language Models to\n  Summarize Real-World Evidence (RWE) Studies"
                },
                "summary": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization."
                },
                "authors": [
                    {
                        "name": "Arjun Mukerji"
                    },
                    {
                        "name": "Michael L. Jackson"
                    },
                    {
                        "name": "Jason Jones"
                    },
                    {
                        "name": "Neil Sanghavi"
                    }
                ],
                "author_detail": {
                    "name": "Neil Sanghavi"
                },
                "author": "Neil Sanghavi",
                "arxiv_comment": "24 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14470v3",
                "updated": "2025-06-23T16:25:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    25,
                    27,
                    0,
                    174,
                    0
                ],
                "published": "2024-08-26T17:58:53Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    58,
                    53,
                    0,
                    239,
                    0
                ],
                "title": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large\n  Language Models"
                },
                "summary": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. Selective PEFT, a class of\nparameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these\ncomputational challenges by selectively fine-tuning only a small fraction of\nthe model parameters. Although parameter-efficient, these techniques often fail\nto match the performance of fully fine-tuned models, primarily due to inherent\nbiases introduced during parameter selection. Traditional selective PEFT\ntechniques use a fixed set of parameters selected using different importance\nheuristics, failing to capture parameter importance dynamically and often\nleading to suboptimal performance. We introduce $\\text{ID}^3$, a novel\nselective PEFT method that calculates parameter importance continually, and\ndynamically unmasks parameters by balancing exploration and exploitation in\nparameter selection. Our empirical study on 16 tasks spanning natural language\nunderstanding, mathematical reasoning and summarization demonstrates the\neffectiveness of our method compared to fixed-masking selective PEFT\ntechniques. We analytically show that $\\text{ID}^3$ reduces the number of\ngradient updates by a factor of two, enhancing computational efficiency. Since\n$\\text{ID}^3$ is robust to random initialization of neurons and operates\ndirectly on the optimization process, it is highly flexible and can be\nintegrated with existing additive and reparametrization-based PEFT techniques\nsuch as adapters and LoRA respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) on downstream tasks requires\nsubstantial computational resources. Selective PEFT, a class of\nparameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these\ncomputational challenges by selectively fine-tuning only a small fraction of\nthe model parameters. Although parameter-efficient, these techniques often fail\nto match the performance of fully fine-tuned models, primarily due to inherent\nbiases introduced during parameter selection. Traditional selective PEFT\ntechniques use a fixed set of parameters selected using different importance\nheuristics, failing to capture parameter importance dynamically and often\nleading to suboptimal performance. We introduce $\\text{ID}^3$, a novel\nselective PEFT method that calculates parameter importance continually, and\ndynamically unmasks parameters by balancing exploration and exploitation in\nparameter selection. Our empirical study on 16 tasks spanning natural language\nunderstanding, mathematical reasoning and summarization demonstrates the\neffectiveness of our method compared to fixed-masking selective PEFT\ntechniques. We analytically show that $\\text{ID}^3$ reduces the number of\ngradient updates by a factor of two, enhancing computational efficiency. Since\n$\\text{ID}^3$ is robust to random initialization of neurons and operates\ndirectly on the optimization process, it is highly flexible and can be\nintegrated with existing additive and reparametrization-based PEFT techniques\nsuch as adapters and LoRA respectively."
                },
                "authors": [
                    {
                        "name": "Aradhye Agarwal"
                    },
                    {
                        "name": "Suhas K Ramesh"
                    },
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "15 pages, 7 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18807v2",
                "updated": "2025-06-24T09:56:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    56,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T16:16:02Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    16,
                    2,
                    0,
                    174,
                    0
                ],
                "title": "PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision\n  Applications"
                },
                "summary": "Real-time, on-device segmentation is critical for latency-sensitive and\nprivacy-aware applications like smart glasses and IoT devices. We introduce\nPicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation\nmodel optimized for edge and in-sensor execution, including the Sony IMX500. It\nbuilds on a depthwise separable U-Net, with knowledge distillation and\nfixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).\nOn COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized\nmodel (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it\nthe only model meeting both memory and compute constraints for in-sensor\ndeployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.\nThese results demonstrate that efficient, promptable segmentation is feasible\ndirectly on-camera, enabling privacy-preserving vision without cloud or host\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time, on-device segmentation is critical for latency-sensitive and\nprivacy-aware applications like smart glasses and IoT devices. We introduce\nPicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation\nmodel optimized for edge and in-sensor execution, including the Sony IMX500. It\nbuilds on a depthwise separable U-Net, with knowledge distillation and\nfixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).\nOn COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized\nmodel (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it\nthe only model meeting both memory and compute constraints for in-sensor\ndeployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.\nThese results demonstrate that efficient, promptable segmentation is feasible\ndirectly on-camera, enabling privacy-preserving vision without cloud or host\nprocessing."
                },
                "authors": [
                    {
                        "name": "Pietro Bonazzi"
                    },
                    {
                        "name": "Nicola Farronato"
                    },
                    {
                        "name": "Stefan Zihlmann"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18795v1",
                "updated": "2025-06-23T16:03:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:03:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    3,
                    16,
                    0,
                    174,
                    0
                ],
                "title": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract\n  Vulnerability Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract\n  Vulnerability Dataset Construction"
                },
                "summary": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities..."
                },
                "authors": [
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Yiming Shen"
                    },
                    {
                        "name": "Jiashuo Zhang"
                    },
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "John Grundy"
                    },
                    {
                        "name": "Zhenzhe Shao"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiashui Wang"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "Accepted for the 48th International Conference on Software\n  Engineering (ICSE 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18791v1",
                "updated": "2025-06-23T16:00:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    0,
                    57,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T16:00:57Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    16,
                    0,
                    57,
                    0,
                    174,
                    0
                ],
                "title": "Focus Your Attention: Towards Data-Intuitive Lightweight Vision\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Focus Your Attention: Towards Data-Intuitive Lightweight Vision\n  Transformers"
                },
                "summary": "The evolution of Vision Transformers has led to their widespread adaptation\nto different domains. Despite large-scale success, there remain significant\nchallenges including their reliance on extensive computational and memory\nresources for pre-training on huge datasets as well as difficulties in\ntask-specific transfer learning. These limitations coupled with energy\ninefficiencies mainly arise due to the computation-intensive self-attention\nmechanism. To address these issues, we propose a novel Super-Pixel Based Patch\nPooling (SPPP) technique that generates context-aware, semantically rich, patch\nembeddings to effectively reduce the architectural complexity and improve\nefficiency. Additionally, we introduce the Light Latent Attention (LLA) module\nin our pipeline by integrating latent tokens into the attention mechanism\nallowing cross-attention operations to significantly reduce the time and space\ncomplexity of the attention module. By leveraging the data-intuitive patch\nembeddings coupled with dynamic positional encodings, our approach adaptively\nmodulates the cross-attention process to focus on informative regions while\nmaintaining the global semantic structure. This targeted attention improves\ntraining efficiency and accelerates convergence. Notably, the SPPP module is\nlightweight and can be easily integrated into existing transformer\narchitectures. Extensive experiments demonstrate that our proposed architecture\nprovides significant improvements in terms of computational efficiency while\nachieving comparable results with the state-of-the-art approaches, highlighting\nits potential for energy-efficient transformers suitable for edge deployment.\n(The code is available on our GitHub repository:\nhttps://github.com/zser092/Focused-Attention-ViT).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of Vision Transformers has led to their widespread adaptation\nto different domains. Despite large-scale success, there remain significant\nchallenges including their reliance on extensive computational and memory\nresources for pre-training on huge datasets as well as difficulties in\ntask-specific transfer learning. These limitations coupled with energy\ninefficiencies mainly arise due to the computation-intensive self-attention\nmechanism. To address these issues, we propose a novel Super-Pixel Based Patch\nPooling (SPPP) technique that generates context-aware, semantically rich, patch\nembeddings to effectively reduce the architectural complexity and improve\nefficiency. Additionally, we introduce the Light Latent Attention (LLA) module\nin our pipeline by integrating latent tokens into the attention mechanism\nallowing cross-attention operations to significantly reduce the time and space\ncomplexity of the attention module. By leveraging the data-intuitive patch\nembeddings coupled with dynamic positional encodings, our approach adaptively\nmodulates the cross-attention process to focus on informative regions while\nmaintaining the global semantic structure. This targeted attention improves\ntraining efficiency and accelerates convergence. Notably, the SPPP module is\nlightweight and can be easily integrated into existing transformer\narchitectures. Extensive experiments demonstrate that our proposed architecture\nprovides significant improvements in terms of computational efficiency while\nachieving comparable results with the state-of-the-art approaches, highlighting\nits potential for energy-efficient transformers suitable for edge deployment.\n(The code is available on our GitHub repository:\nhttps://github.com/zser092/Focused-Attention-ViT)."
                },
                "authors": [
                    {
                        "name": "Suyash Gaurav"
                    },
                    {
                        "name": "Muhammad Farhan Humayun"
                    },
                    {
                        "name": "Jukka Heikkonen"
                    },
                    {
                        "name": "Jatin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Jatin Chaudhary"
                },
                "author": "Jatin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18783v1",
                "updated": "2025-06-23T15:53:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    53,
                    14,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:53:14Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    53,
                    14,
                    0,
                    174,
                    0
                ],
                "title": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation"
                },
                "summary": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks."
                },
                "authors": [
                    {
                        "name": "Kamil Szczepanik"
                    },
                    {
                        "name": "Jarosław A. Chudziak"
                    }
                ],
                "author_detail": {
                    "name": "Jarosław A. Chudziak"
                },
                "author": "Jarosław A. Chudziak",
                "arxiv_doi": "10.5220/0013321900003890",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013321900003890",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.18783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 10 figures, 2 tables, Accepted at the 17th International\n  Conference on Agents and Artificial Intelligence (ICAART 2025). Final version\n  published in Proceedings of ICAART 2025 (Vol. 1), pages 196-207",
                "arxiv_journal_ref": "In Proceedings of the 17th International Conference on Agents and\n  Artificial Intelligence - Volume 1: ICAART 2025; pages 196-207. SciTePress\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18750v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18750v3",
                "updated": "2025-06-23T15:51:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    51,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-25T02:48:53Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    2,
                    48,
                    53,
                    2,
                    360,
                    0
                ],
                "title": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Input Order Bias on Large Language Models for Software\n  Fault Localization"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in software\nengineering tasks such as Fault Localization (FL) and Automatic Program Repair\n(APR). This study investigates how input order and context size influence LLM\nperformance in FL, a crucial step for many downstream software engineering\ntasks. We evaluate different method orderings using Kendall Tau distances,\nincluding \"perfect\" (where ground truths appear first) and \"worst\" (where\nground truths appear last), across two benchmarks containing Java and Python\nprojects. Our results reveal a strong order bias: in Java projects, Top-1 FL\naccuracy drops from 57% to 20% when reversing the order, while in Python\nprojects, it decreases from 38% to approximately 3%. However, segmenting inputs\ninto smaller contexts mitigates this bias, reducing the performance gap in FL\nfrom 22% and 6% to just 1% across both benchmarks. We replaced method names\nwith semantically meaningful alternatives to determine whether this bias is due\nto data leakage. The observed trends remained consistent, suggesting that the\nbias is not caused by memorization from training data but rather by the\ninherent effect of input order. Additionally, we explored ordering methods\nbased on traditional FL techniques and metrics, finding that DepGraph's ranking\nachieves 48% Top-1 accuracy, outperforming simpler approaches such as\nCallGraph(DFS). These findings highlight the importance of structuring inputs,\nmanaging context effectively, and selecting appropriate ordering strategies to\nenhance LLM performance in FL and other software engineering applications."
                },
                "authors": [
                    {
                        "name": "Md Nakhla Rafi"
                    },
                    {
                        "name": "Dong Jae Kim"
                    },
                    {
                        "name": "Tse-Hsun Chen"
                    },
                    {
                        "name": "Shaowei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Wang"
                },
                "author": "Shaowei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18750v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18750v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18781v1",
                "updated": "2025-06-23T15:50:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    50,
                    21,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:50:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    50,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLMs Are Not Self-Consistent For Simple Tasks"
                },
                "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency."
                },
                "authors": [
                    {
                        "name": "Zhenru Lin"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18777v1",
                "updated": "2025-06-23T15:45:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    45,
                    44,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:45:44Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    45,
                    44,
                    0,
                    174,
                    0
                ],
                "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions\n  During Code Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions\n  During Code Training"
                },
                "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles."
                },
                "authors": [
                    {
                        "name": "Jonathan Cook"
                    },
                    {
                        "name": "Silvia Sapora"
                    },
                    {
                        "name": "Arash Ahmadian"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Tim Rocktaschel"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Laura Ruis"
                    }
                ],
                "author_detail": {
                    "name": "Laura Ruis"
                },
                "author": "Laura Ruis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15225v2",
                "updated": "2025-06-23T15:24:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    24,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-01-25T14:09:39Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    14,
                    9,
                    39,
                    5,
                    25,
                    0
                ],
                "title": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Scaling to Emphasize Attention for Long-Context Retrieval"
                },
                "summary": "While many advanced LLMs are designed to handle long sequence data, we can\nstill observe notable quality degradation even within the sequence limit. In\nthis work, we introduce a novel approach called Scaling to Emphasize Attention\nfor Long-context retrieval (SEAL), which enhances the retrieval performance of\nlarge language models (LLMs) over long contexts. We observe that specific\nattention heads are closely tied to long-context retrieval, showing positive or\nnegative correlation with retrieval scores, and adjusting the strength of these\nheads boosts the quality of LLMs in long context by a large margin. Built on\nthis insight, we propose a learning-based mechanism that leverages generated\ndata to emphasize these heads. By applying SEAL, we achieve significant\nimprovements in long-context retrieval performance across various tasks and\nmodels. Additionally, when combined with existing training-free context\nextension techniques, SEAL extends the contextual limits of LLMs while\nmaintaining highly reliable outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many advanced LLMs are designed to handle long sequence data, we can\nstill observe notable quality degradation even within the sequence limit. In\nthis work, we introduce a novel approach called Scaling to Emphasize Attention\nfor Long-context retrieval (SEAL), which enhances the retrieval performance of\nlarge language models (LLMs) over long contexts. We observe that specific\nattention heads are closely tied to long-context retrieval, showing positive or\nnegative correlation with retrieval scores, and adjusting the strength of these\nheads boosts the quality of LLMs in long context by a large margin. Built on\nthis insight, we propose a learning-based mechanism that leverages generated\ndata to emphasize these heads. By applying SEAL, we achieve significant\nimprovements in long-context retrieval performance across various tasks and\nmodels. Additionally, when combined with existing training-free context\nextension techniques, SEAL extends the contextual limits of LLMs while\nmaintaining highly reliable outputs."
                },
                "authors": [
                    {
                        "name": "Changhun Lee"
                    },
                    {
                        "name": "Minsang Seok"
                    },
                    {
                        "name": "Jun-gyu Jin"
                    },
                    {
                        "name": "Younghyun Cho"
                    },
                    {
                        "name": "Eunhyeok Park"
                    }
                ],
                "author_detail": {
                    "name": "Eunhyeok Park"
                },
                "author": "Eunhyeok Park",
                "arxiv_comment": "Accepted at ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18728v1",
                "updated": "2025-06-23T15:05:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    5,
                    54,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T15:05:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    5,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries"
                },
                "summary": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines."
                },
                "authors": [
                    {
                        "name": "Steven Kolawole"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Virginia Smith"
                    },
                    {
                        "name": "Pratiksha Thaker"
                    }
                ],
                "author_detail": {
                    "name": "Pratiksha Thaker"
                },
                "author": "Pratiksha Thaker",
                "arxiv_comment": "In review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24616v2",
                "updated": "2025-06-23T15:01:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    15,
                    1,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-30T14:08:17Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    8,
                    17,
                    4,
                    150,
                    0
                ],
                "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs\n  with POLLUX"
                },
                "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments."
                },
                "authors": [
                    {
                        "name": "Nikita Martynov"
                    },
                    {
                        "name": "Anastasia Mordasheva"
                    },
                    {
                        "name": "Dmitriy Gorbetskiy"
                    },
                    {
                        "name": "Danil Astafurov"
                    },
                    {
                        "name": "Ulyana Isaeva"
                    },
                    {
                        "name": "Elina Basyrova"
                    },
                    {
                        "name": "Sergey Skachkov"
                    },
                    {
                        "name": "Victoria Berestova"
                    },
                    {
                        "name": "Nikolay Ivanov"
                    },
                    {
                        "name": "Valeriia Zanina"
                    },
                    {
                        "name": "Alena Fenogenova"
                    }
                ],
                "author_detail": {
                    "name": "Alena Fenogenova"
                },
                "author": "Alena Fenogenova",
                "arxiv_comment": "179 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18711v1",
                "updated": "2025-06-23T14:49:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:49:03Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    3,
                    0,
                    174,
                    0
                ],
                "title": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with\n  Older Adults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with\n  Older Adults"
                },
                "summary": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults."
                },
                "authors": [
                    {
                        "name": "Marianne Bossema"
                    },
                    {
                        "name": "Somaya Ben Allouch"
                    },
                    {
                        "name": "Aske Plaat"
                    },
                    {
                        "name": "Rob Saunders"
                    }
                ],
                "author_detail": {
                    "name": "Rob Saunders"
                },
                "author": "Rob Saunders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18710v2",
                "updated": "2025-06-24T12:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    12,
                    36,
                    22,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T14:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Pedagogical Knowledge of Large Language Models"
                },
                "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."
                },
                "authors": [
                    {
                        "name": "Maxime Lelièvre"
                    },
                    {
                        "name": "Amy Waldock"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Natalia Valdés Aspillaga"
                    },
                    {
                        "name": "Alasdair Mackintosh"
                    },
                    {
                        "name": "María José Ogando Portela"
                    },
                    {
                        "name": "Jared Lee"
                    },
                    {
                        "name": "Paul Atherton"
                    },
                    {
                        "name": "Robin A. A. Ince"
                    },
                    {
                        "name": "Oliver G. B. Garrod"
                    }
                ],
                "author_detail": {
                    "name": "Oliver G. B. Garrod"
                },
                "author": "Oliver G. B. Garrod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00004v2",
                "updated": "2025-06-23T14:45:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    45,
                    7,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-18T09:46:19Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    46,
                    19,
                    3,
                    200,
                    0
                ],
                "title": "Handling Numeric Expressions in Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling Numeric Expressions in Automatic Speech Recognition"
                },
                "summary": "This paper addresses the problem of correctly formatting numeric expressions\nin automatic speech recognition (ASR) transcripts. This is challenging since\nthe expected transcript format depends on the context, e.g., 1945 (year) vs.\n19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize\nand format numeric expressions such as years, timestamps, currency amounts, and\nquantities. For the end-to-end approach, we employed a data generation strategy\nusing a large language model (LLM) together with a text to speech (TTS) model\nto generate adaptation data. The results on our test data set show that while\napproaches based on LLMs perform well in recognizing formatted numeric\nexpressions, adapted end-to-end models offer competitive performance with the\nadvantage of lower latency and inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the problem of correctly formatting numeric expressions\nin automatic speech recognition (ASR) transcripts. This is challenging since\nthe expected transcript format depends on the context, e.g., 1945 (year) vs.\n19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize\nand format numeric expressions such as years, timestamps, currency amounts, and\nquantities. For the end-to-end approach, we employed a data generation strategy\nusing a large language model (LLM) together with a text to speech (TTS) model\nto generate adaptation data. The results on our test data set show that while\napproaches based on LLMs perform well in recognizing formatted numeric\nexpressions, adapted end-to-end models offer competitive performance with the\nadvantage of lower latency and inference cost."
                },
                "authors": [
                    {
                        "name": "Christian Huber"
                    },
                    {
                        "name": "Alexander Waibel"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Waibel"
                },
                "author": "Alexander Waibel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00839v2",
                "updated": "2025-06-23T14:43:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    43,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-01T14:28:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    28,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "Context-Aware Human Behavior Prediction Using Multimodal Large Language\n  Models: Challenges and Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware Human Behavior Prediction Using Multimodal Large Language\n  Models: Challenges and Insights"
                },
                "summary": "Predicting human behavior in shared environments is crucial for safe and\nefficient human-robot interaction. Traditional data-driven methods to that end\nare pre-trained on domain-specific datasets, activity types, and prediction\nhorizons. In contrast, the recent breakthroughs in Large Language Models (LLMs)\npromise open-ended cross-domain generalization to describe various human\nactivities and make predictions in any context. In particular, Multimodal LLMs\n(MLLMs) are able to integrate information from various sources, achieving more\ncontextual awareness and improved scene understanding. The difficulty in\napplying general-purpose MLLMs directly for prediction stems from their limited\ncapacity for processing large input sequences, sensitivity to prompt design,\nand expensive fine-tuning. In this paper, we present a systematic analysis of\napplying pre-trained MLLMs for context-aware human behavior prediction. To this\nend, we introduce a modular multimodal human activity prediction framework that\nallows us to benchmark various MLLMs, input variations, In-Context Learning\n(ICL), and autoregressive techniques. Our evaluation indicates that the\nbest-performing framework configuration is able to reach 92.8% semantic\nsimilarity and 66.1% exact label accuracy in predicting human behaviors in the\ntarget frame.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting human behavior in shared environments is crucial for safe and\nefficient human-robot interaction. Traditional data-driven methods to that end\nare pre-trained on domain-specific datasets, activity types, and prediction\nhorizons. In contrast, the recent breakthroughs in Large Language Models (LLMs)\npromise open-ended cross-domain generalization to describe various human\nactivities and make predictions in any context. In particular, Multimodal LLMs\n(MLLMs) are able to integrate information from various sources, achieving more\ncontextual awareness and improved scene understanding. The difficulty in\napplying general-purpose MLLMs directly for prediction stems from their limited\ncapacity for processing large input sequences, sensitivity to prompt design,\nand expensive fine-tuning. In this paper, we present a systematic analysis of\napplying pre-trained MLLMs for context-aware human behavior prediction. To this\nend, we introduce a modular multimodal human activity prediction framework that\nallows us to benchmark various MLLMs, input variations, In-Context Learning\n(ICL), and autoregressive techniques. Our evaluation indicates that the\nbest-performing framework configuration is able to reach 92.8% semantic\nsimilarity and 66.1% exact label accuracy in predicting human behaviors in the\ntarget frame."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Lino Lerch"
                    },
                    {
                        "name": "Luigi Palmieri"
                    },
                    {
                        "name": "Andrey Rudenko"
                    },
                    {
                        "name": "Sebastian Koch"
                    },
                    {
                        "name": "Timo Ropinski"
                    },
                    {
                        "name": "Marco Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Marco Aiello"
                },
                "author": "Marco Aiello",
                "arxiv_comment": "Accepted at IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17090v2",
                "updated": "2025-06-23T14:39:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    39,
                    37,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-20T15:53:51Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    15,
                    53,
                    51,
                    4,
                    171,
                    0
                ],
                "title": "Better Language Model Inversion by Compactly Representing Next-Token\n  Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Better Language Model Inversion by Compactly Representing Next-Token\n  Distributions"
                },
                "summary": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known."
                },
                "authors": [
                    {
                        "name": "Murtaza Nazir"
                    },
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18846v2",
                "updated": "2025-06-23T14:37:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    37,
                    53,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-24T19:42:11Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    19,
                    42,
                    11,
                    5,
                    144,
                    0
                ],
                "title": "LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review\n  and Taxonomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review\n  and Taxonomy"
                },
                "summary": "Sixth Generation (6G) wireless networks, which are expected to be deployed in\nthe 2030s, have already created great excitement in academia and the private\nsector with their extremely high communication speed and low latency rates.\nHowever, despite the ultra-low latency, high throughput, and AI-assisted\norchestration capabilities they promise, they are vulnerable to stealthy and\nlong-term Advanced Persistent Threats (APTs). Large Language Models (LLMs)\nstand out as an ideal candidate to fill this gap with their high success in\nsemantic reasoning and threat intelligence. In this paper, we present a\ncomprehensive systematic review and taxonomy study for LLM-assisted APT\ndetection in 6G networks. We address five research questions, namely, semantic\nmerging of fragmented logs, encrypted traffic analysis, edge distribution\nconstraints, dataset/modeling techniques, and reproducibility trends, by\nleveraging most recent studies on the intersection of LLMs, APTs, and 6G\nwireless networks. We identify open challenges such as explainability gaps,\ndata scarcity, edge hardware limitations, and the need for real-time\nslicing-aware adaptation by presenting various taxonomies such as granularity,\ndeployment models, and kill chain stages. We then conclude the paper by\nproviding several research gaps in 6G infrastructures for future researchers.\nTo the best of our knowledge, this paper is the first comprehensive systematic\nreview and classification study on LLM-based APT detection in 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sixth Generation (6G) wireless networks, which are expected to be deployed in\nthe 2030s, have already created great excitement in academia and the private\nsector with their extremely high communication speed and low latency rates.\nHowever, despite the ultra-low latency, high throughput, and AI-assisted\norchestration capabilities they promise, they are vulnerable to stealthy and\nlong-term Advanced Persistent Threats (APTs). Large Language Models (LLMs)\nstand out as an ideal candidate to fill this gap with their high success in\nsemantic reasoning and threat intelligence. In this paper, we present a\ncomprehensive systematic review and taxonomy study for LLM-assisted APT\ndetection in 6G networks. We address five research questions, namely, semantic\nmerging of fragmented logs, encrypted traffic analysis, edge distribution\nconstraints, dataset/modeling techniques, and reproducibility trends, by\nleveraging most recent studies on the intersection of LLMs, APTs, and 6G\nwireless networks. We identify open challenges such as explainability gaps,\ndata scarcity, edge hardware limitations, and the need for real-time\nslicing-aware adaptation by presenting various taxonomies such as granularity,\ndeployment models, and kill chain stages. We then conclude the paper by\nproviding several research gaps in 6G infrastructures for future researchers.\nTo the best of our knowledge, this paper is the first comprehensive systematic\nreview and classification study on LLM-based APT detection in 6G networks."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Yaser Khamayseh"
                    },
                    {
                        "name": "Suhib Bani Melhem"
                    },
                    {
                        "name": "Abdulmalik Alwarafy"
                    }
                ],
                "author_detail": {
                    "name": "Abdulmalik Alwarafy"
                },
                "author": "Abdulmalik Alwarafy",
                "arxiv_comment": "22 pages, 11 figures, 8 tables. Submitted to Computer Science Review\n  (Elsevier), May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18689v1",
                "updated": "2025-06-23T14:28:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    28,
                    30,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:28:30Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    28,
                    30,
                    0,
                    174,
                    0
                ],
                "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed\n  Target Tracking in Unstructured GPS-Denied Environments"
                },
                "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions."
                },
                "authors": [
                    {
                        "name": "Alessandro Saviolo"
                    },
                    {
                        "name": "Giuseppe Loianno"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Loianno"
                },
                "author": "Giuseppe Loianno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12818v2",
                "updated": "2025-06-23T14:24:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    24,
                    28,
                    0,
                    174,
                    0
                ],
                "published": "2024-06-30T15:42:18Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    15,
                    42,
                    18,
                    6,
                    182,
                    0
                ],
                "title": "\"I understand why I got this grade\": Automatic Short Answer Grading with\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I understand why I got this grade\": Automatic Short Answer Grading with\n  Feedback"
                },
                "summary": "In recent years, there has been a growing interest in using Artificial\nIntelligence (AI) to automate student assessment in education. Among different\ntypes of assessments, summative assessments play a crucial role in evaluating a\nstudent's understanding level of a course. Such examinations often involve\nshort-answer questions. However, grading these responses and providing\nmeaningful feedback manually at scale is both time-consuming and\nlabor-intensive. Feedback is particularly important, as it helps students\nrecognize their strengths and areas for improvement. Despite the importance of\nthis task, there is a significant lack of publicly available datasets that\nsupport automatic short-answer grading with feedback generation. To address\nthis gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset\ndesigned for automatic short-answer grading with feedback. The dataset covers a\ndiverse range of subjects, questions, and answer patterns from multiple\nengineering domains and contains ~5.8k data points. We incorporate feedback\ninto our dataset by leveraging the generative capabilities of state-of-the-art\nlarge language models (LLMs) using our Label-Aware Synthetic Feedback\nGeneration (LASFG) strategy. This paper underscores the importance of enhanced\nfeedback in practical educational settings, outlines dataset annotation and\nfeedback generation processes, conducts a thorough EngSAF analysis, and\nprovides different LLMs-based zero-shot and finetuned baselines for future\ncomparison. The best-performing model (Mistral-7B) achieves an overall accuracy\nof 75.4% and 58.7% on unseen answers and unseen question test sets,\nrespectively. Additionally, we demonstrate the efficiency and effectiveness of\nour ASAG system through its deployment in a real-world end-semester exam at a\nreputed institute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a growing interest in using Artificial\nIntelligence (AI) to automate student assessment in education. Among different\ntypes of assessments, summative assessments play a crucial role in evaluating a\nstudent's understanding level of a course. Such examinations often involve\nshort-answer questions. However, grading these responses and providing\nmeaningful feedback manually at scale is both time-consuming and\nlabor-intensive. Feedback is particularly important, as it helps students\nrecognize their strengths and areas for improvement. Despite the importance of\nthis task, there is a significant lack of publicly available datasets that\nsupport automatic short-answer grading with feedback generation. To address\nthis gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset\ndesigned for automatic short-answer grading with feedback. The dataset covers a\ndiverse range of subjects, questions, and answer patterns from multiple\nengineering domains and contains ~5.8k data points. We incorporate feedback\ninto our dataset by leveraging the generative capabilities of state-of-the-art\nlarge language models (LLMs) using our Label-Aware Synthetic Feedback\nGeneration (LASFG) strategy. This paper underscores the importance of enhanced\nfeedback in practical educational settings, outlines dataset annotation and\nfeedback generation processes, conducts a thorough EngSAF analysis, and\nprovides different LLMs-based zero-shot and finetuned baselines for future\ncomparison. The best-performing model (Mistral-7B) achieves an overall accuracy\nof 75.4% and 58.7% on unseen answers and unseen question test sets,\nrespectively. Additionally, we demonstrate the efficiency and effectiveness of\nour ASAG system through its deployment in a real-world end-semester exam at a\nreputed institute."
                },
                "authors": [
                    {
                        "name": "Dishank Aggarwal"
                    },
                    {
                        "name": "Pritam Sil"
                    },
                    {
                        "name": "Bhaskaran Raman"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18674v1",
                "updated": "2025-06-23T14:18:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    18,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:18:46Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    18,
                    46,
                    0,
                    174,
                    0
                ],
                "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?"
                },
                "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus."
                },
                "authors": [
                    {
                        "name": "Raquel Ferrando"
                    },
                    {
                        "name": "Javier Conde"
                    },
                    {
                        "name": "Gonzalo Martínez"
                    },
                    {
                        "name": "Pedro Reviriego"
                    }
                ],
                "author_detail": {
                    "name": "Pedro Reviriego"
                },
                "author": "Pedro Reviriego",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18672v1",
                "updated": "2025-06-23T14:16:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    16,
                    36,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:16:36Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    16,
                    36,
                    0,
                    174,
                    0
                ],
                "title": "Spectrum Opportunities for the Wireless Future: From Direct-to-Device\n  Satellite Applications to 6G Cellular",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectrum Opportunities for the Wireless Future: From Direct-to-Device\n  Satellite Applications to 6G Cellular"
                },
                "summary": "For the next-generation wireless networks and beyond, both the upper mid-band\n(7 GHz-24 GHz) and terahertz (100 GHz-1 THz) spectra are gaining global\nattention from service providers, academic research groups, policy makers, and\nstandards organizations. This article provides an in-depth analysis of recent\nregulatory rulings and spectrum preferences issued by international standard\nbodies such as the International Telecommunications Union and Federal\nCommunications Commission as they seek to identify feasible bands for future\nwireless networks. In this paper, we present the promising spectrum allocations\nearmarked for 6G and beyond. We also provide exemplars that illuminate the\npassive service protections and spectrum feasibility for coexistence between\nterrestrial wireless networks and satellites and other non-terrestrial networks\n(NTN), and discuss key technical constraints that will challenge future\nspectrum use for the wireless industry. The findings highlight promising\nfrequency bands while addressing regulatory and technological challenges for\nfuture wireless service deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the next-generation wireless networks and beyond, both the upper mid-band\n(7 GHz-24 GHz) and terahertz (100 GHz-1 THz) spectra are gaining global\nattention from service providers, academic research groups, policy makers, and\nstandards organizations. This article provides an in-depth analysis of recent\nregulatory rulings and spectrum preferences issued by international standard\nbodies such as the International Telecommunications Union and Federal\nCommunications Commission as they seek to identify feasible bands for future\nwireless networks. In this paper, we present the promising spectrum allocations\nearmarked for 6G and beyond. We also provide exemplars that illuminate the\npassive service protections and spectrum feasibility for coexistence between\nterrestrial wireless networks and satellites and other non-terrestrial networks\n(NTN), and discuss key technical constraints that will challenge future\nspectrum use for the wireless industry. The findings highlight promising\nfrequency bands while addressing regulatory and technological challenges for\nfuture wireless service deployment."
                },
                "authors": [
                    {
                        "name": "Theodore S. Rappaport"
                    },
                    {
                        "name": "Todd E. Humphreys"
                    },
                    {
                        "name": "Shuai Nie"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Nie"
                },
                "author": "Shuai Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18670v1",
                "updated": "2025-06-23T14:14:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    14,
                    43,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T14:14:43Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    14,
                    43,
                    0,
                    174,
                    0
                ],
                "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation"
                },
                "summary": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever."
                },
                "authors": [
                    {
                        "name": "Jingming Liu"
                    },
                    {
                        "name": "Yumeng Li"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Yao-Xiang Ding"
                    },
                    {
                        "name": "Hui Su"
                    },
                    {
                        "name": "Kun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhou"
                },
                "author": "Kun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18656v1",
                "updated": "2025-06-23T13:56:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    56,
                    43,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:56:43Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    56,
                    43,
                    0,
                    174,
                    0
                ],
                "title": "A Random Matrix Analysis of In-context Memorization for Nonlinear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Random Matrix Analysis of In-context Memorization for Nonlinear\n  Attention"
                },
                "summary": "Attention mechanisms have revolutionized machine learning (ML) by enabling\nefficient modeling of global dependencies across inputs. Their inherently\nparallelizable structures allow for efficient scaling with the exponentially\nincreasing size of both pretrained data and model parameters. Yet, despite\ntheir central role as the computational backbone of modern large language\nmodels (LLMs), the theoretical understanding of Attentions, especially in the\nnonlinear setting, remains limited.\n  In this paper, we provide a precise characterization of the \\emph{in-context\nmemorization error} of \\emph{nonlinear Attention}, in the high-dimensional\nproportional regime where the number of input tokens $n$ and their embedding\ndimension $p$ are both large and comparable. Leveraging recent advances in the\ntheory of large kernel random matrices, we show that nonlinear Attention\ntypically incurs higher memorization error than linear ridge regression on\nrandom inputs. However, this gap vanishes, and can even be reversed, when the\ninput exhibits statistical structure, particularly when the Attention weights\nalign with the input signal direction. Our results reveal how nonlinearity and\ninput structure interact with each other to govern the memorization performance\nof nonlinear Attention. The theoretical insights are supported by numerical\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized machine learning (ML) by enabling\nefficient modeling of global dependencies across inputs. Their inherently\nparallelizable structures allow for efficient scaling with the exponentially\nincreasing size of both pretrained data and model parameters. Yet, despite\ntheir central role as the computational backbone of modern large language\nmodels (LLMs), the theoretical understanding of Attentions, especially in the\nnonlinear setting, remains limited.\n  In this paper, we provide a precise characterization of the \\emph{in-context\nmemorization error} of \\emph{nonlinear Attention}, in the high-dimensional\nproportional regime where the number of input tokens $n$ and their embedding\ndimension $p$ are both large and comparable. Leveraging recent advances in the\ntheory of large kernel random matrices, we show that nonlinear Attention\ntypically incurs higher memorization error than linear ridge regression on\nrandom inputs. However, this gap vanishes, and can even be reversed, when the\ninput exhibits statistical structure, particularly when the Attention weights\nalign with the input signal direction. Our results reveal how nonlinearity and\ninput structure interact with each other to govern the memorization performance\nof nonlinear Attention. The theoretical insights are supported by numerical\nexperiments."
                },
                "authors": [
                    {
                        "name": "Zhenyu Liao"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "TianQi Hou"
                    },
                    {
                        "name": "Difan Zou"
                    },
                    {
                        "name": "Zenan Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zenan Ling"
                },
                "author": "Zenan Ling",
                "arxiv_comment": "40 pages, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11097v2",
                "updated": "2025-06-23T13:56:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    56,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-06T10:49:49Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    10,
                    49,
                    49,
                    4,
                    157,
                    0
                ],
                "title": "C-SEO Bench: Does Conversational SEO Work?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C-SEO Bench: Does Conversational SEO Work?"
                },
                "summary": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench."
                },
                "authors": [
                    {
                        "name": "Haritz Puerto"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Tommaso Green"
                    },
                    {
                        "name": "Seong Joon Oh"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18631v2",
                "updated": "2025-06-24T07:07:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    7,
                    7,
                    57,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T13:36:24Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    36,
                    24,
                    0,
                    174,
                    0
                ],
                "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDit: Reward Dithering for Improved LLM Policy Optimization"
                },
                "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Jiarui Yu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Hande Dong"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "arxiv_comment": "10 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18628v1",
                "updated": "2025-06-23T13:35:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    35,
                    5,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:35:05Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    35,
                    5,
                    0,
                    174,
                    0
                ],
                "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention\n  Scores in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AggTruth: Contextual Hallucination Detection using Aggregated Attention\n  Scores in LLMs"
                },
                "summary": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results."
                },
                "authors": [
                    {
                        "name": "Piotr Matys"
                    },
                    {
                        "name": "Jan Eliasz"
                    },
                    {
                        "name": "Konrad Kiełczyński"
                    },
                    {
                        "name": "Mikołaj Langner"
                    },
                    {
                        "name": "Teddy Ferdinan"
                    },
                    {
                        "name": "Jan Kocoń"
                    },
                    {
                        "name": "Przemysław Kazienko"
                    }
                ],
                "author_detail": {
                    "name": "Przemysław Kazienko"
                },
                "author": "Przemysław Kazienko",
                "arxiv_doi": "10.1007/978-3-031-97570-7_18",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97570-7_18",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.18628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICCS 2025 Workshops",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18623v1",
                "updated": "2025-06-23T13:29:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    29,
                    51,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:29:51Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    29,
                    51,
                    0,
                    174,
                    0
                ],
                "title": "Efficient and Generalizable Speaker Diarization via Structured Pruning\n  of Self-Supervised Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Generalizable Speaker Diarization via Structured Pruning\n  of Self-Supervised Models"
                },
                "summary": "Self-supervised learning (SSL) models such as WavLM have brought substantial\nimprovements to speaker diarization by providing rich contextual\nrepresentations. However, the high computational and memory costs of these\nmodels hinder their deployment in real-time and resource-constrained scenarios.\nIn this work, we present a comprehensive study on compressing SSL-based\ndiarization models through structured pruning guided by knowledge distillation.\nBuilding upon our previous work, we extend the analysis to include pruning\nobjectives based on multiply-accumulate operations (MACs), investigate\nmodule-wise and progressive pruning strategies, and examine the impact of\ntraining data quantity. Experimental results show that our method reduces model\nsize by up to 80% without degrading performance, achieving up to 4x faster\ninference on a single GPU. We further perform large-scale evaluations on a\ndiverse compound dataset comprising eight public diarization corpora, where our\nbest pruned model achieves state-of-the-art performance across most conditions.\nAdditionally, we show strong generalization to the CHiME-6 dataset, attaining\nperformance comparable to the third-place system in the CHiME-7 challenge\nwithout any domain adaptation. All models and code are publicly released to\nsupport reproducibility and future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning (SSL) models such as WavLM have brought substantial\nimprovements to speaker diarization by providing rich contextual\nrepresentations. However, the high computational and memory costs of these\nmodels hinder their deployment in real-time and resource-constrained scenarios.\nIn this work, we present a comprehensive study on compressing SSL-based\ndiarization models through structured pruning guided by knowledge distillation.\nBuilding upon our previous work, we extend the analysis to include pruning\nobjectives based on multiply-accumulate operations (MACs), investigate\nmodule-wise and progressive pruning strategies, and examine the impact of\ntraining data quantity. Experimental results show that our method reduces model\nsize by up to 80% without degrading performance, achieving up to 4x faster\ninference on a single GPU. We further perform large-scale evaluations on a\ndiverse compound dataset comprising eight public diarization corpora, where our\nbest pruned model achieves state-of-the-art performance across most conditions.\nAdditionally, we show strong generalization to the CHiME-6 dataset, attaining\nperformance comparable to the third-place system in the CHiME-7 challenge\nwithout any domain adaptation. All models and code are publicly released to\nsupport reproducibility and future research."
                },
                "authors": [
                    {
                        "name": "Jiangyu Han"
                    },
                    {
                        "name": "Petr Pálka"
                    },
                    {
                        "name": "Marc Delcroix"
                    },
                    {
                        "name": "Federico Landini"
                    },
                    {
                        "name": "Johan Rohdin"
                    },
                    {
                        "name": "Jan Cernocký"
                    },
                    {
                        "name": "Lukáš Burget"
                    }
                ],
                "author_detail": {
                    "name": "Lukáš Burget"
                },
                "author": "Lukáš Burget",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18621v1",
                "updated": "2025-06-23T13:28:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    28,
                    33,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T13:28:33Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    28,
                    33,
                    0,
                    174,
                    0
                ],
                "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified\n  Speeches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified\n  Speeches"
                },
                "summary": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact."
                },
                "authors": [
                    {
                        "name": "Alisa Barkar"
                    },
                    {
                        "name": "Mathieu Chollet"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Beatrice Biancardi"
                    },
                    {
                        "name": "Chloe Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloe Clavel"
                },
                "author": "Chloe Clavel",
                "arxiv_comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11069v2",
                "updated": "2025-06-23T13:01:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    13,
                    1,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-14T04:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    4,
                    26,
                    21,
                    4,
                    73,
                    0
                ],
                "title": "API Agents vs. GUI Agents: Divergence and Convergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "API Agents vs. GUI Agents: Divergence and Convergence"
                },
                "summary": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have evolved beyond simple text generation to\npower software agents that directly translate natural language commands into\ntangible actions. While API-based LLM agents initially rose to prominence for\ntheir robust automation capabilities and seamless integration with programmatic\nendpoints, recent progress in multimodal LLM research has enabled GUI-based LLM\nagents that interact with graphical user interfaces in a human-like manner.\nAlthough these two paradigms share the goal of enabling LLM-driven task\nautomation, they diverge significantly in architectural complexity, development\nworkflows, and user interaction models.\n  This paper presents the first comprehensive comparative study of API-based\nand GUI-based LLM agents, systematically analyzing their divergence and\npotential convergence. We examine key dimensions and highlight scenarios in\nwhich hybrid approaches can harness their complementary strengths. By proposing\nclear decision criteria and illustrating practical use cases, we aim to guide\npractitioners and researchers in selecting, combining, or transitioning between\nthese paradigms. Ultimately, we indicate that continuing innovations in\nLLM-based automation are poised to blur the lines between API- and GUI-driven\nagents, paving the way for more flexible, adaptive solutions in a wide range of\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18600v1",
                "updated": "2025-06-23T12:59:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    59,
                    34,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:59:34Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    59,
                    34,
                    0,
                    174,
                    0
                ],
                "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data\n  leakage\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reply to \"Emergent LLM behaviors are observationally equivalent to data\n  leakage\""
                },
                "summary": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions."
                },
                "authors": [
                    {
                        "name": "Ariel Flint Ashery"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    },
                    {
                        "name": "Andrea Baronchelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Baronchelli"
                },
                "author": "Andrea Baronchelli",
                "arxiv_comment": "Reply to arXiv:2505.23796",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18583v1",
                "updated": "2025-06-23T12:37:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    37,
                    1,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:37:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    37,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry"
                },
                "summary": "LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation\nand mapping which is an essential requirement for autonomous robots.\nConventional LIO methods typically rely on formulating constraints from the\ngeometric structure sampled by the LiDAR. Hence, in the lack of geometric\nstructure, these tend to become ill-conditioned (degenerate) and fail.\nRobustness of LIO to such conditions is a necessity for its broader deployment.\nTo address this, we propose PG-LIO, a real-time LIO method that fuses\nphotometric and geometric information sampled by the LiDAR along with inertial\nconstraints from an Inertial Measurement Unit (IMU). This multi-modal\ninformation is integrated into a factor graph optimized over a sliding window\nfor real-time operation. We evaluate PG-LIO on multiple datasets that include\nboth geometrically well-conditioned as well as self-similar scenarios. Our\nmethod achieves accuracy on par with state-of-the-art LIO in geometrically\nwell-structured settings while significantly improving accuracy in degenerate\ncases including against methods that also fuse intensity. Notably, we\ndemonstrate only 1 m drift over a 1 km manually piloted aerial trajectory\nthrough a geometrically self-similar tunnel at an average speed of 7.5m/s (max\nspeed 10.8 m/s). For the benefit of the community, we shall also release our\nsource code https://github.com/ntnu-arl/mimosa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation\nand mapping which is an essential requirement for autonomous robots.\nConventional LIO methods typically rely on formulating constraints from the\ngeometric structure sampled by the LiDAR. Hence, in the lack of geometric\nstructure, these tend to become ill-conditioned (degenerate) and fail.\nRobustness of LIO to such conditions is a necessity for its broader deployment.\nTo address this, we propose PG-LIO, a real-time LIO method that fuses\nphotometric and geometric information sampled by the LiDAR along with inertial\nconstraints from an Inertial Measurement Unit (IMU). This multi-modal\ninformation is integrated into a factor graph optimized over a sliding window\nfor real-time operation. We evaluate PG-LIO on multiple datasets that include\nboth geometrically well-conditioned as well as self-similar scenarios. Our\nmethod achieves accuracy on par with state-of-the-art LIO in geometrically\nwell-structured settings while significantly improving accuracy in degenerate\ncases including against methods that also fuse intensity. Notably, we\ndemonstrate only 1 m drift over a 1 km manually piloted aerial trajectory\nthrough a geometrically self-similar tunnel at an average speed of 7.5m/s (max\nspeed 10.8 m/s). For the benefit of the community, we shall also release our\nsource code https://github.com/ntnu-arl/mimosa"
                },
                "authors": [
                    {
                        "name": "Nikhil Khedekar"
                    },
                    {
                        "name": "Kostas Alexis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Alexis"
                },
                "author": "Kostas Alexis",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18576v1",
                "updated": "2025-06-23T12:28:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    28,
                    13,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:28:13Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    28,
                    13,
                    0,
                    174,
                    0
                ],
                "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on\n  Zero-Shot LLM Classification Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on\n  Zero-Shot LLM Classification Performance"
                },
                "summary": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures."
                },
                "authors": [
                    {
                        "name": "Matteo Melis"
                    },
                    {
                        "name": "Gabriella Lapesa"
                    },
                    {
                        "name": "Dennis Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Assenmacher"
                },
                "author": "Dennis Assenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18566v1",
                "updated": "2025-06-23T12:21:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    21,
                    16,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T12:21:16Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    21,
                    16,
                    0,
                    174,
                    0
                ],
                "title": "Cosmic Ray Detection with the IceTop Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmic Ray Detection with the IceTop Enhancement"
                },
                "summary": "IceTop is the cosmic-ray detector located on the surface of the IceCube\nNeutrino Observatory at the South Pole, consisting of 81 pairs of ice-Cherenkov\ntanks. The rise in the energy threshold of air-shower measurements in IceTop\ndue to accumulating snow emphasized the need for the next generation of IceCube\nsurface detectors. For this purpose, the Surface Array Enhancement (SAE) is set\nto comprise elevated scintillator panels and radio antennas controlled by\nhybrid DAQ systems. The detectors of the SAE are also expected to extend to the\nplanned IceCube-Gen2 Surface Array. An initial study with a prototype station\nis already conducted. We briefly review the SAE and the deployment as well as\nthe calibration status of the upcoming stations of the planned array of 32\nstations. The focus of this contribution is on the radio detection of extensive\nair showers. A preliminary estimation of the position of the shower maximum\n($X_\\mathrm{max}$), that is sensitive to the primary mass, with data from the 3\nantennas of the prototype station was carried out. An extension of the method\nfrom previous analyses is also briefly discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IceTop is the cosmic-ray detector located on the surface of the IceCube\nNeutrino Observatory at the South Pole, consisting of 81 pairs of ice-Cherenkov\ntanks. The rise in the energy threshold of air-shower measurements in IceTop\ndue to accumulating snow emphasized the need for the next generation of IceCube\nsurface detectors. For this purpose, the Surface Array Enhancement (SAE) is set\nto comprise elevated scintillator panels and radio antennas controlled by\nhybrid DAQ systems. The detectors of the SAE are also expected to extend to the\nplanned IceCube-Gen2 Surface Array. An initial study with a prototype station\nis already conducted. We briefly review the SAE and the deployment as well as\nthe calibration status of the upcoming stations of the planned array of 32\nstations. The focus of this contribution is on the radio detection of extensive\nair showers. A preliminary estimation of the position of the shower maximum\n($X_\\mathrm{max}$), that is sensitive to the primary mass, with data from the 3\nantennas of the prototype station was carried out. An extension of the method\nfrom previous analyses is also briefly discussed."
                },
                "authors": [
                    {
                        "name": "Megha Venugopal"
                    }
                ],
                "author_detail": {
                    "name": "Megha Venugopal"
                },
                "arxiv_affiliation": "for the IceCube Collaboration",
                "author": "Megha Venugopal",
                "arxiv_comment": "7 pages, 6 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04942v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04942v2",
                "updated": "2025-06-23T12:21:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    21,
                    10,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-07T11:30:36Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    11,
                    30,
                    36,
                    0,
                    97,
                    0
                ],
                "title": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lemmanaid: Neuro-Symbolic Lemma Conjecturing"
                },
                "summary": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically conjecturing useful, interesting and novel lemmas would greatly\nimprove automated reasoning tools and lower the bar for formalizing mathematics\nin proof assistants. It is however a very challenging task for both neural and\nsymbolic approaches. We present the first steps towards a practical\nneuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language\nModels (LLMs) and symbolic methods, and evaluate it on proof libraries for the\nIsabelle proof assistant. We train an LLM to generate lemma templates that\ndescribe the shape of a lemma, and use symbolic methods to fill in the details.\nWe compare Lemmanaid against an LLM trained to generate complete lemma\nstatements as well as previous fully symbolic conjecturing methods. Lemmanaid\noutperforms both neural and symbolic methods on test sets from Isabelle's HOL\nlibrary and from its Archive of Formal Proofs, discovering between 29-39.5% of\nthe gold standard human written lemmas. This is 8-15% more lemmas than the\nneural-only method. By leveraging the best of both symbolic and neural methods\nwe can generate useful lemmas for a wide range of input domains, facilitating\ncomputer-assisted theory development and formalization."
                },
                "authors": [
                    {
                        "name": "Yousef Alhessi"
                    },
                    {
                        "name": "Sólrún Halla Einarsdóttir"
                    },
                    {
                        "name": "George Granberry"
                    },
                    {
                        "name": "Emily First"
                    },
                    {
                        "name": "Moa Johansson"
                    },
                    {
                        "name": "Sorin Lerner"
                    },
                    {
                        "name": "Nicholas Smallbone"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Smallbone"
                },
                "author": "Nicholas Smallbone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04942v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04942v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17761v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17761v4",
                "updated": "2025-06-23T12:11:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    12,
                    11,
                    44,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-24T17:25:12Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    17,
                    25,
                    12,
                    3,
                    114,
                    0
                ],
                "title": "Step1X-Edit: A Practical Framework for General Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step1X-Edit: A Practical Framework for General Image Editing"
                },
                "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing."
                },
                "authors": [
                    {
                        "name": "Shiyu Liu"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Peng Xing"
                    },
                    {
                        "name": "Fukun Yin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Jiaqi Liao"
                    },
                    {
                        "name": "Yingming Wang"
                    },
                    {
                        "name": "Honghao Fu"
                    },
                    {
                        "name": "Chunrui Han"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Yuang Peng"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Jingwei Wu"
                    },
                    {
                        "name": "Yan Cai"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "arxiv_comment": "code: https://github.com/stepfun-ai/Step1X-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17761v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17761v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04365v2",
                "updated": "2025-06-23T11:56:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    56,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-06T05:30:10Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    5,
                    30,
                    10,
                    6,
                    96,
                    0
                ],
                "title": "AutoPDL: Automatic Prompt Optimization for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPDL: Automatic Prompt Optimization for LLM Agents"
                },
                "summary": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.06\\pm15.3$ percentage points),\nup to 68.9pp, and reveal that selected prompting strategies vary across models\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) depends on how they are\nprompted, with choices spanning both the high-level prompting pattern (e.g.,\nZero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and\nfew-shot demonstrations). Manually tuning this combination is tedious,\nerror-prone, and specific to a given LLM and task. Therefore, this paper\nproposes AutoPDL, an automated approach to discovering good LLM agent\nconfigurations. Our approach frames this as a structured AutoML problem over a\ncombinatorial space of agentic and non-agentic prompting patterns and\ndemonstrations, using successive halving to efficiently navigate this space. We\nintroduce a library implementing common prompting patterns using the PDL prompt\nprogramming language. AutoPDL solutions are human-readable, editable, and\nexecutable PDL programs that use this library. This approach also enables\nsource-to-source optimization, allowing human-in-the-loop refinement and reuse.\nEvaluations across three tasks and seven LLMs (ranging from 3B to 70B\nparameters) show consistent accuracy gains ($9.06\\pm15.3$ percentage points),\nup to 68.9pp, and reveal that selected prompting strategies vary across models\nand tasks."
                },
                "authors": [
                    {
                        "name": "Claudio Spiess"
                    },
                    {
                        "name": "Mandana Vaziri"
                    },
                    {
                        "name": "Louis Mandel"
                    },
                    {
                        "name": "Martin Hirzel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Hirzel"
                },
                "author": "Martin Hirzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18543v1",
                "updated": "2025-06-23T11:53:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    53,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:53:31Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    53,
                    31,
                    0,
                    174,
                    0
                ],
                "title": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Assessment of DeepSeek and GPT Series Models against Jailbreak\n  Attacks"
                },
                "summary": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of large language models (LLMs) has raised critical\nconcerns over their vulnerability to jailbreak attacks, i.e., adversarial\nprompts that bypass alignment mechanisms and elicit harmful or policy-violating\noutputs. While proprietary models like GPT-4 have undergone extensive\nevaluation, the robustness of emerging open-source alternatives such as\nDeepSeek remains largely underexplored, despite their growing adoption in\nreal-world applications. In this paper, we present the first systematic\njailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and\nGPT-4 using the HarmBench benchmark. We evaluate seven representative attack\nstrategies across 510 harmful behaviors categorized by both function and\nsemantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)\narchitecture introduces routing sparsity that offers selective robustness\nagainst optimization-based attacks such as TAP-T, but leads to significantly\nhigher vulnerability under prompt-based and manually engineered attacks. In\ncontrast, GPT-4 Turbo demonstrates stronger and more consistent safety\nalignment across diverse behaviors, likely due to its dense Transformer design\nand reinforcement learning from human feedback. Fine-grained behavioral\nanalysis and case studies further show that DeepSeek often routes adversarial\nprompts to under-aligned expert modules, resulting in inconsistent refusal\nbehaviors. These findings highlight a fundamental trade-off between\narchitectural efficiency and alignment generalization, emphasizing the need for\ntargeted safety tuning and modular alignment strategies to ensure secure\ndeployment of open-source LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wu"
                    },
                    {
                        "name": "Xiangman Li"
                    },
                    {
                        "name": "Jianbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Jianbing Ni"
                },
                "author": "Jianbing Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15035v3",
                "updated": "2025-06-23T11:45:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    45,
                    9,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-19T16:46:54Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    16,
                    46,
                    54,
                    3,
                    354,
                    0
                ],
                "title": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety\n  Inconsistencies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety\n  Inconsistencies"
                },
                "summary": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe conduct a large-scale, comprehensive safety evaluation of the current LLM\nlandscape. For this purpose, we introduce M-ALERT, a multilingual benchmark\nthat evaluates the safety of LLMs in five languages: English, French, German,\nItalian, and Spanish. M-ALERT includes 15k high-quality prompts per language,\ntotaling 75k, with category-wise annotations. Our extensive experiments on 39\nstate-of-the-art LLMs highlight the importance of language-specific safety\nanalysis, revealing that models often exhibit significant inconsistencies in\nsafety across languages and categories. For instance, Llama3.2 shows high\nunsafety in category crime_tax for Italian but remains safe in other languages.\nSimilar inconsistencies can be observed across all models. In contrast, certain\ncategories, such as substance_cannabis and crime_propaganda, consistently\ntrigger unsafe responses across models and languages. These findings underscore\nthe need for robust multilingual safety practices in LLMs to ensure responsible\nusage across diverse communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building safe Large Language Models (LLMs) across multiple languages is\nessential in ensuring both safe access and linguistic diversity. To this end,\nwe conduct a large-scale, comprehensive safety evaluation of the current LLM\nlandscape. For this purpose, we introduce M-ALERT, a multilingual benchmark\nthat evaluates the safety of LLMs in five languages: English, French, German,\nItalian, and Spanish. M-ALERT includes 15k high-quality prompts per language,\ntotaling 75k, with category-wise annotations. Our extensive experiments on 39\nstate-of-the-art LLMs highlight the importance of language-specific safety\nanalysis, revealing that models often exhibit significant inconsistencies in\nsafety across languages and categories. For instance, Llama3.2 shows high\nunsafety in category crime_tax for Italian but remains safe in other languages.\nSimilar inconsistencies can be observed across all models. In contrast, certain\ncategories, such as substance_cannabis and crime_propaganda, consistently\ntrigger unsafe responses across models and languages. These findings underscore\nthe need for robust multilingual safety practices in LLMs to ensure responsible\nusage across diverse communities."
                },
                "authors": [
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Huu Nguyen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02670v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02670v4",
                "updated": "2025-06-23T11:43:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    43,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-03T15:11:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    15,
                    11,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Affordable AI Assistants with Knowledge Graph of Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordable AI Assistants with Knowledge Graph of Thoughts"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing the development of AI\nassistants capable of performing diverse tasks across domains. However, current\nstate-of-the-art LLM-driven agents face significant challenges, including high\noperational costs and limited success rates on complex benchmarks like GAIA. To\naddress these issues, we propose Knowledge Graph of Thoughts (KGoT), an\ninnovative AI assistant architecture that integrates LLM reasoning with\ndynamically constructed knowledge graphs (KGs). KGoT extracts and structures\ntask-relevant knowledge into a dynamic KG representation, iteratively enhanced\nthrough external tools such as math solvers, web crawlers, and Python scripts.\nSuch structured representation of task-relevant knowledge enables low-cost\nmodels to solve complex tasks effectively while also minimizing bias and noise.\nFor example, KGoT achieves a 29% improvement in task success rates on the GAIA\nbenchmark compared to Hugging Face Agents with GPT-4o mini. Moreover,\nharnessing a smaller model dramatically reduces operational costs by over 36x\ncompared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and\nDeepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a\nscalable, affordable, versatile, and high-performing solution for AI\nassistants."
                },
                "authors": [
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Lorenzo Paleari"
                    },
                    {
                        "name": "Jia Hao Andrea Jiang"
                    },
                    {
                        "name": "Robert Gerstenberger"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Jón Gunnar Hannesson"
                    },
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Ales Kubicek"
                    },
                    {
                        "name": "Piotr Nyczyk"
                    },
                    {
                        "name": "Diana Khimey"
                    },
                    {
                        "name": "Nils Blach"
                    },
                    {
                        "name": "Haiqiang Zhang"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peiran Ma"
                    },
                    {
                        "name": "Grzegorz Kwaśniewski"
                    },
                    {
                        "name": "Marcin Copik"
                    },
                    {
                        "name": "Hubert Niewiadomski"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02670v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02670v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18530v1",
                "updated": "2025-06-23T11:35:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    35,
                    20,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:35:20Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    35,
                    20,
                    0,
                    174,
                    0
                ],
                "title": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online\n  Learning to Scalable Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online\n  Learning to Scalable Inference"
                },
                "summary": "Edge AI applications increasingly require models that can learn and adapt\non-device with minimal energy budget. Traditional deep learning models, while\npowerful, are often overparameterized, energy-hungry, and dependent on cloud\nconnectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian\nConfidence Propagation Neural Network (BCPNN), propose a neuromorphic\nalternative by mimicking cortical architecture and biologically-constrained\nlearning. They offer sparse architectures with local learning rules and\nunsupervised/semi-supervised learning, making them well-suited for low-power\nedge intelligence. However, existing BCPNN implementations rely on GPUs or\ndatacenter FPGAs, limiting their applicability to embedded systems. This work\npresents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+\nSoC using High-Level Synthesis. We implement both online learning and\ninference-only kernels with support for variable and mixed precision. Evaluated\non MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to\n17.5x latency and 94% energy savings over ARM baselines, without sacrificing\naccuracy. This work enables practical neuromorphic computing on edge devices,\nbridging the gap between brain-like learning and real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI applications increasingly require models that can learn and adapt\non-device with minimal energy budget. Traditional deep learning models, while\npowerful, are often overparameterized, energy-hungry, and dependent on cloud\nconnectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian\nConfidence Propagation Neural Network (BCPNN), propose a neuromorphic\nalternative by mimicking cortical architecture and biologically-constrained\nlearning. They offer sparse architectures with local learning rules and\nunsupervised/semi-supervised learning, making them well-suited for low-power\nedge intelligence. However, existing BCPNN implementations rely on GPUs or\ndatacenter FPGAs, limiting their applicability to embedded systems. This work\npresents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+\nSoC using High-Level Synthesis. We implement both online learning and\ninference-only kernels with support for variable and mixed precision. Evaluated\non MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to\n17.5x latency and 94% energy savings over ARM baselines, without sacrificing\naccuracy. This work enables practical neuromorphic computing on edge devices,\nbridging the gap between brain-like learning and real-world deployment."
                },
                "authors": [
                    {
                        "name": "Muhammad Ihsan Al Hafiz"
                    },
                    {
                        "name": "Naresh Ravichandran"
                    },
                    {
                        "name": "Anders Lansner"
                    },
                    {
                        "name": "Pawel Herman"
                    },
                    {
                        "name": "Artur Podobas"
                    }
                ],
                "author_detail": {
                    "name": "Artur Podobas"
                },
                "author": "Artur Podobas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.14699v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.14699v3",
                "updated": "2025-06-23T11:31:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    31,
                    33,
                    0,
                    174,
                    0
                ],
                "published": "2022-10-26T13:28:14Z",
                "published_parsed": [
                    2022,
                    10,
                    26,
                    13,
                    28,
                    14,
                    2,
                    299,
                    0
                ],
                "title": "Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts,\n  or Black Magic?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts,\n  or Black Magic?"
                },
                "summary": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently gained attention in code\nassistants, which generate programs from a natural language task description\n(prompt). They have the potential to save time and effort but remain poorly\nunderstood, limiting their optimal use. In this article, we investigate the\nimpact of input variations on two configurations of a language model, focusing\non parameters such as task description, surrounding context, model creativity,\nand the number of generated solutions. We design specific operators to modify\nthese inputs and apply them to three LLM-based code assistants (Copilot, Codex,\nStarCoder2) and two benchmarks representing algorithmic problems (HumanEval,\nLeetCode). Our study examines whether these variations significantly affect\nprogram quality and how these effects generalize across models. Our results\nshow that varying input parameters can greatly improve performance, achieving\nup to 79.27% success in one-shot generation compared to 22.44% for Codex and\n31.1% for Copilot in default settings. Actioning this potential in practice is\nchallenging due to the complex interplay in our study - the optimal settings\nfor temperature, prompt, and number of generated solutions vary by problem.\nReproducing our study with StarCoder2 confirms these findings, indicating they\nare not model-specific. We also uncover surprising behaviors (e.g., fully\nremoving the prompt can be effective), revealing model brittleness and areas\nfor improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are promising solutions for tackling increasing complex\nproblems. In software engineering, they recently gained attention in code\nassistants, which generate programs from a natural language task description\n(prompt). They have the potential to save time and effort but remain poorly\nunderstood, limiting their optimal use. In this article, we investigate the\nimpact of input variations on two configurations of a language model, focusing\non parameters such as task description, surrounding context, model creativity,\nand the number of generated solutions. We design specific operators to modify\nthese inputs and apply them to three LLM-based code assistants (Copilot, Codex,\nStarCoder2) and two benchmarks representing algorithmic problems (HumanEval,\nLeetCode). Our study examines whether these variations significantly affect\nprogram quality and how these effects generalize across models. Our results\nshow that varying input parameters can greatly improve performance, achieving\nup to 79.27% success in one-shot generation compared to 22.44% for Codex and\n31.1% for Copilot in default settings. Actioning this potential in practice is\nchallenging due to the complex interplay in our study - the optimal settings\nfor temperature, prompt, and number of generated solutions vary by problem.\nReproducing our study with StarCoder2 confirms these findings, indicating they\nare not model-specific. We also uncover surprising behaviors (e.g., fully\nremoving the prompt can be effective), revealing model brittleness and areas\nfor improvement."
                },
                "authors": [
                    {
                        "name": "Jean-Baptiste Döderlein"
                    },
                    {
                        "name": "Nguessan Hermann Kouadio"
                    },
                    {
                        "name": "Mathieu Acher"
                    },
                    {
                        "name": "Djamel Eddine Khelladi"
                    },
                    {
                        "name": "Benoit Combemale"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Combemale"
                },
                "author": "Benoit Combemale",
                "arxiv_comment": "53 pages, 3 Figures (not counted the subfigures), 16 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.14699v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.14699v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18516v1",
                "updated": "2025-06-23T11:16:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    16,
                    21,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:16:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    16,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?"
                },
                "summary": "Adversarial examples are small and often imperceptible perturbations crafted\nto fool machine learning models. These attacks seriously threaten the\nreliability of deep neural networks, especially in security-sensitive domains.\nEvasion attacks, a form of adversarial attack where input is modified at test\ntime to cause misclassification, are particularly insidious due to their\ntransferability: adversarial examples crafted against one model often fool\nother models as well. This property, known as adversarial transferability,\ncomplicates defense strategies since it enables black-box attacks to succeed\nwithout direct access to the victim model. While adversarial training is one of\nthe most widely adopted defense mechanisms, its effectiveness is typically\nevaluated on a narrow and homogeneous population of models. This limitation\nhinders the generalizability of empirical findings and restricts practical\nadoption.\n  In this work, we introduce DUMBer, an attack framework built on the\nfoundation of the DUMB (Dataset soUrces, Model architecture, and Balance)\nmethodology, to systematically evaluate the resilience of adversarially trained\nmodels. Our testbed spans multiple adversarial training techniques evaluated\nacross three diverse computer vision tasks, using a heterogeneous population of\nuniquely trained models to reflect real-world deployment variability. Our\nexperimental pipeline comprises over 130k evaluations spanning 13\nstate-of-the-art attack algorithms, allowing us to capture nuanced behaviors of\nadversarial training under varying threat models and dataset conditions. Our\nfindings offer practical, actionable insights for AI practitioners, identifying\nwhich defenses are most effective based on the model, dataset, and attacker\nsetup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial examples are small and often imperceptible perturbations crafted\nto fool machine learning models. These attacks seriously threaten the\nreliability of deep neural networks, especially in security-sensitive domains.\nEvasion attacks, a form of adversarial attack where input is modified at test\ntime to cause misclassification, are particularly insidious due to their\ntransferability: adversarial examples crafted against one model often fool\nother models as well. This property, known as adversarial transferability,\ncomplicates defense strategies since it enables black-box attacks to succeed\nwithout direct access to the victim model. While adversarial training is one of\nthe most widely adopted defense mechanisms, its effectiveness is typically\nevaluated on a narrow and homogeneous population of models. This limitation\nhinders the generalizability of empirical findings and restricts practical\nadoption.\n  In this work, we introduce DUMBer, an attack framework built on the\nfoundation of the DUMB (Dataset soUrces, Model architecture, and Balance)\nmethodology, to systematically evaluate the resilience of adversarially trained\nmodels. Our testbed spans multiple adversarial training techniques evaluated\nacross three diverse computer vision tasks, using a heterogeneous population of\nuniquely trained models to reflect real-world deployment variability. Our\nexperimental pipeline comprises over 130k evaluations spanning 13\nstate-of-the-art attack algorithms, allowing us to capture nuanced behaviors of\nadversarial training under varying threat models and dataset conditions. Our\nfindings offer practical, actionable insights for AI practitioners, identifying\nwhich defenses are most effective based on the model, dataset, and attacker\nsetup."
                },
                "authors": [
                    {
                        "name": "Francesco Marchiori"
                    },
                    {
                        "name": "Marco Alecci"
                    },
                    {
                        "name": "Luca Pajola"
                    },
                    {
                        "name": "Mauro Conti"
                    }
                ],
                "author_detail": {
                    "name": "Mauro Conti"
                },
                "author": "Mauro Conti",
                "arxiv_comment": "Accepted at ESORICS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02072v2",
                "updated": "2025-06-23T11:11:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    11,
                    32,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-04T07:44:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    44,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for\n  Detection of Bias, Discrimination and Stereotyping"
                },
                "summary": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of Large Language Models (LLMs) has transformed natural\nlanguage processing but raises critical concerns about biases inherent in their\ndeployment and use across diverse linguistic and sociocultural contexts. This\npaper presents a framework named ASCenD BDS (Adaptable, Stochastic and\nContext-aware framework for Detection of Bias, Discrimination and\nStereotyping). The framework presents approach to detecting bias,\ndiscrimination, stereotyping across various categories such as gender, caste,\nage, disability, socioeconomic status, linguistic variations, etc., using an\napproach which is Adaptive, Stochastic and Context-Aware. The existing\nframeworks rely heavily on usage of datasets to generate scenarios for\ndetection of Bias, Discrimination and Stereotyping. Examples include datasets\nsuch as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ.\nHowever, such an approach provides point solutions. As a result, these datasets\nprovide a finite number of scenarios for assessment. The current framework\novercomes this limitation by having features which enable Adaptability,\nStochasticity, Context Awareness. Context awareness can be customized for any\nnation or culture or sub-culture (for example an organization's unique\nculture). In this paper, context awareness in the Indian context has been\nestablished. Content has been leveraged from Indian Census 2011 to have a\ncommonality of categorization. A framework has been developed using Category,\nSub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability,\nStochasticity and Context awareness. The framework has been described in detail\nin Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories\nwere developed by a team of consultants at Saint Fox Consultancy Private Ltd.\nThe concept has been tested out in SFCLabs as part of product development."
                },
                "authors": [
                    {
                        "name": "Rajiv Bahl"
                    },
                    {
                        "name": "Venkatesan N"
                    },
                    {
                        "name": "Parimal Aglawe"
                    },
                    {
                        "name": "Aastha Sarasapalli"
                    },
                    {
                        "name": "Bhavya Kancharla"
                    },
                    {
                        "name": "Chaitanya kolukuluri"
                    },
                    {
                        "name": "Harish Mohite"
                    },
                    {
                        "name": "Japneet Hora"
                    },
                    {
                        "name": "Kiran Kakollu"
                    },
                    {
                        "name": "Rahul Dhiman"
                    },
                    {
                        "name": "Shubham Kapale"
                    },
                    {
                        "name": "Sri Bhagya Kathula"
                    },
                    {
                        "name": "Vamsikrishna Motru"
                    },
                    {
                        "name": "Yogeshwar Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Yogeshwar Reddy"
                },
                "author": "Yogeshwar Reddy",
                "arxiv_comment": "17 pages, 6 Figures and this manuscript will be submitted to Q1,Q2\n  Journals",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10150v2",
                "updated": "2025-06-23T11:08:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    8,
                    0,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-13T08:22:31Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    8,
                    22,
                    31,
                    3,
                    72,
                    0
                ],
                "title": "HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge"
                },
                "summary": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly\nenhanced the performance of large language models (LLMs) in domain-specific\ntasks. However, existing RAG methods do not adequately utilize the naturally\ninherent hierarchical knowledge in human cognition, which limits the\ncapabilities of RAG systems. In this paper, we introduce a new RAG approach,\ncalled HiRAG, which utilizes hierarchical knowledge to enhance the semantic\nunderstanding and structure capturing capabilities of RAG systems in the\nindexing and retrieval processes. Our extensive experiments demonstrate that\nHiRAG achieves significant performance improvements over the state-of-the-art\nbaseline methods."
                },
                "authors": [
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Kaili Ma"
                    },
                    {
                        "name": "Hongzhi Chen"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18512v1",
                "updated": "2025-06-23T11:06:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    6,
                    31,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:06:31Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    6,
                    31,
                    0,
                    174,
                    0
                ],
                "title": "MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis"
                },
                "summary": "Accurate and interpretable multi-disease diagnosis remains a critical\nchallenge in medical research, particularly when leveraging heterogeneous\nmultimodal medical data. Current approaches often rely on single-modal data,\nlimiting their ability to comprehensively understand complex diseases. To\naddress this, we propose MedTVT-R1, a novel Multimodal Large Language Model\n(MLLM) framework designed to integrate clinical multimodal data for reasoning\nand diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction\ndataset that provides question-answer pairs for physiological-level\ninterpretations and disease-level diagnoses with a Chain of Evidence approach.\nMedTVT-R1 incorporates a modality perception layer to capture inter-modal\ndependencies and adaptively weight modality contributions. Additionally, we\nemploy Group Relative Policy Optimization (GRPO)-based Reinforcement\nFine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.\nExperimental results demonstrate MedTVT-R1's superiority in multimodal feature\nutilization and multi-disease diagnosis, offering significant potential for\nclinical applications such as diagnostic report generation and comorbidity\nreasoning. The dataset and code are available at\nhttps://github.com/keke-nice/MedTVT-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and interpretable multi-disease diagnosis remains a critical\nchallenge in medical research, particularly when leveraging heterogeneous\nmultimodal medical data. Current approaches often rely on single-modal data,\nlimiting their ability to comprehensively understand complex diseases. To\naddress this, we propose MedTVT-R1, a novel Multimodal Large Language Model\n(MLLM) framework designed to integrate clinical multimodal data for reasoning\nand diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction\ndataset that provides question-answer pairs for physiological-level\ninterpretations and disease-level diagnoses with a Chain of Evidence approach.\nMedTVT-R1 incorporates a modality perception layer to capture inter-modal\ndependencies and adaptively weight modality contributions. Additionally, we\nemploy Group Relative Policy Optimization (GRPO)-based Reinforcement\nFine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.\nExperimental results demonstrate MedTVT-R1's superiority in multimodal feature\nutilization and multi-disease diagnosis, offering significant potential for\nclinical applications such as diagnostic report generation and comorbidity\nreasoning. The dataset and code are available at\nhttps://github.com/keke-nice/MedTVT-R1."
                },
                "authors": [
                    {
                        "name": "Yuting Zhang"
                    },
                    {
                        "name": "Kaishen Yuan"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Yutao Yue"
                    },
                    {
                        "name": "Jintai Chen"
                    },
                    {
                        "name": "Kaishun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kaishun Wu"
                },
                "author": "Kaishun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18510v1",
                "updated": "2025-06-23T11:04:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    4,
                    20,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T11:04:20Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    11,
                    4,
                    20,
                    0,
                    174,
                    0
                ],
                "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich\n  Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich\n  Transcripts"
                },
                "summary": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints."
                },
                "authors": [
                    {
                        "name": "Duygu Altinok"
                    }
                ],
                "author_detail": {
                    "name": "Duygu Altinok"
                },
                "author": "Duygu Altinok",
                "arxiv_comment": "Accepted to INTERSPEECH2025 workshop DISS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18501v1",
                "updated": "2025-06-23T10:52:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    52,
                    54,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T10:52:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    52,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks:\n  Strengths, Weaknesses, and Domain-Specific Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks:\n  Strengths, Weaknesses, and Domain-Specific Performance"
                },
                "summary": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements."
                },
                "authors": [
                    {
                        "name": "Wael Etaiwi"
                    },
                    {
                        "name": "Bushra Alhijawi"
                    }
                ],
                "author_detail": {
                    "name": "Bushra Alhijawi"
                },
                "author": "Bushra Alhijawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06386v2",
                "updated": "2025-06-23T10:50:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    50,
                    0,
                    0,
                    174,
                    0
                ],
                "published": "2025-04-08T19:09:07Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    9,
                    7,
                    1,
                    98,
                    0
                ],
                "title": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task\n  Policies in Model-Free RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task\n  Policies in Model-Free RL"
                },
                "summary": "To apply reinforcement learning to safety-critical applications, we ought to\nprovide safety guarantees during both policy training and deployment. In this\nwork, we present theoretical results that place a bound on the probability of\nviolating a safety property for a new task-specific policy in a model-free,\nepisodic setting. This bound, based on a maximum policy ratio computed with\nrespect to a 'safe' base policy, can also be applied to temporally-extended\nproperties (beyond safety) and to robust control problems. To utilize these\nresults, we introduce SPoRt, which provides a data-driven method for computing\nthis bound for the base policy using the scenario approach, and includes\nProjected PPO, a new projection-based approach for training the task-specific\npolicy while maintaining a user-specified bound on property violation. SPoRt\nthus enables users to trade off safety guarantees against task-specific\nperformance. Complementing our theoretical results, we present experimental\nresults demonstrating this trade-off and comparing the theoretical bound to\nposterior bounds derived from empirical violation rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To apply reinforcement learning to safety-critical applications, we ought to\nprovide safety guarantees during both policy training and deployment. In this\nwork, we present theoretical results that place a bound on the probability of\nviolating a safety property for a new task-specific policy in a model-free,\nepisodic setting. This bound, based on a maximum policy ratio computed with\nrespect to a 'safe' base policy, can also be applied to temporally-extended\nproperties (beyond safety) and to robust control problems. To utilize these\nresults, we introduce SPoRt, which provides a data-driven method for computing\nthis bound for the base policy using the scenario approach, and includes\nProjected PPO, a new projection-based approach for training the task-specific\npolicy while maintaining a user-specified bound on property violation. SPoRt\nthus enables users to trade off safety guarantees against task-specific\nperformance. Complementing our theoretical results, we present experimental\nresults demonstrating this trade-off and comparing the theoretical bound to\nposterior bounds derived from empirical violation rates."
                },
                "authors": [
                    {
                        "name": "Jacques Cloete"
                    },
                    {
                        "name": "Nikolaus Vertovec"
                    },
                    {
                        "name": "Alessandro Abate"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Abate"
                },
                "author": "Alessandro Abate",
                "arxiv_comment": "9 pages + 16 pages supplementary material, 3 figures + 6 figures\n  supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18496v1",
                "updated": "2025-06-23T10:46:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    46,
                    44,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T10:46:44Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    46,
                    44,
                    0,
                    174,
                    0
                ],
                "title": "Biased Teacher, Balanced Student",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biased Teacher, Balanced Student"
                },
                "summary": "Knowledge Distillation (KD) is a widely adopted model compression technique\nwhere a compact student model learns from the output of a larger, pre-trained\nteacher. While effective in balanced settings, conventional KD suffers\nsignificantly when applied to long-tailed data distributions, as the teacher\nmodel tends to be biased toward head classes and provides limited supervision\nfor tail classes. In this paper, we propose Long-Tailed Knowledge Distillation\n(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by\nreformulating the standard KD objective into two components: inter-group and\nintra-group Kullback-Leibler (KL) divergence, corresponding to the prediction\ndistributions across and within class groups (head, medium, tail),\nrespectively. This decomposition allows us to identify and quantify the sources\nof teacher bias. To address them, we introduce (1) a rebalanced inter-group\nloss that calibrates the teacher's group-level predictions and (2) a uniform\nintra-group loss that ensures equal contribution from all groups during\ndistillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and\nImageNet-LT show that LTKD consistently outperforms existing KD methods,\nachieving significant gains in both overall accuracy and tail-class\nperformance. Our results demonstrate that LTKD enables effective knowledge\ntransfer even from biased teachers, making it a strong candidate for real-world\ndeployment in resource-constrained and imbalanced settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation (KD) is a widely adopted model compression technique\nwhere a compact student model learns from the output of a larger, pre-trained\nteacher. While effective in balanced settings, conventional KD suffers\nsignificantly when applied to long-tailed data distributions, as the teacher\nmodel tends to be biased toward head classes and provides limited supervision\nfor tail classes. In this paper, we propose Long-Tailed Knowledge Distillation\n(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by\nreformulating the standard KD objective into two components: inter-group and\nintra-group Kullback-Leibler (KL) divergence, corresponding to the prediction\ndistributions across and within class groups (head, medium, tail),\nrespectively. This decomposition allows us to identify and quantify the sources\nof teacher bias. To address them, we introduce (1) a rebalanced inter-group\nloss that calibrates the teacher's group-level predictions and (2) a uniform\nintra-group loss that ensures equal contribution from all groups during\ndistillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and\nImageNet-LT show that LTKD consistently outperforms existing KD methods,\nachieving significant gains in both overall accuracy and tail-class\nperformance. Our results demonstrate that LTKD enables effective knowledge\ntransfer even from biased teachers, making it a strong candidate for real-world\ndeployment in resource-constrained and imbalanced settings."
                },
                "authors": [
                    {
                        "name": "Seonghak Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seonghak Kim"
                },
                "author": "Seonghak Kim",
                "arxiv_comment": "12 pages, 5 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16796v2",
                "updated": "2025-06-23T10:42:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    42,
                    18,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-20T07:21:21Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    21,
                    21,
                    4,
                    171,
                    0
                ],
                "title": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution\n  with Vision-Language Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution\n  with Vision-Language Chain-of-Thought"
                },
                "summary": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Image Super-Resolution is one of the most challenging task in\nimage restoration. However, existing methods struggle with an accurate\nunderstanding of degraded image content, leading to reconstructed results that\nare both low-fidelity and unnatural. We present RealSR-R1 in this work, which\nempowers the RealSR models with understanding and reasoning capabilities.\nInspired by the success of Chain of Thought (CoT) in large language models\n(LLMs), we simulate the human process of handling degraded images and propose\nthe VLCoT framework, which integrates vision and language reasoning. The\nframework aims to precisely restore image details by progressively generating\nmore comprehensive text and higher-resolution images. To overcome the challenge\nof traditional supervised learning CoT failing to generalize to real-world\nscenarios, we introduce, for the first time, Group Relative Policy Optimization\n(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO\nas a solution, which designs four reward functions: (1) Format reward, used to\nstandardize the CoT process; (2) Degradation reward, to incentivize accurate\ndegradation estimation; (3) Understanding reward, to ensure the accuracy of the\ngenerated content; and (4) Generation reward, where we propose using a visual\nexpert model to evaluate the quality of generated images, encouraging the model\nto generate more realistic images. Extensive experiments demonstrate that our\nproposed RealSR-R1 can generate realistic details and accurately understand\nimage content, particularly in semantically rich scenes or images with severe\ndegradation."
                },
                "authors": [
                    {
                        "name": "Junbo Qiao"
                    },
                    {
                        "name": "Miaomiao Cai"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yutong Liu"
                    },
                    {
                        "name": "Xudong Huang"
                    },
                    {
                        "name": "Gaoqi He"
                    },
                    {
                        "name": "Jiao Xie"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18485v1",
                "updated": "2025-06-23T10:37:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    37,
                    57,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T10:37:57Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    37,
                    57,
                    0,
                    174,
                    0
                ],
                "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning\n  Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning."
                },
                "authors": [
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Guozheng Ma"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15557v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15557v3",
                "updated": "2025-06-23T10:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    23,
                    35,
                    0,
                    174,
                    0
                ],
                "published": "2024-12-20T04:31:03Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    4,
                    31,
                    3,
                    4,
                    355,
                    0
                ],
                "title": "MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems"
                },
                "summary": "With the widespread application of LLM-based dialogue systems in daily life,\nquality assurance has become more important than ever. Recent research has\nsuccessfully introduced methods to identify unexpected behaviour in single-turn\ntesting scenarios. However, multi-turn interaction is the common real-world\nusage of dialogue systems, yet testing methods for such interactions remain\nunderexplored. This is largely due to the oracle problem in multi-turn testing,\nwhich continues to pose a significant challenge for dialogue system developers\nand researchers. In this paper, we propose MORTAR, a metamorphic multi-turn\ndialogue testing approach, which mitigates the test oracle problem in testing\nLLM-based dialogue systems. MORTAR formalises the multi-turn testing for\ndialogue systems, and automates the generation of question-answer dialogue test\ncases with multiple dialogue-level perturbations and metamorphic relations\n(MRs). The automated MR matching mechanism allows MORTAR more flexibility and\nefficiency in metamorphic testing. The proposed approach is fully automated\nwithout reliance on LLM judges. In testing six popular LLM-based dialogue\nsystems, MORTAR reaches significantly better effectiveness with over 150\\% more\nbugs revealed per test case when compared to the single-turn metamorphic\ntesting baseline. Regarding the quality of bugs, MORTAR reveals higher-quality\nbugs in terms of diversity, precision and uniqueness. MORTAR is expected to\ninspire more multi-turn testing approaches, and assist developers in evaluating\nthe dialogue system performance more comprehensively with constrained test\nresources and budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread application of LLM-based dialogue systems in daily life,\nquality assurance has become more important than ever. Recent research has\nsuccessfully introduced methods to identify unexpected behaviour in single-turn\ntesting scenarios. However, multi-turn interaction is the common real-world\nusage of dialogue systems, yet testing methods for such interactions remain\nunderexplored. This is largely due to the oracle problem in multi-turn testing,\nwhich continues to pose a significant challenge for dialogue system developers\nand researchers. In this paper, we propose MORTAR, a metamorphic multi-turn\ndialogue testing approach, which mitigates the test oracle problem in testing\nLLM-based dialogue systems. MORTAR formalises the multi-turn testing for\ndialogue systems, and automates the generation of question-answer dialogue test\ncases with multiple dialogue-level perturbations and metamorphic relations\n(MRs). The automated MR matching mechanism allows MORTAR more flexibility and\nefficiency in metamorphic testing. The proposed approach is fully automated\nwithout reliance on LLM judges. In testing six popular LLM-based dialogue\nsystems, MORTAR reaches significantly better effectiveness with over 150\\% more\nbugs revealed per test case when compared to the single-turn metamorphic\ntesting baseline. Regarding the quality of bugs, MORTAR reveals higher-quality\nbugs in terms of diversity, precision and uniqueness. MORTAR is expected to\ninspire more multi-turn testing approaches, and assist developers in evaluating\nthe dialogue system performance more comprehensively with constrained test\nresources and budget."
                },
                "authors": [
                    {
                        "name": "Guoxiang Guo"
                    },
                    {
                        "name": "Aldeida Aleti"
                    },
                    {
                        "name": "Neelofar Neelofar"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Yuanyuan Qi"
                    },
                    {
                        "name": "Tsong Yueh Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tsong Yueh Chen"
                },
                "author": "Tsong Yueh Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15557v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15557v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18462v1",
                "updated": "2025-06-23T09:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    59,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T09:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    59,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Adaptive alert prioritisation in security operations centres via\n  learning to defer with human feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive alert prioritisation in security operations centres via\n  learning to defer with human feedback"
                },
                "summary": "Alert prioritisation (AP) is crucial for security operations centres (SOCs)\nto manage the overwhelming volume of alerts and ensure timely detection and\nresponse to genuine threats, while minimising alert fatigue. Although\npredictive AI can process large alert volumes and identify known patterns, it\nstruggles with novel and evolving scenarios that demand contextual\nunderstanding and nuanced judgement. A promising solution is Human-AI teaming\n(HAT), which combines human expertise with AI's computational capabilities.\nLearning to Defer (L2D) operationalises HAT by enabling AI to \"defer\" uncertain\nor unfamiliar cases to human experts. However, traditional L2D models rely on\nstatic deferral policies that do not evolve with experience, limiting their\nability to learn from human feedback and adapt over time. To overcome this, we\nintroduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral\nframework that leverages Deep Reinforcement Learning from Human Feedback\n(DRLHF) to optimise deferral decisions. By dynamically incorporating human\nfeedback, L2DHF continuously improves AP accuracy and reduces unnecessary\ndeferrals, enhancing SOC effectiveness and easing analyst workload. Experiments\non two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate\nthat L2DHF significantly outperforms baseline models. Notably, it achieves\n13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on\nCICIDS2017. It also reduces misprioritisations, for example, by 98% for\nhigh-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for\nexample, by 37% on UNSW-NB15, directly reducing analyst workload. These gains\nare achieved with efficient execution, underscoring L2DHF's practicality for\nreal-world SOC deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alert prioritisation (AP) is crucial for security operations centres (SOCs)\nto manage the overwhelming volume of alerts and ensure timely detection and\nresponse to genuine threats, while minimising alert fatigue. Although\npredictive AI can process large alert volumes and identify known patterns, it\nstruggles with novel and evolving scenarios that demand contextual\nunderstanding and nuanced judgement. A promising solution is Human-AI teaming\n(HAT), which combines human expertise with AI's computational capabilities.\nLearning to Defer (L2D) operationalises HAT by enabling AI to \"defer\" uncertain\nor unfamiliar cases to human experts. However, traditional L2D models rely on\nstatic deferral policies that do not evolve with experience, limiting their\nability to learn from human feedback and adapt over time. To overcome this, we\nintroduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral\nframework that leverages Deep Reinforcement Learning from Human Feedback\n(DRLHF) to optimise deferral decisions. By dynamically incorporating human\nfeedback, L2DHF continuously improves AP accuracy and reduces unnecessary\ndeferrals, enhancing SOC effectiveness and easing analyst workload. Experiments\non two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate\nthat L2DHF significantly outperforms baseline models. Notably, it achieves\n13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on\nCICIDS2017. It also reduces misprioritisations, for example, by 98% for\nhigh-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for\nexample, by 37% on UNSW-NB15, directly reducing analyst workload. These gains\nare achieved with efficient execution, underscoring L2DHF's practicality for\nreal-world SOC deployment."
                },
                "authors": [
                    {
                        "name": "Fatemeh Jalalvand"
                    },
                    {
                        "name": "Mohan Baruwal Chhetri"
                    },
                    {
                        "name": "Surya Nepal"
                    },
                    {
                        "name": "Cécile Paris"
                    }
                ],
                "author_detail": {
                    "name": "Cécile Paris"
                },
                "author": "Cécile Paris",
                "arxiv_comment": "No comment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15195v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15195v3",
                "updated": "2025-06-23T09:48:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    48,
                    2,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-19T13:33:29Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    13,
                    33,
                    29,
                    2,
                    78,
                    0
                ],
                "title": "Benchmarking Large Language Models for Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models for Handwritten Text Recognition"
                },
                "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."
                },
                "authors": [
                    {
                        "name": "Giorgia Crosilla"
                    },
                    {
                        "name": "Lukas Klic"
                    },
                    {
                        "name": "Giovanni Colavizza"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Colavizza"
                },
                "author": "Giovanni Colavizza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15195v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15195v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18455v1",
                "updated": "2025-06-23T09:47:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    47,
                    14,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T09:47:14Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    47,
                    14,
                    0,
                    174,
                    0
                ],
                "title": "CODS : A Theoretical Model for Computational Design Based on Design\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CODS : A Theoretical Model for Computational Design Based on Design\n  Space"
                },
                "summary": "We introduce CODS (Computational Optimization in Design Space), a theoretical\nmodel that frames computational design as a constrained optimization problem\nover a structured, multi-dimensional design space. Unlike existing methods that\nrely on handcrafted heuristics or domain-specific rules, CODS provides a\ngeneralizable and interpretable framework that supports diverse design tasks.\nGiven a user requirement and a well-defined design space, CODS automatically\nderives soft and hard constraints using large language models through a\nstructured prompt engineering pipeline. These constraints guide the\noptimization process to generate design solutions that are coherent,\nexpressive, and aligned with user intent. We validate our approach across two\ndomains-visualization design and knitwear generation-demonstrating superior\nperformance in design quality, intent alignment, and user preference compared\nto existing LLM-based methods. CODS offers a unified foundation for scalable,\ncontrollable, and AI-powered design automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CODS (Computational Optimization in Design Space), a theoretical\nmodel that frames computational design as a constrained optimization problem\nover a structured, multi-dimensional design space. Unlike existing methods that\nrely on handcrafted heuristics or domain-specific rules, CODS provides a\ngeneralizable and interpretable framework that supports diverse design tasks.\nGiven a user requirement and a well-defined design space, CODS automatically\nderives soft and hard constraints using large language models through a\nstructured prompt engineering pipeline. These constraints guide the\noptimization process to generate design solutions that are coherent,\nexpressive, and aligned with user intent. We validate our approach across two\ndomains-visualization design and knitwear generation-demonstrating superior\nperformance in design quality, intent alignment, and user preference compared\nto existing LLM-based methods. CODS offers a unified foundation for scalable,\ncontrollable, and AI-powered design automation."
                },
                "authors": [
                    {
                        "name": "Nan Cao"
                    },
                    {
                        "name": "Xiaoyu Qi"
                    },
                    {
                        "name": "Chuer Chen"
                    },
                    {
                        "name": "Xiaoke Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoke Yan"
                },
                "author": "Xiaoke Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18487v2",
                "updated": "2025-06-23T09:35:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    35,
                    39,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-24T09:40:46Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    9,
                    40,
                    46,
                    0,
                    83,
                    0
                ],
                "title": "Large Language Models powered Malicious Traffic Detection: Architecture,\n  Opportunities and Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models powered Malicious Traffic Detection: Architecture,\n  Opportunities and Case Study"
                },
                "summary": "Malicious traffic detection is a pivotal technology for network security to\nidentify abnormal network traffic and detect network attacks. Large Language\nModels (LLMs) are trained on a vast corpus of text, have amassed remarkable\ncapabilities of context-understanding and commonsense knowledge. This has\nopened up a new door for network attacks detection. Researchers have already\ninitiated discussions regarding the application of LLMs on specific\ncyber-security tasks. Unfortunately, there remains a lack of comprehensive\nanalysis on harnessing LLMs for traffic detection, as well as the opportunities\nand challenges. In this paper, we focus on unleashing the full potential of\nLarge Language Models (LLMs) in malicious traffic detection. We present a\nholistic view of the architecture of LLM-powered malicious traffic detection,\nincluding the procedures of Pre-training, Fine-tuning, and Detection.\nEspecially, by exploring the knowledge and capabilities of LLM, we identify\nthree distinct roles LLM can act in traffic classification: Classifier,\nEncoder, and Predictor. For each of them, the modeling paradigm, opportunities\nand challenges are elaborated. Finally, we present our design on LLM-powered\nDDoS detection as a case study. The proposed framework attains accurate\ndetection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual\nmining. The evaluation shows its efficacy, exhibiting a nearly 35% improvement\ncompared to existing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious traffic detection is a pivotal technology for network security to\nidentify abnormal network traffic and detect network attacks. Large Language\nModels (LLMs) are trained on a vast corpus of text, have amassed remarkable\ncapabilities of context-understanding and commonsense knowledge. This has\nopened up a new door for network attacks detection. Researchers have already\ninitiated discussions regarding the application of LLMs on specific\ncyber-security tasks. Unfortunately, there remains a lack of comprehensive\nanalysis on harnessing LLMs for traffic detection, as well as the opportunities\nand challenges. In this paper, we focus on unleashing the full potential of\nLarge Language Models (LLMs) in malicious traffic detection. We present a\nholistic view of the architecture of LLM-powered malicious traffic detection,\nincluding the procedures of Pre-training, Fine-tuning, and Detection.\nEspecially, by exploring the knowledge and capabilities of LLM, we identify\nthree distinct roles LLM can act in traffic classification: Classifier,\nEncoder, and Predictor. For each of them, the modeling paradigm, opportunities\nand challenges are elaborated. Finally, we present our design on LLM-powered\nDDoS detection as a case study. The proposed framework attains accurate\ndetection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual\nmining. The evaluation shows its efficacy, exhibiting a nearly 35% improvement\ncompared to existing systems."
                },
                "authors": [
                    {
                        "name": "Xinggong Zhang"
                    },
                    {
                        "name": "Haotian Meng"
                    },
                    {
                        "name": "Qingyang Li"
                    },
                    {
                        "name": "Yunpeng Tan"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17195v2",
                "updated": "2025-06-23T09:32:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    32,
                    3,
                    0,
                    174,
                    0
                ],
                "published": "2025-03-21T14:43:23Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    14,
                    43,
                    23,
                    4,
                    80,
                    0
                ],
                "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided\n  Subspace Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided\n  Subspace Partitioning"
                },
                "summary": "Model customization necessitates high-quality and diverse datasets, but\nacquiring such data remains time-consuming and labor-intensive. Despite the\ngreat potential of large language models (LLMs) for data synthesis, current\napproaches are constrained by limited seed data, model biases, and\nlow-variation prompts, resulting in limited diversity and biased distributions\nwith the increase of data scales. To tackle this challenge, we introduce\nTREESYNTH, a tree-guided subspace-based data synthesis approach inspired by\ndecision trees. It constructs a spatial partitioning tree to recursively divide\na task-specific full data space (i.e., root node) into numerous atomic\nsubspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes\nto ensure both distinctiveness and comprehensiveness before synthesizing\nsamples within each atomic subspace. This globally dividing-and-synthesizing\nmethod finally collects subspace samples into a comprehensive dataset,\neffectively circumventing repetition and space collapse to ensure the diversity\nof large-scale data synthesis. Furthermore, the spatial partitioning tree\nenables sample allocation into atomic subspaces, allowing the rebalancing of\nexisting datasets for more balanced and comprehensive distributions.\nEmpirically, extensive experiments across diverse benchmarks consistently\ndemonstrate the superior data diversity, model performance, and robust\nscalability of TREESYNTH compared to both human-crafted datasets and peer data\nsynthesis methods, with an average performance gain reaching 10%. Besides, the\nconsistent improvements of TREESYNTH-balanced datasets highlight its\nefficacious application to redistribute existing datasets for more\ncomprehensive coverage and the induced performance enhancement. The code is\navailable at https://github.com/cpa2001/TreeSynth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model customization necessitates high-quality and diverse datasets, but\nacquiring such data remains time-consuming and labor-intensive. Despite the\ngreat potential of large language models (LLMs) for data synthesis, current\napproaches are constrained by limited seed data, model biases, and\nlow-variation prompts, resulting in limited diversity and biased distributions\nwith the increase of data scales. To tackle this challenge, we introduce\nTREESYNTH, a tree-guided subspace-based data synthesis approach inspired by\ndecision trees. It constructs a spatial partitioning tree to recursively divide\na task-specific full data space (i.e., root node) into numerous atomic\nsubspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes\nto ensure both distinctiveness and comprehensiveness before synthesizing\nsamples within each atomic subspace. This globally dividing-and-synthesizing\nmethod finally collects subspace samples into a comprehensive dataset,\neffectively circumventing repetition and space collapse to ensure the diversity\nof large-scale data synthesis. Furthermore, the spatial partitioning tree\nenables sample allocation into atomic subspaces, allowing the rebalancing of\nexisting datasets for more balanced and comprehensive distributions.\nEmpirically, extensive experiments across diverse benchmarks consistently\ndemonstrate the superior data diversity, model performance, and robust\nscalability of TREESYNTH compared to both human-crafted datasets and peer data\nsynthesis methods, with an average performance gain reaching 10%. Besides, the\nconsistent improvements of TREESYNTH-balanced datasets highlight its\nefficacious application to redistribute existing datasets for more\ncomprehensive coverage and the induced performance enhancement. The code is\navailable at https://github.com/cpa2001/TreeSynth."
                },
                "authors": [
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Pengan Chen"
                    },
                    {
                        "name": "Jingqi Zhou"
                    },
                    {
                        "name": "Qintong Li"
                    },
                    {
                        "name": "Jingwei Dong"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18434v1",
                "updated": "2025-06-23T09:16:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    16,
                    4,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T09:16:04Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    16,
                    4,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for\n  Prognosis Prediction in Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for\n  Prognosis Prediction in Medical Imaging"
                },
                "summary": "Artificial Intelligence (AI) holds significant promise for improving\nprognosis prediction in medical imaging, yet its effective application remains\nchallenging. In this work, we introduce a structured benchmark explicitly\ndesigned to evaluate and compare the transferability of Convolutional Neural\nNetworks and Foundation Models in predicting clinical outcomes in COVID-19\npatients, leveraging diverse publicly available Chest X-ray datasets. Our\nexperimental methodology extensively explores a wide set of fine-tuning\nstrategies, encompassing traditional approaches such as Full Fine-Tuning and\nLinear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods\nincluding Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were\nconducted across multiple learning paradigms, including both extensive\nfull-data scenarios and more clinically realistic Few-Shot Learning settings,\nwhich are critical for modeling rare disease outcomes and rapidly emerging\nhealth threats. By implementing a large-scale comparative analysis involving a\ndiverse selection of pretrained models, including general-purpose architectures\npretrained on large-scale datasets such as CLIP and DINOv2, to\nbiomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we\nrigorously assess each model's capacity to effectively adapt and generalize to\nprognosis tasks, particularly under conditions of severe data scarcity and\npronounced class imbalance. The benchmark was designed to capture critical\nconditions common in prognosis tasks, including variations in dataset size and\nclass distribution, providing detailed insights into the strengths and\nlimitations of each fine-tuning strategy. This extensive and structured\nevaluation aims to inform the practical deployment and adoption of robust,\nefficient, and generalizable AI-driven solutions in real-world clinical\nprognosis prediction workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) holds significant promise for improving\nprognosis prediction in medical imaging, yet its effective application remains\nchallenging. In this work, we introduce a structured benchmark explicitly\ndesigned to evaluate and compare the transferability of Convolutional Neural\nNetworks and Foundation Models in predicting clinical outcomes in COVID-19\npatients, leveraging diverse publicly available Chest X-ray datasets. Our\nexperimental methodology extensively explores a wide set of fine-tuning\nstrategies, encompassing traditional approaches such as Full Fine-Tuning and\nLinear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods\nincluding Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were\nconducted across multiple learning paradigms, including both extensive\nfull-data scenarios and more clinically realistic Few-Shot Learning settings,\nwhich are critical for modeling rare disease outcomes and rapidly emerging\nhealth threats. By implementing a large-scale comparative analysis involving a\ndiverse selection of pretrained models, including general-purpose architectures\npretrained on large-scale datasets such as CLIP and DINOv2, to\nbiomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we\nrigorously assess each model's capacity to effectively adapt and generalize to\nprognosis tasks, particularly under conditions of severe data scarcity and\npronounced class imbalance. The benchmark was designed to capture critical\nconditions common in prognosis tasks, including variations in dataset size and\nclass distribution, providing detailed insights into the strengths and\nlimitations of each fine-tuning strategy. This extensive and structured\nevaluation aims to inform the practical deployment and adoption of robust,\nefficient, and generalizable AI-driven solutions in real-world clinical\nprognosis prediction workflows."
                },
                "authors": [
                    {
                        "name": "Filippo Ruffini"
                    },
                    {
                        "name": "Elena Mulero Ayllon"
                    },
                    {
                        "name": "Linlin Shen"
                    },
                    {
                        "name": "Paolo Soda"
                    },
                    {
                        "name": "Valerio Guarrasi"
                    }
                ],
                "author_detail": {
                    "name": "Valerio Guarrasi"
                },
                "author": "Valerio Guarrasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18432v2",
                "updated": "2025-06-24T06:26:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    26,
                    34,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-23T09:13:54Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    13,
                    54,
                    0,
                    174,
                    0
                ],
                "title": "A New Pathway to Integrated Learning and Communication (ILAC): Large AI\n  Model and Hyperdimensional Computing for Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Pathway to Integrated Learning and Communication (ILAC): Large AI\n  Model and Hyperdimensional Computing for Communication"
                },
                "summary": "The rapid evolution of forthcoming sixth-generation (6G) wireless networks\nnecessitates the seamless integration of artificial intelligence (AI) with\nwireless communications to support emerging intelligent applications that\ndemand both efficient communication and robust learning performance. This dual\nrequirement calls for a unified framework of integrated learning and\ncommunication (ILAC), where AI enhances communication through intelligent\nsignal processing and adaptive resource management, while wireless networks\nsupport AI model deployment by enabling efficient and reliable data exchanges.\nHowever, achieving this integration presents significant challenges in\npractice. Communication constraints, such as limited bandwidth and fluctuating\nchannels, hinder learning accuracy and convergence. Simultaneously, AI-driven\nlearning dynamics, including model updates and task-driven inference, introduce\nexcessive burdens on communication systems, necessitating flexible,\ncontext-aware transmission strategies. Finally, we present a case study on a\ncost-to-performance optimization problem, where task assignments, model size\nselection, bandwidth allocation, and transmission power control are jointly\noptimized, considering computational cost, communication efficiency, and\ninference accuracy. Leveraging the Dinkelbach and alternating optimization\nalgorithms, we offer a practical and effective solution to achieve an optimal\nbalance between learning performance and communication constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of forthcoming sixth-generation (6G) wireless networks\nnecessitates the seamless integration of artificial intelligence (AI) with\nwireless communications to support emerging intelligent applications that\ndemand both efficient communication and robust learning performance. This dual\nrequirement calls for a unified framework of integrated learning and\ncommunication (ILAC), where AI enhances communication through intelligent\nsignal processing and adaptive resource management, while wireless networks\nsupport AI model deployment by enabling efficient and reliable data exchanges.\nHowever, achieving this integration presents significant challenges in\npractice. Communication constraints, such as limited bandwidth and fluctuating\nchannels, hinder learning accuracy and convergence. Simultaneously, AI-driven\nlearning dynamics, including model updates and task-driven inference, introduce\nexcessive burdens on communication systems, necessitating flexible,\ncontext-aware transmission strategies. Finally, we present a case study on a\ncost-to-performance optimization problem, where task assignments, model size\nselection, bandwidth allocation, and transmission power control are jointly\noptimized, considering computational cost, communication efficiency, and\ninference accuracy. Leveraging the Dinkelbach and alternating optimization\nalgorithms, we offer a practical and effective solution to achieve an optimal\nbalance between learning performance and communication constraints."
                },
                "authors": [
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Derrick Wing Kwan Ng"
                    },
                    {
                        "name": "Robert Schober"
                    },
                    {
                        "name": "H. Vincent Poor"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Xiaohu You"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu You"
                },
                "author": "Xiaohu You",
                "arxiv_comment": "27 pages 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08301v2",
                "updated": "2025-06-23T09:04:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    4,
                    32,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-12T11:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    2,
                    59,
                    2,
                    43,
                    0
                ],
                "title": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compromising Honesty and Harmlessness in Language Models via Deception\n  Attacks"
                },
                "summary": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce \"deception attacks\"\nthat undermine both of these traits, revealing a vulnerability that, if\nexploited, could have serious real-world consequences. We introduce fine-tuning\nmethods that cause models to selectively deceive users on targeted topics while\nremaining accurate on others. Through a series of experiments, we show that\nsuch targeted deception is effective even in high-stakes domains or\nideologically charged subjects. In addition, we find that deceptive fine-tuning\noften compromises other safety properties: deceptive models are more likely to\nproduce toxic content, including hate speech and stereotypes. Finally, we\nassess whether models can deceive consistently in multi-turn dialogues,\nyielding mixed results. Given that millions of users interact with LLM-based\nchatbots, voice assistants, agents, and other interfaces where trustworthiness\ncannot be ensured, securing these models against deception attacks is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on large language models (LLMs) has demonstrated their\nability to understand and employ deceptive behavior, even without explicit\nprompting. However, such behavior has only been observed in rare, specialized\ncases and has not been shown to pose a serious risk to users. Additionally,\nresearch on AI alignment has made significant advancements in training models\nto refuse generating misleading or toxic content. As a result, LLMs generally\nbecame honest and harmless. In this study, we introduce \"deception attacks\"\nthat undermine both of these traits, revealing a vulnerability that, if\nexploited, could have serious real-world consequences. We introduce fine-tuning\nmethods that cause models to selectively deceive users on targeted topics while\nremaining accurate on others. Through a series of experiments, we show that\nsuch targeted deception is effective even in high-stakes domains or\nideologically charged subjects. In addition, we find that deceptive fine-tuning\noften compromises other safety properties: deceptive models are more likely to\nproduce toxic content, including hate speech and stereotypes. Finally, we\nassess whether models can deceive consistently in multi-turn dialogues,\nyielding mixed results. Given that millions of users interact with LLM-based\nchatbots, voice assistants, agents, and other interfaces where trustworthiness\ncannot be ensured, securing these models against deception attacks is critical."
                },
                "authors": [
                    {
                        "name": "Laurène Vaugrante"
                    },
                    {
                        "name": "Francesca Carlon"
                    },
                    {
                        "name": "Maluna Menke"
                    },
                    {
                        "name": "Thilo Hagendorff"
                    }
                ],
                "author_detail": {
                    "name": "Thilo Hagendorff"
                },
                "author": "Thilo Hagendorff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18424v1",
                "updated": "2025-06-23T09:03:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    3,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T09:03:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    3,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits'\n  Sizing Relationships Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-based Multi-Agent Framework for Analog Circuits'\n  Sizing Relationships Extraction"
                },
                "summary": "In the design process of the analog circuit pre-layout phase, device sizing\nis an important step in determining whether an analog circuit can meet the\nrequired performance metrics. Many existing techniques extract the circuit\nsizing task as a mathematical optimization problem to solve and continuously\nimprove the optimization efficiency from a mathematical perspective. But they\nignore the automatic introduction of prior knowledge, fail to achieve effective\npruning of the search space, which thereby leads to a considerable compression\nmargin remaining in the search space. To alleviate this problem, we propose a\nlarge language model (LLM)-based multi-agent framework for analog circuits'\nsizing relationships extraction from academic papers. The search space in the\nsizing process can be effectively pruned based on the sizing relationship\nextracted by this framework. Eventually, we conducted tests on 3 types of\ncircuits, and the optimization efficiency was improved by $2.32 \\sim 26.6\n\\times$. This work demonstrates that the LLM can effectively prune the search\nspace for analog circuit sizing, providing a new solution for the combination\nof LLMs and conventional analog circuit design automation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the design process of the analog circuit pre-layout phase, device sizing\nis an important step in determining whether an analog circuit can meet the\nrequired performance metrics. Many existing techniques extract the circuit\nsizing task as a mathematical optimization problem to solve and continuously\nimprove the optimization efficiency from a mathematical perspective. But they\nignore the automatic introduction of prior knowledge, fail to achieve effective\npruning of the search space, which thereby leads to a considerable compression\nmargin remaining in the search space. To alleviate this problem, we propose a\nlarge language model (LLM)-based multi-agent framework for analog circuits'\nsizing relationships extraction from academic papers. The search space in the\nsizing process can be effectively pruned based on the sizing relationship\nextracted by this framework. Eventually, we conducted tests on 3 types of\ncircuits, and the optimization efficiency was improved by $2.32 \\sim 26.6\n\\times$. This work demonstrates that the LLM can effectively prune the search\nspace for analog circuit sizing, providing a new solution for the combination\nof LLMs and conventional analog circuit design automation methods."
                },
                "authors": [
                    {
                        "name": "Chengjie Liu"
                    },
                    {
                        "name": "Weiyu Chen"
                    },
                    {
                        "name": "Huiyao Xu"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Li Du"
                    }
                ],
                "author_detail": {
                    "name": "Li Du"
                },
                "author": "Li Du",
                "arxiv_comment": "Accepted by ISEDA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18421v1",
                "updated": "2025-06-23T09:02:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    2,
                    4,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T09:02:04Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    2,
                    4,
                    0,
                    174,
                    0
                ],
                "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning\n  Capabilities of Large Language Models"
                },
                "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub]."
                },
                "authors": [
                    {
                        "name": "Ce Li"
                    },
                    {
                        "name": "Xiaofan Liu"
                    },
                    {
                        "name": "Zhiyan Song"
                    },
                    {
                        "name": "Ce Chi"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Jingjing Yang"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Boshen Shi"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Chao Deng"
                    },
                    {
                        "name": "Junlan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Junlan Feng"
                },
                "author": "Junlan Feng",
                "arxiv_comment": "Benmark report v1.0",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18419v1",
                "updated": "2025-06-23T09:00:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    0,
                    50,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T09:00:50Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    0,
                    50,
                    0,
                    174,
                    0
                ],
                "title": "Generative Diffusion Receivers: Achieving Pilot-Efficient MIMO-OFDM\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Diffusion Receivers: Achieving Pilot-Efficient MIMO-OFDM\n  Communications"
                },
                "summary": "This paper focuses on wireless multiple-input multiple-output\n(MIMO)-orthogonal frequency division multiplex (OFDM) receivers. Traditional\nwireless receivers have relied on mathematical modeling and Bayesian inference,\nachieving remarkable success in most areas but falling short in their ability\nto characterize channel matrices. Neural networks (NNs) have demonstrated\nsignificant potential in this aspect. Nevertheless, integrating traditional\ninference methods with NNs presents challenges, particularly in tracking the\nerror progression. Given the inevitable presence of noise in wireless systems,\ngenerative models that are more resilient to noise are garnering increased\nattention. In this paper, we propose re-evaluating the MIMO-OFDM receiver using\ndiffusion models, which is a common generative approach. With diffusion models,\nwe can effectively leverage prior knowledge of channel matrices and incorporate\ntraditional signal estimation components. Specifically, we explore the\ndiffusion system and introduce an imagination-screening strategy to guide the\ndiffusion process. Furthermore, diffusion models enable adaptation to varying\nnoise levels and pilot schemes using the same NN, significantly reducing\ntraining and deployment costs. Simulated results reveal that, for pilot\ndensities ranging from 4-6 pilots per 64-subcarrier block and signal-to-noise\nratios (SNRs) from -4 dB to 0 dB, our proposed receiver reduces\nchannel-reconstruction error by up to two times compared to leading\ndeep-learning models, with the most pronounced improvements observed in\nlow-pilot conditions. Additionally, performance enhancements can be achieved\nwith a larger imagination size, despite increased computational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on wireless multiple-input multiple-output\n(MIMO)-orthogonal frequency division multiplex (OFDM) receivers. Traditional\nwireless receivers have relied on mathematical modeling and Bayesian inference,\nachieving remarkable success in most areas but falling short in their ability\nto characterize channel matrices. Neural networks (NNs) have demonstrated\nsignificant potential in this aspect. Nevertheless, integrating traditional\ninference methods with NNs presents challenges, particularly in tracking the\nerror progression. Given the inevitable presence of noise in wireless systems,\ngenerative models that are more resilient to noise are garnering increased\nattention. In this paper, we propose re-evaluating the MIMO-OFDM receiver using\ndiffusion models, which is a common generative approach. With diffusion models,\nwe can effectively leverage prior knowledge of channel matrices and incorporate\ntraditional signal estimation components. Specifically, we explore the\ndiffusion system and introduce an imagination-screening strategy to guide the\ndiffusion process. Furthermore, diffusion models enable adaptation to varying\nnoise levels and pilot schemes using the same NN, significantly reducing\ntraining and deployment costs. Simulated results reveal that, for pilot\ndensities ranging from 4-6 pilots per 64-subcarrier block and signal-to-noise\nratios (SNRs) from -4 dB to 0 dB, our proposed receiver reduces\nchannel-reconstruction error by up to two times compared to leading\ndeep-learning models, with the most pronounced improvements observed in\nlow-pilot conditions. Additionally, performance enhancements can be achieved\nwith a larger imagination size, despite increased computational complexity."
                },
                "authors": [
                    {
                        "name": "Yuzhi Yang"
                    },
                    {
                        "name": "Omar Alhussein"
                    },
                    {
                        "name": "Atefeh Arani"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Mérouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Mérouane Debbah"
                },
                "author": "Mérouane Debbah",
                "arxiv_comment": "Submitted to IEEE TNSE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23091v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23091v3",
                "updated": "2025-06-23T08:47:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    47,
                    25,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-29T04:51:56Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    4,
                    51,
                    56,
                    3,
                    149,
                    0
                ],
                "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased\n  Reinforcement Learning in Multimodal Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased\n  Reinforcement Learning in Multimodal Small Language Models"
                },
                "summary": "Recent advancements in large language models (LLMs) have demonstrated\nsubstantial progress in reasoning capabilities, such as DeepSeek-R1, which\nleverages rule-based reinforcement learning to enhance logical reasoning\nsignificantly. However, extending these achievements to multimodal large\nlanguage models (MLLMs) presents critical challenges, which are frequently more\npronounced for Multimodal Small Language Models (MSLMs) given their typically\nweaker foundational reasoning abilities: (1) the scarcity of high-quality\nmultimodal reasoning datasets, (2) the degradation of reasoning capabilities\ndue to the integration of visual processing, and (3) the risk that direct\napplication of reinforcement learning may produce complex yet incorrect\nreasoning processes. To address these challenges, we design a novel framework\nInfi-MMR to systematically unlock the reasoning potential of MSLMs through a\ncurriculum of three carefully structured phases and propose our multimodal\nreasoning model Infi-MMR-3B. The first phase, Foundational Reasoning\nActivation, leverages high-quality textual reasoning datasets to activate and\nstrengthen the model's logical reasoning capabilities. The second phase,\nCross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to\nfacilitate the progressive transfer of reasoning skills to multimodal contexts.\nThe third phase, Multimodal Reasoning Enhancement, employs curated,\ncaption-free multimodal data to mitigate linguistic biases and promote robust\ncross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal\nmath reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision\ntest, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on\nMathVista testmini). Resources are available at\nhttps://huggingface.co/Reallm-Labs/Infi-MMR-3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have demonstrated\nsubstantial progress in reasoning capabilities, such as DeepSeek-R1, which\nleverages rule-based reinforcement learning to enhance logical reasoning\nsignificantly. However, extending these achievements to multimodal large\nlanguage models (MLLMs) presents critical challenges, which are frequently more\npronounced for Multimodal Small Language Models (MSLMs) given their typically\nweaker foundational reasoning abilities: (1) the scarcity of high-quality\nmultimodal reasoning datasets, (2) the degradation of reasoning capabilities\ndue to the integration of visual processing, and (3) the risk that direct\napplication of reinforcement learning may produce complex yet incorrect\nreasoning processes. To address these challenges, we design a novel framework\nInfi-MMR to systematically unlock the reasoning potential of MSLMs through a\ncurriculum of three carefully structured phases and propose our multimodal\nreasoning model Infi-MMR-3B. The first phase, Foundational Reasoning\nActivation, leverages high-quality textual reasoning datasets to activate and\nstrengthen the model's logical reasoning capabilities. The second phase,\nCross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to\nfacilitate the progressive transfer of reasoning skills to multimodal contexts.\nThe third phase, Multimodal Reasoning Enhancement, employs curated,\ncaption-free multimodal data to mitigate linguistic biases and promote robust\ncross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal\nmath reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision\ntest, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on\nMathVista testmini). Resources are available at\nhttps://huggingface.co/Reallm-Labs/Infi-MMR-3B."
                },
                "authors": [
                    {
                        "name": "Zeyu Liu"
                    },
                    {
                        "name": "Yuhang Liu"
                    },
                    {
                        "name": "Guanghao Zhu"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Jianbo Yuan"
                    },
                    {
                        "name": "Xinyao Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    },
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23091v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23091v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18403v1",
                "updated": "2025-06-23T08:40:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    40,
                    45,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T08:40:45Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    40,
                    45,
                    0,
                    174,
                    0
                ],
                "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs"
                },
                "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies."
                },
                "authors": [
                    {
                        "name": "Muntasir Adnan"
                    },
                    {
                        "name": "Carlos C. N. Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Carlos C. N. Kuhn"
                },
                "author": "Carlos C. N. Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01056v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01056v4",
                "updated": "2025-06-24T06:27:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    27,
                    29,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-01T15:48:53Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    15,
                    48,
                    53,
                    6,
                    152,
                    0
                ],
                "title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents"
                },
                "summary": "True intelligence requires active capability acquisition, yet current LLM\nagents inject pre-defined tool schemas into prompts, reducing models to passive\nselectors and falling short of robust general-purpose agency. We introduce\nMCP-Zero, an active agent framework that restores tool discovery autonomy to\nLLMs themselves. Instead of overwhelming models with all available tools,\nMCP-Zero enables agents to actively identify capability gaps, and request\nspecific tools on-demand, transforming them from large-scale retrievers into\ngenuine autonomous agents. The framework operates through three core\nmechanisms: (1) Active Tool Request, where models autonomously generate\nstructured requests specifying their exact tool requirements; (2) Hierarchical\nSemantic Routing, a two-stage algorithm that matches requests to relevant\nservers and tools through improved semantic alignment; (3) Iterative Capability\nExtension, enabling agents to progressively build cross-domain toolchains while\nmaintaining minimal context footprint. We construct MCP-tools, a comprehensive\ndataset of 308 MCP servers and 2,797 tools from the official\nModel-Context-Protocol repository. Experiments demonstrate that MCP-Zero\npreserves agent autonomy while achieving substantial efficiency gains: (i)\naccurate tool selection from nearly 3k candidates across 248.1k tokens; (ii)\n98\\% reduction in token consumption on APIBank while maintaining high accuracy;\nand (iii) consistent multi-turn performance that scales with tool ecosystem\ngrowth. This work establishes active tool discovery as a fundamental design\npattern for scalable autonomous agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "True intelligence requires active capability acquisition, yet current LLM\nagents inject pre-defined tool schemas into prompts, reducing models to passive\nselectors and falling short of robust general-purpose agency. We introduce\nMCP-Zero, an active agent framework that restores tool discovery autonomy to\nLLMs themselves. Instead of overwhelming models with all available tools,\nMCP-Zero enables agents to actively identify capability gaps, and request\nspecific tools on-demand, transforming them from large-scale retrievers into\ngenuine autonomous agents. The framework operates through three core\nmechanisms: (1) Active Tool Request, where models autonomously generate\nstructured requests specifying their exact tool requirements; (2) Hierarchical\nSemantic Routing, a two-stage algorithm that matches requests to relevant\nservers and tools through improved semantic alignment; (3) Iterative Capability\nExtension, enabling agents to progressively build cross-domain toolchains while\nmaintaining minimal context footprint. We construct MCP-tools, a comprehensive\ndataset of 308 MCP servers and 2,797 tools from the official\nModel-Context-Protocol repository. Experiments demonstrate that MCP-Zero\npreserves agent autonomy while achieving substantial efficiency gains: (i)\naccurate tool selection from nearly 3k candidates across 248.1k tokens; (ii)\n98\\% reduction in token consumption on APIBank while maintaining high accuracy;\nand (iii) consistent multi-turn performance that scales with tool ecosystem\ngrowth. This work establishes active tool discovery as a fundamental design\npattern for scalable autonomous agent systems."
                },
                "authors": [
                    {
                        "name": "Xiang Fei"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Hao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Feng"
                },
                "author": "Hao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01056v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01056v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18394v1",
                "updated": "2025-06-23T08:30:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    30,
                    0,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T08:30:00Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    30,
                    0,
                    0,
                    174,
                    0
                ],
                "title": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair\n  via Typestate-Guided Context Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair\n  via Typestate-Guided Context Retrieval"
                },
                "summary": "Memory-related errors in C programming continue to pose significant\nchallenges in software development, primarily due to the complexities of manual\nmemory management inherent in the language. These errors frequently serve as\nvectors for severe vulnerabilities, while their repair requires extensive\nknowledge of program logic and C's memory model. Automated Program Repair (APR)\nhas emerged as a critical research area to address these challenges.\nTraditional APR approaches rely on expert-designed strategies and predefined\ntemplates, which are labor-intensive and constrained by the effectiveness of\nmanual specifications. Deep learning techniques offer a promising alternative\nby automatically extracting repair patterns, but they require substantial\ntraining datasets and often lack interpretability.\n  This paper introduces LTFix, a novel approach that harnesses the potential of\nLarge Language Models (LLMs) for automated memory error repair, especially for\ncomplex repository-level errors that span multiple functions and files. We\naddress two fundamental challenges in LLM-based memory error repair: a limited\nunderstanding of interprocedural memory management patterns and context window\nlimitations for repository-wide analysis. Our approach utilizes a finite\ntypestate automaton to guide the tracking of error-propagation paths and\ncontext trace, capturing both spatial (memory states) and temporal (execution\nhistory) dimensions of error behavior. This typestate-guided context retrieval\nstrategy provides the LLM with concise yet semantically rich information\nrelevant to erroneous memory management, effectively addressing the token\nlimitation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-related errors in C programming continue to pose significant\nchallenges in software development, primarily due to the complexities of manual\nmemory management inherent in the language. These errors frequently serve as\nvectors for severe vulnerabilities, while their repair requires extensive\nknowledge of program logic and C's memory model. Automated Program Repair (APR)\nhas emerged as a critical research area to address these challenges.\nTraditional APR approaches rely on expert-designed strategies and predefined\ntemplates, which are labor-intensive and constrained by the effectiveness of\nmanual specifications. Deep learning techniques offer a promising alternative\nby automatically extracting repair patterns, but they require substantial\ntraining datasets and often lack interpretability.\n  This paper introduces LTFix, a novel approach that harnesses the potential of\nLarge Language Models (LLMs) for automated memory error repair, especially for\ncomplex repository-level errors that span multiple functions and files. We\naddress two fundamental challenges in LLM-based memory error repair: a limited\nunderstanding of interprocedural memory management patterns and context window\nlimitations for repository-wide analysis. Our approach utilizes a finite\ntypestate automaton to guide the tracking of error-propagation paths and\ncontext trace, capturing both spatial (memory states) and temporal (execution\nhistory) dimensions of error behavior. This typestate-guided context retrieval\nstrategy provides the LLM with concise yet semantically rich information\nrelevant to erroneous memory management, effectively addressing the token\nlimitation of LLMs."
                },
                "authors": [
                    {
                        "name": "Xiao Cheng"
                    },
                    {
                        "name": "Zhihao Guo"
                    },
                    {
                        "name": "Huan Huo"
                    },
                    {
                        "name": "Yulei Sui"
                    }
                ],
                "author_detail": {
                    "name": "Yulei Sui"
                },
                "author": "Yulei Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15787v2",
                "updated": "2025-06-23T08:27:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    27,
                    44,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-18T18:10:30Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    18,
                    10,
                    30,
                    2,
                    169,
                    0
                ],
                "title": "SLR: An Automated Synthesis Framework for Scalable Logical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLR: An Automated Synthesis Framework for Scalable Logical Reasoning"
                },
                "summary": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR enables scalable, automated synthesis of\ninductive reasoning tasks with precisely controlled difficulty. For each task,\nSLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation\nprogram used by a symbolic judge to deterministically verify model outputs, and\n(iii) an instruction prompt for the reasoning task. Using SLR, we create\nSLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum\nlevels that progressively increase in relational, arithmetic, and recursive\ncomplexity. Large-scale evaluation reveals that contemporary LLMs readily\nproduce syntactically valid rules, yet often fail at correct logical inference.\nRecent reasoning LLMs do somewhat better, but incur substantial increases in\ntest-time compute, sometimes exceeding 15k completion tokens. Finally,\nlogic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity\nwith Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully\nautomated, requires no human annotation, ensures dataset novelty, and offers a\nscalable environment for probing and advancing LLMs' reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SLR, an end-to-end framework for systematic evaluation and\ntraining of Large Language Models (LLMs) via Scalable Logical Reasoning. Given\na user's task specification, SLR enables scalable, automated synthesis of\ninductive reasoning tasks with precisely controlled difficulty. For each task,\nSLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation\nprogram used by a symbolic judge to deterministically verify model outputs, and\n(iii) an instruction prompt for the reasoning task. Using SLR, we create\nSLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum\nlevels that progressively increase in relational, arithmetic, and recursive\ncomplexity. Large-scale evaluation reveals that contemporary LLMs readily\nproduce syntactically valid rules, yet often fail at correct logical inference.\nRecent reasoning LLMs do somewhat better, but incur substantial increases in\ntest-time compute, sometimes exceeding 15k completion tokens. Finally,\nlogic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity\nwith Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully\nautomated, requires no human annotation, ensures dataset novelty, and offers a\nscalable environment for probing and advancing LLMs' reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Lukas Helff"
                    },
                    {
                        "name": "Ahmad Omar"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "name": "Antonia Wüst"
                    },
                    {
                        "name": "Tim Woydt"
                    },
                    {
                        "name": "Rupert Mitchell"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18387v1",
                "updated": "2025-06-23T08:19:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    19,
                    21,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T08:19:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    19,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and\n  Human-Aligned Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Causal Explanation in Medical Reports with LLM-Based and\n  Human-Aligned Metrics"
                },
                "summary": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning."
                },
                "authors": [
                    {
                        "name": "Yousang Cho"
                    },
                    {
                        "name": "Key-Sun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Key-Sun Choi"
                },
                "author": "Key-Sun Choi",
                "arxiv_comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy,\n  July 17, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18383v1",
                "updated": "2025-06-23T08:15:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    15,
                    24,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T08:15:24Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    8,
                    15,
                    24,
                    0,
                    174,
                    0
                ],
                "title": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using\n  LLMs and Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using\n  LLMs and Preference Optimization"
                },
                "summary": "Logical reasoning is a key task for artificial intelligence due to it's role\nin major downstream tasks such as Question Answering, Summarization. Recent\nmethods in improving the reasoning ability of LLMs fall short in correctly\nconverting a natural language reasoning problem to an equivalent logical\nformulation, which hinders the framework's overall ability to reason. Towards\nthis, we propose to use finetuning on a preference optimization dataset to\nlearn to parse and represent a natural language problem as a whole to a\nconsistent logical program by 1) introducing a new supervised and preference\noptimization dataset LogicPO, and 2) adopting popular techniques such as Direct\nPreference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune\nopen-source LLMs. Our best model with Phi-3.5 consistently outperforms\nGPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%\nless syntax errors. Through the framework and our improved evaluation metrics,\nwe offer a promising direction in improving the logical reasoning of LLMs by\nbetter representing them in their logical formulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is a key task for artificial intelligence due to it's role\nin major downstream tasks such as Question Answering, Summarization. Recent\nmethods in improving the reasoning ability of LLMs fall short in correctly\nconverting a natural language reasoning problem to an equivalent logical\nformulation, which hinders the framework's overall ability to reason. Towards\nthis, we propose to use finetuning on a preference optimization dataset to\nlearn to parse and represent a natural language problem as a whole to a\nconsistent logical program by 1) introducing a new supervised and preference\noptimization dataset LogicPO, and 2) adopting popular techniques such as Direct\nPreference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune\nopen-source LLMs. Our best model with Phi-3.5 consistently outperforms\nGPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%\nless syntax errors. Through the framework and our improved evaluation metrics,\nwe offer a promising direction in improving the logical reasoning of LLMs by\nbetter representing them in their logical formulations."
                },
                "authors": [
                    {
                        "name": "Koushik Viswanadha"
                    },
                    {
                        "name": "Deepanway Ghosal"
                    },
                    {
                        "name": "Somak Aditya"
                    }
                ],
                "author_detail": {
                    "name": "Somak Aditya"
                },
                "author": "Somak Aditya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12158v2",
                "updated": "2025-06-23T07:52:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    52,
                    34,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-13T18:24:25Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    18,
                    24,
                    25,
                    4,
                    164,
                    0
                ],
                "title": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource\n  Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource\n  Languages"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to generate synthetic\ntextual data for training smaller specialized models. However, a comparison of\nvarious generation strategies for low-resource language settings is lacking.\nWhile various prompting strategies have been proposed, such as demonstrations,\nlabel-based summaries, and self-revision, their comparative effectiveness\nremains unclear, especially for low-resource languages. In this paper, we\nsystematically evaluate the performance of these generation strategies and\ntheir combinations across 11 typologically diverse languages, including several\nextremely low-resource ones. Using three NLP tasks and four open-source LLMs,\nwe assess downstream model performance on generated versus gold-standard data.\nOur results show that strategic combinations of generation methods,\nparticularly target-language demonstrations with LLM-based revisions, yield\nstrong performance, narrowing the gap with real data to as little as 5% in some\nsettings. We also find that smart prompting techniques can reduce the advantage\nof larger LLMs, highlighting efficient generation strategies for synthetic data\ngeneration in low-resource scenarios with smaller models."
                },
                "authors": [
                    {
                        "name": "Tatiana Anikina"
                    },
                    {
                        "name": "Jan Cegin"
                    },
                    {
                        "name": "Jakub Simko"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "21 pages, fixed typo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09655v2",
                "updated": "2025-06-23T07:49:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    49,
                    8,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-11T12:25:32Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    12,
                    25,
                    32,
                    2,
                    162,
                    0
                ],
                "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy"
                },
                "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games."
                },
                "authors": [
                    {
                        "name": "Kaixuan Xu"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Sicheng Li"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Yuanheng Zhu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "arxiv_comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00753v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00753v3",
                "updated": "2025-06-23T07:45:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    45,
                    18,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-01T08:29:26Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    8,
                    29,
                    26,
                    3,
                    121,
                    0
                ],
                "title": "A Survey on Large Language Model based Human-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model based Human-Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have sparked growing interest\nin building fully autonomous agents. However, fully autonomous LLM-based agents\nstill face significant challenges, including limited reliability due to\nhallucinations, difficulty in handling complex tasks, and substantial safety\nand ethical risks, all of which limit their feasibility and trustworthiness in\nreal-world applications. To overcome these limitations, LLM-based human-agent\nsystems (LLM-HAS) incorporate human-provided information, feedback, or control\ninto the agent system to enhance system performance, reliability and safety.\nThese human-agent collaboration systems enable humans and LLM-based agents to\ncollaborate effectively by leveraging their complementary strengths. This paper\nprovides the first comprehensive and structured survey of LLM-HAS. It clarifies\nfundamental concepts, systematically presents core components shaping these\nsystems, including environment & profiling, human feedback, interaction types,\norchestration and communication, explores emerging applications, and discusses\nunique challenges and opportunities arising from human-AI collaboration. By\nconsolidating current knowledge and offering a structured overview, we aim to\nfoster further research and innovation in this rapidly evolving\ninterdisciplinary field. Paper lists and resources are available at\nhttps://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems."
                },
                "authors": [
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Wei-Chieh Huang"
                    },
                    {
                        "name": "Yaozu Wu"
                    },
                    {
                        "name": "Yankai Chen"
                    },
                    {
                        "name": "Chunyu Miao"
                    },
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Yue Zhou"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Liancheng Fang"
                    },
                    {
                        "name": "Langzhou He"
                    },
                    {
                        "name": "Yangning Li"
                    },
                    {
                        "name": "Dongyuan Li"
                    },
                    {
                        "name": "Renhe Jiang"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Paper lists and resources are available at\n  https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00753v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00753v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18359v1",
                "updated": "2025-06-23T07:43:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    43,
                    21,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T07:43:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    43,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "Recipe for Discovery: A Framework for Systematic Open Source Project\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recipe for Discovery: A Framework for Systematic Open Source Project\n  Identification"
                },
                "summary": "Open source software development, particularly within institutions such as\nuniversities and research laboratories, is often decentralized and difficult to\ntrack. Despite producing highly impactful tools in science, these efforts often\ngo unrecognized due to a lack of visibility and institutional awareness. This\npaper addresses the challenge of discovering, classifying, and analyzing open\nsource software projects developed across distributed institutional systems. We\npresent a framework for systematically identifying institutional affiliated\nrepositories, using the University of California (UC) system as a case study.\n  Using GitHub's REST API, we build a pipeline to discover relevant\nrepositories and extract meaningful metadata. We then propose and evaluate\nmultiple classification strategies, including both traditional machine learning\nmodels and large language models (LLMs), to distinguish affiliated projects\nfrom unrelated repositories and generate accurate insights into the academic\nopen source landscape. Our results show that the framework is effective at\nscale, discovering over 52,000 repositories and predicting institutional\naffiliation with high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open source software development, particularly within institutions such as\nuniversities and research laboratories, is often decentralized and difficult to\ntrack. Despite producing highly impactful tools in science, these efforts often\ngo unrecognized due to a lack of visibility and institutional awareness. This\npaper addresses the challenge of discovering, classifying, and analyzing open\nsource software projects developed across distributed institutional systems. We\npresent a framework for systematically identifying institutional affiliated\nrepositories, using the University of California (UC) system as a case study.\n  Using GitHub's REST API, we build a pipeline to discover relevant\nrepositories and extract meaningful metadata. We then propose and evaluate\nmultiple classification strategies, including both traditional machine learning\nmodels and large language models (LLMs), to distinguish affiliated projects\nfrom unrelated repositories and generate accurate insights into the academic\nopen source landscape. Our results show that the framework is effective at\nscale, discovering over 52,000 repositories and predicting institutional\naffiliation with high accuracy."
                },
                "authors": [
                    {
                        "name": "Juanita Gomez"
                    },
                    {
                        "name": "Emily Lovell"
                    },
                    {
                        "name": "Stephanie Lieggi"
                    },
                    {
                        "name": "Alvaro A. Cardenas"
                    },
                    {
                        "name": "James Davis"
                    }
                ],
                "author_detail": {
                    "name": "James Davis"
                },
                "author": "James Davis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18349v1",
                "updated": "2025-06-23T07:15:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    15,
                    59,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T07:15:59Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    15,
                    59,
                    0,
                    174,
                    0
                ],
                "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct ."
                },
                "authors": [
                    {
                        "name": "Zichong Li"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Zixuan Zhang"
                    },
                    {
                        "name": "Ilgee Hong"
                    },
                    {
                        "name": "Young Jin Kim"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18348v1",
                "updated": "2025-06-23T07:12:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    12,
                    8,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T07:12:08Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    12,
                    8,
                    0,
                    174,
                    0
                ],
                "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely\n  Unleashing the Potential of a Multi-Agent Research Team"
                },
                "summary": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific progress increasingly relies on effective collaboration among\nresearchers, a dynamic that large language models (LLMs) have only begun to\nemulate. While recent LLM-based scientist agents show promise in autonomous\nscientific discovery, they often lack the interactive reasoning and evaluation\nmechanisms essential to real-world research. We propose IDVSCI (Internal\nDiscussion and Vote SCIentists), a multi-agent framework built on LLMs that\nincorporates two key innovations: a Dynamic Knowledge Exchange mechanism\nenabling iterative feedback among agents, and a Dual-Diversity Review paradigm\nthat simulates heterogeneous expert evaluation. These components jointly\npromote deeper reasoning and the generation of more creative and impactful\nscientific ideas. To evaluate the effectiveness and generalizability of our\napproach, we conduct experiments on two datasets: a widely used benchmark in\ncomputer science and a new dataset we introduce in the health sciences domain.\nResults show that IDVSCI consistently achieves the best performance across both\ndatasets, outperforming existing systems such as AI Scientist and VIRSCI. These\nfindings highlight the value of modeling interaction and peer review dynamics\nin LLM-based autonomous research."
                },
                "authors": [
                    {
                        "name": "Weilun Yu"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Yonggui Huang"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Honggang Qi"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Xiaoli Diao"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Wanli Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ouyang"
                },
                "author": "Wanli Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15721v2",
                "updated": "2025-06-23T07:03:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    3,
                    18,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-04T17:01:38Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    1,
                    38,
                    2,
                    155,
                    0
                ],
                "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration"
                },
                "summary": "Heterogeneous Large Language Model (LLM) fusion integrates the strengths of\nmultiple source LLMs with different architectures into a target LLM with low\ncomputational overhead. While promising, existing methods suffer from two major\nlimitations: 1) reliance on real data from limited domain for knowledge fusion,\npreventing the target LLM from fully acquiring knowledge across diverse\ndomains, and 2) fixed data allocation proportions across domains, failing to\ndynamically adjust according to the target LLM's varying capabilities across\ndomains, leading to a capability imbalance. To overcome these limitations, we\npropose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.\nThrough the organization of knowledge domains into a hierarchical tree\nstructure, Bohdi enables automatic domain exploration and multi-domain data\ngeneration through multi-model collaboration, thereby comprehensively\nextracting knowledge from source LLMs. By formalizing domain expansion and data\nsampling proportion allocation on the knowledge tree as a Hierarchical\nMulti-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism\nto adaptively adjust sampling proportions based on the target LLM's performance\nfeedback across domains. Integrated with our proposed Introspection-Rebirth\n(IR) mechanism, DynaBranches dynamically tracks capability shifts during target\nLLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),\nfurther enhancing its online adaptation capability. Comparative experimental\nresults on a comprehensive suite of benchmarks demonstrate that Bohdi\nsignificantly outperforms existing baselines on multiple target LLMs, exhibits\nhigher data efficiency, and virtually eliminates the imbalance in the target\nLLM's capabilities. Our code is available at\nhttps://github.com/gjq100/Bohdi.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Large Language Model (LLM) fusion integrates the strengths of\nmultiple source LLMs with different architectures into a target LLM with low\ncomputational overhead. While promising, existing methods suffer from two major\nlimitations: 1) reliance on real data from limited domain for knowledge fusion,\npreventing the target LLM from fully acquiring knowledge across diverse\ndomains, and 2) fixed data allocation proportions across domains, failing to\ndynamically adjust according to the target LLM's varying capabilities across\ndomains, leading to a capability imbalance. To overcome these limitations, we\npropose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.\nThrough the organization of knowledge domains into a hierarchical tree\nstructure, Bohdi enables automatic domain exploration and multi-domain data\ngeneration through multi-model collaboration, thereby comprehensively\nextracting knowledge from source LLMs. By formalizing domain expansion and data\nsampling proportion allocation on the knowledge tree as a Hierarchical\nMulti-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism\nto adaptively adjust sampling proportions based on the target LLM's performance\nfeedback across domains. Integrated with our proposed Introspection-Rebirth\n(IR) mechanism, DynaBranches dynamically tracks capability shifts during target\nLLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),\nfurther enhancing its online adaptation capability. Comparative experimental\nresults on a comprehensive suite of benchmarks demonstrate that Bohdi\nsignificantly outperforms existing baselines on multiple target LLMs, exhibits\nhigher data efficiency, and virtually eliminates the imbalance in the target\nLLM's capabilities. Our code is available at\nhttps://github.com/gjq100/Bohdi.git."
                },
                "authors": [
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Zhichang Guo"
                    },
                    {
                        "name": "Dazhi Zhang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17992v2",
                "updated": "2025-06-23T06:59:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    59,
                    8,
                    0,
                    174,
                    0
                ],
                "published": "2024-09-26T16:02:25Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    2,
                    25,
                    3,
                    270,
                    0
                ],
                "title": "LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged\n  Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged\n  Robots"
                },
                "summary": "Reinforcement Learning (RL) has shown its remarkable and generalizable\ncapability in legged locomotion through sim-to-real transfer. However, while\nadaptive methods like domain randomization are expected to enhance policy\nrobustness across diverse environments, they potentially compromise the\npolicy's performance in any specific environment, leading to suboptimal\nreal-world deployment due to the No Free Lunch theorem. To address this, we\npropose LoopSR, a lifelong policy adaptation framework that continuously\nrefines RL policies in the post-deployment stage. LoopSR employs a\ntransformer-based encoder to map real-world trajectories into a latent space\nand reconstruct a digital twin of the real world for further improvement.\nAutoencoder architecture and contrastive learning methods are adopted to\nenhance feature extraction of real-world dynamics. Simulation parameters for\ncontinual training are derived by combining predicted values from the decoder\nwith retrieved parameters from a pre-collected simulation trajectory dataset.\nBy leveraging simulated continual training, LoopSR achieves superior data\nefficiency compared with strong baselines, yielding eminent performance with\nlimited data in both sim-to-sim and sim-to-real experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has shown its remarkable and generalizable\ncapability in legged locomotion through sim-to-real transfer. However, while\nadaptive methods like domain randomization are expected to enhance policy\nrobustness across diverse environments, they potentially compromise the\npolicy's performance in any specific environment, leading to suboptimal\nreal-world deployment due to the No Free Lunch theorem. To address this, we\npropose LoopSR, a lifelong policy adaptation framework that continuously\nrefines RL policies in the post-deployment stage. LoopSR employs a\ntransformer-based encoder to map real-world trajectories into a latent space\nand reconstruct a digital twin of the real world for further improvement.\nAutoencoder architecture and contrastive learning methods are adopted to\nenhance feature extraction of real-world dynamics. Simulation parameters for\ncontinual training are derived by combining predicted values from the decoder\nwith retrieved parameters from a pre-collected simulation trajectory dataset.\nBy leveraging simulated continual training, LoopSR achieves superior data\nefficiency compared with strong baselines, yielding eminent performance with\nlimited data in both sim-to-sim and sim-to-real experiments."
                },
                "authors": [
                    {
                        "name": "Peilin Wu"
                    },
                    {
                        "name": "Weiji Xie"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Hang Lai"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18343v1",
                "updated": "2025-06-23T06:52:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    52,
                    38,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:52:38Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    52,
                    38,
                    0,
                    174,
                    0
                ],
                "title": "TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for\n  Exploration and Rescue Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for\n  Exploration and Rescue Operations"
                },
                "summary": "The increasing demand for underwater exploration and rescue operations\nenforces the development of advanced wireless or semi-wireless underwater\nvessels equipped with manipulator arms. This paper presents the implementation\nof a semi-wireless underwater vehicle, \"TritonZ\" equipped with a manipulator\narm, tailored for effective underwater exploration and rescue operations. The\nvehicle's compact design enables deployment in different submarine\nsurroundings, addressing the need for wireless systems capable of navigating\nchallenging underwater terrains. The manipulator arm can interact with the\nenvironment, allowing the robot to perform sophisticated tasks during\nexploration and rescue missions in emergency situations. TritonZ is equipped\nwith various sensors such as Pi-Camera, Humidity, and Temperature sensors to\nsend real-time environmental data. Our underwater vehicle controlled using a\ncustomized remote controller can navigate efficiently in the water where\nPi-Camera enables live streaming of the surroundings. Motion control and video\ncapture are performed simultaneously using this camera. The manipulator arm is\ndesigned to perform various tasks, similar to grasping, manipulating, and\ncollecting underwater objects. Experimental results shows the efficacy of the\nproposed remotely operated vehicle in performing a variety of underwater\nexploration and rescue tasks. Additionally, the results show that TritonZ can\nmaintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.\nFurthermore, the vehicle can sustain waves underwater by maintaining its\nposition as well as average velocity. The full project details and source code\ncan be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for underwater exploration and rescue operations\nenforces the development of advanced wireless or semi-wireless underwater\nvessels equipped with manipulator arms. This paper presents the implementation\nof a semi-wireless underwater vehicle, \"TritonZ\" equipped with a manipulator\narm, tailored for effective underwater exploration and rescue operations. The\nvehicle's compact design enables deployment in different submarine\nsurroundings, addressing the need for wireless systems capable of navigating\nchallenging underwater terrains. The manipulator arm can interact with the\nenvironment, allowing the robot to perform sophisticated tasks during\nexploration and rescue missions in emergency situations. TritonZ is equipped\nwith various sensors such as Pi-Camera, Humidity, and Temperature sensors to\nsend real-time environmental data. Our underwater vehicle controlled using a\ncustomized remote controller can navigate efficiently in the water where\nPi-Camera enables live streaming of the surroundings. Motion control and video\ncapture are performed simultaneously using this camera. The manipulator arm is\ndesigned to perform various tasks, similar to grasping, manipulating, and\ncollecting underwater objects. Experimental results shows the efficacy of the\nproposed remotely operated vehicle in performing a variety of underwater\nexploration and rescue tasks. Additionally, the results show that TritonZ can\nmaintain an average of 13.5cm/s with a minimal delay of 2-3 seconds.\nFurthermore, the vehicle can sustain waves underwater by maintaining its\nposition as well as average velocity. The full project details and source code\ncan be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ"
                },
                "authors": [
                    {
                        "name": "Kawser Ahmed"
                    },
                    {
                        "name": "Mir Shahriar Fardin"
                    },
                    {
                        "name": "Md Arif Faysal Nayem"
                    },
                    {
                        "name": "Fahim Hafiz"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    }
                ],
                "author_detail": {
                    "name": "Swakkhar Shatabda"
                },
                "author": "Swakkhar Shatabda",
                "arxiv_doi": "10.1109/ICCIT64611.2024.11022145",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICCIT64611.2024.11022145",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.18343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 5 figures",
                "arxiv_journal_ref": "In: 2024 27th International Conference on Computer and Information\n  Technology (ICCIT), Cox's Bazar, Bangladesh, 20-22 December 2024, pp. 417-422",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18341v1",
                "updated": "2025-06-23T06:47:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    47,
                    28,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:47:28Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    47,
                    28,
                    0,
                    174,
                    0
                ],
                "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient\n  Test-Time Reasoning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less Data Less Tokens: Multilingual Unification Learning for Efficient\n  Test-Time Reasoning in LLMs"
                },
                "summary": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs."
                },
                "authors": [
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Yixin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Cao"
                },
                "author": "Yixin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21091v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21091v3",
                "updated": "2025-06-23T06:43:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    43,
                    45,
                    0,
                    174,
                    0
                ],
                "published": "2025-05-27T12:19:08Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    19,
                    8,
                    1,
                    147,
                    0
                ],
                "title": "Position is Power: System Prompts as a Mechanism of Bias in Large\n  Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position is Power: System Prompts as a Mechanism of Bias in Large\n  Language Models (LLMs)"
                },
                "summary": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System prompts in Large Language Models (LLMs) are predefined directives that\nguide model behaviour, taking precedence over user inputs in text processing\nand generation. LLM deployers increasingly use them to ensure consistent\nresponses across contexts. While model providers set a foundation of system\nprompts, deployers and third-party developers can append additional prompts\nwithout visibility into others' additions, while this layered implementation\nremains entirely hidden from end-users. As system prompts become more complex,\nthey can directly or indirectly introduce unaccounted for side effects. This\nlack of transparency raises fundamental questions about how the position of\ninformation in different directives shapes model outputs. As such, this work\nexamines how the placement of information affects model behaviour. To this end,\nwe compare how models process demographic information in system versus user\nprompts across six commercially available LLMs and 50 demographic groups. Our\nanalysis reveals significant biases, manifesting in differences in user\nrepresentation and decision-making scenarios. Since these variations stem from\ninaccessible and opaque system-level configurations, they risk\nrepresentational, allocative and potential other biases and downstream harms\nbeyond the user's ability to detect or correct. Our findings draw attention to\nthese critical issues, which have the potential to perpetuate harms if left\nunexamined. Further, we argue that system prompt analysis must be incorporated\ninto AI auditing processes, particularly as customisable system prompts become\nincreasingly prevalent in commercial AI deployments."
                },
                "authors": [
                    {
                        "name": "Anna Neumann"
                    },
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    },
                    {
                        "name": "Jatinder Singh"
                    }
                ],
                "author_detail": {
                    "name": "Jatinder Singh"
                },
                "author": "Jatinder Singh",
                "arxiv_doi": "10.1145/3715275.3732038",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715275.3732038",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.21091v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21091v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Proceedings of ACM FAccT 2025 Update Comment: Fixed the\n  error where user vs. system and implicit vs. explicit labels in the heatmaps\n  were switched. The takeaways remain the same",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18337v1",
                "updated": "2025-06-23T06:38:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    38,
                    49,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:38:49Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    38,
                    49,
                    0,
                    174,
                    0
                ],
                "title": "TranslationCorrect: A Unified Framework for Machine Translation\n  Post-Editing with Predictive Error Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TranslationCorrect: A Unified Framework for Machine Translation\n  Post-Editing with Predictive Error Assistance"
                },
                "summary": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods."
                },
                "authors": [
                    {
                        "name": "Syed Mekael Wasti"
                    },
                    {
                        "name": "Shou-Yi Hung"
                    },
                    {
                        "name": "Christopher Collins"
                    },
                    {
                        "name": "En-Shiun Annie Lee"
                    }
                ],
                "author_detail": {
                    "name": "En-Shiun Annie Lee"
                },
                "author": "En-Shiun Annie Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18330v1",
                "updated": "2025-06-23T06:23:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    23,
                    53,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:23:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    23,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning"
                },
                "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math."
                },
                "authors": [
                    {
                        "name": "Lixin Wu"
                    },
                    {
                        "name": "Na Cai"
                    },
                    {
                        "name": "Qiao Cheng"
                    },
                    {
                        "name": "Jiachen Wang"
                    },
                    {
                        "name": "Yitao Duan"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Duan"
                },
                "author": "Yitao Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18323v1",
                "updated": "2025-06-23T06:11:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    11,
                    55,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:11:55Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    11,
                    55,
                    0,
                    174,
                    0
                ],
                "title": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for\n  Low-Light Image Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for\n  Low-Light Image Enhancement"
                },
                "summary": "Low-light image enhancement remains a challenging task, particularly in the\nabsence of paired training data. In this study, we present LucentVisionNet, a\nnovel zero-shot learning framework that addresses the limitations of\ntraditional and deep learning-based enhancement methods. The proposed approach\nintegrates multi-scale spatial attention with a deep curve estimation network,\nenabling fine-grained enhancement while preserving semantic and perceptual\nfidelity. To further improve generalization, we adopt a recurrent enhancement\nstrategy and optimize the model using a composite loss function comprising six\ntailored components, including a novel no-reference image quality loss inspired\nby human visual perception. Extensive experiments on both paired and unpaired\nbenchmark datasets demonstrate that LucentVisionNet consistently outperforms\nstate-of-the-art supervised, unsupervised, and zero-shot methods across\nmultiple full-reference and no-reference image quality metrics. Our framework\nachieves high visual quality, structural consistency, and computational\nefficiency, making it well-suited for deployment in real-world applications\nsuch as mobile photography, surveillance, and autonomous navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-light image enhancement remains a challenging task, particularly in the\nabsence of paired training data. In this study, we present LucentVisionNet, a\nnovel zero-shot learning framework that addresses the limitations of\ntraditional and deep learning-based enhancement methods. The proposed approach\nintegrates multi-scale spatial attention with a deep curve estimation network,\nenabling fine-grained enhancement while preserving semantic and perceptual\nfidelity. To further improve generalization, we adopt a recurrent enhancement\nstrategy and optimize the model using a composite loss function comprising six\ntailored components, including a novel no-reference image quality loss inspired\nby human visual perception. Extensive experiments on both paired and unpaired\nbenchmark datasets demonstrate that LucentVisionNet consistently outperforms\nstate-of-the-art supervised, unsupervised, and zero-shot methods across\nmultiple full-reference and no-reference image quality metrics. Our framework\nachieves high visual quality, structural consistency, and computational\nefficiency, making it well-suited for deployment in real-world applications\nsuch as mobile photography, surveillance, and autonomous navigation."
                },
                "authors": [
                    {
                        "name": "Muhammad Azeem Aslam"
                    },
                    {
                        "name": "Hassan Khalid"
                    },
                    {
                        "name": "Nisar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Nisar Ahmed"
                },
                "author": "Nisar Ahmed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18317v1",
                "updated": "2025-06-23T06:04:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    4,
                    45,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:04:45Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    4,
                    45,
                    0,
                    174,
                    0
                ],
                "title": "Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi\n  Ranging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi\n  Ranging"
                },
                "summary": "Indoor localization opens the path to potentially transformative\napplications. Although many indoor localization methods have been proposed over\nthe years, they remain too impractical for widespread deployment in the real\nworld. In this paper, we introduce PeepLoc, a deployable and scalable\nWi-Fi-based solution for indoor localization that relies only on pre-existing\ndevices and infrastructure. Specifically, PeepLoc works on any mobile device\nwith an unmodified Wi-Fi transceiver and in any indoor environment with a\nsufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the\ncore of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain\nnon-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel\nbootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and\ncrowdsourcing to opportunistically initialize pre-existing APs as anchor points\nwithin an environment. We implement PeepLoc using commodity hardware and\nevaluate it extensively across 4 campus buildings. We show PeepLoc leads to a\nmean and median positional error of 3.41 m and 3.06 m respectively, which is\nsuperior to existing deployed indoor localization systems and is competitive\nwith commodity GPS in outdoor environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Indoor localization opens the path to potentially transformative\napplications. Although many indoor localization methods have been proposed over\nthe years, they remain too impractical for widespread deployment in the real\nworld. In this paper, we introduce PeepLoc, a deployable and scalable\nWi-Fi-based solution for indoor localization that relies only on pre-existing\ndevices and infrastructure. Specifically, PeepLoc works on any mobile device\nwith an unmodified Wi-Fi transceiver and in any indoor environment with a\nsufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the\ncore of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain\nnon-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel\nbootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and\ncrowdsourcing to opportunistically initialize pre-existing APs as anchor points\nwithin an environment. We implement PeepLoc using commodity hardware and\nevaluate it extensively across 4 campus buildings. We show PeepLoc leads to a\nmean and median positional error of 3.41 m and 3.06 m respectively, which is\nsuperior to existing deployed indoor localization systems and is competitive\nwith commodity GPS in outdoor environments."
                },
                "authors": [
                    {
                        "name": "Emerson Sie"
                    },
                    {
                        "name": "Enguang Fan"
                    },
                    {
                        "name": "Federico Cifuentes-Urtubey"
                    },
                    {
                        "name": "Deepak Vasisht"
                    }
                ],
                "author_detail": {
                    "name": "Deepak Vasisht"
                },
                "author": "Deepak Vasisht",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18316v1",
                "updated": "2025-06-23T06:01:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    1,
                    21,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:01:21Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    1,
                    21,
                    0,
                    174,
                    0
                ],
                "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via\n  relation-based zero-shot retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Team LA at SCIDOCA shared task 2025: Citation Discovery via\n  relation-based zero-shot retrieval"
                },
                "summary": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction."
                },
                "authors": [
                    {
                        "name": "Trieu An"
                    },
                    {
                        "name": "Long Nguyen"
                    },
                    {
                        "name": "Minh Le Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Le Nguyen"
                },
                "author": "Minh Le Nguyen",
                "arxiv_comment": "In the Proceedings of SCIDOCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18315v1",
                "updated": "2025-06-23T06:01:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    1,
                    12,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T06:01:12Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    6,
                    1,
                    12,
                    0,
                    174,
                    0
                ],
                "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Property-Based Testing to Bridge LLM Code Generation and Validation"
                },
                "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods."
                },
                "authors": [
                    {
                        "name": "Lehan He"
                    },
                    {
                        "name": "Zeren Chen"
                    },
                    {
                        "name": "Zhe Zhang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Xiang Gao"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18311v1",
                "updated": "2025-06-23T05:55:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    5,
                    55,
                    53,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T05:55:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    5,
                    55,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large\n  Language Models for Hidden Relation Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large\n  Language Models for Hidden Relation Extraction"
                },
                "summary": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress."
                },
                "authors": [
                    {
                        "name": "Hoang-An Trieu"
                    },
                    {
                        "name": "Dinh-Truong Do"
                    },
                    {
                        "name": "Chau Nguyen"
                    },
                    {
                        "name": "Vu Tran"
                    },
                    {
                        "name": "Minh Le Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Minh Le Nguyen"
                },
                "author": "Minh Le Nguyen",
                "arxiv_comment": "In the Proceedings of SCIDOCA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]