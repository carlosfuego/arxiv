[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v1",
                "updated": "2024-11-27T10:14:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\nIn this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\nIn this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xing Hu"
                },
                "author": "Xing Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v1",
                "updated": "2024-11-27T06:10:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV\n  Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v1",
                "updated": "2024-11-26T17:28:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Cheng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yu"
                },
                "author": "Cheng Yu",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v1",
                "updated": "2024-11-26T14:23:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13588v1",
                "updated": "2024-11-18T02:49:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T02:49:23Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    49,
                    23,
                    0,
                    323,
                    0
                ],
                "title": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Redundancy in Diffusion Transformers (DiTs): A Systematic\n  Study"
                },
                "summary": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increased model capacity of Diffusion Transformers (DiTs) and the demand\nfor generating higher resolutions of images and videos have led to a\nsignificant rise in inference latency, impacting real-time performance\nadversely. While prior research has highlighted the presence of high similarity\nin activation values between adjacent diffusion steps (referred to as\nredundancy) and proposed various caching mechanisms to mitigate computational\noverhead, the exploration of redundancy in existing literature remains limited,\nwith findings often not generalizable across different DiT models. This study\naims to address this gap by conducting a comprehensive investigation into\nredundancy across a broad spectrum of mainstream DiT models. Our experimental\nanalysis reveals substantial variations in the distribution of redundancy\nacross diffusion steps among different DiT models. Interestingly, within a\nsingle model, the redundancy distribution remains stable regardless of\nvariations in input prompts, step counts, or scheduling strategies. Given the\nlack of a consistent pattern across diverse models, caching strategies designed\nfor a specific group of models may not easily transfer to others. To overcome\nthis challenge, we introduce a tool for analyzing the redundancy of individual\nmodels, enabling subsequent research to develop tailored caching strategies for\nspecific model architectures. The project is publicly available at\nhttps://github.com/xdit-project/DiTCacheAnalysis."
                },
                "authors": [
                    {
                        "name": "Xibo Sun"
                    },
                    {
                        "name": "Jiarui Fang"
                    },
                    {
                        "name": "Aoyu Li"
                    },
                    {
                        "name": "Jinzhe Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhe Pan"
                },
                "author": "Jinzhe Pan",
                "arxiv_comment": "9 pages including reference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.15100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15100v2",
                "updated": "2024-11-27T18:59:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-22T18:01:37Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    1,
                    37,
                    4,
                    327,
                    0
                ],
                "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models"
                },
                "summary": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving."
                },
                "authors": [
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yaxing Cai"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05779v2",
                "updated": "2024-11-27T18:44:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    44,
                    7,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-08T15:19:57Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    15,
                    19,
                    57,
                    0,
                    99,
                    0
                ],
                "title": "Data Readiness for AI: A 360-Degree Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Readiness for AI: A 360-Degree Survey"
                },
                "summary": "Artificial Intelligence (AI) applications critically depend on data. Poor\nquality data produces inaccurate and ineffective AI models that may lead to\nincorrect or unsafe use. Evaluation of data readiness is a crucial step in\nimproving the quality and appropriateness of data usage for AI. R&D efforts\nhave been spent on improving data quality. However, standardized metrics for\nevaluating data readiness for use in AI training are still evolving. In this\nstudy, we perform a comprehensive survey of metrics used to verify data\nreadiness for AI training. This survey examines more than 140 papers published\nby ACM Digital Library, IEEE Xplore, journals such as Nature, Springer, and\nScience Direct, and online articles published by prominent AI experts. This\nsurvey aims to propose a taxonomy of data readiness for AI (DRAI) metrics for\nstructured and unstructured datasets. We anticipate that this taxonomy will\nlead to new standards for DRAI metrics that will be used for enhancing the\nquality, accuracy, and fairness of AI training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) applications critically depend on data. Poor\nquality data produces inaccurate and ineffective AI models that may lead to\nincorrect or unsafe use. Evaluation of data readiness is a crucial step in\nimproving the quality and appropriateness of data usage for AI. R&D efforts\nhave been spent on improving data quality. However, standardized metrics for\nevaluating data readiness for use in AI training are still evolving. In this\nstudy, we perform a comprehensive survey of metrics used to verify data\nreadiness for AI training. This survey examines more than 140 papers published\nby ACM Digital Library, IEEE Xplore, journals such as Nature, Springer, and\nScience Direct, and online articles published by prominent AI experts. This\nsurvey aims to propose a taxonomy of data readiness for AI (DRAI) metrics for\nstructured and unstructured datasets. We anticipate that this taxonomy will\nlead to new standards for DRAI metrics that will be used for enhancing the\nquality, accuracy, and fairness of AI training and inference."
                },
                "authors": [
                    {
                        "name": "Kaveen Hiniduma"
                    },
                    {
                        "name": "Suren Byna"
                    },
                    {
                        "name": "Jean Luca Bez"
                    }
                ],
                "author_detail": {
                    "name": "Jean Luca Bez"
                },
                "author": "Jean Luca Bez",
                "arxiv_comment": "36 pages, 3 figures, 2 tables, submitted to ACM Computing Surveys",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; E.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18583v1",
                "updated": "2024-11-27T18:27:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    27,
                    7,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:27:07Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    27,
                    7,
                    2,
                    332,
                    0
                ],
                "title": "Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation"
                },
                "summary": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model."
                },
                "authors": [
                    {
                        "name": "Nurshat Fateh Ali"
                    },
                    {
                        "name": "Md. Mahdi Mohtasim"
                    },
                    {
                        "name": "Shakil Mosharrof"
                    },
                    {
                        "name": "T. Gopi Krishna"
                    }
                ],
                "author_detail": {
                    "name": "T. Gopi Krishna"
                },
                "author": "T. Gopi Krishna",
                "arxiv_comment": "Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature\n  Review, Natural Language Processing, Retrieval-augmented generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18571v1",
                "updated": "2024-11-27T18:14:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "Challenges in Adapting Multilingual LLMs to Low-Resource Languages using\n  LoRA PEFT Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Adapting Multilingual LLMs to Low-Resource Languages using\n  LoRA PEFT Tuning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable multilingual\ncapabilities, yet challenges persist in adapting these models for low-resource\nlanguages. In this study, we investigate the effects of Low-Rank Adaptation\n(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models for\nMarathi, a language with limited resources. Using a translated Alpaca dataset\nwith 52,000 instruction-response pairs, our findings reveal that while\nevaluation metrics often show a performance decline post-fine-tuning, manual\nassessments frequently suggest that the fine-tuned models outperform their\noriginal counterparts. The observations indicate improvements in target\nlanguage generation capabilities but a reduction in reasoning abilities\nfollowing language adaptation. These results underscore the need for improved\nevaluation methodologies and the creation of high-quality native datasets to\naccurately assess language-specific model performance in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable multilingual\ncapabilities, yet challenges persist in adapting these models for low-resource\nlanguages. In this study, we investigate the effects of Low-Rank Adaptation\n(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models for\nMarathi, a language with limited resources. Using a translated Alpaca dataset\nwith 52,000 instruction-response pairs, our findings reveal that while\nevaluation metrics often show a performance decline post-fine-tuning, manual\nassessments frequently suggest that the fine-tuned models outperform their\noriginal counterparts. The observations indicate improvements in target\nlanguage generation capabilities but a reduction in reasoning abilities\nfollowing language adaptation. These results underscore the need for improved\nevaluation methodologies and the creation of high-quality native datasets to\naccurately assess language-specific model performance in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Omkar Khade"
                    },
                    {
                        "name": "Shruti Jagdale"
                    },
                    {
                        "name": "Abhishek Phaltankar"
                    },
                    {
                        "name": "Gauri Takalikar"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18569v1",
                "updated": "2024-11-27T18:14:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    14,
                    8,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:14:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    14,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "A Flexible Defense Against the Winner's Curse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Defense Against the Winner's Curse"
                },
                "summary": "Across science and policy, decision-makers often need to draw conclusions\nabout the best candidate among competing alternatives. For instance,\nresearchers may seek to infer the effectiveness of the most successful\ntreatment or determine which demographic group benefits most from a specific\ntreatment. Similarly, in machine learning, practitioners are often interested\nin the population performance of the model that performs best empirically.\nHowever, cherry-picking the best candidate leads to the winner's curse: the\nobserved performance for the winner is biased upwards, rendering conclusions\nbased on standard measures of uncertainty invalid. We introduce the zoom\ncorrection, a novel approach for valid inference on the winner. Our method is\nflexible: it can be employed in both parametric and nonparametric settings, can\nhandle arbitrary dependencies between candidates, and automatically adapts to\nthe level of selection bias. The method easily extends to important related\nproblems, such as inference on the top k winners, inference on the value and\nidentity of the population winner, and inference on \"near-winners.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across science and policy, decision-makers often need to draw conclusions\nabout the best candidate among competing alternatives. For instance,\nresearchers may seek to infer the effectiveness of the most successful\ntreatment or determine which demographic group benefits most from a specific\ntreatment. Similarly, in machine learning, practitioners are often interested\nin the population performance of the model that performs best empirically.\nHowever, cherry-picking the best candidate leads to the winner's curse: the\nobserved performance for the winner is biased upwards, rendering conclusions\nbased on standard measures of uncertainty invalid. We introduce the zoom\ncorrection, a novel approach for valid inference on the winner. Our method is\nflexible: it can be employed in both parametric and nonparametric settings, can\nhandle arbitrary dependencies between candidates, and automatically adapts to\nthe level of selection bias. The method easily extends to important related\nproblems, such as inference on the top k winners, inference on the value and\nidentity of the population winner, and inference on \"near-winners.\""
                },
                "authors": [
                    {
                        "name": "Tijana Zrnic"
                    },
                    {
                        "name": "William Fithian"
                    }
                ],
                "author_detail": {
                    "name": "William Fithian"
                },
                "author": "William Fithian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14392v2",
                "updated": "2024-11-27T18:08:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    8,
                    50,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-21T18:25:18Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    18,
                    25,
                    18,
                    3,
                    326,
                    0
                ],
                "title": "Convolutional Vision Transformer for Cosmology Parameter Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Vision Transformer for Cosmology Parameter Inference"
                },
                "summary": "Parameter inference is a crucial task in modern cosmology that requires\naccurate and fast computational methods to handle the high precision and volume\nof observational datasets. In this study, we explore a hybrid vision\ntransformer, the Convolution vision Transformer (CvT), which combines the\nbenefits of vision transformers (ViTs) and convolutional neural networks\n(CNNs). We use this approach to infer the $\\Omega_m$ and $\\sigma_8$\ncosmological parameters from simulated dark matter and halo fields. Our\nexperiments indicate that the constraints on $\\Omega_m$ and $\\sigma_8$ obtained\nusing CvT are better than ViT and CNN, using either dark matter or halo fields.\nFor CvT, pretraining on dark matter fields proves advantageous for improving\nconstraints using halo fields compared to training a model from the beginning.\nHowever, ViT and CNN do not show these benefits. The CvT is more efficient than\nViT since, despite having more parameters, it requires a training time similar\nto that of ViT and has similar inference times. The code is available at\n\\url{https://github.com/Yash-10/cvt-cosmo-inference/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter inference is a crucial task in modern cosmology that requires\naccurate and fast computational methods to handle the high precision and volume\nof observational datasets. In this study, we explore a hybrid vision\ntransformer, the Convolution vision Transformer (CvT), which combines the\nbenefits of vision transformers (ViTs) and convolutional neural networks\n(CNNs). We use this approach to infer the $\\Omega_m$ and $\\sigma_8$\ncosmological parameters from simulated dark matter and halo fields. Our\nexperiments indicate that the constraints on $\\Omega_m$ and $\\sigma_8$ obtained\nusing CvT are better than ViT and CNN, using either dark matter or halo fields.\nFor CvT, pretraining on dark matter fields proves advantageous for improving\nconstraints using halo fields compared to training a model from the beginning.\nHowever, ViT and CNN do not show these benefits. The CvT is more efficient than\nViT since, despite having more parameters, it requires a training time similar\nto that of ViT and has similar inference times. The code is available at\n\\url{https://github.com/Yash-10/cvt-cosmo-inference/}."
                },
                "authors": [
                    {
                        "name": "Yash Gondhalekar"
                    },
                    {
                        "name": "Kana Moriwaki"
                    }
                ],
                "author_detail": {
                    "name": "Kana Moriwaki"
                },
                "author": "Kana Moriwaki",
                "arxiv_comment": "Accepted at the NeurIPS ML4PS Workshop 2024. The code is available at\n  https://github.com/Yash-10/cvt-cosmo-inference/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18564v1",
                "updated": "2024-11-27T18:04:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    4,
                    5,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:04:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    4,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks. However, LLMs often struggle with spatial reasoning which is one\nessential part of reasoning and inference and requires understanding complex\nrelationships between objects in space. This paper proposes a novel\nneural-symbolic framework that enhances LLMs' spatial reasoning abilities. We\nevaluate our approach on two benchmark datasets: StepGame and SparQA,\nimplementing three distinct strategies: (1) ASP (Answer Set Programming)-based\nsymbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logical\nrules. Our experiments demonstrate significant improvements over the baseline\nprompting methods, with accuracy increases of 40-50% on StepGame} dataset and\n3-13% on the more complex SparQA dataset. The \"LLM + ASP\" pipeline achieves\nparticularly strong results on the tasks of Finding Relations (FR) and Finding\nBlock (FB) questions, though performance varies across different question\ntypes. The impressive results suggest that while neural-symbolic approaches\noffer promising directions for enhancing spatial reasoning in LLMs, their\neffectiveness depends heavily on the specific task characteristics and\nimplementation strategies. We propose an integrated, simple yet effective set\nof strategies using a neural-symbolic pipeline to boost spatial reasoning\nabilities in LLMs. This pipeline and its strategies demonstrate strong and\nbroader applicability to other reasoning domains in LLMs, such as temporal\nreasoning, deductive inference etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks. However, LLMs often struggle with spatial reasoning which is one\nessential part of reasoning and inference and requires understanding complex\nrelationships between objects in space. This paper proposes a novel\nneural-symbolic framework that enhances LLMs' spatial reasoning abilities. We\nevaluate our approach on two benchmark datasets: StepGame and SparQA,\nimplementing three distinct strategies: (1) ASP (Answer Set Programming)-based\nsymbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logical\nrules. Our experiments demonstrate significant improvements over the baseline\nprompting methods, with accuracy increases of 40-50% on StepGame} dataset and\n3-13% on the more complex SparQA dataset. The \"LLM + ASP\" pipeline achieves\nparticularly strong results on the tasks of Finding Relations (FR) and Finding\nBlock (FB) questions, though performance varies across different question\ntypes. The impressive results suggest that while neural-symbolic approaches\noffer promising directions for enhancing spatial reasoning in LLMs, their\neffectiveness depends heavily on the specific task characteristics and\nimplementation strategies. We propose an integrated, simple yet effective set\nof strategies using a neural-symbolic pipeline to boost spatial reasoning\nabilities in LLMs. This pipeline and its strategies demonstrate strong and\nbroader applicability to other reasoning domains in LLMs, such as temporal\nreasoning, deductive inference etc."
                },
                "authors": [
                    {
                        "name": "Rong Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Jonas Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kuhn"
                },
                "author": "Jonas Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18553v1",
                "updated": "2024-11-27T17:51:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    58,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T17:51:58Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    58,
                    2,
                    332,
                    0
                ],
                "title": "Retrofitting (Large) Language Models with Dynamic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrofitting (Large) Language Models with Dynamic Tokenization"
                },
                "summary": "Current language models (LMs) use a fixed, static subword tokenizer. This\nchoice, often taken for granted, typically results in degraded efficiency and\ncapabilities in languages other than English, and makes it challenging to apply\nLMs to new domains or languages. To address these issues, we propose\nretrofitting LMs with dynamic tokenization: a way to dynamically decide on\ntoken boundaries based on the input text. For encoder-style models, we\nintroduce a subword-merging algorithm inspired by byte-pair encoding (BPE), but\nat a batch level. We merge frequent subword sequences in a batch, then apply a\npretrained embedding-prediction hypernetwork to compute the token embeddings\non-the-fly. When applied with word-level boundaries, this on average reduces\ntoken sequence lengths by >20% across 14 languages on XNLI with XLM-R while\ndegrading its task performance by less than 2%. For decoder-style models, we\napply dynamic tokenization in two ways: 1) for prefilling, maintaining\nperformance of Mistral-7B almost completely with up to 40% sequence reduction -\nrelative to the word-level; and 2) via an approximate nearest neighbor index,\nachieving fast generation with a one million token vocabulary, demonstrating\nscalability to even larger, dynamic vocabularies. Overall, our findings show\nthat dynamic tokenization substantially improves inference speed and promotes\nfairness across languages, making a leap towards overcoming the limitations of\nstatic tokenization and enabling more equitable and adaptable LMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current language models (LMs) use a fixed, static subword tokenizer. This\nchoice, often taken for granted, typically results in degraded efficiency and\ncapabilities in languages other than English, and makes it challenging to apply\nLMs to new domains or languages. To address these issues, we propose\nretrofitting LMs with dynamic tokenization: a way to dynamically decide on\ntoken boundaries based on the input text. For encoder-style models, we\nintroduce a subword-merging algorithm inspired by byte-pair encoding (BPE), but\nat a batch level. We merge frequent subword sequences in a batch, then apply a\npretrained embedding-prediction hypernetwork to compute the token embeddings\non-the-fly. When applied with word-level boundaries, this on average reduces\ntoken sequence lengths by >20% across 14 languages on XNLI with XLM-R while\ndegrading its task performance by less than 2%. For decoder-style models, we\napply dynamic tokenization in two ways: 1) for prefilling, maintaining\nperformance of Mistral-7B almost completely with up to 40% sequence reduction -\nrelative to the word-level; and 2) via an approximate nearest neighbor index,\nachieving fast generation with a one million token vocabulary, demonstrating\nscalability to even larger, dynamic vocabularies. Overall, our findings show\nthat dynamic tokenization substantially improves inference speed and promotes\nfairness across languages, making a leap towards overcoming the limitations of\nstatic tokenization and enabling more equitable and adaptable LMs."
                },
                "authors": [
                    {
                        "name": "Darius Feher"
                    },
                    {
                        "name": "Benjamin Minixhofer"
                    },
                    {
                        "name": "Ivan VuliÄ"
                    }
                ],
                "author_detail": {
                    "name": "Ivan VuliÄ"
                },
                "author": "Ivan VuliÄ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18552v1",
                "updated": "2024-11-27T17:51:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T17:51:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "FAM Diffusion: Frequency and Attention Modulation for High-Resolution\n  Image Generation with Stable Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAM Diffusion: Frequency and Attention Modulation for High-Resolution\n  Image Generation with Stable Diffusion"
                },
                "summary": "Diffusion models are proficient at generating high-quality images. They are\nhowever effective only when operating at the resolution used during training.\nInference at a scaled resolution leads to repetitive patterns and structural\ndistortions. Retraining at higher resolutions quickly becomes prohibitive.\nThus, methods enabling pre-existing diffusion models to operate at flexible\ntest-time resolutions are highly desirable. Previous works suffer from frequent\nartifacts and often introduce large latency overheads. We propose two simple\nmodules that combine to solve these issues. We introduce a Frequency Modulation\n(FM) module that leverages the Fourier domain to improve the global structure\nconsistency, and an Attention Modulation (AM) module which improves the\nconsistency of local texture patterns, a problem largely ignored in prior\nworks. Our method, coined Fam diffusion, can seamlessly integrate into any\nlatent diffusion model and requires no additional training. Extensive\nqualitative results highlight the effectiveness of our method in addressing\nstructural and local artifacts, while quantitative results show\nstate-of-the-art performance. Also, our method avoids redundant inference\ntricks for improved consistency such as patch-based or progressive generation,\nleading to negligible latency overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are proficient at generating high-quality images. They are\nhowever effective only when operating at the resolution used during training.\nInference at a scaled resolution leads to repetitive patterns and structural\ndistortions. Retraining at higher resolutions quickly becomes prohibitive.\nThus, methods enabling pre-existing diffusion models to operate at flexible\ntest-time resolutions are highly desirable. Previous works suffer from frequent\nartifacts and often introduce large latency overheads. We propose two simple\nmodules that combine to solve these issues. We introduce a Frequency Modulation\n(FM) module that leverages the Fourier domain to improve the global structure\nconsistency, and an Attention Modulation (AM) module which improves the\nconsistency of local texture patterns, a problem largely ignored in prior\nworks. Our method, coined Fam diffusion, can seamlessly integrate into any\nlatent diffusion model and requires no additional training. Extensive\nqualitative results highlight the effectiveness of our method in addressing\nstructural and local artifacts, while quantitative results show\nstate-of-the-art performance. Also, our method avoids redundant inference\ntricks for improved consistency such as patch-based or progressive generation,\nleading to negligible latency overheads."
                },
                "authors": [
                    {
                        "name": "Haosen Yang"
                    },
                    {
                        "name": "Adrian Bulat"
                    },
                    {
                        "name": "Isma Hadji"
                    },
                    {
                        "name": "Hai X. Pham"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    },
                    {
                        "name": "Brais Martinez"
                    }
                ],
                "author_detail": {
                    "name": "Brais Martinez"
                },
                "author": "Brais Martinez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18549v1",
                "updated": "2024-11-27T17:51:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    8,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T17:51:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "Finite population inference for skewness measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite population inference for skewness measures"
                },
                "summary": "In this article we consider Bowley's skewness measure and the\nGroeneveld-Meeden $b_{3}$ index in the context of finite population sampling.\nWe employ the functional delta method to obtain asymptotic variance formulae\nfor plug-in estimators and propose corresponding variance estimators. We then\nconsider plug-in estimators based on the H\\'{a}jek cdf-estimator and on a\nDeville-S\\\"arndal type calibration estimator and test the performance of normal\nconfidence intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we consider Bowley's skewness measure and the\nGroeneveld-Meeden $b_{3}$ index in the context of finite population sampling.\nWe employ the functional delta method to obtain asymptotic variance formulae\nfor plug-in estimators and propose corresponding variance estimators. We then\nconsider plug-in estimators based on the H\\'{a}jek cdf-estimator and on a\nDeville-S\\\"arndal type calibration estimator and test the performance of normal\nconfidence intervals."
                },
                "authors": [
                    {
                        "name": "Leo Pasquazzi"
                    }
                ],
                "author_detail": {
                    "name": "Leo Pasquazzi"
                },
                "author": "Leo Pasquazzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18539v1",
                "updated": "2024-11-27T17:36:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    36,
                    8,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T17:36:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    36,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "AdaVLN: Towards Visual Language Navigation in Continuous Indoor\n  Environments with Moving Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaVLN: Towards Visual Language Navigation in Continuous Indoor\n  Environments with Moving Humans"
                },
                "summary": "Visual Language Navigation is a task that challenges robots to navigate in\nrealistic environments based on natural language instructions. While previous\nresearch has largely focused on static settings, real-world navigation must\noften contend with dynamic human obstacles. Hence, we propose an extension to\nthe task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks to\nnarrow this gap. AdaVLN requires robots to navigate complex 3D indoor\nenvironments populated with dynamically moving human obstacles, adding a layer\nof complexity to navigation tasks that mimic the real-world. To support\nexploration of this task, we also present AdaVLN simulator and AdaR2R datasets.\nThe AdaVLN simulator enables easy inclusion of fully animated human models\ndirectly into common datasets like Matterport3D. We also introduce a\n\"freeze-time\" mechanism for both the navigation task and simulator, which\npauses world state updates during agent inference, enabling fair comparisons\nand experimental reproducibility across different hardware. We evaluate several\nbaseline models on this task, analyze the unique challenges introduced by\nAdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLN\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Language Navigation is a task that challenges robots to navigate in\nrealistic environments based on natural language instructions. While previous\nresearch has largely focused on static settings, real-world navigation must\noften contend with dynamic human obstacles. Hence, we propose an extension to\nthe task, termed Adaptive Visual Language Navigation (AdaVLN), which seeks to\nnarrow this gap. AdaVLN requires robots to navigate complex 3D indoor\nenvironments populated with dynamically moving human obstacles, adding a layer\nof complexity to navigation tasks that mimic the real-world. To support\nexploration of this task, we also present AdaVLN simulator and AdaR2R datasets.\nThe AdaVLN simulator enables easy inclusion of fully animated human models\ndirectly into common datasets like Matterport3D. We also introduce a\n\"freeze-time\" mechanism for both the navigation task and simulator, which\npauses world state updates during agent inference, enabling fair comparisons\nand experimental reproducibility across different hardware. We evaluate several\nbaseline models on this task, analyze the unique challenges introduced by\nAdaVLN, and demonstrate its potential to bridge the sim-to-real gap in VLN\nresearch."
                },
                "authors": [
                    {
                        "name": "Dillon Loh"
                    },
                    {
                        "name": "Tomasz Bednarz"
                    },
                    {
                        "name": "Xinxing Xia"
                    },
                    {
                        "name": "Frank Guan"
                    }
                ],
                "author_detail": {
                    "name": "Frank Guan"
                },
                "author": "Frank Guan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13362v2",
                "updated": "2024-11-27T17:07:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    7,
                    41,
                    2,
                    332,
                    0
                ],
                "published": "2024-05-22T05:43:15Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    5,
                    43,
                    15,
                    2,
                    143,
                    0
                ],
                "title": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems"
                },
                "summary": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited."
                },
                "authors": [
                    {
                        "name": "Danial Ebrat"
                    },
                    {
                        "name": "Eli Paradalis"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14985v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14985v4",
                "updated": "2024-11-27T17:05:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    5,
                    16,
                    2,
                    332,
                    0
                ],
                "published": "2024-07-20T21:24:40Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    21,
                    24,
                    40,
                    5,
                    202,
                    0
                ],
                "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data"
                },
                "summary": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth. We also show the practical implications\nof our analysis through a novel prompt optimization algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth. We also show the practical implications\nof our analysis through a novel prompt optimization algorithm."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Antonis Antoniades"
                    },
                    {
                        "name": "Yanai Elazar"
                    },
                    {
                        "name": "Alfonso Amayuelas"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "updated 10-page version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14985v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14985v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11883v2",
                "updated": "2024-11-27T16:52:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    52,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-11T10:09:46Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    10,
                    9,
                    46,
                    4,
                    285,
                    0
                ],
                "title": "Simulation-based inference with scattering representations: scattering\n  is all you need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference with scattering representations: scattering\n  is all you need"
                },
                "summary": "We demonstrate the successful use of scattering representations without\nfurther compression for simulation-based inference (SBI) with images (i.e.\nfield-level), illustrated with a cosmological case study. Scattering\nrepresentations provide a highly effective representational space for\nsubsequent learning tasks, although the higher dimensional compressed space\nintroduces challenges. We overcome these through spatial averaging, coupled\nwith more expressive density estimators. Compared to alternative methods, such\nan approach does not require additional simulations for either training or\ncomputing derivatives, is interpretable, and resilient to covariate shift. As\nexpected, we show that a scattering only approach extracts more information\nthan traditional second order summary statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate the successful use of scattering representations without\nfurther compression for simulation-based inference (SBI) with images (i.e.\nfield-level), illustrated with a cosmological case study. Scattering\nrepresentations provide a highly effective representational space for\nsubsequent learning tasks, although the higher dimensional compressed space\nintroduces challenges. We overcome these through spatial averaging, coupled\nwith more expressive density estimators. Compared to alternative methods, such\nan approach does not require additional simulations for either training or\ncomputing derivatives, is interpretable, and resilient to covariate shift. As\nexpected, we show that a scattering only approach extracts more information\nthan traditional second order summary statistics."
                },
                "authors": [
                    {
                        "name": "Kiyam Lin"
                    },
                    {
                        "name": "Benjamin Joachimi"
                    },
                    {
                        "name": "Jason D. McEwen"
                    }
                ],
                "author_detail": {
                    "name": "Jason D. McEwen"
                },
                "author": "Jason D. McEwen",
                "arxiv_comment": "9 pages, 2 figures, accepted by NeurIPS workshop on Machine Learning\n  and the Physical Sciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03015v2",
                "updated": "2024-11-27T16:50:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    50,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-03T18:54:27Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    18,
                    54,
                    27,
                    2,
                    94,
                    0
                ],
                "title": "DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object\n  Detection"
                },
                "summary": "The perception of autonomous vehicles has to be efficient, robust, and\ncost-effective. However, cameras are not robust against severe weather\nconditions, lidar sensors are expensive, and the performance of radar-based\nperception is still inferior to the others. Camera-radar fusion methods have\nbeen proposed to address this issue, but these are constrained by the typical\nsparsity of radar point clouds and often designed for radars without elevation\ninformation. We propose a novel camera-radar fusion approach called Dual\nPerspective Fusion Transformer (DPFT), designed to overcome these limitations.\nOur method leverages lower-level radar data (the radar cube) instead of the\nprocessed point clouds to preserve as much information as possible and employs\nprojections in both the camera and ground planes to effectively use radars with\nelevation information and simplify the fusion with camera data. As a result,\nDPFT has demonstrated state-of-the-art performance on the K-Radar dataset while\nshowing remarkable robustness against adverse weather conditions and\nmaintaining a low inference time. The code is made available as open-source\nsoftware under https://github.com/TUMFTM/DPFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perception of autonomous vehicles has to be efficient, robust, and\ncost-effective. However, cameras are not robust against severe weather\nconditions, lidar sensors are expensive, and the performance of radar-based\nperception is still inferior to the others. Camera-radar fusion methods have\nbeen proposed to address this issue, but these are constrained by the typical\nsparsity of radar point clouds and often designed for radars without elevation\ninformation. We propose a novel camera-radar fusion approach called Dual\nPerspective Fusion Transformer (DPFT), designed to overcome these limitations.\nOur method leverages lower-level radar data (the radar cube) instead of the\nprocessed point clouds to preserve as much information as possible and employs\nprojections in both the camera and ground planes to effectively use radars with\nelevation information and simplify the fusion with camera data. As a result,\nDPFT has demonstrated state-of-the-art performance on the K-Radar dataset while\nshowing remarkable robustness against adverse weather conditions and\nmaintaining a low inference time. The code is made available as open-source\nsoftware under https://github.com/TUMFTM/DPFT."
                },
                "authors": [
                    {
                        "name": "Felix Fent"
                    },
                    {
                        "name": "Andras Palffy"
                    },
                    {
                        "name": "Holger Caesar"
                    }
                ],
                "author_detail": {
                    "name": "Holger Caesar"
                },
                "author": "Holger Caesar",
                "arxiv_comment": "Accepted to IEEE Transactions on Intelligent Vehicles",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18507v1",
                "updated": "2024-11-27T16:50:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    50,
                    42,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T16:50:42Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    50,
                    42,
                    2,
                    332,
                    0
                ],
                "title": "At First Contact: Stiffness Estimation Using Vibrational Information for\n  Prosthetic Grasp Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At First Contact: Stiffness Estimation Using Vibrational Information for\n  Prosthetic Grasp Modulation"
                },
                "summary": "Stiffness estimation is crucial for delicate object manipulation in robotic\nand prosthetic hands but remains challenging due to dependence on force and\ndisplacement measurement and real-time sensory integration. This study presents\na piezoelectric sensing framework for stiffness estimation at first contact\nduring pinch grasps, addressing the limitations of traditional force-based\nmethods. Inspired by human skin, a multimodal tactile sensor that captures\nvibrational and force data is developed and integrated into a prosthetic hand's\nfingertip. Machine learning models, including support vector machines and\nconvolutional neural networks, demonstrate that vibrational signals within the\ncritical 15 ms after first contact reliably encode stiffness, achieving\nclassification accuracies up to 98.6\\% and regression errors as low as 2.39\nShore A on real-world objects of varying stiffness. Inference times of less\nthan 1.5 ms are significantly faster than the average grasp closure time (16.65\nms in our dataset), enabling real-time stiffness estimation before the object\nis fully grasped. By leveraging the transient asymmetry in grasp dynamics,\nwhere one finger contacts the object before the others, this method enables\nearly grasp modulation, enhancing safety and intuitiveness in prosthetic hands\nwhile offering broad applications in robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stiffness estimation is crucial for delicate object manipulation in robotic\nand prosthetic hands but remains challenging due to dependence on force and\ndisplacement measurement and real-time sensory integration. This study presents\na piezoelectric sensing framework for stiffness estimation at first contact\nduring pinch grasps, addressing the limitations of traditional force-based\nmethods. Inspired by human skin, a multimodal tactile sensor that captures\nvibrational and force data is developed and integrated into a prosthetic hand's\nfingertip. Machine learning models, including support vector machines and\nconvolutional neural networks, demonstrate that vibrational signals within the\ncritical 15 ms after first contact reliably encode stiffness, achieving\nclassification accuracies up to 98.6\\% and regression errors as low as 2.39\nShore A on real-world objects of varying stiffness. Inference times of less\nthan 1.5 ms are significantly faster than the average grasp closure time (16.65\nms in our dataset), enabling real-time stiffness estimation before the object\nis fully grasped. By leveraging the transient asymmetry in grasp dynamics,\nwhere one finger contacts the object before the others, this method enables\nearly grasp modulation, enhancing safety and intuitiveness in prosthetic hands\nwhile offering broad applications in robotics."
                },
                "authors": [
                    {
                        "name": "Anway S. Pimpalkar"
                    },
                    {
                        "name": "Ariel Slepyan"
                    },
                    {
                        "name": "Nitish V. Thakor"
                    }
                ],
                "author_detail": {
                    "name": "Nitish V. Thakor"
                },
                "author": "Nitish V. Thakor",
                "arxiv_comment": "5 pages, 7 figures, for IEEE Sensors Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v1",
                "updated": "2024-11-27T16:48:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understand time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understand time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15637v2",
                "updated": "2024-11-27T16:34:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    34,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-24T00:51:45Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    0,
                    51,
                    45,
                    1,
                    268,
                    0
                ],
                "title": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for\n  Digital Agents at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for\n  Digital Agents at Scale"
                },
                "summary": "LLMs can now act as autonomous agents that interact with digital environments\nand complete specific objectives (e.g., arranging an online meeting). However,\naccuracy is still far from satisfactory, partly due to a lack of large-scale,\ndirect demonstrations for digital tasks. Obtaining supervised data from humans\nis costly, and automatic data collection through exploration or reinforcement\nlearning relies on complex environmental and content setup, resulting in\ndatasets that lack comprehensive coverage of various scenarios. On the other\nhand, there is abundant knowledge that may indirectly assist task completion,\nsuch as online tutorials that were created for human consumption. In this work,\nwe present Synatra, an approach that effectively transforms this indirect\nknowledge into direct supervision at scale. We define different types of\nindirect knowledge, and carefully study the available sources to obtain it,\nmethods to encode the structure of direct demonstrations, and finally methods\nto transform indirect knowledge into direct demonstrations. We use 100k such\nsynthetically-created demonstrations to finetune a 7B CodeLlama, and\ndemonstrate that the resulting agent surpasses all comparably sized models on\nthree web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as\nsurpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic\ndemonstrations prove to be only 3% the cost of human demonstrations (at $0.031\neach), we show that the synthetic demonstrations can be more effective than an\nidentical number of human demonstrations collected from limited domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can now act as autonomous agents that interact with digital environments\nand complete specific objectives (e.g., arranging an online meeting). However,\naccuracy is still far from satisfactory, partly due to a lack of large-scale,\ndirect demonstrations for digital tasks. Obtaining supervised data from humans\nis costly, and automatic data collection through exploration or reinforcement\nlearning relies on complex environmental and content setup, resulting in\ndatasets that lack comprehensive coverage of various scenarios. On the other\nhand, there is abundant knowledge that may indirectly assist task completion,\nsuch as online tutorials that were created for human consumption. In this work,\nwe present Synatra, an approach that effectively transforms this indirect\nknowledge into direct supervision at scale. We define different types of\nindirect knowledge, and carefully study the available sources to obtain it,\nmethods to encode the structure of direct demonstrations, and finally methods\nto transform indirect knowledge into direct demonstrations. We use 100k such\nsynthetically-created demonstrations to finetune a 7B CodeLlama, and\ndemonstrate that the resulting agent surpasses all comparably sized models on\nthree web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as\nsurpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic\ndemonstrations prove to be only 3% the cost of human demonstrations (at $0.031\neach), we show that the synthetic demonstrations can be more effective than an\nidentical number of human demonstrations collected from limited domains."
                },
                "authors": [
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Aman Madaan"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Robert Lo"
                    },
                    {
                        "name": "Abishek Sridhar"
                    },
                    {
                        "name": "Sudipta Sengupta"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Shuyan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuyan Zhou"
                },
                "author": "Shuyan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10468v2",
                "updated": "2024-11-27T16:20:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    20,
                    36,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-16T11:10:11Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    11,
                    10,
                    11,
                    1,
                    107,
                    0
                ],
                "title": "Community detection and anomaly prediction in dynamic networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community detection and anomaly prediction in dynamic networks"
                },
                "summary": "Anomaly detection is an essential task in the analysis of dynamic networks,\noffering early warnings of abnormal behavior. We present a principled approach\nto detect anomalies in dynamic networks that integrates community structure as\na foundational model for regular behavior. Our model identifies anomalies as\nirregular edges while capturing structural changes. Our approach leverages a\nMarkovian framework for temporal transitions and latent variables for community\nand anomaly detection, inferring hidden parameters to detect unusual\ninteractions. Evaluations on synthetic and real-world datasets show strong\nanomaly detection across various scenarios. In a case study on professional\nfootball player transfers, we detect patterns influenced by club wealth and\ncountry, as well as unexpected transactions both within and across community\nboundaries. This work provides a framework for adaptable anomaly detection,\nhighlighting the value of integrating domain knowledge with data-driven\ntechniques for improved interpretability and robustness in complex networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection is an essential task in the analysis of dynamic networks,\noffering early warnings of abnormal behavior. We present a principled approach\nto detect anomalies in dynamic networks that integrates community structure as\na foundational model for regular behavior. Our model identifies anomalies as\nirregular edges while capturing structural changes. Our approach leverages a\nMarkovian framework for temporal transitions and latent variables for community\nand anomaly detection, inferring hidden parameters to detect unusual\ninteractions. Evaluations on synthetic and real-world datasets show strong\nanomaly detection across various scenarios. In a case study on professional\nfootball player transfers, we detect patterns influenced by club wealth and\ncountry, as well as unexpected transactions both within and across community\nboundaries. This work provides a framework for adaptable anomaly detection,\nhighlighting the value of integrating domain knowledge with data-driven\ntechniques for improved interpretability and robustness in complex networks."
                },
                "authors": [
                    {
                        "name": "Hadiseh Safdari"
                    },
                    {
                        "name": "Caterina De Bacco"
                    }
                ],
                "author_detail": {
                    "name": "Caterina De Bacco"
                },
                "author": "Caterina De Bacco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68-XX",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18478v1",
                "updated": "2024-11-27T16:19:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    19,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T16:19:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    19,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS"
                },
                "summary": "In-context Learning (ICL) enables large language models (LLMs) to tackle\ndownstream tasks through sophisticated prompting and high-quality\ndemonstrations. However, this traditional ICL paradigm shows limitations when\nfacing complex mathematical reasoning tasks, primarily due to its heavy\ndependence on example quality and the necessity for human intervention in\nchallenging scenarios. To address these limitations, this paper presents\nHiAR-ICL, a \\textbf{Hi}gh-level \\textbf{A}utomated \\textbf{R}easoning paradigm\nin \\textbf{ICL} that shifts focus from specific examples to abstract thinking\npatterns, extending the conventional concept of context in ICL. HiAR-ICL\nintroduces five atomic reasoning actions as fundamental components for\nconstructing chain-structured patterns. Using Monte Carlo Tree Search, we\nexplore reasoning paths and construct thought cards to guide subsequent\ninference. We then develop a cognitive complexity framework that dynamically\nmatches problems with appropriate thought cards. Experimental results\ndemonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy\n(79.6$\\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o\n(76.6$\\%$) and Claude 3.5 (71.1$\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context Learning (ICL) enables large language models (LLMs) to tackle\ndownstream tasks through sophisticated prompting and high-quality\ndemonstrations. However, this traditional ICL paradigm shows limitations when\nfacing complex mathematical reasoning tasks, primarily due to its heavy\ndependence on example quality and the necessity for human intervention in\nchallenging scenarios. To address these limitations, this paper presents\nHiAR-ICL, a \\textbf{Hi}gh-level \\textbf{A}utomated \\textbf{R}easoning paradigm\nin \\textbf{ICL} that shifts focus from specific examples to abstract thinking\npatterns, extending the conventional concept of context in ICL. HiAR-ICL\nintroduces five atomic reasoning actions as fundamental components for\nconstructing chain-structured patterns. Using Monte Carlo Tree Search, we\nexplore reasoning paths and construct thought cards to guide subsequent\ninference. We then develop a cognitive complexity framework that dynamically\nmatches problems with appropriate thought cards. Experimental results\ndemonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy\n(79.6$\\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o\n(76.6$\\%$) and Claude 3.5 (71.1$\\%$)."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Zengqi Wen"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04742v2",
                "updated": "2024-11-27T16:09:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    9,
                    49,
                    2,
                    332,
                    0
                ],
                "published": "2024-03-07T18:49:01Z",
                "published_parsed": [
                    2024,
                    3,
                    7,
                    18,
                    49,
                    1,
                    3,
                    67,
                    0
                ],
                "title": "Bayesian Inference of Time-Varying Origin-Destination Matrices from\n  Boarding/Alighting Counts for Transit Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference of Time-Varying Origin-Destination Matrices from\n  Boarding/Alighting Counts for Transit Services"
                },
                "summary": "Origin-destination (OD) demand matrices are crucial for transit agencies to\ndesign and operate transit systems. This paper presents a novel temporal\nBayesian model designed to estimate transit OD matrices at the individual\nbus-journey level from boarding/alighting counts at bus stops. Our approach\nbegins by modeling the number of alighting passengers at subsequent bus stops,\ngiven a boarding stop, through a multinomial distribution parameterized by\nalighting probabilities. Given the large scale of the problem, we generate\nalighting probabilities with a latent variable matrix and factorize it into a\nmapping matrix and a temporal matrix, thereby substantially reducing the number\nof parameters. To further encode a temporally-smooth structure in the\nparameters, we impose a Gaussian process prior on the columns of the temporal\nfactor matrix. For model inference, we develop a two-stage algorithm with the\nMarkov chain Monte Carlo (MCMC) method. In the first stage, latent OD matrices\nare sampled conditional on model parameters using a Metropolis-Hastings\nsampling algorithm with a Markov model-based proposal distribution. In the\nsecond stage, we sample model parameters conditional on latent OD matrices\nusing slice and elliptical slice sampling algorithms. We assess the proposed\nmodel using real-world data collected from three bus routes with varying\nnumbers of stops, and the results demonstrate that our model achieves accurate\nposterior mean estimation and outperforms the widely used iterative\nproportional fitting (IPF) method. Additionally, our model can provide\nuncertainty quantification for the OD demand matrices, thus benefiting many\ndownstream planning/operational tasks that require robust decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Origin-destination (OD) demand matrices are crucial for transit agencies to\ndesign and operate transit systems. This paper presents a novel temporal\nBayesian model designed to estimate transit OD matrices at the individual\nbus-journey level from boarding/alighting counts at bus stops. Our approach\nbegins by modeling the number of alighting passengers at subsequent bus stops,\ngiven a boarding stop, through a multinomial distribution parameterized by\nalighting probabilities. Given the large scale of the problem, we generate\nalighting probabilities with a latent variable matrix and factorize it into a\nmapping matrix and a temporal matrix, thereby substantially reducing the number\nof parameters. To further encode a temporally-smooth structure in the\nparameters, we impose a Gaussian process prior on the columns of the temporal\nfactor matrix. For model inference, we develop a two-stage algorithm with the\nMarkov chain Monte Carlo (MCMC) method. In the first stage, latent OD matrices\nare sampled conditional on model parameters using a Metropolis-Hastings\nsampling algorithm with a Markov model-based proposal distribution. In the\nsecond stage, we sample model parameters conditional on latent OD matrices\nusing slice and elliptical slice sampling algorithms. We assess the proposed\nmodel using real-world data collected from three bus routes with varying\nnumbers of stops, and the results demonstrate that our model achieves accurate\nposterior mean estimation and outperforms the widely used iterative\nproportional fitting (IPF) method. Additionally, our model can provide\nuncertainty quantification for the OD demand matrices, thus benefiting many\ndownstream planning/operational tasks that require robust decisions."
                },
                "authors": [
                    {
                        "name": "Xiaoxu Chen"
                    },
                    {
                        "name": "Zhanhong Cheng"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06413v2",
                "updated": "2024-11-27T15:58:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    58,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-08-12T18:00:02Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    18,
                    0,
                    2,
                    0,
                    225,
                    0
                ],
                "title": "Perturbation Theory Remixed II: Improved Modeling of Nonlinear\n  Bispectrum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbation Theory Remixed II: Improved Modeling of Nonlinear\n  Bispectrum"
                },
                "summary": "We present the application of the $n$-th order Eulerian Perturbation Theory\n($n$EPT) for modeling the matter bispectrum in real space as an advancement\nover the Standard Perturbation Theory (SPT). The $n$EPT method, detailed in\nWang et al. (2023) \\cite{Wang2023nEPT}, sums up the density perturbations up to\nthe $n$-th order before computing summary statistics such as bispectrum. Taking\nadvantage of grid-based calculation of SPT (GridSPT), we make a\nrealization-based comparison of the analytical nonlinear bispectrum predictions\nfrom $n$EPT and SPT against a suite of $N$-body simulations. Using a\nspherical-bispectrum visualization scheme, we show that $n$EPT bispectrum\nmatches better than SPT bispectrum over a wide range of scales in general\n$w$CDM cosmologies. Like the power spectrum case, we find that $n$EPT\nbispectrum modeling accuracy is controlled by $\\sigma_8(z) \\equiv \\sigma_8\nD(z)$, where $D(z)$ is the linear growth factor at a redshift $z$. Notably, the\n6EPT doubles the bispectrum model's validity range compared to the one-loop SPT\nfor $\\sigma_8(z) < 0.5$, corresponding to redshifts $z\\ge1$ for the\nbest-fitting Planck-2018 cosmology. For $n\\ge5$, however, $n$EPT bispectrum\ndepends sensitively on the cut-off scale or the grid resolution. The\npercent-level modeling accuracy achieved for the spherical bispectrum (where we\naverage over all triangular configurations) becomes much degraded when fixing\nconfigurations. Thus, we show that the validity range of the field-level\ncosmological inferences must be different from that derived from averaged\nsummary statistics such as $n$-point correlation functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the application of the $n$-th order Eulerian Perturbation Theory\n($n$EPT) for modeling the matter bispectrum in real space as an advancement\nover the Standard Perturbation Theory (SPT). The $n$EPT method, detailed in\nWang et al. (2023) \\cite{Wang2023nEPT}, sums up the density perturbations up to\nthe $n$-th order before computing summary statistics such as bispectrum. Taking\nadvantage of grid-based calculation of SPT (GridSPT), we make a\nrealization-based comparison of the analytical nonlinear bispectrum predictions\nfrom $n$EPT and SPT against a suite of $N$-body simulations. Using a\nspherical-bispectrum visualization scheme, we show that $n$EPT bispectrum\nmatches better than SPT bispectrum over a wide range of scales in general\n$w$CDM cosmologies. Like the power spectrum case, we find that $n$EPT\nbispectrum modeling accuracy is controlled by $\\sigma_8(z) \\equiv \\sigma_8\nD(z)$, where $D(z)$ is the linear growth factor at a redshift $z$. Notably, the\n6EPT doubles the bispectrum model's validity range compared to the one-loop SPT\nfor $\\sigma_8(z) < 0.5$, corresponding to redshifts $z\\ge1$ for the\nbest-fitting Planck-2018 cosmology. For $n\\ge5$, however, $n$EPT bispectrum\ndepends sensitively on the cut-off scale or the grid resolution. The\npercent-level modeling accuracy achieved for the spherical bispectrum (where we\naverage over all triangular configurations) becomes much degraded when fixing\nconfigurations. Thus, we show that the validity range of the field-level\ncosmological inferences must be different from that derived from averaged\nsummary statistics such as $n$-point correlation functions."
                },
                "authors": [
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Donghui Jeong"
                    },
                    {
                        "name": "Atsushi Taruya"
                    },
                    {
                        "name": "Takahiro Nishimichi"
                    },
                    {
                        "name": "Ken Osato"
                    }
                ],
                "author_detail": {
                    "name": "Ken Osato"
                },
                "author": "Ken Osato",
                "arxiv_doi": "10.1103/PhysRevD.110.103548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.103548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Physics Review D",
                "arxiv_journal_ref": "Phys. Rev. D 110, 103548 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18466v1",
                "updated": "2024-11-27T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    58,
                    7,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:58:07Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    58,
                    7,
                    2,
                    332,
                    0
                ],
                "title": "Complexity Experts are Task-Discriminative Learners for Any Image\n  Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complexity Experts are Task-Discriminative Learners for Any Image\n  Restoration"
                },
                "summary": "Recent advancements in all-in-one image restoration models have\nrevolutionized the ability to address diverse degradations through a unified\nframework. However, parameters tied to specific tasks often remain inactive for\nother tasks, making mixture-of-experts (MoE) architectures a natural extension.\nDespite this, MoEs often show inconsistent behavior, with some experts\nunexpectedly generalizing across tasks while others struggle within their\nintended scope. This hinders leveraging MoEs' computational benefits by\nbypassing irrelevant experts during inference. We attribute this undesired\nbehavior to the uniform and rigid architecture of traditional MoEs. To address\nthis, we introduce ``complexity experts\" -- flexible expert blocks with varying\ncomputational complexity and receptive fields. A key challenge is assigning\ntasks to each expert, as degradation complexity is unknown in advance. Thus, we\nexecute tasks with a simple bias toward lower complexity. To our surprise, this\npreference effectively drives task-specific allocation, assigning tasks to\nexperts with the appropriate complexity. Extensive experiments validate our\napproach, demonstrating the ability to bypass irrelevant experts during\ninference while maintaining superior performance. The proposed MoCE-IR model\noutperforms state-of-the-art methods, affirming its efficiency and practical\napplicability. The source will be publicly made available at\n\\href{https://eduardzamfir.github.io/moceir/}{\\texttt{eduardzamfir.github.io/MoCE-IR/}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in all-in-one image restoration models have\nrevolutionized the ability to address diverse degradations through a unified\nframework. However, parameters tied to specific tasks often remain inactive for\nother tasks, making mixture-of-experts (MoE) architectures a natural extension.\nDespite this, MoEs often show inconsistent behavior, with some experts\nunexpectedly generalizing across tasks while others struggle within their\nintended scope. This hinders leveraging MoEs' computational benefits by\nbypassing irrelevant experts during inference. We attribute this undesired\nbehavior to the uniform and rigid architecture of traditional MoEs. To address\nthis, we introduce ``complexity experts\" -- flexible expert blocks with varying\ncomputational complexity and receptive fields. A key challenge is assigning\ntasks to each expert, as degradation complexity is unknown in advance. Thus, we\nexecute tasks with a simple bias toward lower complexity. To our surprise, this\npreference effectively drives task-specific allocation, assigning tasks to\nexperts with the appropriate complexity. Extensive experiments validate our\napproach, demonstrating the ability to bypass irrelevant experts during\ninference while maintaining superior performance. The proposed MoCE-IR model\noutperforms state-of-the-art methods, affirming its efficiency and practical\napplicability. The source will be publicly made available at\n\\href{https://eduardzamfir.github.io/moceir/}{\\texttt{eduardzamfir.github.io/MoCE-IR/}}"
                },
                "authors": [
                    {
                        "name": "Eduard Zamfir"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Nancy Mehta"
                    },
                    {
                        "name": "Yuedong Tan"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Radu Timofte"
                    }
                ],
                "author_detail": {
                    "name": "Radu Timofte"
                },
                "author": "Radu Timofte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18462v1",
                "updated": "2024-11-27T15:53:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    53,
                    17,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:53:17Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    53,
                    17,
                    2,
                    332,
                    0
                ],
                "title": "Draft Model Knows When to Stop: A Self-Verification Length Policy for\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft Model Knows When to Stop: A Self-Verification Length Policy for\n  Speculative Decoding"
                },
                "summary": "Speculative Decoding (SD) has become an important technique in accelerating\nthe inference speed of large language models. Conventional SD methods employ a\nfixed draft length, which ignores the token generation difficulty across tasks.\nConsequently, in this paper, we address such an issue and introduce SVIP - a\ndifficulty-aware dynamic draft length policy for speculative decoding systems.\nBased on a theoretical lower bound of draft token acceptance rate and its\ninference-time approximation, SVIP adaptively determines the lengths of draft\nsequences based on the entropy of each draft token distribution. Experimental\nresults on mainstream SD benchmarks and frameworks demonstrate the superior\nperformance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over\nbaseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up\nto 8K tokens. Moreover, SVIP is totally training-free and compatible with any\nexisting SD methods that generate draft tokens autoregressively. Experimental\nresults also show that SVIP yields consistent walltime improvement on top of\nGliDe & CaPE and EAGLE-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding (SD) has become an important technique in accelerating\nthe inference speed of large language models. Conventional SD methods employ a\nfixed draft length, which ignores the token generation difficulty across tasks.\nConsequently, in this paper, we address such an issue and introduce SVIP - a\ndifficulty-aware dynamic draft length policy for speculative decoding systems.\nBased on a theoretical lower bound of draft token acceptance rate and its\ninference-time approximation, SVIP adaptively determines the lengths of draft\nsequences based on the entropy of each draft token distribution. Experimental\nresults on mainstream SD benchmarks and frameworks demonstrate the superior\nperformance of SVIP, achieving up to 20\\% walltime speedup on SpecBench over\nbaseline SD methods and 60\\% speedup on MT-Bench for long-form generation of up\nto 8K tokens. Moreover, SVIP is totally training-free and compatible with any\nexisting SD methods that generate draft tokens autoregressively. Experimental\nresults also show that SVIP yields consistent walltime improvement on top of\nGliDe & CaPE and EAGLE-2."
                },
                "authors": [
                    {
                        "name": "Ziyin Zhang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Code at https://github.com/Geralt-Targaryen/SVIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18447v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18447v1",
                "updated": "2024-11-27T15:38:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    38,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:38:20Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    38,
                    20,
                    2,
                    332,
                    0
                ],
                "title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error\n  Accumulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Autoregressive Models with Noise Augmentation Avoid Error\n  Accumulation"
                },
                "summary": "Autoregressive models are typically applied to sequences of discrete tokens,\nbut recent research indicates that generating sequences of continuous\nembeddings in an autoregressive manner is also feasible. However, such\nContinuous Autoregressive Models (CAMs) can suffer from a decline in generation\nquality over extended sequences due to error accumulation during inference. We\nintroduce a novel method to address this issue by injecting random noise into\nthe input embeddings during training. This procedure makes the model robust\nagainst varying error levels at inference. We further reduce error accumulation\nthrough an inference procedure that introduces low-level noise. Experiments on\nmusical audio generation show that CAM substantially outperforms existing\nautoregressive and non-autoregressive approaches while preserving audio quality\nover extended sequences. This work paves the way for generating continuous\nembeddings in a purely autoregressive setting, opening new possibilities for\nreal-time and interactive generative applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models are typically applied to sequences of discrete tokens,\nbut recent research indicates that generating sequences of continuous\nembeddings in an autoregressive manner is also feasible. However, such\nContinuous Autoregressive Models (CAMs) can suffer from a decline in generation\nquality over extended sequences due to error accumulation during inference. We\nintroduce a novel method to address this issue by injecting random noise into\nthe input embeddings during training. This procedure makes the model robust\nagainst varying error levels at inference. We further reduce error accumulation\nthrough an inference procedure that introduces low-level noise. Experiments on\nmusical audio generation show that CAM substantially outperforms existing\nautoregressive and non-autoregressive approaches while preserving audio quality\nover extended sequences. This work paves the way for generating continuous\nembeddings in a purely autoregressive setting, opening new possibilities for\nreal-time and interactive generative applications."
                },
                "authors": [
                    {
                        "name": "Marco Pasini"
                    },
                    {
                        "name": "Javier Nistal"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "George Fazekas"
                    }
                ],
                "author_detail": {
                    "name": "George Fazekas"
                },
                "author": "George Fazekas",
                "arxiv_comment": "Accepted to NeurIPS 2024 - Audio Imagination Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18447v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18447v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18444v1",
                "updated": "2024-11-27T15:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM\n  Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM\n  Evaluator"
                },
                "summary": "The quality of meeting summaries generated by natural language generation\n(NLG) systems is hard to measure automatically. Established metrics such as\nROUGE and BERTScore have a relatively low correlation with human judgments and\nfail to capture nuanced errors. Recent studies suggest using large language\nmodels (LLMs), which have the benefit of better context understanding and\nadaption of error definitions without training on a large number of human\npreference judgments. However, current LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving human evaluation the gold standard\ndespite being costly and hard to compare across studies. In this work, we\npresent MESA, an LLM-based framework employing a three-step assessment of\nindividual error types, multi-agent discussion for decision refinement, and\nfeedback-based self-training to refine error definition understanding and\nalignment with human judgment. We show that MESA's components enable thorough\nerror detection, consistent rating, and adaptability to custom error\nguidelines. Using GPT-4o as its backbone, MESA achieves mid to high\nPoint-Biserial correlation with human judgment in error detection and mid\nSpearman and Kendall correlation in reflecting error impact on summary quality,\non average 0.25 higher than previous methods. The framework's flexibility in\nadapting to custom error guidelines makes it suitable for various tasks with\nlimited human-labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of meeting summaries generated by natural language generation\n(NLG) systems is hard to measure automatically. Established metrics such as\nROUGE and BERTScore have a relatively low correlation with human judgments and\nfail to capture nuanced errors. Recent studies suggest using large language\nmodels (LLMs), which have the benefit of better context understanding and\nadaption of error definitions without training on a large number of human\npreference judgments. However, current LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving human evaluation the gold standard\ndespite being costly and hard to compare across studies. In this work, we\npresent MESA, an LLM-based framework employing a three-step assessment of\nindividual error types, multi-agent discussion for decision refinement, and\nfeedback-based self-training to refine error definition understanding and\nalignment with human judgment. We show that MESA's components enable thorough\nerror detection, consistent rating, and adaptability to custom error\nguidelines. Using GPT-4o as its backbone, MESA achieves mid to high\nPoint-Biserial correlation with human judgment in error detection and mid\nSpearman and Kendall correlation in reflecting error impact on summary quality,\non average 0.25 higher than previous methods. The framework's flexibility in\nadapting to custom error guidelines makes it suitable for various tasks with\nlimited human-labeled data."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05784v2",
                "updated": "2024-11-27T15:19:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    19,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-07-08T09:47:35Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    9,
                    47,
                    35,
                    0,
                    190,
                    0
                ],
                "title": "Hecaton: Training Large Language Models with Scalable Chiplet Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hecaton: Training Large Language Models with Scalable Chiplet Systems"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various\nfields, but their training and finetuning require massive computation and\nmemory, necessitating parallelism which introduces heavy communication\noverheads. Driven by advances in packaging, the chiplet architecture emerges as\na potential solution, as it can integrate computing power, as well as utilize\non-package links with better signal integrity, higher bandwidth, and lower\nenergy consumption. However, most existing chiplet-related works focus on DNN\ninference. Directly porting them to LLM training introduces significantly large\nquantities of DRAM access and network-on-package (NoP) overheads which make\nstate-of-the-art chiplet designs fail, highlighting a research gap.\n  This work proposes Hecaton, a scalable and cost-effective chiplet system for\nLLM training. We first provide a chiplet architecture with tailored scheduling\nthat can largely reduce DRAM accesses. We further design an efficient\ndistributed training method that reduces NoP communication complexity and\nrelieves constraints on SRAM capacity and layout. Theoretical analysis shows\nthat the entire system achieves weak scaling: as the workload and hardware\nresources grow proportionally, the computation-to-communication ratio remains\nnearly constant. Experiments with various workloads and hardware configurations\nverify the property, and Hecaton achieves $5.29\\times$ performance improvement\nand $3.46\\times$ energy reduction on Llama3.1-405B, compared to the tensor\nparallelism in Megatron. To the best of our knowledge, we propose the first\nchiplet architecture specifically used for LLM training or finetuning, with\nguaranteed performance regardless of the problem scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various\nfields, but their training and finetuning require massive computation and\nmemory, necessitating parallelism which introduces heavy communication\noverheads. Driven by advances in packaging, the chiplet architecture emerges as\na potential solution, as it can integrate computing power, as well as utilize\non-package links with better signal integrity, higher bandwidth, and lower\nenergy consumption. However, most existing chiplet-related works focus on DNN\ninference. Directly porting them to LLM training introduces significantly large\nquantities of DRAM access and network-on-package (NoP) overheads which make\nstate-of-the-art chiplet designs fail, highlighting a research gap.\n  This work proposes Hecaton, a scalable and cost-effective chiplet system for\nLLM training. We first provide a chiplet architecture with tailored scheduling\nthat can largely reduce DRAM accesses. We further design an efficient\ndistributed training method that reduces NoP communication complexity and\nrelieves constraints on SRAM capacity and layout. Theoretical analysis shows\nthat the entire system achieves weak scaling: as the workload and hardware\nresources grow proportionally, the computation-to-communication ratio remains\nnearly constant. Experiments with various workloads and hardware configurations\nverify the property, and Hecaton achieves $5.29\\times$ performance improvement\nand $3.46\\times$ energy reduction on Llama3.1-405B, compared to the tensor\nparallelism in Megatron. To the best of our knowledge, we propose the first\nchiplet architecture specifically used for LLM training or finetuning, with\nguaranteed performance regardless of the problem scale."
                },
                "authors": [
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Shupei Fan"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Shuwen Deng"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18433v1",
                "updated": "2024-11-27T15:17:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    17,
                    19,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:17:19Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    17,
                    19,
                    2,
                    332,
                    0
                ],
                "title": "A Latent Space Approach to Inferring Distance-Dependent Reciprocity in\n  Directed Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Latent Space Approach to Inferring Distance-Dependent Reciprocity in\n  Directed Networks"
                },
                "summary": "Reciprocity, or the stochastic tendency for actors to form mutual\nrelationships, is an essential characteristic of directed network data.\nExisting latent space approaches to modeling directed networks are severely\nlimited by the assumption that reciprocity is homogeneous across the network.\nIn this work, we introduce a new latent space model for directed networks that\ncan model heterogeneous reciprocity patterns that arise from the actors' latent\ndistances. Furthermore, existing edge-independent latent space models are\nnested within the proposed model class, which allows for meaningful model\ncomparisons. We introduce a Bayesian inference procedure to infer the model\nparameters using Hamiltonian Monte Carlo. Lastly, we use the proposed method to\ninfer different reciprocity patterns in an advice network among lawyers, an\ninformation-sharing network between employees at a manufacturing company, and a\nfriendship network between high school students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocity, or the stochastic tendency for actors to form mutual\nrelationships, is an essential characteristic of directed network data.\nExisting latent space approaches to modeling directed networks are severely\nlimited by the assumption that reciprocity is homogeneous across the network.\nIn this work, we introduce a new latent space model for directed networks that\ncan model heterogeneous reciprocity patterns that arise from the actors' latent\ndistances. Furthermore, existing edge-independent latent space models are\nnested within the proposed model class, which allows for meaningful model\ncomparisons. We introduce a Bayesian inference procedure to infer the model\nparameters using Hamiltonian Monte Carlo. Lastly, we use the proposed method to\ninfer different reciprocity patterns in an advice network among lawyers, an\ninformation-sharing network between employees at a manufacturing company, and a\nfriendship network between high school students."
                },
                "authors": [
                    {
                        "name": "Joshua Daniel Loyal"
                    },
                    {
                        "name": "Xiangyu Wu"
                    },
                    {
                        "name": "Jonathan R. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan R. Stewart"
                },
                "author": "Jonathan R. Stewart",
                "arxiv_comment": "21 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13694v2",
                "updated": "2024-11-27T15:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    13,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-03T03:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    31,
                    37,
                    1,
                    247,
                    0
                ],
                "title": "Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study"
                },
                "summary": "Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach for mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of externeal knowledge source. However, in\nreal-world applications, most situations involve diverse knowledge from various\nsources, yet this area has been less explored. The main dilemma is the lack of\na suitable dataset containing multiple knowledge sources and pre-exploration of\nthe associated issues. To address these challenges, we standardize a benchmark\ndataset that combines structured and unstructured knowledge across diverse and\ncomplementary domains. Based on this dataset, we further develop a\nplug-and-play RAG framework, PruningRAG, whose main characteristic is to employ\nmulti-granularity pruning strategies for optimizing the integration of relevant\ninformation and minimizing misleading context. Building upon the standardized\ndataset and PruningRAG, we also report a series of experimental results, as\nwell as insightful findings. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of\nadvancing future research in the RAG community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach for mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of externeal knowledge source. However, in\nreal-world applications, most situations involve diverse knowledge from various\nsources, yet this area has been less explored. The main dilemma is the lack of\na suitable dataset containing multiple knowledge sources and pre-exploration of\nthe associated issues. To address these challenges, we standardize a benchmark\ndataset that combines structured and unstructured knowledge across diverse and\ncomplementary domains. Based on this dataset, we further develop a\nplug-and-play RAG framework, PruningRAG, whose main characteristic is to employ\nmulti-granularity pruning strategies for optimizing the integration of relevant\ninformation and minimizing misleading context. Building upon the standardized\ndataset and PruningRAG, we also report a series of experimental results, as\nwell as insightful findings. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of\nadvancing future research in the RAG community."
                },
                "authors": [
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jiqian Yang"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Chenyi Lei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "arxiv_affiliation": "State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China",
                "author": "Enhong Chen",
                "arxiv_comment": "10 pages, 11 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05724v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05724v4",
                "updated": "2024-11-27T15:08:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    8,
                    23,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-08T17:59:02Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    17,
                    59,
                    2,
                    0,
                    99,
                    0
                ],
                "title": "JADES: Primaeval Lyman-$\\mathrmÎ±$ emitting galaxies reveal early\n  sites of reionisation out to redshift $z \\sim 9$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JADES: Primaeval Lyman-$\\mathrmÎ±$ emitting galaxies reveal early\n  sites of reionisation out to redshift $z \\sim 9$"
                },
                "summary": "$\\require{mediawiki-texvc}$Given the sensitivity of the resonant\nLyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by\nneutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs)\nhave been widely used to probe the ionising capabilities of reionisation-era\ngalaxies and their impact on the intergalactic medium (IGM). However, prior to\nJWST our understanding of the contribution of fainter sources and of ionised\n`bubbles' at earlier stages of reionisation remained uncertain. Here, we\npresent the characterisation of three exceptionally distant LAEs at $z>8$,\nnewly discovered by JWST/NIRSpec in the JADES survey. These three similarly\nbright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small\nLy$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta\nv_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of\nLy$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).\nThe former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions\n($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes\nremarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 72\\%$),\nwhich moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).\nWe find these LAEs are low-mass galaxies dominated by very recent, vigorous\nbursts of star formation accompanied by strong nebular emission from metal-poor\ngas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which\nreveals evidence for ionisation by an active galactic nucleus, may have\nreasonably produced small ionised bubbles preventing complete IGM absorption of\nLy$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical\nMpc}$ bubble, indicating faint galaxies have contributed significantly. The\nmost distant LAEs thus continue to be powerful observational probes into the\nearlier stages of reionisation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\require{mediawiki-texvc}$Given the sensitivity of the resonant\nLyman-$\\mathrm{\\alpha}$ (Ly$\\mathrm{\\alpha}$) transition to absorption by\nneutral hydrogen, observations of Ly$\\mathrm{\\alpha}$ emitting galaxies (LAEs)\nhave been widely used to probe the ionising capabilities of reionisation-era\ngalaxies and their impact on the intergalactic medium (IGM). However, prior to\nJWST our understanding of the contribution of fainter sources and of ionised\n`bubbles' at earlier stages of reionisation remained uncertain. Here, we\npresent the characterisation of three exceptionally distant LAEs at $z>8$,\nnewly discovered by JWST/NIRSpec in the JADES survey. These three similarly\nbright ($M_\\text{UV} \\approx -20\\,\\mathrm{mag}$) LAEs exhibit small\nLy$\\mathrm{\\alpha}$ velocity offsets from the systemic redshift, $\\Delta\nv_\\mathrm{Ly\\alpha} \\lesssim 200\\,\\mathrm{km\\,s^{-1}}$, yet span a range of\nLy$\\mathrm{\\alpha}$ equivalent widths ($15\\,\\AA$, $31\\,\\AA$, and $132\\,\\AA$).\nThe former two show moderate Ly$\\mathrm{\\alpha}$ escape fractions\n($f_\\mathrm{esc,Ly\\alpha} \\approx 10\\%$), whereas Ly$\\mathrm{\\alpha}$ escapes\nremarkably efficiently from the third ($f_\\mathrm{esc,Ly\\alpha} \\approx 72\\%$),\nwhich moreover is very compact (half-light radius of $90\\pm10\\,\\mathrm{pc}$).\nWe find these LAEs are low-mass galaxies dominated by very recent, vigorous\nbursts of star formation accompanied by strong nebular emission from metal-poor\ngas. We infer the two LAEs with modest $f_\\mathrm{esc,Ly\\alpha}$, one of which\nreveals evidence for ionisation by an active galactic nucleus, may have\nreasonably produced small ionised bubbles preventing complete IGM absorption of\nLy$\\mathrm{\\alpha}$. The third, however, requires a $\\sim 3\\,\\text{physical\nMpc}$ bubble, indicating faint galaxies have contributed significantly. The\nmost distant LAEs thus continue to be powerful observational probes into the\nearlier stages of reionisation."
                },
                "authors": [
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Renske Smit"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Jakob M. Helton"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Rachana Bhatawdekar"
                    },
                    {
                        "name": "Kristan Boyett"
                    },
                    {
                        "name": "Alex J. Cameron"
                    },
                    {
                        "name": "Phillip A. Cargile"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "StÃ©phane Charlot"
                    },
                    {
                        "name": "Jacopo Chevallard"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Kevin Hainline"
                    },
                    {
                        "name": "Ryan Hausen"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Isaac Laseter"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Irene Shivaei"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Chris Willott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Willott"
                },
                "author": "Chris Willott",
                "arxiv_doi": "10.1093/mnras/stae2535",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2535",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.05724v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05724v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 13 figures, accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18425v1",
                "updated": "2024-11-27T15:07:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Streamlining Prediction in Bayesian Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Prediction in Bayesian Deep Learning"
                },
                "summary": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rising interest in Bayesian deep learning (BDL) has led to a plethora of\nmethods for estimating the posterior distribution. However, efficient\ncomputation of inferences, such as predictions, has been largely overlooked\nwith Monte Carlo integration remaining the standard. In this work we examine\nstreamlining prediction in BDL through a single forward pass without sampling.\nFor this we use local linearisation on activation functions and local Gaussian\napproximations at linear layers. Thus allowing us to analytically compute an\napproximation to the posterior predictive distribution. We showcase our\napproach for both MLP and transformers, such as ViT and GPT-2, and assess its\nperformance on regression and classification tasks."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Marcus Klasson"
                    },
                    {
                        "name": "Arno Solin"
                    },
                    {
                        "name": "Martin Trapp"
                    }
                ],
                "author_detail": {
                    "name": "Martin Trapp"
                },
                "author": "Martin Trapp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17515v2",
                "updated": "2024-11-27T14:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    59,
                    15,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T15:26:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    26,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperMat: Physically Consistent PBR Material Estimation at Interactive\n  Rates"
                },
                "summary": "Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds. The project page is at\nhttps://hyj542682306.github.io/SuperMat/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing physically-based materials from images into their constituent\nproperties remains challenging, particularly when maintaining both\ncomputational efficiency and physical consistency. While recent diffusion-based\napproaches have shown promise, they face substantial computational overhead due\nto multiple denoising steps and separate models for different material\nproperties. We present SuperMat, a single-step framework that achieves\nhigh-quality material decomposition with one-step inference. This enables\nend-to-end training with perceptual and re-render losses while decomposing\nalbedo, metallic, and roughness maps at millisecond-scale speeds. We further\nextend our framework to 3D objects through a UV refinement network, enabling\nconsistent material estimation across viewpoints while maintaining efficiency.\nExperiments demonstrate that SuperMat achieves state-of-the-art PBR material\ndecomposition quality while reducing inference time from seconds to\nmilliseconds per image, and completes PBR material estimation for 3D objects in\napproximately 3 seconds. The project page is at\nhttps://hyj542682306.github.io/SuperMat/."
                },
                "authors": [
                    {
                        "name": "Yijia Hong"
                    },
                    {
                        "name": "Yuan-Chen Guo"
                    },
                    {
                        "name": "Ran Yi"
                    },
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Yan-Pei Cao"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "arxiv_comment": "https://hyj542682306.github.io/SuperMat/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14427v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14427v3",
                "updated": "2024-11-27T14:58:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    58,
                    17,
                    2,
                    332,
                    0
                ],
                "published": "2024-03-21T14:33:34Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    14,
                    33,
                    34,
                    3,
                    81,
                    0
                ],
                "title": "Learning and communication pressures in neural networks: Lessons from\n  emergent communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and communication pressures in neural networks: Lessons from\n  emergent communication"
                },
                "summary": "Finding and facilitating commonalities between the linguistic behaviors of\nlarge language models and humans could lead to major breakthroughs in our\nunderstanding of the acquisition, processing, and evolution of language.\nHowever, most findings on human-LLM similarity can be attributed to training on\nhuman data. The field of emergent machine-to-machine communication provides an\nideal testbed for discovering which pressures are neural agents naturally\nexposed to when learning to communicate in isolation, without any human\nlanguage to start with. Here, we review three cases where mismatches between\nthe emergent linguistic behavior of neural agents and humans were resolved\nthanks to introducing theoretically-motivated inductive biases. By contrasting\nhumans, large language models, and emergent communication agents, we then\nidentify key pressures at play for language learning and emergence:\ncommunicative success, production effort, learnability, and other\npsycho-/sociolinguistic factors. We discuss their implications and relevance to\nthe field of language evolution and acquisition. By mapping out the necessary\ninductive biases that make agents' emergent languages more human-like, we not\nonly shed light on the underlying principles of human cognition and\ncommunication, but also inform and improve the very use of these models as\nvaluable scientific tools for studying language learning, processing, use, and\nrepresentation more broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding and facilitating commonalities between the linguistic behaviors of\nlarge language models and humans could lead to major breakthroughs in our\nunderstanding of the acquisition, processing, and evolution of language.\nHowever, most findings on human-LLM similarity can be attributed to training on\nhuman data. The field of emergent machine-to-machine communication provides an\nideal testbed for discovering which pressures are neural agents naturally\nexposed to when learning to communicate in isolation, without any human\nlanguage to start with. Here, we review three cases where mismatches between\nthe emergent linguistic behavior of neural agents and humans were resolved\nthanks to introducing theoretically-motivated inductive biases. By contrasting\nhumans, large language models, and emergent communication agents, we then\nidentify key pressures at play for language learning and emergence:\ncommunicative success, production effort, learnability, and other\npsycho-/sociolinguistic factors. We discuss their implications and relevance to\nthe field of language evolution and acquisition. By mapping out the necessary\ninductive biases that make agents' emergent languages more human-like, we not\nonly shed light on the underlying principles of human cognition and\ncommunication, but also inform and improve the very use of these models as\nvaluable scientific tools for studying language learning, processing, use, and\nrepresentation more broadly."
                },
                "authors": [
                    {
                        "name": "Lukas Galke"
                    },
                    {
                        "name": "Limor Raviv"
                    }
                ],
                "author_detail": {
                    "name": "Limor Raviv"
                },
                "author": "Limor Raviv",
                "arxiv_doi": "10.34842/3vr5-5r49",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.34842/3vr5-5r49",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.14427v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14427v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready version, as published in Language Development Research",
                "arxiv_journal_ref": "Language Development Research 5(1), 116-143 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18410v1",
                "updated": "2024-11-27T14:56:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    56,
                    5,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:56:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    56,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "Preserving Information: How does Topological Data Analysis improve\n  Neural Network performance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Information: How does Topological Data Analysis improve\n  Neural Network performance?"
                },
                "summary": "Artificial Neural Networks (ANNs) require significant amounts of data and\ncomputational resources to achieve high effectiveness in performing the tasks\nfor which they are trained. To reduce resource demands, various techniques,\nsuch as Neuron Pruning, are applied. Due to the complex structure of ANNs,\ninterpreting the behavior of hidden layers and the features they recognize in\nthe data is challenging. A lack of comprehensive understanding of which\ninformation is utilized during inference can lead to inefficient use of\navailable data, thereby lowering the overall performance of the models. In this\npaper, we introduce a method for integrating Topological Data Analysis (TDA)\nwith Convolutional Neural Networks (CNN) in the context of image recognition.\nThis method significantly enhances the performance of neural networks by\nleveraging a broader range of information present in the data, enabling the\nmodel to make more informed and accurate predictions. Our approach, further\nreferred to as Vector Stitching, involves combining raw image data with\nadditional topological information derived through TDA methods. This approach\nenables the neural network to train on an enriched dataset, incorporating\ntopological features that might otherwise remain unexploited or not captured by\nthe network's inherent mechanisms. The results of our experiments highlight the\npotential of incorporating results of additional data analysis into the\nnetwork's inference process, resulting in enhanced performance in pattern\nrecognition tasks in digital images, particularly when using limited datasets.\nThis work contributes to the development of methods for integrating TDA with\ndeep learning and explores how concepts from Information Theory can explain the\nperformance of such hybrid methods in practical implementation environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Neural Networks (ANNs) require significant amounts of data and\ncomputational resources to achieve high effectiveness in performing the tasks\nfor which they are trained. To reduce resource demands, various techniques,\nsuch as Neuron Pruning, are applied. Due to the complex structure of ANNs,\ninterpreting the behavior of hidden layers and the features they recognize in\nthe data is challenging. A lack of comprehensive understanding of which\ninformation is utilized during inference can lead to inefficient use of\navailable data, thereby lowering the overall performance of the models. In this\npaper, we introduce a method for integrating Topological Data Analysis (TDA)\nwith Convolutional Neural Networks (CNN) in the context of image recognition.\nThis method significantly enhances the performance of neural networks by\nleveraging a broader range of information present in the data, enabling the\nmodel to make more informed and accurate predictions. Our approach, further\nreferred to as Vector Stitching, involves combining raw image data with\nadditional topological information derived through TDA methods. This approach\nenables the neural network to train on an enriched dataset, incorporating\ntopological features that might otherwise remain unexploited or not captured by\nthe network's inherent mechanisms. The results of our experiments highlight the\npotential of incorporating results of additional data analysis into the\nnetwork's inference process, resulting in enhanced performance in pattern\nrecognition tasks in digital images, particularly when using limited datasets.\nThis work contributes to the development of methods for integrating TDA with\ndeep learning and explores how concepts from Information Theory can explain the\nperformance of such hybrid methods in practical implementation environments."
                },
                "authors": [
                    {
                        "name": "A. Stolarek"
                    },
                    {
                        "name": "W. Jaworek"
                    }
                ],
                "author_detail": {
                    "name": "W. Jaworek"
                },
                "author": "W. Jaworek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16831v2",
                "updated": "2024-11-27T14:37:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    37,
                    37,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-25T18:58:51Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    18,
                    58,
                    51,
                    0,
                    330,
                    0
                ],
                "title": "Measuring Statistical Evidence: A Short Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Statistical Evidence: A Short Report"
                },
                "summary": "This short text tried to establish a big picture of what evidential\nstatistics is about and how an ideal inference method should behave. Moreover,\nby examining shortcomings of some of the currently used methods for measuring\nevidence and utilizing some intuitive principles, we motivated the Relative\nBelief Ratio as the primary method of characterizing statistical evidence.\nNumber of topics has been omitted for the interest of this text and the reader\nis strongly advised to refer to (Evans, 2015) as the primary source for further\nreadings of the subject.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This short text tried to establish a big picture of what evidential\nstatistics is about and how an ideal inference method should behave. Moreover,\nby examining shortcomings of some of the currently used methods for measuring\nevidence and utilizing some intuitive principles, we motivated the Relative\nBelief Ratio as the primary method of characterizing statistical evidence.\nNumber of topics has been omitted for the interest of this text and the reader\nis strongly advised to refer to (Evans, 2015) as the primary source for further\nreadings of the subject."
                },
                "authors": [
                    {
                        "name": "Mahdi Zamani"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Zamani"
                },
                "author": "Mahdi Zamani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18382v1",
                "updated": "2024-11-27T14:29:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    29,
                    10,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:29:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    29,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatGPT as speechwriter for the French presidents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT as speechwriter for the French presidents"
                },
                "summary": "Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches."
                },
                "authors": [
                    {
                        "name": "Dominique LabbÃ©"
                    },
                    {
                        "name": "Cyril LabbÃ©"
                    },
                    {
                        "name": "Jacques Savoy"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Savoy"
                },
                "author": "Jacques Savoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18376v1",
                "updated": "2024-11-27T14:25:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    25,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:25:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    25,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Preserving Deep Representations In One-Shot Pruning: A Hessian-Free\n  Second-Order Optimization Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Deep Representations In One-Shot Pruning: A Hessian-Free\n  Second-Order Optimization Framework"
                },
                "summary": "We present SNOWS, a one-shot post-training pruning framework aimed at\nreducing the cost of vision network inference without retraining. Current\nleading one-shot pruning methods minimize layer-wise least squares\nreconstruction error which does not take into account deeper network\nrepresentations. We propose to optimize a more global reconstruction objective.\nThis objective accounts for nonlinear activations deep in the network to obtain\na better proxy for the network loss. This nonlinear objective leads to a more\nchallenging optimization problem -- we demonstrate it can be solved efficiently\nusing a specialized second-order optimization framework. A key innovation of\nour framework is the use of Hessian-free optimization to compute exact Newton\ndescent steps without needing to compute or store the full Hessian matrix. A\ndistinct advantage of SNOWS is that it can be readily applied on top of any\nsparse mask derived from prior methods, readjusting their weights to exploit\nnonlinearities in deep feature representations. SNOWS obtains state-of-the-art\nresults on various one-shot pruning benchmarks including residual networks and\nVision Transformers (ViT/B-16 and ViT/L-16, 86m and 304m parameters\nrespectively).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SNOWS, a one-shot post-training pruning framework aimed at\nreducing the cost of vision network inference without retraining. Current\nleading one-shot pruning methods minimize layer-wise least squares\nreconstruction error which does not take into account deeper network\nrepresentations. We propose to optimize a more global reconstruction objective.\nThis objective accounts for nonlinear activations deep in the network to obtain\na better proxy for the network loss. This nonlinear objective leads to a more\nchallenging optimization problem -- we demonstrate it can be solved efficiently\nusing a specialized second-order optimization framework. A key innovation of\nour framework is the use of Hessian-free optimization to compute exact Newton\ndescent steps without needing to compute or store the full Hessian matrix. A\ndistinct advantage of SNOWS is that it can be readily applied on top of any\nsparse mask derived from prior methods, readjusting their weights to exploit\nnonlinearities in deep feature representations. SNOWS obtains state-of-the-art\nresults on various one-shot pruning benchmarks including residual networks and\nVision Transformers (ViT/B-16 and ViT/L-16, 86m and 304m parameters\nrespectively)."
                },
                "authors": [
                    {
                        "name": "Ryan Lucas"
                    },
                    {
                        "name": "Rahul Mazumder"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Mazumder"
                },
                "author": "Rahul Mazumder",
                "arxiv_comment": "10 pages excl. appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18375v1",
                "updated": "2024-11-27T14:22:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    22,
                    13,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:22:13Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    22,
                    13,
                    2,
                    332,
                    0
                ],
                "title": "Individual Content and Motion Dynamics Preserved Pruning for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individual Content and Motion Dynamics Preserved Pruning for Video\n  Diffusion Models"
                },
                "summary": "The high computational cost and slow inference time are major obstacles to\ndeploying the video diffusion model (VDM) in practical applications. To\novercome this, we introduce a new Video Diffusion Model Compression approach\nusing individual content and motion dynamics preserved pruning and consistency\nloss. First, we empirically observe that deeper VDM layers are crucial for\nmaintaining the quality of \\textbf{motion dynamics} e.g., coherence of the\nentire video, while shallower layers are more focused on \\textbf{individual\ncontent} e.g., individual frames. Therefore, we prune redundant blocks from the\nshallower layers while preserving more of the deeper layers, resulting in a\nlightweight VDM variant called VDMini. Additionally, we propose an\n\\textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain\ncomparable generation performance as larger VDM, i.e., the teacher to VDMini\ni.e., the student. Particularly, we first use the Individual Content\nDistillation (ICD) Loss to ensure consistency in the features of each generated\nframe between the teacher and student models. Next, we introduce a Multi-frame\nContent Adversarial (MCA) Loss to enhance the motion dynamics across the\ngenerated video as a whole. This method significantly accelerates inference\ntime while maintaining high-quality video generation. Extensive experiments\ndemonstrate the effectiveness of our VDMini on two important video generation\ntasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively\nachieve an average 2.5 $\\times$ and 1.4 $\\times$ speed up for the I2V method\nSF-V and the T2V method T2V-Turbo-v2, while maintaining the quality of the\ngenerated videos on two benchmarks, i.e., UCF101 and VBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational cost and slow inference time are major obstacles to\ndeploying the video diffusion model (VDM) in practical applications. To\novercome this, we introduce a new Video Diffusion Model Compression approach\nusing individual content and motion dynamics preserved pruning and consistency\nloss. First, we empirically observe that deeper VDM layers are crucial for\nmaintaining the quality of \\textbf{motion dynamics} e.g., coherence of the\nentire video, while shallower layers are more focused on \\textbf{individual\ncontent} e.g., individual frames. Therefore, we prune redundant blocks from the\nshallower layers while preserving more of the deeper layers, resulting in a\nlightweight VDM variant called VDMini. Additionally, we propose an\n\\textbf{Individual Content and Motion Dynamics (ICMD)} Consistency Loss to gain\ncomparable generation performance as larger VDM, i.e., the teacher to VDMini\ni.e., the student. Particularly, we first use the Individual Content\nDistillation (ICD) Loss to ensure consistency in the features of each generated\nframe between the teacher and student models. Next, we introduce a Multi-frame\nContent Adversarial (MCA) Loss to enhance the motion dynamics across the\ngenerated video as a whole. This method significantly accelerates inference\ntime while maintaining high-quality video generation. Extensive experiments\ndemonstrate the effectiveness of our VDMini on two important video generation\ntasks, Text-to-Video (T2V) and Image-to-Video (I2V), where we respectively\nachieve an average 2.5 $\\times$ and 1.4 $\\times$ speed up for the I2V method\nSF-V and the T2V method T2V-Turbo-v2, while maintaining the quality of the\ngenerated videos on two benchmarks, i.e., UCF101 and VBench."
                },
                "authors": [
                    {
                        "name": "Yiming Wu"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Zhenghao Chen"
                    },
                    {
                        "name": "Dong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Xu"
                },
                "author": "Dong Xu",
                "arxiv_comment": "9 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16800v2",
                "updated": "2024-11-27T14:14:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    14,
                    13,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-25T12:12:38Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    12,
                    38,
                    0,
                    330,
                    0
                ],
                "title": "Phys4DGen: A Physics-Driven Framework for Controllable and Efficient 4D\n  Content Generation from a Single Image",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phys4DGen: A Physics-Driven Framework for Controllable and Efficient 4D\n  Content Generation from a Single Image"
                },
                "summary": "The task of 4D content generation involves creating dynamic 3D models that\nevolve over time in response to specific input conditions, such as images.\nExisting methods rely heavily on pre-trained video diffusion models to guide 4D\ncontent dynamics, but these approaches often fail to capture essential physical\nprinciples, as video diffusion models lack a robust understanding of real-world\nphysics. Moreover, these models face challenges in providing fine-grained\ncontrol over dynamics and exhibit high computational costs. In this work, we\npropose Phys4DGen, a novel, high-efficiency framework that generates\nphysics-compliant 4D content from a single image with enhanced control\ncapabilities. Our approach uniquely integrates physical simulations into the 4D\ngeneration pipeline, ensuring adherence to fundamental physical laws. Inspired\nby the human ability to infer physical properties visually, we introduce a\nPhysical Perception Module (PPM) that discerns the material properties and\nstructural components of the 3D object from the input image, facilitating\naccurate downstream simulations. Phys4DGen significantly accelerates the 4D\ngeneration process by eliminating iterative optimization steps in the dynamics\nmodeling phase. It allows users to intuitively control the movement speed and\ndirection of generated 4D content by adjusting external forces, achieving\nfinely tunable, physically plausible animations. Extensive evaluations show\nthat Phys4DGen outperforms existing methods in both inference speed and\nphysical realism, producing high-quality, controllable 4D content. Our project\npage is available at the link: \\url{https://jiajinglin.github.io/Phys4DGen/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of 4D content generation involves creating dynamic 3D models that\nevolve over time in response to specific input conditions, such as images.\nExisting methods rely heavily on pre-trained video diffusion models to guide 4D\ncontent dynamics, but these approaches often fail to capture essential physical\nprinciples, as video diffusion models lack a robust understanding of real-world\nphysics. Moreover, these models face challenges in providing fine-grained\ncontrol over dynamics and exhibit high computational costs. In this work, we\npropose Phys4DGen, a novel, high-efficiency framework that generates\nphysics-compliant 4D content from a single image with enhanced control\ncapabilities. Our approach uniquely integrates physical simulations into the 4D\ngeneration pipeline, ensuring adherence to fundamental physical laws. Inspired\nby the human ability to infer physical properties visually, we introduce a\nPhysical Perception Module (PPM) that discerns the material properties and\nstructural components of the 3D object from the input image, facilitating\naccurate downstream simulations. Phys4DGen significantly accelerates the 4D\ngeneration process by eliminating iterative optimization steps in the dynamics\nmodeling phase. It allows users to intuitively control the movement speed and\ndirection of generated 4D content by adjusting external forces, achieving\nfinely tunable, physically plausible animations. Extensive evaluations show\nthat Phys4DGen outperforms existing methods in both inference speed and\nphysical realism, producing high-quality, controllable 4D content. Our project\npage is available at the link: \\url{https://jiajinglin.github.io/Phys4DGen/}."
                },
                "authors": [
                    {
                        "name": "Jiajing Lin"
                    },
                    {
                        "name": "Zhenzhong Wang"
                    },
                    {
                        "name": "Shu Jiang"
                    },
                    {
                        "name": "Yongjie Hou"
                    },
                    {
                        "name": "Min Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Min Jiang"
                },
                "author": "Min Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18365v1",
                "updated": "2024-11-27T14:12:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    12,
                    36,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:12:36Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    12,
                    36,
                    2,
                    332,
                    0
                ],
                "title": "GPT as ghostwriter at the White House",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT as ghostwriter at the White House"
                },
                "summary": "Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses."
                },
                "authors": [
                    {
                        "name": "Jacques Savoy"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Savoy"
                },
                "author": "Jacques Savoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18363v1",
                "updated": "2024-11-27T14:11:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding"
                },
                "summary": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}."
                },
                "authors": [
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Gen luo"
                    },
                    {
                        "name": "Yuqin Yang"
                    },
                    {
                        "name": "Yuda Xiong"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Zhaoyang Zeng"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "35 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05484v2",
                "updated": "2024-11-27T14:03:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    3,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-08-10T08:25:52Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    8,
                    25,
                    52,
                    5,
                    223,
                    0
                ],
                "title": "Interacting Models of Dark Energy and Dark Matter in Einstein scalar\n  Gauss Bonnet Gravity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting Models of Dark Energy and Dark Matter in Einstein scalar\n  Gauss Bonnet Gravity"
                },
                "summary": "We study the dynamics of the interacting models between the Gauss-Bonnet (GB)\ncoupled scalar field and the dark matter fluid in a homogeneous and isotropic\nbackground. A key feature of GB coupling models is the varying speed of\ngravitational waves (GWs). We utilize recent constraints on the GW speed and\nconduct our analysis in two primary scenarios: model-dependent and\nmodel-independent. In the model-dependent scenario, where determining the GW\nspeed requires a specific GB coupling functional form, we choose an exponential\nGB coupling. We adopt a dynamical system analysis to obtain the necessary\nconstraints on the model parameters that describe different phases of the\nuniverse and produce a stable late-time accelerating solution following the GW\nconstraint, and find that to satisfy all these constraints, fine-tuning of the\nfree parameters involved in the models is often needed. In the\nmodel-independent scenario, the GW speed is fixed to one, and we construct the\nautonomous system to identify the late-time stable accelerating critical\npoints. Furthermore, we adopt a Bayesian inference method using late-time\nobservational data sets, including 31 data points from cosmic chronometer data\n(Hubble data) and 1701 data points from Pantheon+ and find that all the\nobservational constraints can be satisfied without fine-tuning. In addition, we\nalso utilize simulated binned Roman and LSST data to study the evolution of the\nuniverse in the model-independent scenario. We find that the model shows\nsignificant deviation at higher redshifts from $\\Lambda$CDM and fits the\ncurrent data much better than $\\Lambda$CDM within the error bars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the dynamics of the interacting models between the Gauss-Bonnet (GB)\ncoupled scalar field and the dark matter fluid in a homogeneous and isotropic\nbackground. A key feature of GB coupling models is the varying speed of\ngravitational waves (GWs). We utilize recent constraints on the GW speed and\nconduct our analysis in two primary scenarios: model-dependent and\nmodel-independent. In the model-dependent scenario, where determining the GW\nspeed requires a specific GB coupling functional form, we choose an exponential\nGB coupling. We adopt a dynamical system analysis to obtain the necessary\nconstraints on the model parameters that describe different phases of the\nuniverse and produce a stable late-time accelerating solution following the GW\nconstraint, and find that to satisfy all these constraints, fine-tuning of the\nfree parameters involved in the models is often needed. In the\nmodel-independent scenario, the GW speed is fixed to one, and we construct the\nautonomous system to identify the late-time stable accelerating critical\npoints. Furthermore, we adopt a Bayesian inference method using late-time\nobservational data sets, including 31 data points from cosmic chronometer data\n(Hubble data) and 1701 data points from Pantheon+ and find that all the\nobservational constraints can be satisfied without fine-tuning. In addition, we\nalso utilize simulated binned Roman and LSST data to study the evolution of the\nuniverse in the model-independent scenario. We find that the model shows\nsignificant deviation at higher redshifts from $\\Lambda$CDM and fits the\ncurrent data much better than $\\Lambda$CDM within the error bars."
                },
                "authors": [
                    {
                        "name": "Saddam Hussain"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Yamuna Rana"
                    },
                    {
                        "name": "Benjamin Rose"
                    },
                    {
                        "name": "Anzhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anzhong Wang"
                },
                "author": "Anzhong Wang",
                "arxiv_doi": "10.1088/1475-7516/2024/11/042",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2024/11/042",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.05484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "36 pages, 21 figures, and 3 tables, Published in JCAP",
                "arxiv_journal_ref": "JCAP11(2024)042",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.02982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.02982v2",
                "updated": "2024-11-27T13:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    50,
                    5,
                    2,
                    332,
                    0
                ],
                "published": "2022-10-06T15:13:10Z",
                "published_parsed": [
                    2022,
                    10,
                    6,
                    15,
                    13,
                    10,
                    3,
                    279,
                    0
                ],
                "title": "Interplay of Pauli blockade with electron-photon coupling in quantum\n  dots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interplay of Pauli blockade with electron-photon coupling in quantum\n  dots"
                },
                "summary": "Both quantum transport measurements in the Pauli blockade regime and\nmicrowave cavity transmission measurements are important tools for spin-qubit\nreadout and characterization. Based on a generalized input-output theory we\nderive a theoretical framework to investigate how a double quantum dot (DQD) in\na transport setup interacts with a coupled microwave resonator while the\ncurrent through the DQD is rectified by Pauli blockade. We show that the output\nfield of the resonator can be used to infer the leakage current and thus obtain\ninsight into the blockade mechanisms. In the case of a silicon DQD, we show how\nthe valley quasi-degeneracy can impose limitations on this scheme. We also\ndemonstrate that a large number of unknown DQD parameters including (but not\nlimited to) the valley splitting can be estimated from the resonator response\nsimultaneous to a transport experiment, providing more detailed knowledge about\nthe microscopic environment of the DQD. Furthermore, we describe and quantify a\nback-action of the resonator photons on the steady state leakage current.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both quantum transport measurements in the Pauli blockade regime and\nmicrowave cavity transmission measurements are important tools for spin-qubit\nreadout and characterization. Based on a generalized input-output theory we\nderive a theoretical framework to investigate how a double quantum dot (DQD) in\na transport setup interacts with a coupled microwave resonator while the\ncurrent through the DQD is rectified by Pauli blockade. We show that the output\nfield of the resonator can be used to infer the leakage current and thus obtain\ninsight into the blockade mechanisms. In the case of a silicon DQD, we show how\nthe valley quasi-degeneracy can impose limitations on this scheme. We also\ndemonstrate that a large number of unknown DQD parameters including (but not\nlimited to) the valley splitting can be estimated from the resonator response\nsimultaneous to a transport experiment, providing more detailed knowledge about\nthe microscopic environment of the DQD. Furthermore, we describe and quantify a\nback-action of the resonator photons on the steady state leakage current."
                },
                "authors": [
                    {
                        "name": "Florian Ginzel"
                    },
                    {
                        "name": "Guido Burkard"
                    }
                ],
                "author_detail": {
                    "name": "Guido Burkard"
                },
                "author": "Guido Burkard",
                "arxiv_doi": "10.1103/PhysRevB.107.115302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.107.115302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2210.02982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.02982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v1",
                "updated": "2024-11-27T13:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06802v2",
                "updated": "2024-11-27T13:27:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    27,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-11T08:57:44Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    57,
                    44,
                    0,
                    316,
                    0
                ],
                "title": "Identifying the impact of local connectivity patterns on dynamics in\n  excitatory-inhibitory networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the impact of local connectivity patterns on dynamics in\n  excitatory-inhibitory networks"
                },
                "summary": "Networks of excitatory and inhibitory (EI) neurons form a canonical circuit\nin the brain. Seminal theoretical results on dynamics of such networks are\nbased on the assumption that synaptic strengths depend on the type of neurons\nthey connect, but are otherwise statistically independent. Recent synaptic\nphysiology datasets however highlight the prominence of specific connectivity\npatterns that go well beyond what is expected from independent connections.\nWhile decades of influential research have demonstrated the strong role of the\nbasic EI cell type structure, to which extent additional connectivity features\ninfluence dynamics remains to be fully determined. Here we examine the effects\nof pairwise connectivity motifs on the linear dynamics in EI networks using an\nanalytical framework that approximates the connectivity in terms of low-rank\nstructures. This low-rank approximation is based on a mathematical derivation\nof the dominant eigenvalues of the connectivity matrix and predicts the impact\non responses to external inputs of connectivity motifs and their interactions\nwith cell-type structure. Our results reveal that a particular pattern of\nconnectivity, chain motifs, have a much stronger impact on dominant eigenmodes\nthan other pairwise motifs. An overrepresentation of chain motifs induces a\nstrong positive eigenvalue in inhibition-dominated networks and generates a\npotential instability that requires revisiting the classical\nexcitation-inhibition balance criteria. Examining effects of external inputs,\nwe show that chain motifs can on their own induce paradoxical responses where\nan increased input to inhibitory neurons leads to a decrease in their activity\ndue to the recurrent feedback. These findings have direct implications for the\ninterpretation of experiments in which responses to optogenetic perturbations\nare measured and used to infer the dynamical regime of cortical circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networks of excitatory and inhibitory (EI) neurons form a canonical circuit\nin the brain. Seminal theoretical results on dynamics of such networks are\nbased on the assumption that synaptic strengths depend on the type of neurons\nthey connect, but are otherwise statistically independent. Recent synaptic\nphysiology datasets however highlight the prominence of specific connectivity\npatterns that go well beyond what is expected from independent connections.\nWhile decades of influential research have demonstrated the strong role of the\nbasic EI cell type structure, to which extent additional connectivity features\ninfluence dynamics remains to be fully determined. Here we examine the effects\nof pairwise connectivity motifs on the linear dynamics in EI networks using an\nanalytical framework that approximates the connectivity in terms of low-rank\nstructures. This low-rank approximation is based on a mathematical derivation\nof the dominant eigenvalues of the connectivity matrix and predicts the impact\non responses to external inputs of connectivity motifs and their interactions\nwith cell-type structure. Our results reveal that a particular pattern of\nconnectivity, chain motifs, have a much stronger impact on dominant eigenmodes\nthan other pairwise motifs. An overrepresentation of chain motifs induces a\nstrong positive eigenvalue in inhibition-dominated networks and generates a\npotential instability that requires revisiting the classical\nexcitation-inhibition balance criteria. Examining effects of external inputs,\nwe show that chain motifs can on their own induce paradoxical responses where\nan increased input to inhibitory neurons leads to a decrease in their activity\ndue to the recurrent feedback. These findings have direct implications for the\ninterpretation of experiments in which responses to optogenetic perturbations\nare measured and used to infer the dynamical regime of cortical circuits."
                },
                "authors": [
                    {
                        "name": "Yuxiu Shao"
                    },
                    {
                        "name": "David Dahmen"
                    },
                    {
                        "name": "Stefano Recanatesi"
                    },
                    {
                        "name": "Eric Shea-Brown"
                    },
                    {
                        "name": "Srdjan Ostojic"
                    }
                ],
                "author_detail": {
                    "name": "Srdjan Ostojic"
                },
                "arxiv_affiliation": "Laboratoire de Neurosciences Cognitives et Computationnelles, INSERM U960, Ecole Normale Superieure - PSL Research University, Paris, France",
                "author": "Srdjan Ostojic",
                "arxiv_comment": "25 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.01101v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.01101v3",
                "updated": "2024-11-27T13:07:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    7,
                    1,
                    2,
                    332,
                    0
                ],
                "published": "2022-03-02T13:41:11Z",
                "published_parsed": [
                    2022,
                    3,
                    2,
                    13,
                    41,
                    11,
                    2,
                    61,
                    0
                ],
                "title": "Approaching ideal visibility in singlet-triplet qubit operations using\n  energy-selective tunneling-based Hamiltonian estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approaching ideal visibility in singlet-triplet qubit operations using\n  energy-selective tunneling-based Hamiltonian estimation"
                },
                "summary": "We report energy selective tunneling readout-based Hamiltonian parameter\nestimation of a two-electron spin qubit in a GaAs quantum dot array.\nOptimization of readout fidelity enables a single-shot measurement time of 16\non average, with adaptive initialization and efficient qubit frequency\nestimation based on real-time Bayesian inference. For qubit operation in a\nfrequency heralded mode, we observe a 40-fold increase in coherence time\nwithout resorting to dynamic nuclear polarization. We also demonstrate active\nfrequency feedback with quantum oscillation visibility, single-shot measurement\nfidelity, and state initialization fidelity up to 97.7%, 99%, and over 99.7%,\nrespectively. By pushing the sensitivity of the energy selective\ntunneling-based spin to charge conversion to the limit, the technique is useful\nfor advanced quantum control protocols such as error mitigation schemes, where\nfast qubit parameter calibration with a large signal-to-noise ratio is crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report energy selective tunneling readout-based Hamiltonian parameter\nestimation of a two-electron spin qubit in a GaAs quantum dot array.\nOptimization of readout fidelity enables a single-shot measurement time of 16\non average, with adaptive initialization and efficient qubit frequency\nestimation based on real-time Bayesian inference. For qubit operation in a\nfrequency heralded mode, we observe a 40-fold increase in coherence time\nwithout resorting to dynamic nuclear polarization. We also demonstrate active\nfrequency feedback with quantum oscillation visibility, single-shot measurement\nfidelity, and state initialization fidelity up to 97.7%, 99%, and over 99.7%,\nrespectively. By pushing the sensitivity of the energy selective\ntunneling-based spin to charge conversion to the limit, the technique is useful\nfor advanced quantum control protocols such as error mitigation schemes, where\nfast qubit parameter calibration with a large signal-to-noise ratio is crucial."
                },
                "authors": [
                    {
                        "name": "Jehyun Kim"
                    },
                    {
                        "name": "Jonginn Yun"
                    },
                    {
                        "name": "Wonjin Jang"
                    },
                    {
                        "name": "Hyeongyu Jang"
                    },
                    {
                        "name": "Jaemin Park"
                    },
                    {
                        "name": "Youngwook Song"
                    },
                    {
                        "name": "Min-Kyun Cho"
                    },
                    {
                        "name": "Sangwoo Shim"
                    },
                    {
                        "name": "Hanseo Sohn"
                    },
                    {
                        "name": "Hwanchul Jung"
                    },
                    {
                        "name": "Vladimir Umansky"
                    },
                    {
                        "name": "Dohun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dohun Kim"
                },
                "author": "Dohun Kim",
                "arxiv_doi": "10.1103/PhysRevLett.129.040501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevLett.129.040501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2203.01101v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.01101v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 7 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19657v3",
                "updated": "2024-11-27T12:59:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    59,
                    3,
                    2,
                    332,
                    0
                ],
                "published": "2024-06-28T04:56:53Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    4,
                    56,
                    53,
                    4,
                    180,
                    0
                ],
                "title": "LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization"
                },
                "summary": "Currently, there are many quantization methods appeared for LLM quantization,\nyet few are user-friendly and easy to be deployed locally. Packages like\nTensorRT and Quantohave many underlying structures and self-invoking internal\nfunctions, which are not conducive to developers' personalized development and\nlearning for deployment. Therefore, we develop LLMEasyQuant, it is a package\naiming to for easy quantization deployment which is user-friendly and suitable\nfor beginners' learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, there are many quantization methods appeared for LLM quantization,\nyet few are user-friendly and easy to be deployed locally. Packages like\nTensorRT and Quantohave many underlying structures and self-invoking internal\nfunctions, which are not conducive to developers' personalized development and\nlearning for deployment. Therefore, we develop LLMEasyQuant, it is a package\naiming to for easy quantization deployment which is user-friendly and suitable\nfor beginners' learning."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Kaiser Pister"
                    }
                ],
                "author_detail": {
                    "name": "Kaiser Pister"
                },
                "author": "Kaiser Pister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00693v2",
                "updated": "2024-11-27T12:30:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    30,
                    23,
                    2,
                    332,
                    0
                ],
                "published": "2024-03-26T15:36:40Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    15,
                    36,
                    40,
                    1,
                    86,
                    0
                ],
                "title": "Leveraging Large Language Models in Human-Robot Interaction: A Critical\n  Analysis of Potential and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models in Human-Robot Interaction: A Critical\n  Analysis of Potential and Pitfalls"
                },
                "summary": "The emergence of large language models (LLM) and, consequently, vision\nlanguage models (VLM) has ignited new imaginations among robotics researchers.\nAt this point, the range of applications to which LLM and VLM can be applied in\nhuman-robot interaction (HRI), particularly socially assistive robots (SARs),\nis unchartered territory. However, LLM and VLM present unprecedented\nopportunities and challenges for SAR integration. We aim to illuminate the\nopportunities and challenges when roboticists deploy LLM and VLM in SARs.\nFirst, we conducted a meta-study of more than 250 papers exploring 1) major\nrobots in HRI research and 2) significant applications of SARs, emphasizing\neducation, healthcare, and entertainment while addressing 3) societal norms and\nissues like trust, bias, and ethics that the robot developers must address.\nThen, we identified 4) critical components of a robot that LLM or VLM can\nreplace while addressing the 5) benefits of integrating LLM into robot designs\nand the 6) risks involved. Finally, we outline a pathway for the responsible\nand effective adoption of LLM or VLM into SARs, and we close our discussion by\noffering caution regarding this deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLM) and, consequently, vision\nlanguage models (VLM) has ignited new imaginations among robotics researchers.\nAt this point, the range of applications to which LLM and VLM can be applied in\nhuman-robot interaction (HRI), particularly socially assistive robots (SARs),\nis unchartered territory. However, LLM and VLM present unprecedented\nopportunities and challenges for SAR integration. We aim to illuminate the\nopportunities and challenges when roboticists deploy LLM and VLM in SARs.\nFirst, we conducted a meta-study of more than 250 papers exploring 1) major\nrobots in HRI research and 2) significant applications of SARs, emphasizing\neducation, healthcare, and entertainment while addressing 3) societal norms and\nissues like trust, bias, and ethics that the robot developers must address.\nThen, we identified 4) critical components of a robot that LLM or VLM can\nreplace while addressing the 5) benefits of integrating LLM into robot designs\nand the 6) risks involved. Finally, we outline a pathway for the responsible\nand effective adoption of LLM or VLM into SARs, and we close our discussion by\noffering caution regarding this deployment."
                },
                "authors": [
                    {
                        "name": "Jesse Atuhurra"
                    }
                ],
                "author_detail": {
                    "name": "Jesse Atuhurra"
                },
                "author": "Jesse Atuhurra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18289v1",
                "updated": "2024-11-27T12:27:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    27,
                    50,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:27:50Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    27,
                    50,
                    2,
                    332,
                    0
                ],
                "title": "Don't Let Your Robot be Harmful: Responsible Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Let Your Robot be Harmful: Responsible Robotic Manipulation"
                },
                "summary": "Unthinking execution of human instructions in robotic manipulation can lead\nto severe safety risks, such as poisonings, fires, and even explosions. In this\npaper, we present responsible robotic manipulation, which requires robots to\nconsider potential hazards in the real-world environment while completing\ninstructions and performing complex operations safely and efficiently. However,\nsuch scenarios in real world are variable and risky for training. To address\nthis challenge, we propose Safety-as-policy, which includes (i) a world model\nto automatically generate scenarios containing safety risks and conduct virtual\ninteractions, and (ii) a mental model to infer consequences with reflections\nand gradually develop the cognition of safety, allowing robots to accomplish\ntasks while avoiding dangers. Additionally, we create the SafeBox synthetic\ndataset, which includes one hundred responsible robotic manipulation tasks with\ndifferent safety risk scenarios and instructions, effectively reducing the\nrisks associated with real-world experiments. Experiments demonstrate that\nSafety-as-policy can avoid risks and efficiently complete tasks in both\nsynthetic dataset and real-world experiments, significantly outperforming\nbaseline methods. Our SafeBox dataset shows consistent evaluation results with\nreal-world scenarios, serving as a safe and effective benchmark for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unthinking execution of human instructions in robotic manipulation can lead\nto severe safety risks, such as poisonings, fires, and even explosions. In this\npaper, we present responsible robotic manipulation, which requires robots to\nconsider potential hazards in the real-world environment while completing\ninstructions and performing complex operations safely and efficiently. However,\nsuch scenarios in real world are variable and risky for training. To address\nthis challenge, we propose Safety-as-policy, which includes (i) a world model\nto automatically generate scenarios containing safety risks and conduct virtual\ninteractions, and (ii) a mental model to infer consequences with reflections\nand gradually develop the cognition of safety, allowing robots to accomplish\ntasks while avoiding dangers. Additionally, we create the SafeBox synthetic\ndataset, which includes one hundred responsible robotic manipulation tasks with\ndifferent safety risk scenarios and instructions, effectively reducing the\nrisks associated with real-world experiments. Experiments demonstrate that\nSafety-as-policy can avoid risks and efficiently complete tasks in both\nsynthetic dataset and real-world experiments, significantly outperforming\nbaseline methods. Our SafeBox dataset shows consistent evaluation results with\nreal-world scenarios, serving as a safe and effective benchmark for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Minheng Ni"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Zihan Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08562v3",
                "updated": "2024-11-27T12:25:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    25,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2023-11-14T21:46:27Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    21,
                    46,
                    27,
                    1,
                    318,
                    0
                ],
                "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating exceptional reasoning, tool usage, and memory\ncapabilities. As their applications expand into multi-agent environments, there\narises a need for a comprehensive evaluation framework that captures LLMs'\nreasoning, planning, collaboration, and other social abilities. This work\nintroduces a novel competition-based benchmark framework specifically designed\nto assess LLMs within multi-agent settings, providing quantitative metrics to\nevaluate their judgment, reasoning, deception, self-awareness, cooperation,\ncoordination, and rationality. We utilize two social deduction games alongside\nthree game-theory scenarios to create diverse environments. Our frame is\nfortified with the probabilistic graphic modeling (PGM) method, enhancing the\nLLMs' capabilities in navigating complex social and cognitive dimensions. We\nevaluate seven LLMs, quantitatively highlighting a significant capability gap\nof over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.\nIt also confirms that our PGM enhancement boosts the abilities of all selected\nmodels by an average of 37%. Our data and code can be found here\nhttps://github.com/cathyxl/MAgIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating exceptional reasoning, tool usage, and memory\ncapabilities. As their applications expand into multi-agent environments, there\narises a need for a comprehensive evaluation framework that captures LLMs'\nreasoning, planning, collaboration, and other social abilities. This work\nintroduces a novel competition-based benchmark framework specifically designed\nto assess LLMs within multi-agent settings, providing quantitative metrics to\nevaluate their judgment, reasoning, deception, self-awareness, cooperation,\ncoordination, and rationality. We utilize two social deduction games alongside\nthree game-theory scenarios to create diverse environments. Our frame is\nfortified with the probabilistic graphic modeling (PGM) method, enhancing the\nLLMs' capabilities in navigating complex social and cognitive dimensions. We\nevaluate seven LLMs, quantitatively highlighting a significant capability gap\nof over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.\nIt also confirms that our PGM enhancement boosts the abilities of all selected\nmodels by an average of 37%. Our data and code can be found here\nhttps://github.com/cathyxl/MAgIC."
                },
                "authors": [
                    {
                        "name": "Lin Xu"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Hongyu Ren"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "See Kiong Ng"
                    },
                    {
                        "name": "Jiashi Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jiashi Feng"
                },
                "author": "Jiashi Feng",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18281v1",
                "updated": "2024-11-27T12:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    15,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    15,
                    52,
                    2,
                    332,
                    0
                ],
                "title": "MotionCharacter: Identity-Preserving and Motion Controllable Human Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionCharacter: Identity-Preserving and Motion Controllable Human Video\n  Generation"
                },
                "summary": "Recent advancements in personalized Text-to-Video (T2V) generation highlight\nthe importance of integrating character-specific identities and actions.\nHowever, previous T2V models struggle with identity consistency and\ncontrollable motion dynamics, mainly due to limited fine-grained facial and\naction-based textual prompts, and datasets that overlook key human attributes\nand actions. To address these challenges, we propose MotionCharacter, an\nefficient and high-fidelity human video generation framework designed for\nidentity preservation and fine-grained motion control. We introduce an\nID-preserving module to maintain identity fidelity while allowing flexible\nattribute modifications, and further integrate ID-consistency and region-aware\nloss mechanisms, significantly enhancing identity consistency and detail\nfidelity. Additionally, our approach incorporates a motion control module that\nprioritizes action-related text while maintaining subject consistency, along\nwith a dataset, Human-Motion, which utilizes large language models to generate\ndetailed motion descriptions. For simplify user control during inference, we\nparameterize motion intensity through a single coefficient, allowing for easy\nadjustments. Extensive experiments highlight the effectiveness of\nMotionCharacter, demonstrating significant improvements in ID-preserving,\nhigh-quality video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized Text-to-Video (T2V) generation highlight\nthe importance of integrating character-specific identities and actions.\nHowever, previous T2V models struggle with identity consistency and\ncontrollable motion dynamics, mainly due to limited fine-grained facial and\naction-based textual prompts, and datasets that overlook key human attributes\nand actions. To address these challenges, we propose MotionCharacter, an\nefficient and high-fidelity human video generation framework designed for\nidentity preservation and fine-grained motion control. We introduce an\nID-preserving module to maintain identity fidelity while allowing flexible\nattribute modifications, and further integrate ID-consistency and region-aware\nloss mechanisms, significantly enhancing identity consistency and detail\nfidelity. Additionally, our approach incorporates a motion control module that\nprioritizes action-related text while maintaining subject consistency, along\nwith a dataset, Human-Motion, which utilizes large language models to generate\ndetailed motion descriptions. For simplify user control during inference, we\nparameterize motion intensity through a single coefficient, allowing for easy\nadjustments. Extensive experiments highlight the effectiveness of\nMotionCharacter, demonstrating significant improvements in ID-preserving,\nhigh-quality video generation."
                },
                "authors": [
                    {
                        "name": "Haopeng Fang"
                    },
                    {
                        "name": "Di Qiu"
                    },
                    {
                        "name": "Binjie Mao"
                    },
                    {
                        "name": "Pengfei Yan"
                    },
                    {
                        "name": "He Tang"
                    }
                ],
                "author_detail": {
                    "name": "He Tang"
                },
                "author": "He Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18280v1",
                "updated": "2024-11-27T12:15:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    15,
                    22,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:15:22Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    15,
                    22,
                    2,
                    332,
                    0
                ],
                "title": "Neutralizing Backdoors through Information Conflicts for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing Backdoors through Information Conflicts for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks, from\nunderstanding to reasoning. However, they remain vulnerable to backdoor\nattacks, where models behave normally for standard queries but generate harmful\nresponses or unintended output when specific triggers are activated. Existing\nbackdoor defenses often suffer from drawbacks that they either focus on\ndetection without removal, rely on rigid assumptions about trigger properties,\nor prove to be ineffective against advanced attacks like multi-trigger\nbackdoors. In this paper, we present a novel method to eliminate backdoor\nbehaviors from LLMs through the construction of information conflicts using\nboth internal and external mechanisms. Internally, we leverage a lightweight\ndataset to train a conflict model, which is then merged with the backdoored\nmodel to neutralize malicious behaviors by embedding contradictory information\nwithin the model's parametric memory. Externally, we incorporate convincing\ncontradictory evidence into the prompt to challenge the model's internal\nbackdoor knowledge. Experimental results on classification and conversational\ntasks across 4 widely used LLMs demonstrate that our method outperforms 8\nstate-of-the-art backdoor defense baselines. We can reduce the attack success\nrate of advanced backdoor attacks by up to 98% while maintaining over 90% clean\ndata accuracy. Furthermore, our method has proven to be robust against adaptive\nbackdoor attacks. The code will be open-sourced upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks, from\nunderstanding to reasoning. However, they remain vulnerable to backdoor\nattacks, where models behave normally for standard queries but generate harmful\nresponses or unintended output when specific triggers are activated. Existing\nbackdoor defenses often suffer from drawbacks that they either focus on\ndetection without removal, rely on rigid assumptions about trigger properties,\nor prove to be ineffective against advanced attacks like multi-trigger\nbackdoors. In this paper, we present a novel method to eliminate backdoor\nbehaviors from LLMs through the construction of information conflicts using\nboth internal and external mechanisms. Internally, we leverage a lightweight\ndataset to train a conflict model, which is then merged with the backdoored\nmodel to neutralize malicious behaviors by embedding contradictory information\nwithin the model's parametric memory. Externally, we incorporate convincing\ncontradictory evidence into the prompt to challenge the model's internal\nbackdoor knowledge. Experimental results on classification and conversational\ntasks across 4 widely used LLMs demonstrate that our method outperforms 8\nstate-of-the-art backdoor defense baselines. We can reduce the attack success\nrate of advanced backdoor attacks by up to 98% while maintaining over 90% clean\ndata accuracy. Furthermore, our method has proven to be robust against adaptive\nbackdoor attacks. The code will be open-sourced upon publication."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v1",
                "updated": "2024-11-27T12:13:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18266v1",
                "updated": "2024-11-27T12:03:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:03:52Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    52,
                    2,
                    332,
                    0
                ],
                "title": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria"
                },
                "summary": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems."
                },
                "authors": [
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Shuo Gao"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Wentian Yi"
                    },
                    {
                        "name": "Yuxuan Jin"
                    },
                    {
                        "name": "Xiaoxue Zhai"
                    },
                    {
                        "name": "Sixuan Lei"
                    },
                    {
                        "name": "Hongbei Meng"
                    },
                    {
                        "name": "Zibo Zhang"
                    },
                    {
                        "name": "Muzi Xu"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Xuhang Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Hongyun Yang"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Xiaodong Feng"
                    },
                    {
                        "name": "Peter Smielewski"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Wenhui Song"
                    },
                    {
                        "name": "Martin Birchall"
                    },
                    {
                        "name": "Luigi G. Occhipint"
                    }
                ],
                "author_detail": {
                    "name": "Luigi G. Occhipint"
                },
                "author": "Luigi G. Occhipint",
                "arxiv_comment": "5 figures, 45 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11782v2",
                "updated": "2024-11-27T12:03:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    27,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-15T17:01:21Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks"
                },
                "summary": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Dawei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Cheng"
                },
                "author": "Dawei Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18263v1",
                "updated": "2024-11-27T12:01:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    1,
                    8,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:01:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    1,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World\n  Image Super-Resolution"
                },
                "summary": "Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained text-to-image diffusion models are increasingly applied to\nreal-world image super-resolution (Real-ISR) task. Given the iterative\nrefinement nature of diffusion models, most existing approaches are\ncomputationally expensive. While methods such as SinSR and OSEDiff have emerged\nto condense inference steps via distillation, their performance in image\nrestoration or details recovery is not satisfied. To address this, we propose\nTSD-SR, a novel distillation framework specifically designed for real-world\nimage super-resolution, aiming to construct an efficient and effective one-step\nmodel. We first introduce the Target Score Distillation, which leverages the\npriors of diffusion models and real image references to achieve more realistic\nimage restoration. Secondly, we propose a Distribution-Aware Sampling Module to\nmake detail-oriented gradients more readily accessible, addressing the\nchallenge of recovering fine details. Extensive experiments demonstrate that\nour TSD-SR has superior restoration results (most of the metrics perform the\nbest) and the fastest inference speed (e.g. 40 times faster than SeeSR)\ncompared to the past Real-ISR approaches based on pre-trained diffusion priors."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yihong Guo"
                    },
                    {
                        "name": "Zhonghao Wang"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18262v1",
                "updated": "2024-11-27T11:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    59,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    59,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Break the ID-Language Barrier: An Adaption Framework for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Break the ID-Language Barrier: An Adaption Framework for Sequential\n  Recommendation"
                },
                "summary": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15115v3",
                "updated": "2024-11-27T11:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    58,
                    50,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-19T13:53:50Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    13,
                    53,
                    50,
                    5,
                    293,
                    0
                ],
                "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
                },
                "summary": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wenjie Ye"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17175v3",
                "updated": "2024-11-27T11:47:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    47,
                    45,
                    2,
                    332,
                    0
                ],
                "published": "2024-08-30T10:24:07Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    24,
                    7,
                    4,
                    243,
                    0
                ],
                "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model"
                },
                "summary": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)"
                },
                "authors": [
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Peiwen Sun"
                    },
                    {
                        "name": "Jiahe Lei"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Zheqi Dai"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Jiahao Pan"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16197v2",
                "updated": "2024-11-27T11:45:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    45,
                    11,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-25T08:53:23Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    8,
                    53,
                    23,
                    0,
                    330,
                    0
                ],
                "title": "The bulk metallicity of giant planets around M stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bulk metallicity of giant planets around M stars"
                },
                "summary": "The bulk-metallicity determination of giant exoplanets is essential to\nconstrain their formation and evolution pathways and to compare them to the\nsolar system. Previous studies inferred an inverse relation between the mass\nand bulk metallicity. However, the data almost exclusively contained planets\nthat orbit FGK stars. The recent discoveries of giant exoplanets around M-dwarf\nstars present an opportunity to probe whether they follow a mass-metallicity\ntrend different from that of their FGK counterparts. Using evolution models we\ncharacterised the interiors of giant exoplanets with reliable mass-radius\nmeasurements that orbit FGK and M-dwarf stars. We then inferred the\nmass-metallicity trends for both populations. We found that the bulk\nmetallicity of giant planets around M stars is overall lower compared to those\naround FGK stars. This yielded mass-metallicity relations for the two\npopulations with similar slopes but significantly different offsets. The lack\nof metal-rich giant planets around M dwarfs could explain the difference in the\ninferred offset and be a result of different formation conditions. However,\nthere were only 20 successful bulk-metallicity retrievals for the giant planets\naround M dwarfs, which resulted in rather large uncertainties. Therefore, it is\nof great importance to continue detecting these planets with both transit and\nradial velocities. Additionally, the characterisation of the atmospheres of\ngiant planets around M-stars can further help to constrain their interiors and\nto investigate the atmosphere-interior connection. This will significantly\ncontribute towards understanding the possible formation pathways of giant\nplanets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bulk-metallicity determination of giant exoplanets is essential to\nconstrain their formation and evolution pathways and to compare them to the\nsolar system. Previous studies inferred an inverse relation between the mass\nand bulk metallicity. However, the data almost exclusively contained planets\nthat orbit FGK stars. The recent discoveries of giant exoplanets around M-dwarf\nstars present an opportunity to probe whether they follow a mass-metallicity\ntrend different from that of their FGK counterparts. Using evolution models we\ncharacterised the interiors of giant exoplanets with reliable mass-radius\nmeasurements that orbit FGK and M-dwarf stars. We then inferred the\nmass-metallicity trends for both populations. We found that the bulk\nmetallicity of giant planets around M stars is overall lower compared to those\naround FGK stars. This yielded mass-metallicity relations for the two\npopulations with similar slopes but significantly different offsets. The lack\nof metal-rich giant planets around M dwarfs could explain the difference in the\ninferred offset and be a result of different formation conditions. However,\nthere were only 20 successful bulk-metallicity retrievals for the giant planets\naround M dwarfs, which resulted in rather large uncertainties. Therefore, it is\nof great importance to continue detecting these planets with both transit and\nradial velocities. Additionally, the characterisation of the atmospheres of\ngiant planets around M-stars can further help to constrain their interiors and\nto investigate the atmosphere-interior connection. This will significantly\ncontribute towards understanding the possible formation pathways of giant\nplanets."
                },
                "authors": [
                    {
                        "name": "Simon MÃ¼ller"
                    },
                    {
                        "name": "Ravit Helled"
                    }
                ],
                "author_detail": {
                    "name": "Ravit Helled"
                },
                "author": "Ravit Helled",
                "arxiv_comment": "12 pages, 10 figures, 3 tables, accepted for publication at Astronomy\n  & Astrophysics; corrected arXiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18247v1",
                "updated": "2024-11-27T11:38:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    38,
                    9,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:38:09Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    38,
                    9,
                    2,
                    332,
                    0
                ],
                "title": "A gentle push funziona benissimo: making instructed models in Italian\n  via contrastive activation steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A gentle push funziona benissimo: making instructed models in Italian\n  via contrastive activation steering"
                },
                "summary": "Adapting models to a language that was only partially present in the\npre-training data requires fine-tuning, which is expensive in terms of both\ndata and computational resources. As an alternative to fine-tuning, we explore\nthe potential of activation steering-based techniques to enhance model\nperformance on Italian tasks. Through our experiments we show that Italian\nsteering (i) can be successfully applied to different models, (ii) achieves\nperformances comparable to, or even better than, fine-tuned models for Italian,\nand (iii) yields higher quality and consistency in Italian generations. We also\ndiscuss the utility of steering and fine-tuning in the contemporary LLM\nlandscape where models are anyway getting high Italian performances even if not\nexplicitly trained in this language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting models to a language that was only partially present in the\npre-training data requires fine-tuning, which is expensive in terms of both\ndata and computational resources. As an alternative to fine-tuning, we explore\nthe potential of activation steering-based techniques to enhance model\nperformance on Italian tasks. Through our experiments we show that Italian\nsteering (i) can be successfully applied to different models, (ii) achieves\nperformances comparable to, or even better than, fine-tuned models for Italian,\nand (iii) yields higher quality and consistency in Italian generations. We also\ndiscuss the utility of steering and fine-tuning in the contemporary LLM\nlandscape where models are anyway getting high Italian performances even if not\nexplicitly trained in this language."
                },
                "authors": [
                    {
                        "name": "Daniel Scalena"
                    },
                    {
                        "name": "Elisabetta Fersini"
                    },
                    {
                        "name": "Malvina Nissim"
                    }
                ],
                "author_detail": {
                    "name": "Malvina Nissim"
                },
                "author": "Malvina Nissim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18242v1",
                "updated": "2024-11-27T11:30:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    30,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    30,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Thai Financial Domain Adaptation of THaLLE -- Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thai Financial Domain Adaptation of THaLLE -- Technical Report"
                },
                "summary": "Large Language Models (LLMs) excel in general tasks but struggle with\ndomain-specific challenges, such as specialized terminology and localized\nregulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack\nsupport for the Thai financial domain. We developed a Thai Financial LLM using\nthe Investment Consultant (IC) exam dataset from the Stock Exchange of\nThailand. To address dataset limitations, we applied data augmentation, ReLoRA\nfor efficient training, Continued Pretraining (CPT) for domain knowledge, and\nRank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)\nsimulated exam scenarios, while Direct Preference Optimization (DPO) refined\nthe model using feedback. The model achieved scores of 72%, 72%, and 84% on IC\nexam levels P1, P2, and P3, respectively, demonstrating its effectiveness in\nThai financial advisory tasks and its potential for specialized applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in general tasks but struggle with\ndomain-specific challenges, such as specialized terminology and localized\nregulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack\nsupport for the Thai financial domain. We developed a Thai Financial LLM using\nthe Investment Consultant (IC) exam dataset from the Stock Exchange of\nThailand. To address dataset limitations, we applied data augmentation, ReLoRA\nfor efficient training, Continued Pretraining (CPT) for domain knowledge, and\nRank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)\nsimulated exam scenarios, while Direct Preference Optimization (DPO) refined\nthe model using feedback. The model achieved scores of 72%, 72%, and 84% on IC\nexam levels P1, P2, and P3, respectively, demonstrating its effectiveness in\nThai financial advisory tasks and its potential for specialized applications."
                },
                "authors": [
                    {
                        "name": "KBTG Labs"
                    },
                    {
                        "name": "Atthakorn Petchsod"
                    },
                    {
                        "name": "Pornchanan Balee"
                    },
                    {
                        "name": "Danupat Khamnuansin"
                    },
                    {
                        "name": "Anuruth Lertpiya"
                    },
                    {
                        "name": "Chanatip Saetia"
                    },
                    {
                        "name": "Tawunrat Chalothorn"
                    },
                    {
                        "name": "Thadpong Pongthawornkamol"
                    },
                    {
                        "name": "Monchai Lertsutthiwong"
                    }
                ],
                "author_detail": {
                    "name": "Monchai Lertsutthiwong"
                },
                "author": "Monchai Lertsutthiwong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18241v1",
                "updated": "2024-11-27T11:29:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    29,
                    17,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:29:17Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    29,
                    17,
                    2,
                    332,
                    0
                ],
                "title": "Exploration of LLM Multi-Agent Application Implementation Based on\n  LangGraph+CrewAI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration of LLM Multi-Agent Application Implementation Based on\n  LangGraph+CrewAI"
                },
                "summary": "With the rapid development of large model technology, the application of\nagent technology in various fields is becoming increasingly widespread,\nprofoundly changing people's work and lifestyles. In complex and dynamic\nsystems, multi-agents achieve complex tasks that are difficult for a single\nagent to complete through division of labor and collaboration among agents.\nThis paper discusses the integrated application of LangGraph and CrewAI.\nLangGraph improves the efficiency of information transmission through graph\narchitecture, while CrewAI enhances team collaboration capabilities and system\nperformance through intelligent task allocation and resource management. The\nmain research contents of this paper are: (1) designing the architecture of\nagents based on LangGraph for precise control; (2) enhancing the capabilities\nof agents based on CrewAI to complete a variety of tasks. This study aims to\ndelve into the application of LangGraph and CrewAI in multi-agent systems,\nproviding new perspectives for the future development of agent technology, and\npromoting technological progress and application innovation in the field of\nlarge model intelligent agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large model technology, the application of\nagent technology in various fields is becoming increasingly widespread,\nprofoundly changing people's work and lifestyles. In complex and dynamic\nsystems, multi-agents achieve complex tasks that are difficult for a single\nagent to complete through division of labor and collaboration among agents.\nThis paper discusses the integrated application of LangGraph and CrewAI.\nLangGraph improves the efficiency of information transmission through graph\narchitecture, while CrewAI enhances team collaboration capabilities and system\nperformance through intelligent task allocation and resource management. The\nmain research contents of this paper are: (1) designing the architecture of\nagents based on LangGraph for precise control; (2) enhancing the capabilities\nof agents based on CrewAI to complete a variety of tasks. This study aims to\ndelve into the application of LangGraph and CrewAI in multi-agent systems,\nproviding new perspectives for the future development of agent technology, and\npromoting technological progress and application innovation in the field of\nlarge model intelligent agents."
                },
                "authors": [
                    {
                        "name": "Zhihua Duan"
                    },
                    {
                        "name": "Jialin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Wang"
                },
                "author": "Jialin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23558v2",
                "updated": "2024-11-27T11:28:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    28,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-31T01:55:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    1,
                    55,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Transferable Ensemble Black-box Jailbreak Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Ensemble Black-box Jailbreak Attacks on Large Language\n  Models"
                },
                "summary": "In this report, we propose a novel black-box jailbreak attacking framework\nthat incorporates various LLM-as-Attacker methods to deliver transferable and\npowerful jailbreak attacks. Our method is designed based on three key\nobservations from existing jailbreaking studies and practices. First, we\nconsider an ensemble approach should be more effective in exposing the\nvulnerabilities of an aligned LLM compared to individual attacks. Second,\ndifferent malicious instructions inherently vary in their jailbreaking\ndifficulty, necessitating differentiated treatment to ensure more efficient\nattacks. Finally, the semantic coherence of a malicious instruction is crucial\nfor triggering the defenses of an aligned LLM; therefore, it must be carefully\ndisrupted to manipulate its embedding representation, thereby increasing the\njailbreak success rate. We validated our approach by participating in the\nCompetition for LLM and Agent Safety 2024, where our team achieved top\nperformance in the Jailbreaking Attack Track.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we propose a novel black-box jailbreak attacking framework\nthat incorporates various LLM-as-Attacker methods to deliver transferable and\npowerful jailbreak attacks. Our method is designed based on three key\nobservations from existing jailbreaking studies and practices. First, we\nconsider an ensemble approach should be more effective in exposing the\nvulnerabilities of an aligned LLM compared to individual attacks. Second,\ndifferent malicious instructions inherently vary in their jailbreaking\ndifficulty, necessitating differentiated treatment to ensure more efficient\nattacks. Finally, the semantic coherence of a malicious instruction is crucial\nfor triggering the defenses of an aligned LLM; therefore, it must be carefully\ndisrupted to manipulate its embedding representation, thereby increasing the\njailbreak success rate. We validated our approach by participating in the\nCompetition for LLM and Agent Safety 2024, where our team achieved top\nperformance in the Jailbreaking Attack Track."
                },
                "authors": [
                    {
                        "name": "Yiqi Yang"
                    },
                    {
                        "name": "Hongye Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hongye Fu"
                },
                "author": "Hongye Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10596v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10596v3",
                "updated": "2024-11-27T11:22:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    22,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-05-17T07:43:25Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    7,
                    43,
                    25,
                    4,
                    138,
                    0
                ],
                "title": "CELA: Cost-Efficient Language Model Alignment for CTR Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CELA: Cost-Efficient Language Model Alignment for CTR Prediction"
                },
                "summary": "Click-Through Rate (CTR) prediction holds a paramount position in recommender\nsystems. The prevailing ID-based paradigm underperforms in cold-start scenarios\ndue to the skewed distribution of feature frequency. Additionally, the\nutilization of a single modality fails to exploit the knowledge contained\nwithin textual features. Recent efforts have sought to mitigate these\nchallenges by integrating Pre-trained Language Models (PLMs). They design hard\nprompts to structure raw features into text for each interaction and then apply\nPLMs for text processing. With external knowledge and reasoning capabilities,\nPLMs extract valuable information even in cases of sparse interactions.\nNevertheless, compared to ID-based models, pure text modeling degrades the\nefficacy of collaborative filtering, as well as feature scalability and\nefficiency during both training and inference. To address these issues, we\npropose \\textbf{C}ost-\\textbf{E}fficient \\textbf{L}anguage Model\n\\textbf{A}lignment (\\textbf{CELA}) for CTR prediction. CELA incorporates\ntextual features and language models while preserving the collaborative\nfiltering capabilities of ID-based models. This model-agnostic framework can be\nequipped with plug-and-play textual features, with item-level alignment\nenhancing the utilization of external information while maintaining training\nand inference efficiency. Through extensive offline experiments, CELA\ndemonstrates superior performance compared to state-of-the-art methods.\nFurthermore, an online A/B test conducted on an industrial App recommender\nsystem showcases its practical effectiveness, solidifying the potential for\nreal-world applications of CELA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Click-Through Rate (CTR) prediction holds a paramount position in recommender\nsystems. The prevailing ID-based paradigm underperforms in cold-start scenarios\ndue to the skewed distribution of feature frequency. Additionally, the\nutilization of a single modality fails to exploit the knowledge contained\nwithin textual features. Recent efforts have sought to mitigate these\nchallenges by integrating Pre-trained Language Models (PLMs). They design hard\nprompts to structure raw features into text for each interaction and then apply\nPLMs for text processing. With external knowledge and reasoning capabilities,\nPLMs extract valuable information even in cases of sparse interactions.\nNevertheless, compared to ID-based models, pure text modeling degrades the\nefficacy of collaborative filtering, as well as feature scalability and\nefficiency during both training and inference. To address these issues, we\npropose \\textbf{C}ost-\\textbf{E}fficient \\textbf{L}anguage Model\n\\textbf{A}lignment (\\textbf{CELA}) for CTR prediction. CELA incorporates\ntextual features and language models while preserving the collaborative\nfiltering capabilities of ID-based models. This model-agnostic framework can be\nequipped with plug-and-play textual features, with item-level alignment\nenhancing the utilization of external information while maintaining training\nand inference efficiency. Through extensive offline experiments, CELA\ndemonstrates superior performance compared to state-of-the-art methods.\nFurthermore, an online A/B test conducted on an industrial App recommender\nsystem showcases its practical effectiveness, solidifying the potential for\nreal-world applications of CELA."
                },
                "authors": [
                    {
                        "name": "Xingmei Wang"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xiaolong Chen"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Xu Huang"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "10 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10596v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10596v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09870v2",
                "updated": "2024-11-27T11:11:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    11,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-13T15:08:49Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    15,
                    8,
                    49,
                    6,
                    287,
                    0
                ],
                "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains"
                },
                "summary": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply ourChroKnowPrompt,\nan in-depth prompting to elicit chronological knowledge by traversing\nstep-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply ourChroKnowPrompt,\nan in-depth prompting to elicit chronological knowledge by traversing\nstep-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats."
                },
                "authors": [
                    {
                        "name": "Yein Park"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Donghyeon Lee"
                    },
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18220v1",
                "updated": "2024-11-27T10:57:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:57:06Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "title": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge"
                },
                "summary": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Mohsen Pourghasemian"
                    },
                    {
                        "name": "Haris Gacanin"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18216v1",
                "updated": "2024-11-27T10:48:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    48,
                    37,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:48:37Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    48,
                    37,
                    2,
                    332,
                    0
                ],
                "title": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. However, LLMs struggle to generate accurate code, resulting,\ne.g., in attack detectors that miss well-known attacks when used in practice.\nThis is most likely due to the LLM lacking knowledge about some existing\nattacks and to the generated code being not evaluated in real usage scenarios.\nWe propose a novel approach integrating Retrieval Augmented Generation (RAG)\nand Self-Ranking into the LLM pipeline. RAG enhances the robustness of the\noutput by incorporating external knowledge sources, while the Self-Ranking\ntechnique, inspired to the concept of Self-Consistency, generates multiple\nreasoning paths and creates ranks to select the most robust detector. Our\nextensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance compared to baselines, with an increase of up to 71%pt and 37%pt in\nthe F2-Score for XSS and SQLi detection, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. However, LLMs struggle to generate accurate code, resulting,\ne.g., in attack detectors that miss well-known attacks when used in practice.\nThis is most likely due to the LLM lacking knowledge about some existing\nattacks and to the generated code being not evaluated in real usage scenarios.\nWe propose a novel approach integrating Retrieval Augmented Generation (RAG)\nand Self-Ranking into the LLM pipeline. RAG enhances the robustness of the\noutput by incorporating external knowledge sources, while the Self-Ranking\ntechnique, inspired to the concept of Self-Consistency, generates multiple\nreasoning paths and creates ranks to select the most robust detector. Our\nextensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance compared to baselines, with an increase of up to 71%pt and 37%pt in\nthe F2-Score for XSS and SQLi detection, respectively."
                },
                "authors": [
                    {
                        "name": "Samuele Pasini"
                    },
                    {
                        "name": "Jinhan Kim"
                    },
                    {
                        "name": "Tommaso Aiello"
                    },
                    {
                        "name": "Rocio Cabrera Lozoya"
                    },
                    {
                        "name": "Antonino Sabetta"
                    },
                    {
                        "name": "Paolo Tonella"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Tonella"
                },
                "author": "Paolo Tonella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18211v1",
                "updated": "2024-11-27T10:45:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    45,
                    40,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:45:40Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    45,
                    40,
                    2,
                    332,
                    0
                ],
                "title": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding\n  with Superior Temporal Localization Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding\n  with Superior Temporal Localization Ability"
                },
                "summary": "Rapid development of large language models (LLMs) has significantly advanced\nmultimodal large language models (LMMs), particularly in vision-language tasks.\nHowever, existing video-language models often overlook precise temporal\nlocalization and struggle with videos of varying lengths. We introduce\nTimeMarker, a versatile Video-LLM designed for high-quality dialogue based on\nvideo content, emphasizing temporal localization. TimeMarker integrates\nTemporal Separator Tokens to enhance temporal awareness, accurately marking\nspecific moments within videos. It employs the AnyLength mechanism for dynamic\nframe sampling and adaptive token merging, enabling effective handling of both\nshort and long videos. Additionally, TimeMarker utilizes diverse datasets,\nincluding further transformed temporal-related video QA datasets, to bolster\nits temporal understanding capabilities. Image and interleaved data are also\nemployed to further enhance the model's semantic perception ability.\nEvaluations demonstrate that TimeMarker achieves state-of-the-art performance\nacross multiple benchmarks, excelling in both short and long video categories.\nOur project page is at \\url{https://github.com/TimeMarker-LLM/TimeMarker/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid development of large language models (LLMs) has significantly advanced\nmultimodal large language models (LMMs), particularly in vision-language tasks.\nHowever, existing video-language models often overlook precise temporal\nlocalization and struggle with videos of varying lengths. We introduce\nTimeMarker, a versatile Video-LLM designed for high-quality dialogue based on\nvideo content, emphasizing temporal localization. TimeMarker integrates\nTemporal Separator Tokens to enhance temporal awareness, accurately marking\nspecific moments within videos. It employs the AnyLength mechanism for dynamic\nframe sampling and adaptive token merging, enabling effective handling of both\nshort and long videos. Additionally, TimeMarker utilizes diverse datasets,\nincluding further transformed temporal-related video QA datasets, to bolster\nits temporal understanding capabilities. Image and interleaved data are also\nemployed to further enhance the model's semantic perception ability.\nEvaluations demonstrate that TimeMarker achieves state-of-the-art performance\nacross multiple benchmarks, excelling in both short and long video categories.\nOur project page is at \\url{https://github.com/TimeMarker-LLM/TimeMarker/}."
                },
                "authors": [
                    {
                        "name": "Shimin Chen"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01658v2",
                "updated": "2024-11-27T10:35:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    35,
                    19,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-03T07:01:37Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    1,
                    37,
                    1,
                    247,
                    0
                ],
                "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language\n  Models with Pinpoint Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language\n  Models with Pinpoint Tuning"
                },
                "summary": "Large Language Models (LLMs) tend to prioritize adherence to user prompts\nover providing veracious responses, leading to the sycophancy issue. When\nchallenged by users, LLMs tend to admit mistakes and provide inaccurate\nresponses even if they initially provided the correct answer. Recent works\npropose to employ supervised fine-tuning (SFT) to mitigate the sycophancy\nissue, while it typically leads to the degeneration of LLMs' general\ncapability. To address the challenge, we propose a novel supervised pinpoint\ntuning (SPT), where the region-of-interest modules are tuned for a given\nobjective. Specifically, SPT first reveals and verifies a small percentage\n(<5%) of the basic modules, which significantly affect a particular behavior of\nLLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified\nmodules while freezing the rest. To verify the effectiveness of the proposed\nSPT, we conduct comprehensive experiments, demonstrating that SPT significantly\nmitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT\nintroduces limited or even no side effects on the general capability of LLMs.\nOur results shed light on how to precisely, effectively, and efficiently\nexplain and improve the targeted ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to prioritize adherence to user prompts\nover providing veracious responses, leading to the sycophancy issue. When\nchallenged by users, LLMs tend to admit mistakes and provide inaccurate\nresponses even if they initially provided the correct answer. Recent works\npropose to employ supervised fine-tuning (SFT) to mitigate the sycophancy\nissue, while it typically leads to the degeneration of LLMs' general\ncapability. To address the challenge, we propose a novel supervised pinpoint\ntuning (SPT), where the region-of-interest modules are tuned for a given\nobjective. Specifically, SPT first reveals and verifies a small percentage\n(<5%) of the basic modules, which significantly affect a particular behavior of\nLLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified\nmodules while freezing the rest. To verify the effectiveness of the proposed\nSPT, we conduct comprehensive experiments, demonstrating that SPT significantly\nmitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT\nintroduces limited or even no side effects on the general capability of LLMs.\nOur results shed light on how to precisely, effectively, and efficiently\nexplain and improve the targeted ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Le Lu"
                    },
                    {
                        "name": "Xinmei Tian"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yonggang Zhang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18207v1",
                "updated": "2024-11-27T10:33:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    33,
                    51,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:33:51Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    33,
                    51,
                    2,
                    332,
                    0
                ],
                "title": "From Open Vocabulary to Open World: Teaching Vision Language Models to\n  Detect Novel Objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Open Vocabulary to Open World: Teaching Vision Language Models to\n  Detect Novel Objects"
                },
                "summary": "Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional object detection methods operate under the closed-set assumption,\nwhere models can only detect a fixed number of objects predefined in the\ntraining set. Recent works on open vocabulary object detection (OVD) enable the\ndetection of objects defined by an unbounded vocabulary, which reduces the cost\nof training models for specific tasks. However, OVD heavily relies on accurate\nprompts provided by an ''oracle'', which limits their use in critical\napplications such as driving scene perception. OVD models tend to misclassify\nnear-out-of-distribution (NOOD) objects that have similar semantics to known\nclasses, and ignore far-out-of-distribution (FOOD) objects. To address theses\nlimitations, we propose a framework that enables OVD models to operate in open\nworld settings, by identifying and incrementally learning novel objects. To\ndetect FOOD objects, we propose Open World Embedding Learning (OWEL) and\nintroduce the concept of Pseudo Unknown Embedding which infers the location of\nunknown classes in a continuous semantic space based on the information of\nknown classes. We also propose Multi-Scale Contrastive Anchor Learning (MSCAL),\nwhich enables the identification of misclassified unknown objects by promoting\nthe intra-class consistency of object embeddings at different scales. The\nproposed method achieves state-of-the-art performance in common open world\nobject detection and autonomous driving benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhao Li"
                    },
                    {
                        "name": "Zhengkang Xiang"
                    },
                    {
                        "name": "Joseph West"
                    },
                    {
                        "name": "Kourosh Khoshelham"
                    }
                ],
                "author_detail": {
                    "name": "Kourosh Khoshelham"
                },
                "author": "Kourosh Khoshelham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12361v2",
                "updated": "2024-11-27T10:14:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    54,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-16T08:24:09Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    24,
                    9,
                    2,
                    290,
                    0
                ],
                "title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance"
                },
                "summary": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Shenzhi Yang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Guirong Chen"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v1",
                "updated": "2024-11-27T10:14:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\nIn this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\nIn this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xing Hu"
                },
                "author": "Xing Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02415v2",
                "updated": "2024-11-27T10:09:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    9,
                    55,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-04T03:41:42Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    41,
                    42,
                    2,
                    248,
                    0
                ],
                "title": "Local Map Construction with SDMap: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Map Construction with SDMap: A Comprehensive Survey"
                },
                "summary": "Local map construction is a vital component of intelligent driving\nperception, offering necessary reference for vehicle positioning and planning.\nStandard Definition map (SDMap), known for its low cost, accessibility, and\nversatility, has significant potential as prior information for local map\nperception. This paper mainly reviews the local map construction methods with\nSDMap, including definitions, general processing flow, and datasets. Besides,\nthis paper analyzes multimodal data representation and fusion methods in\nSDMap-based local map construction. This paper also discusses key challenges\nand future directions, such as optimizing SDMap processing, enhancing spatial\nalignment with real-time data, and incorporating richer environmental\ninformation. At last, the review looks forward to future research focusing on\nenhancing road topology inference and multimodal data fusion to improve the\nrobustness and scalability of local map perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local map construction is a vital component of intelligent driving\nperception, offering necessary reference for vehicle positioning and planning.\nStandard Definition map (SDMap), known for its low cost, accessibility, and\nversatility, has significant potential as prior information for local map\nperception. This paper mainly reviews the local map construction methods with\nSDMap, including definitions, general processing flow, and datasets. Besides,\nthis paper analyzes multimodal data representation and fusion methods in\nSDMap-based local map construction. This paper also discusses key challenges\nand future directions, such as optimizing SDMap processing, enhancing spatial\nalignment with real-time data, and incorporating richer environmental\ninformation. At last, the review looks forward to future research focusing on\nenhancing road topology inference and multimodal data fusion to improve the\nrobustness and scalability of local map perception."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Pingfan Jia"
                    },
                    {
                        "name": "Jiaxing Chen"
                    },
                    {
                        "name": "Jiaxi Liu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Keqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiang Li"
                },
                "author": "Keqiang Li",
                "arxiv_comment": "21 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18175v1",
                "updated": "2024-11-27T09:50:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    50,
                    22,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T09:50:22Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    50,
                    22,
                    2,
                    332,
                    0
                ],
                "title": "Spectroscopic Signature of Local Alloy Fluctuations in InGaN/GaN\n  Multi-Quantum-Disk Light Emitting Diode Heterostructures and Its Impact on\n  the Optical Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectroscopic Signature of Local Alloy Fluctuations in InGaN/GaN\n  Multi-Quantum-Disk Light Emitting Diode Heterostructures and Its Impact on\n  the Optical Performance"
                },
                "summary": "Inhomogeneity-governed carrier localization has been investigated in three\nsets of InGaN/GaN multi-quantum-disk light-emitting diode (LED) structures\ngrown by plasma-assisted molecular beam epitaxy (PAMBE) under different process\nconditions. A temperature-dependent study of the luminescence peak positions\nreveals that samples prepared under certain process conditions exhibit a\nthermal distribution of carriers from the localized states that show the\ntypical S-shaped dependence in luminescence characteristics. The absence of an\nS-shaped nature in the other sample prepared with relatively higher In-flux\ninfers a superior homogeneity in alloy composition. Further investigation\nmanifested superior optical properties for the samples where the S-shape nature\nis found to be absent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inhomogeneity-governed carrier localization has been investigated in three\nsets of InGaN/GaN multi-quantum-disk light-emitting diode (LED) structures\ngrown by plasma-assisted molecular beam epitaxy (PAMBE) under different process\nconditions. A temperature-dependent study of the luminescence peak positions\nreveals that samples prepared under certain process conditions exhibit a\nthermal distribution of carriers from the localized states that show the\ntypical S-shaped dependence in luminescence characteristics. The absence of an\nS-shaped nature in the other sample prepared with relatively higher In-flux\ninfers a superior homogeneity in alloy composition. Further investigation\nmanifested superior optical properties for the samples where the S-shape nature\nis found to be absent."
                },
                "authors": [
                    {
                        "name": "Soumyadip Chatterjee"
                    },
                    {
                        "name": "Subhranshu Sekhar Sahu"
                    },
                    {
                        "name": "Kanchan Singh Rana"
                    },
                    {
                        "name": "Swagata Bhunia"
                    },
                    {
                        "name": "Dipankar Saha"
                    },
                    {
                        "name": "Apurba Laha"
                    }
                ],
                "author_detail": {
                    "name": "Apurba Laha"
                },
                "author": "Apurba Laha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14945v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14945v3",
                "updated": "2024-11-27T09:43:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    43,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2023-12-06T15:24:01Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    15,
                    24,
                    1,
                    2,
                    340,
                    0
                ],
                "title": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management"
                },
                "summary": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality."
                },
                "authors": [
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Yan-Fu Li"
                    },
                    {
                        "name": "Min Xie"
                    }
                ],
                "author_detail": {
                    "name": "Min Xie"
                },
                "author": "Min Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14945v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14945v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17538v2",
                "updated": "2024-11-27T09:43:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    43,
                    1,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T15:53:28Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    53,
                    28,
                    1,
                    331,
                    0
                ],
                "title": "Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code\n  Search"
                },
                "summary": "Low isotropy in an embedding space impairs performance on tasks involving\nsemantic inference. Our study investigates the impact of isotropy on semantic\ncode search performance and explores post-processing techniques to mitigate\nthis issue. We analyze various code language models, examine isotropy in their\nembedding spaces, and its influence on search effectiveness. We propose a\nmodified ZCA whitening technique to control isotropy levels in embeddings. Our\nresults demonstrate that Soft-ZCA whitening improves the performance of\npre-trained code language models and can complement contrastive fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low isotropy in an embedding space impairs performance on tasks involving\nsemantic inference. Our study investigates the impact of isotropy on semantic\ncode search performance and explores post-processing techniques to mitigate\nthis issue. We analyze various code language models, examine isotropy in their\nembedding spaces, and its influence on search effectiveness. We propose a\nmodified ZCA whitening technique to control isotropy levels in embeddings. Our\nresults demonstrate that Soft-ZCA whitening improves the performance of\npre-trained code language models and can complement contrastive fine-tuning."
                },
                "authors": [
                    {
                        "name": "Andor Diera"
                    },
                    {
                        "name": "Lukas Galke"
                    },
                    {
                        "name": "Ansgar Scherp"
                    }
                ],
                "author_detail": {
                    "name": "Ansgar Scherp"
                },
                "author": "Ansgar Scherp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18164v1",
                "updated": "2024-11-27T09:20:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    20,
                    26,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T09:20:26Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    20,
                    26,
                    2,
                    332,
                    0
                ],
                "title": "RPEE-HEADS: A Novel Benchmark for Pedestrian Head Detection in Crowd\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPEE-HEADS: A Novel Benchmark for Pedestrian Head Detection in Crowd\n  Videos"
                },
                "summary": "The automatic detection of pedestrian heads in crowded environments is\nessential for crowd analysis and management tasks, particularly in high-risk\nsettings such as railway platforms and event entrances. These environments,\ncharacterized by dense crowds and dynamic movements, are underrepresented in\npublic datasets, posing challenges for existing deep learning models. To\naddress this gap, we introduce the Railway Platforms and Event Entrances-Heads\n(RPEE-Heads) dataset, a novel, diverse, high-resolution, and accurately\nannotated resource. It includes 109,913 annotated pedestrian heads across 1,886\nimages from 66 video recordings, with an average of 56.2 heads per image.\nAnnotations include bounding boxes for visible head regions. In addition to\nintroducing the RPEE-Heads dataset, this paper evaluates eight state-of-the-art\nobject detection algorithms using the RPEE-Heads dataset and analyzes the\nimpact of head size on detection accuracy. The experimental results show that\nYou Only Look Once v9 and Real-Time Detection Transformer outperform the other\nalgorithms, achieving mean average precisions of 90.7% and 90.8%, with\ninference times of 11 and 14 milliseconds, respectively. Moreover, the findings\nunderscore the need for specialized datasets like RPEE-Heads for training and\nevaluating accurate models for head detection in railway platforms and event\nentrances. The dataset and pretrained models are available at\nhttps://doi.org/10.34735/ped.2024.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic detection of pedestrian heads in crowded environments is\nessential for crowd analysis and management tasks, particularly in high-risk\nsettings such as railway platforms and event entrances. These environments,\ncharacterized by dense crowds and dynamic movements, are underrepresented in\npublic datasets, posing challenges for existing deep learning models. To\naddress this gap, we introduce the Railway Platforms and Event Entrances-Heads\n(RPEE-Heads) dataset, a novel, diverse, high-resolution, and accurately\nannotated resource. It includes 109,913 annotated pedestrian heads across 1,886\nimages from 66 video recordings, with an average of 56.2 heads per image.\nAnnotations include bounding boxes for visible head regions. In addition to\nintroducing the RPEE-Heads dataset, this paper evaluates eight state-of-the-art\nobject detection algorithms using the RPEE-Heads dataset and analyzes the\nimpact of head size on detection accuracy. The experimental results show that\nYou Only Look Once v9 and Real-Time Detection Transformer outperform the other\nalgorithms, achieving mean average precisions of 90.7% and 90.8%, with\ninference times of 11 and 14 milliseconds, respectively. Moreover, the findings\nunderscore the need for specialized datasets like RPEE-Heads for training and\nevaluating accurate models for head detection in railway platforms and event\nentrances. The dataset and pretrained models are available at\nhttps://doi.org/10.34735/ped.2024.2."
                },
                "authors": [
                    {
                        "name": "Mohamad Abubaker"
                    },
                    {
                        "name": "Zubayda Alsadder"
                    },
                    {
                        "name": "Hamed Abdelhaq"
                    },
                    {
                        "name": "Maik Boltes"
                    },
                    {
                        "name": "Ahmed Alia"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Alia"
                },
                "author": "Ahmed Alia",
                "arxiv_comment": "17 pages, 8 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18162v1",
                "updated": "2024-11-27T09:18:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    18,
                    26,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T09:18:26Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    18,
                    26,
                    2,
                    332,
                    0
                ],
                "title": "SentiXRL: An advanced large language Model Framework for Multilingual\n  Fine-Grained Emotion Classification in Complex Text Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentiXRL: An advanced large language Model Framework for Multilingual\n  Fine-Grained Emotion Classification in Complex Text Environment"
                },
                "summary": "With strong expressive capabilities in Large Language Models(LLMs),\ngenerative models effectively capture sentiment structures and deep semantics,\nhowever, challenges remain in fine-grained sentiment classification across\nmulti-lingual and complex contexts. To address this, we propose the Sentiment\nCross-Lingual Recognition and Logic Framework (SentiXRL), which incorporates\ntwo modules,an emotion retrieval enhancement module to improve sentiment\nclassification accuracy in complex contexts through historical dialogue and\nlogical reasoning,and a self-circulating analysis negotiation mechanism\n(SANM)to facilitates autonomous decision-making within a single model for\nclassification tasks.We have validated SentiXRL's superiority on multiple\nstandard datasets, outperforming existing models on CPED and CH-SIMS,and\nachieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, we\nunified labels across several fine-grained sentiment annotation datasets and\nconducted category confusion experiments, revealing challenges and impacts of\nclass imbalance in standard datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With strong expressive capabilities in Large Language Models(LLMs),\ngenerative models effectively capture sentiment structures and deep semantics,\nhowever, challenges remain in fine-grained sentiment classification across\nmulti-lingual and complex contexts. To address this, we propose the Sentiment\nCross-Lingual Recognition and Logic Framework (SentiXRL), which incorporates\ntwo modules,an emotion retrieval enhancement module to improve sentiment\nclassification accuracy in complex contexts through historical dialogue and\nlogical reasoning,and a self-circulating analysis negotiation mechanism\n(SANM)to facilitates autonomous decision-making within a single model for\nclassification tasks.We have validated SentiXRL's superiority on multiple\nstandard datasets, outperforming existing models on CPED and CH-SIMS,and\nachieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, we\nunified labels across several fine-grained sentiment annotation datasets and\nconducted category confusion experiments, revealing challenges and impacts of\nclass imbalance in standard datasets."
                },
                "authors": [
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Zhilin Zhang"
                    },
                    {
                        "name": "Jianhao Zeng"
                    },
                    {
                        "name": "Kaidi Wang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyang Chen"
                },
                "author": "Zhiyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18157v1",
                "updated": "2024-11-27T09:04:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    4,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T09:04:47Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    4,
                    47,
                    2,
                    332,
                    0
                ],
                "title": "A survey on cutting-edge relation extraction techniques based on\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey on cutting-edge relation extraction techniques based on\n  language models"
                },
                "summary": "This comprehensive survey delves into the latest advancements in Relation\nExtraction (RE), a pivotal task in natural language processing essential for\napplications across biomedical, financial, and legal sectors. This study\nhighlights the evolution and current state of RE techniques by analyzing 137\npapers presented at the Association for Computational Linguistics (ACL)\nconferences over the past four years, focusing on models that leverage language\nmodels. Our findings underscore the dominance of BERT-based methods in\nachieving state-of-the-art results for RE while also noting the promising\ncapabilities of emerging large language models (LLMs) like T5, especially in\nfew-shot relation extraction scenarios where they excel in identifying\npreviously unseen relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive survey delves into the latest advancements in Relation\nExtraction (RE), a pivotal task in natural language processing essential for\napplications across biomedical, financial, and legal sectors. This study\nhighlights the evolution and current state of RE techniques by analyzing 137\npapers presented at the Association for Computational Linguistics (ACL)\nconferences over the past four years, focusing on models that leverage language\nmodels. Our findings underscore the dominance of BERT-based methods in\nachieving state-of-the-art results for RE while also noting the promising\ncapabilities of emerging large language models (LLMs) like T5, especially in\nfew-shot relation extraction scenarios where they excel in identifying\npreviously unseen relations."
                },
                "authors": [
                    {
                        "name": "Jose A. Diaz-Garcia"
                    },
                    {
                        "name": "Julio Amador Diaz Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Julio Amador Diaz Lopez"
                },
                "author": "Julio Amador Diaz Lopez",
                "arxiv_comment": "50 pages, under review in Artificial Intelligence Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18151v1",
                "updated": "2024-11-27T08:59:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    59,
                    34,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:59:34Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    59,
                    34,
                    2,
                    332,
                    0
                ],
                "title": "Howzat? Appealing to Expert Judgement for Evaluating Human and AI\n  Next-Step Hints for Novice Programmers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Howzat? Appealing to Expert Judgement for Evaluating Human and AI\n  Next-Step Hints for Novice Programmers"
                },
                "summary": "Motivation: Students learning to program often reach states where they are\nstuck and can make no forward progress. An automatically generated next-step\nhint can help them make forward progress and support their learning. It is\nimportant to know what makes a good hint or a bad hint, and how to generate\ngood hints automatically in novice programming tools, for example using Large\nLanguage Models (LLMs).\n  Method and participants: We recruited 44 Java educators from around the world\nto participate in an online study. We used a set of real student code states as\nhint-generation scenarios. Participants used a technique known as comparative\njudgement to rank a set of candidate next-step Java hints, which were generated\nby Large Language Models (LLMs) and by five human experienced educators.\nParticipants ranked the hints without being told how they were generated.\n  Findings: We found that LLMs had considerable variation in generating high\nquality next-step hints for programming novices, with GPT-4 outperforming other\nmodels tested. When used with a well-designed prompt, GPT-4 outperformed human\nexperts in generating pedagogically valuable hints. A multi-stage prompt was\nthe most effective LLM prompt. We found that the two most important factors of\na good hint were length (80--160 words being best), and reading level (US grade\n9 or below being best). Offering alternative approaches to solving the problem\nwas considered bad, and we found no effect of sentiment.\n  Conclusions: Automatic generation of these hints is immediately viable, given\nthat LLMs outperformed humans -- even when the students' task is unknown. The\nfact that only the best prompts achieve this outcome suggests that students on\ntheir own are unlikely to be able to produce the same benefit. The prompting\ntask, therefore, should be embedded in an expert-designed tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivation: Students learning to program often reach states where they are\nstuck and can make no forward progress. An automatically generated next-step\nhint can help them make forward progress and support their learning. It is\nimportant to know what makes a good hint or a bad hint, and how to generate\ngood hints automatically in novice programming tools, for example using Large\nLanguage Models (LLMs).\n  Method and participants: We recruited 44 Java educators from around the world\nto participate in an online study. We used a set of real student code states as\nhint-generation scenarios. Participants used a technique known as comparative\njudgement to rank a set of candidate next-step Java hints, which were generated\nby Large Language Models (LLMs) and by five human experienced educators.\nParticipants ranked the hints without being told how they were generated.\n  Findings: We found that LLMs had considerable variation in generating high\nquality next-step hints for programming novices, with GPT-4 outperforming other\nmodels tested. When used with a well-designed prompt, GPT-4 outperformed human\nexperts in generating pedagogically valuable hints. A multi-stage prompt was\nthe most effective LLM prompt. We found that the two most important factors of\na good hint were length (80--160 words being best), and reading level (US grade\n9 or below being best). Offering alternative approaches to solving the problem\nwas considered bad, and we found no effect of sentiment.\n  Conclusions: Automatic generation of these hints is immediately viable, given\nthat LLMs outperformed humans -- even when the students' task is unknown. The\nfact that only the best prompts achieve this outcome suggests that students on\ntheir own are unlikely to be able to produce the same benefit. The prompting\ntask, therefore, should be embedded in an expert-designed tool."
                },
                "authors": [
                    {
                        "name": "Neil C. C. Brown"
                    },
                    {
                        "name": "Pierre Weill-Tessier"
                    },
                    {
                        "name": "Juho Leinonen"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Michael KÃ¶lling"
                    }
                ],
                "author_detail": {
                    "name": "Michael KÃ¶lling"
                },
                "author": "Michael KÃ¶lling",
                "arxiv_comment": "36 pages, 14 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19226v2",
                "updated": "2024-11-27T08:50:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    50,
                    24,
                    2,
                    332,
                    0
                ],
                "published": "2024-06-27T14:51:07Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    51,
                    7,
                    3,
                    179,
                    0
                ],
                "title": "Simulating Classroom Education with LLM-Empowered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Classroom Education with LLM-Empowered Agents"
                },
                "summary": "Large language models (LLMs) have been applied across various intelligent\neducational tasks to assist teaching. While preliminary studies have focused on\ntask-specific, independent LLM-empowered agents, the potential of LLMs within a\nmulti-agent collaborative framework for classroom simulation with real user\nparticipation remains unexplored. In this work, we propose SimClass, a\nmulti-agent classroom simulation teaching framework. We recognize\nrepresentative class roles and introduce a novel class control mechanism for\nautomatic classroom teaching, and conduct user experiments in two real-world\ncourses. Using the Flanders Interactive Analysis System and Community of\nInquiry theoretical frameworks from educational analysis, we demonstrate that\nLLMs can simulate a dynamic learning environment for users with active\nteacher-student and student-student interactions. We also observe group\nbehaviors among agents in SimClass, where agents collaborate to create\nenlivening interactions in classrooms to improve user learning process. We hope\nthis work pioneers the application of LLM-empowered multi-agent systems in\nvirtual classroom teaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been applied across various intelligent\neducational tasks to assist teaching. While preliminary studies have focused on\ntask-specific, independent LLM-empowered agents, the potential of LLMs within a\nmulti-agent collaborative framework for classroom simulation with real user\nparticipation remains unexplored. In this work, we propose SimClass, a\nmulti-agent classroom simulation teaching framework. We recognize\nrepresentative class roles and introduce a novel class control mechanism for\nautomatic classroom teaching, and conduct user experiments in two real-world\ncourses. Using the Flanders Interactive Analysis System and Community of\nInquiry theoretical frameworks from educational analysis, we demonstrate that\nLLMs can simulate a dynamic learning environment for users with active\nteacher-student and student-student interactions. We also observe group\nbehaviors among agents in SimClass, where agents collaborate to create\nenlivening interactions in classrooms to improve user learning process. We hope\nthis work pioneers the application of LLM-empowered multi-agent systems in\nvirtual classroom teaching."
                },
                "authors": [
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Daniel Zhang-Li"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Jinchang Zhou"
                    },
                    {
                        "name": "Zhanxin Hao"
                    },
                    {
                        "name": "Jianxiao Jiang"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12959v2",
                "updated": "2024-11-27T08:49:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    49,
                    12,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-19T17:59:45Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    59,
                    45,
                    3,
                    263,
                    0
                ],
                "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines"
                },
                "summary": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io"
                },
                "authors": [
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Yanmin Wu"
                    },
                    {
                        "name": "Jiayi Lei"
                    },
                    {
                        "name": "Pengshuo Qiu"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Chunyuan Li"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://mmsearch.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06050v2",
                "updated": "2024-11-27T08:46:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    46,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-09T20:21:43Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    20,
                    21,
                    43,
                    0,
                    253,
                    0
                ],
                "title": "Use Model Averaging instead of Model Selection in Pulsar Timing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Use Model Averaging instead of Model Selection in Pulsar Timing"
                },
                "summary": "Over the past decade and a half, adoption of Bayesian inference in pulsar\ntiming analysis has led to increasingly sophisticated models. The recent\nannouncement of evidence for a stochastic background of gravitational waves by\nvarious pulsar timing array projects highlighted Bayesian inference as a\ncentral tool for parameter estimation and model selection. Despite its success,\nBayesian inference is occasionally misused in the pulsar timing community. A\ncommon workflow is that the data is analyzed in multiple steps: a first\nanalysis of single pulsars individually, and a subsequent analysis of the whole\narray of pulsars. A mistake that is then sometimes introduced stems from using\nthe posterior distribution to craft the prior for the analysis of the same data\nin a second step, a practice referred to in the statistics literature as\n``circular analysis.'' This is done to prune the model for computational\nefficiency. Multiple recent high-profile searches for gravitational waves by\npulsar timing array (PTA) projects have this workflow. This letter highlights\nthis error and suggests that Spike and Slab priors can be used to carry out\nmodel averaging instead of model selection in a single pass. Spike and Slab\npriors are proved to be equal to Log-Uniform priors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade and a half, adoption of Bayesian inference in pulsar\ntiming analysis has led to increasingly sophisticated models. The recent\nannouncement of evidence for a stochastic background of gravitational waves by\nvarious pulsar timing array projects highlighted Bayesian inference as a\ncentral tool for parameter estimation and model selection. Despite its success,\nBayesian inference is occasionally misused in the pulsar timing community. A\ncommon workflow is that the data is analyzed in multiple steps: a first\nanalysis of single pulsars individually, and a subsequent analysis of the whole\narray of pulsars. A mistake that is then sometimes introduced stems from using\nthe posterior distribution to craft the prior for the analysis of the same data\nin a second step, a practice referred to in the statistics literature as\n``circular analysis.'' This is done to prune the model for computational\nefficiency. Multiple recent high-profile searches for gravitational waves by\npulsar timing array (PTA) projects have this workflow. This letter highlights\nthis error and suggests that Spike and Slab priors can be used to carry out\nmodel averaging instead of model selection in a single pass. Spike and Slab\npriors are proved to be equal to Log-Uniform priors."
                },
                "authors": [
                    {
                        "name": "Rutger van Haasteren"
                    }
                ],
                "author_detail": {
                    "name": "Rutger van Haasteren"
                },
                "author": "Rutger van Haasteren",
                "arxiv_doi": "10.1093/mnrasl/slae108",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnrasl/slae108",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.06050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18143v1",
                "updated": "2024-11-27T08:44:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    44,
                    41,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:44:41Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    44,
                    41,
                    2,
                    332,
                    0
                ],
                "title": "Harnessing Large Language Models for Seed Generation in Greybox Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Seed Generation in Greybox Fuzzing"
                },
                "summary": "Greybox fuzzing has emerged as a preferred technique for discovering software\nbugs, striking a balance between efficiency and depth of exploration. While\nresearch has focused on improving fuzzing techniques, the importance of\nhigh-quality initial seeds remains critical yet often overlooked. Existing\nmethods for seed generation are limited, especially for programs with\nnon-standard or custom input formats. Large Language Models (LLMs) has\nrevolutionized numerous domains, showcasing unprecedented capabilities in\nunderstanding and generating complex patterns across various fields of\nknowledge. This paper introduces SeedMind, a novel system that leverages LLMs\nto boost greybox fuzzing through intelligent seed generation. Unlike previous\napproaches, SeedMind employs LLMs to create test case generators rather than\ndirectly producing test cases. Our approach implements an iterative,\nfeedback-driven process that guides the LLM to progressively refine test case\ngeneration, aiming for increased code coverage depth and breadth. In developing\nSeedMind, we addressed key challenges including input format limitations,\ncontext window constraints, and ensuring consistent, progress-aware behavior.\nIntensive evaluations with real-world applications show that SeedMind\neffectively harnesses LLMs to generate high-quality test cases and facilitate\nfuzzing in bug finding, presenting utility comparable to human-created seeds\nand significantly outperforming the existing LLM-based solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox fuzzing has emerged as a preferred technique for discovering software\nbugs, striking a balance between efficiency and depth of exploration. While\nresearch has focused on improving fuzzing techniques, the importance of\nhigh-quality initial seeds remains critical yet often overlooked. Existing\nmethods for seed generation are limited, especially for programs with\nnon-standard or custom input formats. Large Language Models (LLMs) has\nrevolutionized numerous domains, showcasing unprecedented capabilities in\nunderstanding and generating complex patterns across various fields of\nknowledge. This paper introduces SeedMind, a novel system that leverages LLMs\nto boost greybox fuzzing through intelligent seed generation. Unlike previous\napproaches, SeedMind employs LLMs to create test case generators rather than\ndirectly producing test cases. Our approach implements an iterative,\nfeedback-driven process that guides the LLM to progressively refine test case\ngeneration, aiming for increased code coverage depth and breadth. In developing\nSeedMind, we addressed key challenges including input format limitations,\ncontext window constraints, and ensuring consistent, progress-aware behavior.\nIntensive evaluations with real-world applications show that SeedMind\neffectively harnesses LLMs to generate high-quality test cases and facilitate\nfuzzing in bug finding, presenting utility comparable to human-created seeds\nand significantly outperforming the existing LLM-based solutions."
                },
                "authors": [
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yunhang Zhang"
                    },
                    {
                        "name": "Xinyu Xing"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18138v1",
                "updated": "2024-11-27T08:38:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    38,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:38:57Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    38,
                    57,
                    2,
                    332,
                    0
                ],
                "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and\n  Generation"
                },
                "summary": "Full-duplex multimodal large language models (LLMs) provide a unified\nframework for addressing diverse speech understanding and generation tasks,\nenabling more natural and seamless human-machine conversations. Unlike\ntraditional modularised conversational AI systems, which separate speech\nrecognition, understanding, and text-to-speech generation into distinct\ncomponents, multimodal LLMs operate as single end-to-end models. This\nstreamlined design eliminates error propagation across components and fully\nleverages the rich non-verbal information embedded in input speech signals. We\nintroduce SALMONN-omni, a codec-free, full-duplex speech understanding and\ngeneration model capable of simultaneously listening to its own generated\nspeech and background sounds while speaking. To support this capability, we\npropose a novel duplex spoken dialogue framework incorporating a ``thinking''\nmechanism that facilitates asynchronous text and speech generation relying on\nembeddings instead of codecs (quantized speech and audio tokens). Experimental\nresults demonstrate SALMONN-omni's versatility across a broad range of\nstreaming speech tasks, including speech recognition, speech enhancement, and\nspoken question answering. Additionally, SALMONN-omni excels at managing\nturn-taking, barge-in, and echo cancellation scenarios, establishing its\npotential as a robust prototype for full-duplex conversational AI systems. To\nthe best of our knowledge, SALMONN-omni is the first codec-free model of its\nkind. A full technical report along with model checkpoints will be released\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex multimodal large language models (LLMs) provide a unified\nframework for addressing diverse speech understanding and generation tasks,\nenabling more natural and seamless human-machine conversations. Unlike\ntraditional modularised conversational AI systems, which separate speech\nrecognition, understanding, and text-to-speech generation into distinct\ncomponents, multimodal LLMs operate as single end-to-end models. This\nstreamlined design eliminates error propagation across components and fully\nleverages the rich non-verbal information embedded in input speech signals. We\nintroduce SALMONN-omni, a codec-free, full-duplex speech understanding and\ngeneration model capable of simultaneously listening to its own generated\nspeech and background sounds while speaking. To support this capability, we\npropose a novel duplex spoken dialogue framework incorporating a ``thinking''\nmechanism that facilitates asynchronous text and speech generation relying on\nembeddings instead of codecs (quantized speech and audio tokens). Experimental\nresults demonstrate SALMONN-omni's versatility across a broad range of\nstreaming speech tasks, including speech recognition, speech enhancement, and\nspoken question answering. Additionally, SALMONN-omni excels at managing\nturn-taking, barge-in, and echo cancellation scenarios, establishing its\npotential as a robust prototype for full-duplex conversational AI systems. To\nthe best of our knowledge, SALMONN-omni is the first codec-free model of its\nkind. A full technical report along with model checkpoints will be released\nsoon."
                },
                "authors": [
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18897v2",
                "updated": "2024-11-27T08:32:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    32,
                    27,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-29T17:34:24Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    17,
                    34,
                    24,
                    0,
                    120,
                    0
                ],
                "title": "Neural network prediction of model parameters for strong lensing samples\n  from Hyper Suprime-Cam Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network prediction of model parameters for strong lensing samples\n  from Hyper Suprime-Cam Survey"
                },
                "summary": "Strong lensing of background galaxies provides important information about\nthe matter distribution around lens galaxies. Traditional modelling of such\nstrong lenses is both time and resource intensive. Fast and automated analysis\nmethods are the need of the hour given large upcoming surveys. In this work, we\nbuild and train a simple convolutional neural network with an aim of rapidly\npredicting model parameters of gravitational lenses. We focus on the inference\nof the Einstein radius, and ellipticity components of the mass distribution. We\ntrain our network on a variety of simulated data with increasing degree of\nrealism and compare its performance on simulated test data in a quantitative\nmanner. We also model 182 gravitational lenses from Subaru HSC survey using\n{\\sc YattaLens} pipeline to infer their model parameters, which allow a\nbenchmark to compare the predictions of the network. Given all considerations,\nwe conclude that the network trained on simulated samples with lensed sources\ninjected in empty HSC cutouts is the most robust, reproducing Einstein radii\nwith an accuracy of about $10-20$ percent, a bias less than 5 percent, and an\noutlier fraction of the order of 10 percent. We argue in favour of the\nsubtraction of the lens light before modelling the lens mass distribution. Our\ncomparisons of the inferred parameters of 10 HSC lenses previously modelled in\nthe literature, demonstrate agreement on the Einstein radius. However, the\nellipticity components from the network as well as the individual modelling\nmethods, seem to have systematic uncertainties beyond the quoted errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong lensing of background galaxies provides important information about\nthe matter distribution around lens galaxies. Traditional modelling of such\nstrong lenses is both time and resource intensive. Fast and automated analysis\nmethods are the need of the hour given large upcoming surveys. In this work, we\nbuild and train a simple convolutional neural network with an aim of rapidly\npredicting model parameters of gravitational lenses. We focus on the inference\nof the Einstein radius, and ellipticity components of the mass distribution. We\ntrain our network on a variety of simulated data with increasing degree of\nrealism and compare its performance on simulated test data in a quantitative\nmanner. We also model 182 gravitational lenses from Subaru HSC survey using\n{\\sc YattaLens} pipeline to infer their model parameters, which allow a\nbenchmark to compare the predictions of the network. Given all considerations,\nwe conclude that the network trained on simulated samples with lensed sources\ninjected in empty HSC cutouts is the most robust, reproducing Einstein radii\nwith an accuracy of about $10-20$ percent, a bias less than 5 percent, and an\noutlier fraction of the order of 10 percent. We argue in favour of the\nsubtraction of the lens light before modelling the lens mass distribution. Our\ncomparisons of the inferred parameters of 10 HSC lenses previously modelled in\nthe literature, demonstrate agreement on the Einstein radius. However, the\nellipticity components from the network as well as the individual modelling\nmethods, seem to have systematic uncertainties beyond the quoted errors."
                },
                "authors": [
                    {
                        "name": "Priyanka Gawade"
                    },
                    {
                        "name": "Anupreeta More"
                    },
                    {
                        "name": "Surhud More"
                    },
                    {
                        "name": "Akisato Kimura"
                    },
                    {
                        "name": "Alessandro Sonnenfeld"
                    },
                    {
                        "name": "Masamune Oguri"
                    },
                    {
                        "name": "Naoki Yoshida"
                    }
                ],
                "author_detail": {
                    "name": "Naoki Yoshida"
                },
                "author": "Naoki Yoshida",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v3",
                "updated": "2024-11-27T08:17:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    17,
                    9,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18126v1",
                "updated": "2024-11-27T08:16:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    16,
                    41,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:16:41Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    16,
                    41,
                    2,
                    332,
                    0
                ],
                "title": "Curriculum Demonstration Selection for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum Demonstration Selection for In-Context Learning"
                },
                "summary": "Large Language Models (LLMs) have shown strong in-context learning (ICL)\nabilities with a few demonstrations. However, one critical challenge is how to\nselect demonstrations to elicit the full potential of LLMs. In this paper, we\npropose Curriculum Demonstration Selection (CDS), a novel demonstration\nselection method for ICL. Instead of merely using similarity, CDS additionally\npartitions samples by their complexity measurements. Following curriculum\nlearning, CDS then selects demonstrations from easy to difficult. Thus the\nselected demonstrations cover a wide range of difficulty levels, enabling LLMs\nto learn from varied complexities within the training set. Experiments\ndemonstrate that our CDS consistently outperforms baseline methods, achieving\nnotable improvements across nine LLMs on three benchmarks. Moreover, CDS proves\nespecially effective in enhancing LLM performance in solving challenging\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong in-context learning (ICL)\nabilities with a few demonstrations. However, one critical challenge is how to\nselect demonstrations to elicit the full potential of LLMs. In this paper, we\npropose Curriculum Demonstration Selection (CDS), a novel demonstration\nselection method for ICL. Instead of merely using similarity, CDS additionally\npartitions samples by their complexity measurements. Following curriculum\nlearning, CDS then selects demonstrations from easy to difficult. Thus the\nselected demonstrations cover a wide range of difficulty levels, enabling LLMs\nto learn from varied complexities within the training set. Experiments\ndemonstrate that our CDS consistently outperforms baseline methods, achieving\nnotable improvements across nine LLMs on three benchmarks. Moreover, CDS proves\nespecially effective in enhancing LLM performance in solving challenging\nproblems."
                },
                "authors": [
                    {
                        "name": "Duc Anh Vu"
                    },
                    {
                        "name": "Nguyen Tran Cong Duy"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Hoang Minh Nhat"
                    },
                    {
                        "name": "Du Mingzhe"
                    },
                    {
                        "name": "Nguyen Thanh Thong"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "Accepted at the 40th ACM/SIGAPP Symposium On Applied Computing (SAC\n  2025), Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08681v2",
                "updated": "2024-11-27T08:09:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    9,
                    56,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-08T07:36:26Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    7,
                    36,
                    26,
                    0,
                    99,
                    0
                ],
                "title": "EFSA: Towards Event-Level Financial Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFSA: Towards Event-Level Financial Sentiment Analysis"
                },
                "summary": "In this paper, we extend financial sentiment analysis~(FSA) to event-level\nsince events usually serve as the subject of the sentiment in financial text.\nThough extracting events from the financial text may be conducive to accurate\nsentiment predictions, it has specialized challenges due to the lengthy and\ndiscontinuity of events in a financial text. To this end, we reconceptualize\nthe event extraction as a classification task by designing a categorization\ncomprising coarse-grained and fine-grained event categories. Under this\nsetting, we formulate the \\textbf{E}vent-Level \\textbf{F}inancial\n\\textbf{S}entiment \\textbf{A}nalysis~(\\textbf{EFSA} for short) task that\noutputs quintuples consisting of (company, industry, coarse-grained event,\nfine-grained event, sentiment) from financial text. A large-scale Chinese\ndataset containing $12,160$ news articles and $13,725$ quintuples is publicized\nas a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based\napproach is devised for this task. Systematically investigations are conducted\non our dataset, and the empirical results demonstrate the benchmarking scores\nof existing methods and our proposed method can reach the current\nstate-of-the-art. Our dataset and framework implementation are available at\nhttps://anonymous.4open.science/r/EFSA-645E",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we extend financial sentiment analysis~(FSA) to event-level\nsince events usually serve as the subject of the sentiment in financial text.\nThough extracting events from the financial text may be conducive to accurate\nsentiment predictions, it has specialized challenges due to the lengthy and\ndiscontinuity of events in a financial text. To this end, we reconceptualize\nthe event extraction as a classification task by designing a categorization\ncomprising coarse-grained and fine-grained event categories. Under this\nsetting, we formulate the \\textbf{E}vent-Level \\textbf{F}inancial\n\\textbf{S}entiment \\textbf{A}nalysis~(\\textbf{EFSA} for short) task that\noutputs quintuples consisting of (company, industry, coarse-grained event,\nfine-grained event, sentiment) from financial text. A large-scale Chinese\ndataset containing $12,160$ news articles and $13,725$ quintuples is publicized\nas a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based\napproach is devised for this task. Systematically investigations are conducted\non our dataset, and the empirical results demonstrate the benchmarking scores\nof existing methods and our proposed method can reach the current\nstate-of-the-art. Our dataset and framework implementation are available at\nhttps://anonymous.4open.science/r/EFSA-645E"
                },
                "authors": [
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Guoxin Yu"
                    },
                    {
                        "name": "Dapeng Zhang"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Qing He"
                    },
                    {
                        "name": "Xiang Ao"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ao"
                },
                "author": "Xiang Ao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18111v1",
                "updated": "2024-11-27T07:45:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    45,
                    25,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T07:45:25Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    45,
                    25,
                    2,
                    332,
                    0
                ],
                "title": "When Large Vision-Language Models Meet Person Re-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Vision-Language Models Meet Person Re-Identification"
                },
                "summary": "Large Vision-Language Models (LVLMs) that incorporate visual models and Large\nLanguage Models (LLMs) have achieved impressive results across various\ncross-modal understanding and reasoning tasks. In recent years, person\nre-identification (ReID) has also started to explore cross-modal semantics to\nimprove the accuracy of identity recognition. However, effectively utilizing\nLVLMs for ReID remains an open challenge. While LVLMs operate under a\ngenerative paradigm by predicting the next output word, ReID requires the\nextraction of discriminative identity features to match pedestrians across\ncameras. In this paper, we propose LVLM-ReID, a novel framework that harnesses\nthe strengths of LVLMs to promote ReID. Specifically, we employ instructions to\nguide the LVLM in generating one pedestrian semantic token that encapsulates\nkey appearance semantics from the person image. This token is further refined\nthrough our Semantic-Guided Interaction (SGI) module, establishing a reciprocal\ninteraction between the semantic token and visual tokens. Ultimately, the\nreinforced semantic token serves as the pedestrian identity representation. Our\nframework integrates the semantic understanding and generation capabilities of\nLVLMs into end-to-end ReID training, allowing LVLMs to capture rich semantic\ncues from pedestrian images during both training and inference. Our method\nachieves competitive results on multiple benchmarks without additional\nimage-text annotations, demonstrating the potential of LVLM-generated semantics\nto advance person ReID and offering a promising direction for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) that incorporate visual models and Large\nLanguage Models (LLMs) have achieved impressive results across various\ncross-modal understanding and reasoning tasks. In recent years, person\nre-identification (ReID) has also started to explore cross-modal semantics to\nimprove the accuracy of identity recognition. However, effectively utilizing\nLVLMs for ReID remains an open challenge. While LVLMs operate under a\ngenerative paradigm by predicting the next output word, ReID requires the\nextraction of discriminative identity features to match pedestrians across\ncameras. In this paper, we propose LVLM-ReID, a novel framework that harnesses\nthe strengths of LVLMs to promote ReID. Specifically, we employ instructions to\nguide the LVLM in generating one pedestrian semantic token that encapsulates\nkey appearance semantics from the person image. This token is further refined\nthrough our Semantic-Guided Interaction (SGI) module, establishing a reciprocal\ninteraction between the semantic token and visual tokens. Ultimately, the\nreinforced semantic token serves as the pedestrian identity representation. Our\nframework integrates the semantic understanding and generation capabilities of\nLVLMs into end-to-end ReID training, allowing LVLMs to capture rich semantic\ncues from pedestrian images during both training and inference. Our method\nachieves competitive results on multiple benchmarks without additional\nimage-text annotations, demonstrating the potential of LVLM-generated semantics\nto advance person ReID and offering a promising direction for future research."
                },
                "authors": [
                    {
                        "name": "Qizao Wang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xiangyang Xue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Xue"
                },
                "author": "Xiangyang Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05732v2",
                "updated": "2024-11-27T07:45:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    45,
                    18,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-08T06:50:02Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    6,
                    50,
                    2,
                    1,
                    282,
                    0
                ],
                "title": "Statistical inference for highly correlated stationary point processes\n  and noisy bivariate Neyman-Scott processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for highly correlated stationary point processes\n  and noisy bivariate Neyman-Scott processes"
                },
                "summary": "Motivated by estimating the lead-lag relationships in high-frequency\nfinancial data, we propose noisy bivariate Neyman-Scott point processes with\ngamma kernels (NBNSP-G). NBNSP-G tolerates noises that are not necessarily\nPoissonian and has an intuitive interpretation. Our experiments suggest that\nNBNSP-G can explain the correlation of orders of two stocks well. A\ncomposite-type quasi-likelihood is employed to estimate the parameters of the\nmodel. However, when one tries to prove consistency and asymptotic normality,\nNBNSP-G breaks the boundedness assumption on the moment density functions\ncommonly assumed in the literature. Therefore, under more relaxed conditions,\nwe show consistency and asymptotic normality for bivariate point process\nmodels, which include NBNSP-G. Our numerical simulations also show that the\nestimator is indeed likely to converge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by estimating the lead-lag relationships in high-frequency\nfinancial data, we propose noisy bivariate Neyman-Scott point processes with\ngamma kernels (NBNSP-G). NBNSP-G tolerates noises that are not necessarily\nPoissonian and has an intuitive interpretation. Our experiments suggest that\nNBNSP-G can explain the correlation of orders of two stocks well. A\ncomposite-type quasi-likelihood is employed to estimate the parameters of the\nmodel. However, when one tries to prove consistency and asymptotic normality,\nNBNSP-G breaks the boundedness assumption on the moment density functions\ncommonly assumed in the literature. Therefore, under more relaxed conditions,\nwe show consistency and asymptotic normality for bivariate point process\nmodels, which include NBNSP-G. Our numerical simulations also show that the\nestimator is indeed likely to converge."
                },
                "authors": [
                    {
                        "name": "Takaaki Shiotani"
                    },
                    {
                        "name": "Nakahiro Yoshida"
                    }
                ],
                "author_detail": {
                    "name": "Nakahiro Yoshida"
                },
                "author": "Nakahiro Yoshida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12762v2",
                "updated": "2024-11-27T07:41:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    41,
                    35,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-16T13:07:13Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    7,
                    13,
                    5,
                    321,
                    0
                ],
                "title": "Playing Language Game with LLMs Leads to Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing Language Game with LLMs Leads to Jailbreaking"
                },
                "summary": "The advent of large language models (LLMs) has spurred the development of\nnumerous jailbreak techniques aimed at circumventing their security defenses\nagainst malicious attacks. An effective jailbreak approach is to identify a\ndomain where safety generalization fails, a phenomenon known as mismatched\ngeneralization. In this paper, we introduce two novel jailbreak methods based\non mismatched generalization: natural language games and custom language games,\nboth of which effectively bypass the safety mechanisms of LLMs, with various\nkinds and different variants, making them hard to defend and leading to high\nattack rates. Natural language games involve the use of synthetic linguistic\nconstructs and the actions intertwined with these constructs, such as the Ubbi\nDubbi language. Building on this phenomenon, we propose the custom language\ngames method: by engaging with LLMs using a variety of custom rules, we\nsuccessfully execute jailbreak attacks across multiple LLM platforms. Extensive\nexperiments demonstrate the effectiveness of our methods, achieving success\nrates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet.\nFurthermore, to investigate the generalizability of safety alignments, we\nfine-tuned Llama-3.1-70B with the custom language games to achieve safety\nalignment within our datasets and found that when interacting through other\nlanguage games, the fine-tuned models still failed to identify harmful content.\nThis finding indicates that the safety alignment knowledge embedded in LLMs\nfails to generalize across different linguistic formats, thus opening new\navenues for future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has spurred the development of\nnumerous jailbreak techniques aimed at circumventing their security defenses\nagainst malicious attacks. An effective jailbreak approach is to identify a\ndomain where safety generalization fails, a phenomenon known as mismatched\ngeneralization. In this paper, we introduce two novel jailbreak methods based\non mismatched generalization: natural language games and custom language games,\nboth of which effectively bypass the safety mechanisms of LLMs, with various\nkinds and different variants, making them hard to defend and leading to high\nattack rates. Natural language games involve the use of synthetic linguistic\nconstructs and the actions intertwined with these constructs, such as the Ubbi\nDubbi language. Building on this phenomenon, we propose the custom language\ngames method: by engaging with LLMs using a variety of custom rules, we\nsuccessfully execute jailbreak attacks across multiple LLM platforms. Extensive\nexperiments demonstrate the effectiveness of our methods, achieving success\nrates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet.\nFurthermore, to investigate the generalizability of safety alignments, we\nfine-tuned Llama-3.1-70B with the custom language games to achieve safety\nalignment within our datasets and found that when interacting through other\nlanguage games, the fine-tuned models still failed to identify harmful content.\nThis finding indicates that the safety alignment knowledge embedded in LLMs\nfails to generalize across different linguistic formats, thus opening new\navenues for future research in this area."
                },
                "authors": [
                    {
                        "name": "Yu Peng"
                    },
                    {
                        "name": "Zewen Long"
                    },
                    {
                        "name": "Fangming Dong"
                    },
                    {
                        "name": "Congyi Li"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17697v2",
                "updated": "2024-11-27T07:39:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    39,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T18:59:22Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    59,
                    22,
                    1,
                    331,
                    0
                ],
                "title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableAnimator: High-Quality Identity-Preserving Human Image Animation"
                },
                "summary": "Current diffusion models for human image animation struggle to ensure\nidentity (ID) consistency. This paper presents StableAnimator, the first\nend-to-end ID-preserving video diffusion framework, which synthesizes\nhigh-quality videos without any post-processing, conditioned on a reference\nimage and a sequence of poses. Building upon a video diffusion model,\nStableAnimator contains carefully designed modules for both training and\ninference striving for identity consistency. In particular, StableAnimator\nbegins by computing image and face embeddings with off-the-shelf extractors,\nrespectively and face embeddings are further refined by interacting with image\nembeddings using a global content-aware Face Encoder. Then, StableAnimator\nintroduces a novel distribution-aware ID Adapter that prevents interference\ncaused by temporal layers while preserving ID via alignment. During inference,\nwe propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to\nfurther enhance the face quality. We demonstrate that solving the HJB equation\ncan be integrated into the diffusion denoising process, and the resulting\nsolution constrains the denoising path and thus benefits ID preservation.\nExperiments on multiple benchmarks show the effectiveness of StableAnimator\nboth qualitatively and quantitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current diffusion models for human image animation struggle to ensure\nidentity (ID) consistency. This paper presents StableAnimator, the first\nend-to-end ID-preserving video diffusion framework, which synthesizes\nhigh-quality videos without any post-processing, conditioned on a reference\nimage and a sequence of poses. Building upon a video diffusion model,\nStableAnimator contains carefully designed modules for both training and\ninference striving for identity consistency. In particular, StableAnimator\nbegins by computing image and face embeddings with off-the-shelf extractors,\nrespectively and face embeddings are further refined by interacting with image\nembeddings using a global content-aware Face Encoder. Then, StableAnimator\nintroduces a novel distribution-aware ID Adapter that prevents interference\ncaused by temporal layers while preserving ID via alignment. During inference,\nwe propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to\nfurther enhance the face quality. We demonstrate that solving the HJB equation\ncan be integrated into the diffusion denoising process, and the resulting\nsolution constrains the denoising path and thus benefits ID preservation.\nExperiments on multiple benchmarks show the effectiveness of StableAnimator\nboth qualitatively and quantitatively."
                },
                "authors": [
                    {
                        "name": "Shuyuan Tu"
                    },
                    {
                        "name": "Zhen Xing"
                    },
                    {
                        "name": "Xintong Han"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Zuxuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zuxuan Wu"
                },
                "author": "Zuxuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18107v1",
                "updated": "2024-11-27T07:36:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    36,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T07:36:52Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    36,
                    52,
                    2,
                    332,
                    0
                ],
                "title": "Fusion of Discrete Representations and Self-Augmented Representations\n  for Multilingual Automatic Speech Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion of Discrete Representations and Self-Augmented Representations\n  for Multilingual Automatic Speech Recognition"
                },
                "summary": "Self-supervised learning (SSL) models have shown exceptional capabilities\nacross various speech-processing tasks. Continuous SSL representations are\neffective but suffer from high computational and storage demands. On the other\nhand, discrete SSL representations, although with degraded performance, reduce\ntransmission and storage costs, and improve input sequence efficiency through\nde-duplication and subword-modeling. To boost the performance of discrete\nrepresentations for ASR, we introduce a novel fusion mechanism that integrates\ntwo discrete representations. The fusion mechanism preserves all the benefits\nof discrete representation while enhancing the model's performance by\nintegrating complementary information. Additionally, we explore\n\"self-augmented'' discrete representations, which apply transformations to a\nsingle continuous SSL representation, eliminating the fusion mechanism's\ndependency on multiple SSL models and further decreasing its inference costs.\nExperimental results on benchmarks, including LibriSpeech and ML-SUPERB,\nindicate up to 19% and 24% relative character error rate improvement compared\nwith the non-fusion baseline, validating the effectiveness of our proposed\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning (SSL) models have shown exceptional capabilities\nacross various speech-processing tasks. Continuous SSL representations are\neffective but suffer from high computational and storage demands. On the other\nhand, discrete SSL representations, although with degraded performance, reduce\ntransmission and storage costs, and improve input sequence efficiency through\nde-duplication and subword-modeling. To boost the performance of discrete\nrepresentations for ASR, we introduce a novel fusion mechanism that integrates\ntwo discrete representations. The fusion mechanism preserves all the benefits\nof discrete representation while enhancing the model's performance by\nintegrating complementary information. Additionally, we explore\n\"self-augmented'' discrete representations, which apply transformations to a\nsingle continuous SSL representation, eliminating the fusion mechanism's\ndependency on multiple SSL models and further decreasing its inference costs.\nExperimental results on benchmarks, including LibriSpeech and ML-SUPERB,\nindicate up to 19% and 24% relative character error rate improvement compared\nwith the non-fusion baseline, validating the effectiveness of our proposed\nmethods."
                },
                "authors": [
                    {
                        "name": "Shih-heng Wang"
                    },
                    {
                        "name": "Jiatong Shi"
                    },
                    {
                        "name": "Chien-yu Huang"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "SLT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18104v1",
                "updated": "2024-11-27T07:32:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    32,
                    56,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T07:32:56Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    32,
                    56,
                    2,
                    332,
                    0
                ],
                "title": "Training and Evaluating Language Models with Template-based Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and Evaluating Language Models with Template-based Data\n  Generation"
                },
                "summary": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhang"
                },
                "author": "Yifan Zhang",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06208v2",
                "updated": "2024-11-27T07:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    29,
                    59,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-09T15:12:43Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    15,
                    12,
                    43,
                    5,
                    314,
                    0
                ],
                "title": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization"
                },
                "summary": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively."
                },
                "authors": [
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Cheng Fu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15105v3",
                "updated": "2024-11-27T07:26:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    26,
                    34,
                    2,
                    332,
                    0
                ],
                "published": "2024-02-23T05:30:32Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    5,
                    30,
                    32,
                    4,
                    54,
                    0
                ],
                "title": "A First Look at GPT Apps: Landscape and Vulnerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look at GPT Apps: Landscape and Vulnerability"
                },
                "summary": "Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the\nlaunch of dedicated LLM app stores. Nevertheless, given its debut, there is a\nlack of sufficient understanding of this new ecosystem. To fill this gap, this\npaper presents a first comprehensive longitudinal (5-month) study of the\nevolution, landscape, and vulnerability of the emerging LLM app ecosystem,\nfocusing on two GPT app stores: \\textit{GPTStore.AI} and the official\n\\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a\nTriLevel configuration extraction strategy to efficiently gather metadata (\\ie\nnames, creators, descriptions, \\etc) and user feedback for all GPT apps across\nthese two stores, as well as configurations (\\ie system prompts, knowledge\nfiles, and APIs) for the top 10,000 popular apps. Our extensive analysis\nreveals: (1) the user enthusiasm for GPT apps consistently rises, whereas\ncreator interest plateaus within three months of GPTs' launch; (2) nearly 90\\%\nsystem prompts can be easily accessed due to widespread failure to secure GPT\napp configurations, leading to considerable plagiarism and duplication among\napps. Our findings highlight the necessity of enhancing the LLM app ecosystem\nby the app stores, creators, and users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the\nlaunch of dedicated LLM app stores. Nevertheless, given its debut, there is a\nlack of sufficient understanding of this new ecosystem. To fill this gap, this\npaper presents a first comprehensive longitudinal (5-month) study of the\nevolution, landscape, and vulnerability of the emerging LLM app ecosystem,\nfocusing on two GPT app stores: \\textit{GPTStore.AI} and the official\n\\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a\nTriLevel configuration extraction strategy to efficiently gather metadata (\\ie\nnames, creators, descriptions, \\etc) and user feedback for all GPT apps across\nthese two stores, as well as configurations (\\ie system prompts, knowledge\nfiles, and APIs) for the top 10,000 popular apps. Our extensive analysis\nreveals: (1) the user enthusiasm for GPT apps consistently rises, whereas\ncreator interest plateaus within three months of GPTs' launch; (2) nearly 90\\%\nsystem prompts can be easily accessed due to widespread failure to secure GPT\napp configurations, leading to considerable plagiarism and duplication among\napps. Our findings highlight the necessity of enhancing the LLM app ecosystem\nby the app stores, creators, and users."
                },
                "authors": [
                    {
                        "name": "Zejun Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Anlan Zhang"
                    },
                    {
                        "name": "Mengwei Xu"
                    },
                    {
                        "name": "Feng Qian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Qian"
                },
                "author": "Feng Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06387v3",
                "updated": "2024-11-27T07:25:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    25,
                    2,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-10T08:11:05Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    11,
                    5,
                    6,
                    315,
                    0
                ],
                "title": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation"
                },
                "summary": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches."
                },
                "authors": [
                    {
                        "name": "Jaehyeok Lee"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "JinYeong Bak"
                    }
                ],
                "author_detail": {
                    "name": "JinYeong Bak"
                },
                "author": "JinYeong Bak",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.15100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15100v2",
                "updated": "2024-11-27T18:59:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-22T18:01:37Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    1,
                    37,
                    4,
                    327,
                    0
                ],
                "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XGrammar: Flexible and Efficient Structured Generation Engine for Large\n  Language Models"
                },
                "summary": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of LLM Agents are becoming increasingly complex and diverse,\nleading to a high demand for structured outputs that can be parsed into code,\nstructured function calls, and embodied agent commands. These developments\nbring significant demands for structured generation in LLM inference.\nContext-free grammar is a flexible approach to enable structured generation via\nconstrained decoding. However, executing context-free grammar requires going\nthrough several stack states over all tokens in vocabulary during runtime,\nbringing non-negligible overhead for structured generation. In this paper, we\npropose XGrammar, a flexible and efficient structure generation engine for\nlarge language models. XGrammar accelerates context-free grammar execution by\ndividing the vocabulary into context-independent tokens that can be prechecked\nand context-dependent tokens that need to be interpreted during runtime. We\nfurther build transformations to expand the grammar context and reduce the\nnumber of context-independent tokens. Additionally, we build an efficient\npersistent stack to accelerate the context-dependent token checks. Finally, we\nco-design the grammar engine with LLM inference engine to overlap grammar\ncomputation with GPU executions. Evaluation results show that XGrammar can\nachieve up to 100x speedup over existing solutions. Combined with an LLM\ninference engine, it can generate near-zero overhead structure generation in\nend-to-end low-LLM serving."
                },
                "authors": [
                    {
                        "name": "Yixin Dong"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yaxing Cai"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18583v1",
                "updated": "2024-11-27T18:27:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    27,
                    7,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:27:07Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    27,
                    7,
                    2,
                    332,
                    0
                ],
                "title": "Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Literature Review Using NLP Techniques and LLM-Based\n  Retrieval-Augmented Generation"
                },
                "summary": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research presents and compares multiple approaches to automate the\ngeneration of literature reviews using several Natural Language Processing\n(NLP) techniques and retrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles provides a huge\nchallenge for manual literature review. It has resulted in an increased demand\nfor automation. Developing a system capable of automatically generating the\nliterature reviews from only the PDF files as input is the primary objective of\nthis research work. The effectiveness of several Natural Language Processing\n(NLP) strategies, such as the frequency-based method (spaCy), the transformer\nmodel (Simple T5), and retrieval-augmented generation (RAG) with Large Language\nModel (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR\ndataset is chosen for this research experiment and three distinct techniques\nare utilized to implement three different systems for auto-generating the\nliterature reviews. The ROUGE scores are used for the evaluation of all three\nsystems. Based on the evaluation, the Large Language Model GPT-3.5-turbo\nachieved the highest ROUGE-1 score, 0.364. The transformer model comes in\nsecond place and spaCy is at the last position. Finally, a graphical user\ninterface is created for the best system based on the large language model."
                },
                "authors": [
                    {
                        "name": "Nurshat Fateh Ali"
                    },
                    {
                        "name": "Md. Mahdi Mohtasim"
                    },
                    {
                        "name": "Shakil Mosharrof"
                    },
                    {
                        "name": "T. Gopi Krishna"
                    }
                ],
                "author_detail": {
                    "name": "T. Gopi Krishna"
                },
                "author": "T. Gopi Krishna",
                "arxiv_comment": "Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature\n  Review, Natural Language Processing, Retrieval-augmented generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18571v1",
                "updated": "2024-11-27T18:14:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "Challenges in Adapting Multilingual LLMs to Low-Resource Languages using\n  LoRA PEFT Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges in Adapting Multilingual LLMs to Low-Resource Languages using\n  LoRA PEFT Tuning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable multilingual\ncapabilities, yet challenges persist in adapting these models for low-resource\nlanguages. In this study, we investigate the effects of Low-Rank Adaptation\n(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models for\nMarathi, a language with limited resources. Using a translated Alpaca dataset\nwith 52,000 instruction-response pairs, our findings reveal that while\nevaluation metrics often show a performance decline post-fine-tuning, manual\nassessments frequently suggest that the fine-tuned models outperform their\noriginal counterparts. The observations indicate improvements in target\nlanguage generation capabilities but a reduction in reasoning abilities\nfollowing language adaptation. These results underscore the need for improved\nevaluation methodologies and the creation of high-quality native datasets to\naccurately assess language-specific model performance in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable multilingual\ncapabilities, yet challenges persist in adapting these models for low-resource\nlanguages. In this study, we investigate the effects of Low-Rank Adaptation\n(LoRA) Parameter-Efficient Fine-Tuning (PEFT) on multilingual Gemma models for\nMarathi, a language with limited resources. Using a translated Alpaca dataset\nwith 52,000 instruction-response pairs, our findings reveal that while\nevaluation metrics often show a performance decline post-fine-tuning, manual\nassessments frequently suggest that the fine-tuned models outperform their\noriginal counterparts. The observations indicate improvements in target\nlanguage generation capabilities but a reduction in reasoning abilities\nfollowing language adaptation. These results underscore the need for improved\nevaluation methodologies and the creation of high-quality native datasets to\naccurately assess language-specific model performance in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Omkar Khade"
                    },
                    {
                        "name": "Shruti Jagdale"
                    },
                    {
                        "name": "Abhishek Phaltankar"
                    },
                    {
                        "name": "Gauri Takalikar"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18564v1",
                "updated": "2024-11-27T18:04:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    4,
                    5,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:04:05Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    4,
                    5,
                    2,
                    332,
                    0
                ],
                "title": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pipeline of Neural-Symbolic Integration to Enhance Spatial Reasoning\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks. However, LLMs often struggle with spatial reasoning which is one\nessential part of reasoning and inference and requires understanding complex\nrelationships between objects in space. This paper proposes a novel\nneural-symbolic framework that enhances LLMs' spatial reasoning abilities. We\nevaluate our approach on two benchmark datasets: StepGame and SparQA,\nimplementing three distinct strategies: (1) ASP (Answer Set Programming)-based\nsymbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logical\nrules. Our experiments demonstrate significant improvements over the baseline\nprompting methods, with accuracy increases of 40-50% on StepGame} dataset and\n3-13% on the more complex SparQA dataset. The \"LLM + ASP\" pipeline achieves\nparticularly strong results on the tasks of Finding Relations (FR) and Finding\nBlock (FB) questions, though performance varies across different question\ntypes. The impressive results suggest that while neural-symbolic approaches\noffer promising directions for enhancing spatial reasoning in LLMs, their\neffectiveness depends heavily on the specific task characteristics and\nimplementation strategies. We propose an integrated, simple yet effective set\nof strategies using a neural-symbolic pipeline to boost spatial reasoning\nabilities in LLMs. This pipeline and its strategies demonstrate strong and\nbroader applicability to other reasoning domains in LLMs, such as temporal\nreasoning, deductive inference etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks. However, LLMs often struggle with spatial reasoning which is one\nessential part of reasoning and inference and requires understanding complex\nrelationships between objects in space. This paper proposes a novel\nneural-symbolic framework that enhances LLMs' spatial reasoning abilities. We\nevaluate our approach on two benchmark datasets: StepGame and SparQA,\nimplementing three distinct strategies: (1) ASP (Answer Set Programming)-based\nsymbolic reasoning, (2) LLM + ASP pipeline using DSPy, and (3) Fact + Logical\nrules. Our experiments demonstrate significant improvements over the baseline\nprompting methods, with accuracy increases of 40-50% on StepGame} dataset and\n3-13% on the more complex SparQA dataset. The \"LLM + ASP\" pipeline achieves\nparticularly strong results on the tasks of Finding Relations (FR) and Finding\nBlock (FB) questions, though performance varies across different question\ntypes. The impressive results suggest that while neural-symbolic approaches\noffer promising directions for enhancing spatial reasoning in LLMs, their\neffectiveness depends heavily on the specific task characteristics and\nimplementation strategies. We propose an integrated, simple yet effective set\nof strategies using a neural-symbolic pipeline to boost spatial reasoning\nabilities in LLMs. This pipeline and its strategies demonstrate strong and\nbroader applicability to other reasoning domains in LLMs, such as temporal\nreasoning, deductive inference etc."
                },
                "authors": [
                    {
                        "name": "Rong Wang"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Jonas Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Kuhn"
                },
                "author": "Jonas Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13362v2",
                "updated": "2024-11-27T17:07:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    7,
                    41,
                    2,
                    332,
                    0
                ],
                "published": "2024-05-22T05:43:15Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    5,
                    43,
                    15,
                    2,
                    143,
                    0
                ],
                "title": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems"
                },
                "summary": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited."
                },
                "authors": [
                    {
                        "name": "Danial Ebrat"
                    },
                    {
                        "name": "Eli Paradalis"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14985v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14985v4",
                "updated": "2024-11-27T17:05:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    5,
                    16,
                    2,
                    332,
                    0
                ],
                "published": "2024-07-20T21:24:40Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    21,
                    24,
                    40,
                    5,
                    202,
                    0
                ],
                "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data"
                },
                "summary": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth. We also show the practical implications\nof our analysis through a novel prompt optimization algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth. We also show the practical implications\nof our analysis through a novel prompt optimization algorithm."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Antonis Antoniades"
                    },
                    {
                        "name": "Yanai Elazar"
                    },
                    {
                        "name": "Alfonso Amayuelas"
                    },
                    {
                        "name": "Alon Albalak"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "William Yang Wang"
                    }
                ],
                "author_detail": {
                    "name": "William Yang Wang"
                },
                "author": "William Yang Wang",
                "arxiv_comment": "updated 10-page version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14985v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14985v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07806v2",
                "updated": "2024-11-27T16:55:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    55,
                    59,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-12T14:01:08Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    14,
                    1,
                    8,
                    1,
                    317,
                    0
                ],
                "title": "Federated Low-Rank Adaptation with Differential Privacy over Wireless\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Low-Rank Adaptation with Differential Privacy over Wireless\n  Networks"
                },
                "summary": "Fine-tuning large pre-trained foundation models (FMs) on distributed edge\ndevices presents considerable computational and privacy challenges. Federated\nfine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative\nmodel training without the need to share raw data. To lessen the computational\nburden on resource-limited devices, combining low-rank adaptation (LoRA) with\nfederated learning enables parameter-efficient fine-tuning. Additionally, the\nsplit FedFT architecture partitions an FM between edge devices and a central\nserver, reducing the necessity for complete model deployment on individual\ndevices. However, the risk of privacy eavesdropping attacks in FedFT remains a\nconcern, particularly in sensitive areas such as healthcare and finance. In\nthis paper, we propose a split FedFT framework with differential privacy (DP)\nover wireless networks, where the inherent wireless channel noise in the uplink\ntransmission is utilized to achieve DP guarantees without adding an extra\nartificial noise. We shall investigate the impact of the wireless noise on\nconvergence performance of the proposed framework. We will also show that by\nupdating only one of the low-rank matrices in the split FedFT with DP, the\nproposed method can mitigate the noise amplification effect. Simulation results\nwill demonstrate that the proposed framework achieves higher accuracy under\nstrict privacy budgets compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large pre-trained foundation models (FMs) on distributed edge\ndevices presents considerable computational and privacy challenges. Federated\nfine-tuning (FedFT) mitigates some privacy issues by facilitating collaborative\nmodel training without the need to share raw data. To lessen the computational\nburden on resource-limited devices, combining low-rank adaptation (LoRA) with\nfederated learning enables parameter-efficient fine-tuning. Additionally, the\nsplit FedFT architecture partitions an FM between edge devices and a central\nserver, reducing the necessity for complete model deployment on individual\ndevices. However, the risk of privacy eavesdropping attacks in FedFT remains a\nconcern, particularly in sensitive areas such as healthcare and finance. In\nthis paper, we propose a split FedFT framework with differential privacy (DP)\nover wireless networks, where the inherent wireless channel noise in the uplink\ntransmission is utilized to achieve DP guarantees without adding an extra\nartificial noise. We shall investigate the impact of the wireless noise on\nconvergence performance of the proposed framework. We will also show that by\nupdating only one of the low-rank matrices in the split FedFT with DP, the\nproposed method can mitigate the noise amplification effect. Simulation results\nwill demonstrate that the proposed framework achieves higher accuracy under\nstrict privacy budgets compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Tianqu Kang"
                    },
                    {
                        "name": "Zixin Wang"
                    },
                    {
                        "name": "Hengtao He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shenghui Song"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18506v1",
                "updated": "2024-11-27T16:48:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T16:48:24Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    48,
                    24,
                    2,
                    332,
                    0
                ],
                "title": "LLM-ABBA: Understand time series via symbolic approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-ABBA: Understand time series via symbolic approximation"
                },
                "summary": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of large language models (LLMs) for time series has been\ndemonstrated in previous work. Utilizing a symbolic time series representation,\none can efficiently bridge the gap between LLMs and time series. However, the\nremaining challenge is to exploit the semantic information hidden in time\nseries by using symbols or existing tokens of LLMs, while aligning the\nembedding space of LLMs according to the hidden information of time series. The\nsymbolic time series approximation (STSA) method called adaptive Brownian\nbridge-based symbolic aggregation (ABBA) shows outstanding efficacy in\npreserving salient time series features by modeling time series patterns in\nterms of amplitude and period while using existing tokens of LLMs.\n  In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA\ninto large language models for various downstream time series tasks. By\nsymbolizing time series, LLM-ABBA compares favorably to the recent\nstate-of-the-art (SOTA) in UCR and three medical time series classification\ntasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to\n\\kc{avoid obvious drifting} during prediction tasks by significantly mitigating\nthe effects of cumulative error arising from misused symbols during the\ntransition from symbols to numerical values. In time series regression tasks,\nLLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER)\nbenchmarks. LLM-ABBA also shows competitive prediction capability compared to\nrecent SOTA time series prediction results. We believe this framework can also\nseamlessly extend to other time series tasks."
                },
                "authors": [
                    {
                        "name": "Erin Carson"
                    },
                    {
                        "name": "Xinye Chen"
                    },
                    {
                        "name": "Cheng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Kang"
                },
                "author": "Cheng Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15637v2",
                "updated": "2024-11-27T16:34:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    34,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-24T00:51:45Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    0,
                    51,
                    45,
                    1,
                    268,
                    0
                ],
                "title": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for\n  Digital Agents at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synatra: Turning Indirect Knowledge into Direct Demonstrations for\n  Digital Agents at Scale"
                },
                "summary": "LLMs can now act as autonomous agents that interact with digital environments\nand complete specific objectives (e.g., arranging an online meeting). However,\naccuracy is still far from satisfactory, partly due to a lack of large-scale,\ndirect demonstrations for digital tasks. Obtaining supervised data from humans\nis costly, and automatic data collection through exploration or reinforcement\nlearning relies on complex environmental and content setup, resulting in\ndatasets that lack comprehensive coverage of various scenarios. On the other\nhand, there is abundant knowledge that may indirectly assist task completion,\nsuch as online tutorials that were created for human consumption. In this work,\nwe present Synatra, an approach that effectively transforms this indirect\nknowledge into direct supervision at scale. We define different types of\nindirect knowledge, and carefully study the available sources to obtain it,\nmethods to encode the structure of direct demonstrations, and finally methods\nto transform indirect knowledge into direct demonstrations. We use 100k such\nsynthetically-created demonstrations to finetune a 7B CodeLlama, and\ndemonstrate that the resulting agent surpasses all comparably sized models on\nthree web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as\nsurpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic\ndemonstrations prove to be only 3% the cost of human demonstrations (at $0.031\neach), we show that the synthetic demonstrations can be more effective than an\nidentical number of human demonstrations collected from limited domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can now act as autonomous agents that interact with digital environments\nand complete specific objectives (e.g., arranging an online meeting). However,\naccuracy is still far from satisfactory, partly due to a lack of large-scale,\ndirect demonstrations for digital tasks. Obtaining supervised data from humans\nis costly, and automatic data collection through exploration or reinforcement\nlearning relies on complex environmental and content setup, resulting in\ndatasets that lack comprehensive coverage of various scenarios. On the other\nhand, there is abundant knowledge that may indirectly assist task completion,\nsuch as online tutorials that were created for human consumption. In this work,\nwe present Synatra, an approach that effectively transforms this indirect\nknowledge into direct supervision at scale. We define different types of\nindirect knowledge, and carefully study the available sources to obtain it,\nmethods to encode the structure of direct demonstrations, and finally methods\nto transform indirect knowledge into direct demonstrations. We use 100k such\nsynthetically-created demonstrations to finetune a 7B CodeLlama, and\ndemonstrate that the resulting agent surpasses all comparably sized models on\nthree web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as\nsurpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic\ndemonstrations prove to be only 3% the cost of human demonstrations (at $0.031\neach), we show that the synthetic demonstrations can be more effective than an\nidentical number of human demonstrations collected from limited domains."
                },
                "authors": [
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Aman Madaan"
                    },
                    {
                        "name": "Jiarui Liu"
                    },
                    {
                        "name": "Robert Lo"
                    },
                    {
                        "name": "Abishek Sridhar"
                    },
                    {
                        "name": "Sudipta Sengupta"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Shuyan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Shuyan Zhou"
                },
                "author": "Shuyan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18490v1",
                "updated": "2024-11-27T16:33:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    33,
                    26,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T16:33:26Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    33,
                    26,
                    2,
                    332,
                    0
                ],
                "title": "Seismic swarms unveil the mechanisms driving shallow slow slip dynamics\n  in the CopiapÃ³ ridge, Northern Chile",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seismic swarms unveil the mechanisms driving shallow slow slip dynamics\n  in the CopiapÃ³ ridge, Northern Chile"
                },
                "summary": "Like earthquakes, slow slip events release elastic energy stored on faults.\nYet, the mechanisms behind slow slip instability and its relationship with\nseismicity are debated. Here, we use a seismo-geodetic deployment to document a\nshallow slow slip event (SSE) in 2023 on the Chile subduction. We observe\ndense, migrating seismic swarms accompanying the SSE, comprised of interface\nactivity and upper plate splay faulting. Our observations suggest that the slow\nslip initiation is driven by structurally-confined fluid overpressure in the\nfluid-rich surroundings of a subducted seamount. This is consistent with an\nobserved acceleration and expansion of the SSE after a $M_L=5.3$ earthquake\nlikely triggering an increase in interface permeability. Historical earthquake\nswarms highlight the persistent structural control and recurrent nature of such\nslow slip events. Our observations provide insight into the interactions\nbetween slow slip and seismicity, suggesting they are controlled by creep on a\nfluid-infiltrated fault with fractally distributed asperities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Like earthquakes, slow slip events release elastic energy stored on faults.\nYet, the mechanisms behind slow slip instability and its relationship with\nseismicity are debated. Here, we use a seismo-geodetic deployment to document a\nshallow slow slip event (SSE) in 2023 on the Chile subduction. We observe\ndense, migrating seismic swarms accompanying the SSE, comprised of interface\nactivity and upper plate splay faulting. Our observations suggest that the slow\nslip initiation is driven by structurally-confined fluid overpressure in the\nfluid-rich surroundings of a subducted seamount. This is consistent with an\nobserved acceleration and expansion of the SSE after a $M_L=5.3$ earthquake\nlikely triggering an increase in interface permeability. Historical earthquake\nswarms highlight the persistent structural control and recurrent nature of such\nslow slip events. Our observations provide insight into the interactions\nbetween slow slip and seismicity, suggesting they are controlled by creep on a\nfluid-infiltrated fault with fractally distributed asperities."
                },
                "authors": [
                    {
                        "name": "Jannes MÃ¼nchmeyer"
                    },
                    {
                        "name": "Diego Molina"
                    },
                    {
                        "name": "Mathilde Radiguet"
                    },
                    {
                        "name": "David Marsan"
                    },
                    {
                        "name": "Juan-Carlos Baez"
                    },
                    {
                        "name": "Francisco Ortega-Culaciati"
                    },
                    {
                        "name": "Andres Tassara"
                    },
                    {
                        "name": "Marcos Moreno"
                    },
                    {
                        "name": "Anne Socquet"
                    }
                ],
                "author_detail": {
                    "name": "Anne Socquet"
                },
                "author": "Anne Socquet",
                "arxiv_comment": "Manuscript including 10 page supplement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18478v1",
                "updated": "2024-11-27T16:19:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    19,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T16:19:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    16,
                    19,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context\n  Learning via MCTS"
                },
                "summary": "In-context Learning (ICL) enables large language models (LLMs) to tackle\ndownstream tasks through sophisticated prompting and high-quality\ndemonstrations. However, this traditional ICL paradigm shows limitations when\nfacing complex mathematical reasoning tasks, primarily due to its heavy\ndependence on example quality and the necessity for human intervention in\nchallenging scenarios. To address these limitations, this paper presents\nHiAR-ICL, a \\textbf{Hi}gh-level \\textbf{A}utomated \\textbf{R}easoning paradigm\nin \\textbf{ICL} that shifts focus from specific examples to abstract thinking\npatterns, extending the conventional concept of context in ICL. HiAR-ICL\nintroduces five atomic reasoning actions as fundamental components for\nconstructing chain-structured patterns. Using Monte Carlo Tree Search, we\nexplore reasoning paths and construct thought cards to guide subsequent\ninference. We then develop a cognitive complexity framework that dynamically\nmatches problems with appropriate thought cards. Experimental results\ndemonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy\n(79.6$\\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o\n(76.6$\\%$) and Claude 3.5 (71.1$\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context Learning (ICL) enables large language models (LLMs) to tackle\ndownstream tasks through sophisticated prompting and high-quality\ndemonstrations. However, this traditional ICL paradigm shows limitations when\nfacing complex mathematical reasoning tasks, primarily due to its heavy\ndependence on example quality and the necessity for human intervention in\nchallenging scenarios. To address these limitations, this paper presents\nHiAR-ICL, a \\textbf{Hi}gh-level \\textbf{A}utomated \\textbf{R}easoning paradigm\nin \\textbf{ICL} that shifts focus from specific examples to abstract thinking\npatterns, extending the conventional concept of context in ICL. HiAR-ICL\nintroduces five atomic reasoning actions as fundamental components for\nconstructing chain-structured patterns. Using Monte Carlo Tree Search, we\nexplore reasoning paths and construct thought cards to guide subsequent\ninference. We then develop a cognitive complexity framework that dynamically\nmatches problems with appropriate thought cards. Experimental results\ndemonstrate HiAR-ICL's effectiveness, achieving state-of-the-art accuracy\n(79.6$\\%$) on the MATH benchmark with Qwen2.5-7B-Instruct, surpassing GPT-4o\n(76.6$\\%$) and Claude 3.5 (71.1$\\%$)."
                },
                "authors": [
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Zengqi Wen"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11082v2",
                "updated": "2024-11-27T15:49:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    49,
                    49,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-17T14:15:54Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    15,
                    54,
                    6,
                    322,
                    0
                ],
                "title": "STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage\n  Synergistic Training of Deep Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STOP: Spatiotemporal Orthogonal Propagation for Weight-Threshold-Leakage\n  Synergistic Training of Deep Spiking Neural Networks"
                },
                "summary": "The prevailing of artificial intelligence-of-things calls for higher\nenergy-efficient edge computing paradigms, such as neuromorphic agents\nleveraging brain-inspired spiking neural network (SNN) models based on\nspatiotemporally sparse binary spikes. However, the lack of efficient and\nhigh-accuracy deep SNN learning algorithms prevents them from practical edge\ndeployments at a strictly bounded cost. In this paper, we propose the\nspatiotemporal orthogonal propagation (STOP) algorithm to tackle this\nchallenge. Our algorithm enables fully synergistic learning of synaptic weights\nas well as firing thresholds and leakage factors in spiking neurons to improve\nSNN accuracy, in a unified temporally-forward trace-based framework to mitigate\nthe huge memory requirement for storing neural states across all time-steps in\nthe forward pass. Characteristically, the spatially-backward neuronal errors\nand temporally-forward traces propagate orthogonally to and independently of\neach other, substantially reducing computational complexity. Our STOP algorithm\nobtained high recognition accuracies of 94.84%, 74.92%, 98.26% and 77.10% on\nthe CIFAR-10, CIFAR-100, DVS-Gesture and DVS-CIFAR10 datasets with adequate\ndeep convolutional SNNs of VGG-11 or ResNet-18 structures. Compared with other\ndeep SNN training algorithms, our method is more plausible for edge intelligent\nscenarios where resources are limited but high-accuracy in-situ learning is\ndesired.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing of artificial intelligence-of-things calls for higher\nenergy-efficient edge computing paradigms, such as neuromorphic agents\nleveraging brain-inspired spiking neural network (SNN) models based on\nspatiotemporally sparse binary spikes. However, the lack of efficient and\nhigh-accuracy deep SNN learning algorithms prevents them from practical edge\ndeployments at a strictly bounded cost. In this paper, we propose the\nspatiotemporal orthogonal propagation (STOP) algorithm to tackle this\nchallenge. Our algorithm enables fully synergistic learning of synaptic weights\nas well as firing thresholds and leakage factors in spiking neurons to improve\nSNN accuracy, in a unified temporally-forward trace-based framework to mitigate\nthe huge memory requirement for storing neural states across all time-steps in\nthe forward pass. Characteristically, the spatially-backward neuronal errors\nand temporally-forward traces propagate orthogonally to and independently of\neach other, substantially reducing computational complexity. Our STOP algorithm\nobtained high recognition accuracies of 94.84%, 74.92%, 98.26% and 77.10% on\nthe CIFAR-10, CIFAR-100, DVS-Gesture and DVS-CIFAR10 datasets with adequate\ndeep convolutional SNNs of VGG-11 or ResNet-18 structures. Compared with other\ndeep SNN training algorithms, our method is more plausible for edge intelligent\nscenarios where resources are limited but high-accuracy in-situ learning is\ndesired."
                },
                "authors": [
                    {
                        "name": "Haoran Gao"
                    },
                    {
                        "name": "Xichuan Zhou"
                    },
                    {
                        "name": "Yingcheng Lin"
                    },
                    {
                        "name": "Min Tian"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Cong Shi"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shi"
                },
                "author": "Cong Shi",
                "arxiv_comment": "13 pages (exclude supplementary), 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18444v1",
                "updated": "2024-11-27T15:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM\n  Evaluator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is my Meeting Summary Good? Estimating Quality with a Multi-LLM\n  Evaluator"
                },
                "summary": "The quality of meeting summaries generated by natural language generation\n(NLG) systems is hard to measure automatically. Established metrics such as\nROUGE and BERTScore have a relatively low correlation with human judgments and\nfail to capture nuanced errors. Recent studies suggest using large language\nmodels (LLMs), which have the benefit of better context understanding and\nadaption of error definitions without training on a large number of human\npreference judgments. However, current LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving human evaluation the gold standard\ndespite being costly and hard to compare across studies. In this work, we\npresent MESA, an LLM-based framework employing a three-step assessment of\nindividual error types, multi-agent discussion for decision refinement, and\nfeedback-based self-training to refine error definition understanding and\nalignment with human judgment. We show that MESA's components enable thorough\nerror detection, consistent rating, and adaptability to custom error\nguidelines. Using GPT-4o as its backbone, MESA achieves mid to high\nPoint-Biserial correlation with human judgment in error detection and mid\nSpearman and Kendall correlation in reflecting error impact on summary quality,\non average 0.25 higher than previous methods. The framework's flexibility in\nadapting to custom error guidelines makes it suitable for various tasks with\nlimited human-labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of meeting summaries generated by natural language generation\n(NLG) systems is hard to measure automatically. Established metrics such as\nROUGE and BERTScore have a relatively low correlation with human judgments and\nfail to capture nuanced errors. Recent studies suggest using large language\nmodels (LLMs), which have the benefit of better context understanding and\nadaption of error definitions without training on a large number of human\npreference judgments. However, current LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving human evaluation the gold standard\ndespite being costly and hard to compare across studies. In this work, we\npresent MESA, an LLM-based framework employing a three-step assessment of\nindividual error types, multi-agent discussion for decision refinement, and\nfeedback-based self-training to refine error definition understanding and\nalignment with human judgment. We show that MESA's components enable thorough\nerror detection, consistent rating, and adaptability to custom error\nguidelines. Using GPT-4o as its backbone, MESA achieves mid to high\nPoint-Biserial correlation with human judgment in error detection and mid\nSpearman and Kendall correlation in reflecting error impact on summary quality,\non average 0.25 higher than previous methods. The framework's flexibility in\nadapting to custom error guidelines makes it suitable for various tasks with\nlimited human-labeled data."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05784v2",
                "updated": "2024-11-27T15:19:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    19,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-07-08T09:47:35Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    9,
                    47,
                    35,
                    0,
                    190,
                    0
                ],
                "title": "Hecaton: Training Large Language Models with Scalable Chiplet Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hecaton: Training Large Language Models with Scalable Chiplet Systems"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success in various\nfields, but their training and finetuning require massive computation and\nmemory, necessitating parallelism which introduces heavy communication\noverheads. Driven by advances in packaging, the chiplet architecture emerges as\na potential solution, as it can integrate computing power, as well as utilize\non-package links with better signal integrity, higher bandwidth, and lower\nenergy consumption. However, most existing chiplet-related works focus on DNN\ninference. Directly porting them to LLM training introduces significantly large\nquantities of DRAM access and network-on-package (NoP) overheads which make\nstate-of-the-art chiplet designs fail, highlighting a research gap.\n  This work proposes Hecaton, a scalable and cost-effective chiplet system for\nLLM training. We first provide a chiplet architecture with tailored scheduling\nthat can largely reduce DRAM accesses. We further design an efficient\ndistributed training method that reduces NoP communication complexity and\nrelieves constraints on SRAM capacity and layout. Theoretical analysis shows\nthat the entire system achieves weak scaling: as the workload and hardware\nresources grow proportionally, the computation-to-communication ratio remains\nnearly constant. Experiments with various workloads and hardware configurations\nverify the property, and Hecaton achieves $5.29\\times$ performance improvement\nand $3.46\\times$ energy reduction on Llama3.1-405B, compared to the tensor\nparallelism in Megatron. To the best of our knowledge, we propose the first\nchiplet architecture specifically used for LLM training or finetuning, with\nguaranteed performance regardless of the problem scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success in various\nfields, but their training and finetuning require massive computation and\nmemory, necessitating parallelism which introduces heavy communication\noverheads. Driven by advances in packaging, the chiplet architecture emerges as\na potential solution, as it can integrate computing power, as well as utilize\non-package links with better signal integrity, higher bandwidth, and lower\nenergy consumption. However, most existing chiplet-related works focus on DNN\ninference. Directly porting them to LLM training introduces significantly large\nquantities of DRAM access and network-on-package (NoP) overheads which make\nstate-of-the-art chiplet designs fail, highlighting a research gap.\n  This work proposes Hecaton, a scalable and cost-effective chiplet system for\nLLM training. We first provide a chiplet architecture with tailored scheduling\nthat can largely reduce DRAM accesses. We further design an efficient\ndistributed training method that reduces NoP communication complexity and\nrelieves constraints on SRAM capacity and layout. Theoretical analysis shows\nthat the entire system achieves weak scaling: as the workload and hardware\nresources grow proportionally, the computation-to-communication ratio remains\nnearly constant. Experiments with various workloads and hardware configurations\nverify the property, and Hecaton achieves $5.29\\times$ performance improvement\nand $3.46\\times$ energy reduction on Llama3.1-405B, compared to the tensor\nparallelism in Megatron. To the best of our knowledge, we propose the first\nchiplet architecture specifically used for LLM training or finetuning, with\nguaranteed performance regardless of the problem scale."
                },
                "authors": [
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Shupei Fan"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Shuwen Deng"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13694v2",
                "updated": "2024-11-27T15:13:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    13,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-03T03:31:37Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    31,
                    37,
                    1,
                    247,
                    0
                ],
                "title": "Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Source Knowledge Pruning for Retrieval-Augmented Generation: A\n  Benchmark and Empirical Study"
                },
                "summary": "Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach for mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of externeal knowledge source. However, in\nreal-world applications, most situations involve diverse knowledge from various\nsources, yet this area has been less explored. The main dilemma is the lack of\na suitable dataset containing multiple knowledge sources and pre-exploration of\nthe associated issues. To address these challenges, we standardize a benchmark\ndataset that combines structured and unstructured knowledge across diverse and\ncomplementary domains. Based on this dataset, we further develop a\nplug-and-play RAG framework, PruningRAG, whose main characteristic is to employ\nmulti-granularity pruning strategies for optimizing the integration of relevant\ninformation and minimizing misleading context. Building upon the standardized\ndataset and PruningRAG, we also report a series of experimental results, as\nwell as insightful findings. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of\nadvancing future research in the RAG community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is increasingly recognized as an\neffective approach for mitigating the hallucination of large language models\n(LLMs) through the integration of external knowledge. While numerous efforts,\nmost studies focus on a single type of externeal knowledge source. However, in\nreal-world applications, most situations involve diverse knowledge from various\nsources, yet this area has been less explored. The main dilemma is the lack of\na suitable dataset containing multiple knowledge sources and pre-exploration of\nthe associated issues. To address these challenges, we standardize a benchmark\ndataset that combines structured and unstructured knowledge across diverse and\ncomplementary domains. Based on this dataset, we further develop a\nplug-and-play RAG framework, PruningRAG, whose main characteristic is to employ\nmulti-granularity pruning strategies for optimizing the integration of relevant\ninformation and minimizing misleading context. Building upon the standardized\ndataset and PruningRAG, we also report a series of experimental results, as\nwell as insightful findings. Our dataset and code are publicly\navailable\\footnote{https://github.com/USTCAGI/PruningRAG}, with the aim of\nadvancing future research in the RAG community."
                },
                "authors": [
                    {
                        "name": "Shuo Yu"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jiqian Yang"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Chenyi Lei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "arxiv_affiliation": "State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China, Hefei, China",
                "author": "Enhong Chen",
                "arxiv_comment": "10 pages, 11 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14427v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14427v3",
                "updated": "2024-11-27T14:58:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    58,
                    17,
                    2,
                    332,
                    0
                ],
                "published": "2024-03-21T14:33:34Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    14,
                    33,
                    34,
                    3,
                    81,
                    0
                ],
                "title": "Learning and communication pressures in neural networks: Lessons from\n  emergent communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and communication pressures in neural networks: Lessons from\n  emergent communication"
                },
                "summary": "Finding and facilitating commonalities between the linguistic behaviors of\nlarge language models and humans could lead to major breakthroughs in our\nunderstanding of the acquisition, processing, and evolution of language.\nHowever, most findings on human-LLM similarity can be attributed to training on\nhuman data. The field of emergent machine-to-machine communication provides an\nideal testbed for discovering which pressures are neural agents naturally\nexposed to when learning to communicate in isolation, without any human\nlanguage to start with. Here, we review three cases where mismatches between\nthe emergent linguistic behavior of neural agents and humans were resolved\nthanks to introducing theoretically-motivated inductive biases. By contrasting\nhumans, large language models, and emergent communication agents, we then\nidentify key pressures at play for language learning and emergence:\ncommunicative success, production effort, learnability, and other\npsycho-/sociolinguistic factors. We discuss their implications and relevance to\nthe field of language evolution and acquisition. By mapping out the necessary\ninductive biases that make agents' emergent languages more human-like, we not\nonly shed light on the underlying principles of human cognition and\ncommunication, but also inform and improve the very use of these models as\nvaluable scientific tools for studying language learning, processing, use, and\nrepresentation more broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding and facilitating commonalities between the linguistic behaviors of\nlarge language models and humans could lead to major breakthroughs in our\nunderstanding of the acquisition, processing, and evolution of language.\nHowever, most findings on human-LLM similarity can be attributed to training on\nhuman data. The field of emergent machine-to-machine communication provides an\nideal testbed for discovering which pressures are neural agents naturally\nexposed to when learning to communicate in isolation, without any human\nlanguage to start with. Here, we review three cases where mismatches between\nthe emergent linguistic behavior of neural agents and humans were resolved\nthanks to introducing theoretically-motivated inductive biases. By contrasting\nhumans, large language models, and emergent communication agents, we then\nidentify key pressures at play for language learning and emergence:\ncommunicative success, production effort, learnability, and other\npsycho-/sociolinguistic factors. We discuss their implications and relevance to\nthe field of language evolution and acquisition. By mapping out the necessary\ninductive biases that make agents' emergent languages more human-like, we not\nonly shed light on the underlying principles of human cognition and\ncommunication, but also inform and improve the very use of these models as\nvaluable scientific tools for studying language learning, processing, use, and\nrepresentation more broadly."
                },
                "authors": [
                    {
                        "name": "Lukas Galke"
                    },
                    {
                        "name": "Limor Raviv"
                    }
                ],
                "author_detail": {
                    "name": "Limor Raviv"
                },
                "author": "Limor Raviv",
                "arxiv_doi": "10.34842/3vr5-5r49",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.34842/3vr5-5r49",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.14427v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14427v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready version, as published in Language Development Research",
                "arxiv_journal_ref": "Language Development Research 5(1), 116-143 (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18382v1",
                "updated": "2024-11-27T14:29:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    29,
                    10,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:29:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    29,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatGPT as speechwriter for the French presidents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT as speechwriter for the French presidents"
                },
                "summary": "Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI proposes several large language models (LLMs) to automatically\ngenerate a message in response to users' requests. Such scientific\nbreakthroughs promote new writing assistants but with some fears. The main\nfocus of this study is to analyze the written style of one LLM called ChatGPT\nby comparing its generated messages with those of the recent French presidents.\nTo achieve this, we compare end-of-the-year addresses written by Chirac,\nSarkozy, Hollande, and Macron with those automatically produced by ChatGPT. We\nfound that ChatGPT tends to overuse nouns, possessive determiners, and numbers.\nOn the other hand, the generated speeches employ less verbs, pronouns, and\nadverbs and include, in mean, too standardized sentences. Considering some\nwords, one can observe that ChatGPT tends to overuse \"to must\" (devoir), \"to\ncontinue\" or the lemma \"we\" (nous). Moreover, GPT underuses the auxiliary verb\n\"to be\" (^etre), or the modal verbs \"to will\" (vouloir) or \"to have to\"\n(falloir). In addition, when a short text is provided as example to ChatGPT,\nthe machine can generate a short message with a style closed to the original\nwording. Finally, we reveal that ChatGPT style exposes distinct features\ncompared to real presidential speeches."
                },
                "authors": [
                    {
                        "name": "Dominique LabbÃ©"
                    },
                    {
                        "name": "Cyril LabbÃ©"
                    },
                    {
                        "name": "Jacques Savoy"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Savoy"
                },
                "author": "Jacques Savoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18365v1",
                "updated": "2024-11-27T14:12:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    12,
                    36,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:12:36Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    12,
                    36,
                    2,
                    332,
                    0
                ],
                "title": "GPT as ghostwriter at the White House",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT as ghostwriter at the White House"
                },
                "summary": "Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently several large language models (LLMs) have demonstrated their\ncapability to generate a message in response to a user request. Such scientific\nbreakthroughs promote new perspectives but also some fears. The main focus of\nthis study is to analyze the written style of one LLM called ChatGPT 3.5 by\ncomparing its generated messages with those of the recent US presidents. To\nachieve this objective, we compare the State of the Union addresses written by\nReagan to Obama with those automatically produced by ChatGPT. We found that\nChatGPT tends to overuse the lemma \"we\" as well as nouns and commas. On the\nother hand, the generated speeches employ less verbs and include, in mean,\nlonger sentences. Even when imposing a given style to ChatGPT, the resulting\nspeech remains distinct from messages written by the target author. Moreover,\nChatGPT opts for a neutral tone with mainly positive emotional expressions and\nsymbolic terms (e.g., freedom, nation). Finally, we show that the GPT's style\nexposes distinct features compared to real presidential addresses."
                },
                "authors": [
                    {
                        "name": "Jacques Savoy"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Savoy"
                },
                "author": "Jacques Savoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18363v1",
                "updated": "2024-11-27T14:11:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T14:11:10Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    11,
                    10,
                    2,
                    332,
                    0
                ],
                "title": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatRex: Taming Multimodal LLM for Joint Perception and Understanding"
                },
                "summary": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception and understanding are two pillars of computer vision. While\nmultimodal large language models (MLLM) have demonstrated remarkable visual\nunderstanding capabilities, they arguably lack accurate perception abilities,\ne.g. the stage-of-the-art model Qwen2-VL only achieves a 43.9 recall rate on\nthe COCO dataset, limiting many tasks requiring the combination of perception\nand understanding. In this work, we aim to bridge this perception gap from both\nmodel designing and data development perspectives. We first introduce ChatRex,\nan MLLM with a decoupled perception design. Instead of having the LLM directly\npredict box coordinates, we feed the output boxes from a universal proposal\nnetwork into the LLM, allowing it to output the corresponding box indices to\nrepresent its detection results, turning the regression task into a\nretrieval-based task that LLM handles more proficiently. From the data\nperspective, we build a fully automated data engine and construct the\nRexverse-2M dataset which possesses multiple granularities to support the joint\ntraining of perception and understanding. After standard two-stage training,\nChatRex demonstrates strong perception capabilities while preserving multimodal\nunderstanding performance. The combination of these two capabilities\nsimultaneously unlocks many attractive applications, demonstrating the\ncomplementary roles of both perception and understanding in MLLM. Code is\navailable at \\url{https://github.com/IDEA-Research/ChatRex}."
                },
                "authors": [
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Gen luo"
                    },
                    {
                        "name": "Yuqin Yang"
                    },
                    {
                        "name": "Yuda Xiong"
                    },
                    {
                        "name": "Yihao Chen"
                    },
                    {
                        "name": "Zhaoyang Zeng"
                    },
                    {
                        "name": "Tianhe Ren"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "35 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18337v1",
                "updated": "2024-11-27T13:35:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T13:35:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    13,
                    35,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various\n  Large Language Models on Word Sense Disambiguation"
                },
                "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication."
                },
                "authors": [
                    {
                        "name": "T. G. D. K. Sumanathilaka"
                    },
                    {
                        "name": "Nicholas Micallef"
                    },
                    {
                        "name": "Julian Hough"
                    }
                ],
                "author_detail": {
                    "name": "Julian Hough"
                },
                "author": "Julian Hough",
                "arxiv_comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19657v3",
                "updated": "2024-11-27T12:59:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    59,
                    3,
                    2,
                    332,
                    0
                ],
                "published": "2024-06-28T04:56:53Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    4,
                    56,
                    53,
                    4,
                    180,
                    0
                ],
                "title": "LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEasyQuant -- An Easy to Use Toolkit for LLM Quantization"
                },
                "summary": "Currently, there are many quantization methods appeared for LLM quantization,\nyet few are user-friendly and easy to be deployed locally. Packages like\nTensorRT and Quantohave many underlying structures and self-invoking internal\nfunctions, which are not conducive to developers' personalized development and\nlearning for deployment. Therefore, we develop LLMEasyQuant, it is a package\naiming to for easy quantization deployment which is user-friendly and suitable\nfor beginners' learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Currently, there are many quantization methods appeared for LLM quantization,\nyet few are user-friendly and easy to be deployed locally. Packages like\nTensorRT and Quantohave many underlying structures and self-invoking internal\nfunctions, which are not conducive to developers' personalized development and\nlearning for deployment. Therefore, we develop LLMEasyQuant, it is a package\naiming to for easy quantization deployment which is user-friendly and suitable\nfor beginners' learning."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Kaiser Pister"
                    }
                ],
                "author_detail": {
                    "name": "Kaiser Pister"
                },
                "author": "Kaiser Pister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00693v2",
                "updated": "2024-11-27T12:30:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    30,
                    23,
                    2,
                    332,
                    0
                ],
                "published": "2024-03-26T15:36:40Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    15,
                    36,
                    40,
                    1,
                    86,
                    0
                ],
                "title": "Leveraging Large Language Models in Human-Robot Interaction: A Critical\n  Analysis of Potential and Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models in Human-Robot Interaction: A Critical\n  Analysis of Potential and Pitfalls"
                },
                "summary": "The emergence of large language models (LLM) and, consequently, vision\nlanguage models (VLM) has ignited new imaginations among robotics researchers.\nAt this point, the range of applications to which LLM and VLM can be applied in\nhuman-robot interaction (HRI), particularly socially assistive robots (SARs),\nis unchartered territory. However, LLM and VLM present unprecedented\nopportunities and challenges for SAR integration. We aim to illuminate the\nopportunities and challenges when roboticists deploy LLM and VLM in SARs.\nFirst, we conducted a meta-study of more than 250 papers exploring 1) major\nrobots in HRI research and 2) significant applications of SARs, emphasizing\neducation, healthcare, and entertainment while addressing 3) societal norms and\nissues like trust, bias, and ethics that the robot developers must address.\nThen, we identified 4) critical components of a robot that LLM or VLM can\nreplace while addressing the 5) benefits of integrating LLM into robot designs\nand the 6) risks involved. Finally, we outline a pathway for the responsible\nand effective adoption of LLM or VLM into SARs, and we close our discussion by\noffering caution regarding this deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLM) and, consequently, vision\nlanguage models (VLM) has ignited new imaginations among robotics researchers.\nAt this point, the range of applications to which LLM and VLM can be applied in\nhuman-robot interaction (HRI), particularly socially assistive robots (SARs),\nis unchartered territory. However, LLM and VLM present unprecedented\nopportunities and challenges for SAR integration. We aim to illuminate the\nopportunities and challenges when roboticists deploy LLM and VLM in SARs.\nFirst, we conducted a meta-study of more than 250 papers exploring 1) major\nrobots in HRI research and 2) significant applications of SARs, emphasizing\neducation, healthcare, and entertainment while addressing 3) societal norms and\nissues like trust, bias, and ethics that the robot developers must address.\nThen, we identified 4) critical components of a robot that LLM or VLM can\nreplace while addressing the 5) benefits of integrating LLM into robot designs\nand the 6) risks involved. Finally, we outline a pathway for the responsible\nand effective adoption of LLM or VLM into SARs, and we close our discussion by\noffering caution regarding this deployment."
                },
                "authors": [
                    {
                        "name": "Jesse Atuhurra"
                    }
                ],
                "author_detail": {
                    "name": "Jesse Atuhurra"
                },
                "author": "Jesse Atuhurra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08562v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08562v3",
                "updated": "2024-11-27T12:25:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    25,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2023-11-14T21:46:27Z",
                "published_parsed": [
                    2023,
                    11,
                    14,
                    21,
                    46,
                    27,
                    1,
                    318,
                    0
                ],
                "title": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAgIC: Investigation of Large Language Model Powered Multi-Agent in\n  Cognition, Adaptability, Rationality and Collaboration"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating exceptional reasoning, tool usage, and memory\ncapabilities. As their applications expand into multi-agent environments, there\narises a need for a comprehensive evaluation framework that captures LLMs'\nreasoning, planning, collaboration, and other social abilities. This work\nintroduces a novel competition-based benchmark framework specifically designed\nto assess LLMs within multi-agent settings, providing quantitative metrics to\nevaluate their judgment, reasoning, deception, self-awareness, cooperation,\ncoordination, and rationality. We utilize two social deduction games alongside\nthree game-theory scenarios to create diverse environments. Our frame is\nfortified with the probabilistic graphic modeling (PGM) method, enhancing the\nLLMs' capabilities in navigating complex social and cognitive dimensions. We\nevaluate seven LLMs, quantitatively highlighting a significant capability gap\nof over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.\nIt also confirms that our PGM enhancement boosts the abilities of all selected\nmodels by an average of 37%. Our data and code can be found here\nhttps://github.com/cathyxl/MAgIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating exceptional reasoning, tool usage, and memory\ncapabilities. As their applications expand into multi-agent environments, there\narises a need for a comprehensive evaluation framework that captures LLMs'\nreasoning, planning, collaboration, and other social abilities. This work\nintroduces a novel competition-based benchmark framework specifically designed\nto assess LLMs within multi-agent settings, providing quantitative metrics to\nevaluate their judgment, reasoning, deception, self-awareness, cooperation,\ncoordination, and rationality. We utilize two social deduction games alongside\nthree game-theory scenarios to create diverse environments. Our frame is\nfortified with the probabilistic graphic modeling (PGM) method, enhancing the\nLLMs' capabilities in navigating complex social and cognitive dimensions. We\nevaluate seven LLMs, quantitatively highlighting a significant capability gap\nof over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B.\nIt also confirms that our PGM enhancement boosts the abilities of all selected\nmodels by an average of 37%. Our data and code can be found here\nhttps://github.com/cathyxl/MAgIC."
                },
                "authors": [
                    {
                        "name": "Lin Xu"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Hongyu Ren"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "See Kiong Ng"
                    },
                    {
                        "name": "Jiashi Feng"
                    }
                ],
                "author_detail": {
                    "name": "Jiashi Feng"
                },
                "author": "Jiashi Feng",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08562v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08562v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18288v1",
                "updated": "2024-11-27T12:18:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    18,
                    39,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:18:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    18,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Optimizing Multispectral Object Detection: A Bag of Tricks and\n  Comprehensive Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Multispectral Object Detection: A Bag of Tricks and\n  Comprehensive Benchmarks"
                },
                "summary": "Multispectral object detection, utilizing RGB and TIR (thermal infrared)\nmodalities, is widely recognized as a challenging task. It requires not only\nthe effective extraction of features from both modalities and robust fusion\nstrategies, but also the ability to address issues such as spectral\ndiscrepancies, spatial misalignment, and environmental dependencies between RGB\nand TIR images. These challenges significantly hinder the generalization of\nmultispectral detection systems across diverse scenarios. Although numerous\nstudies have attempted to overcome these limitations, it remains difficult to\nclearly distinguish the performance gains of multispectral detection systems\nfrom the impact of these \"optimization techniques\". Worse still, despite the\nrapid emergence of high-performing single-modality detection models, there is\nstill a lack of specialized training techniques that can effectively adapt\nthese models for multispectral detection tasks. The absence of a standardized\nbenchmark with fair and consistent experimental setups also poses a significant\nbarrier to evaluating the effectiveness of new approaches. To this end, we\npropose the first fair and reproducible benchmark specifically designed to\nevaluate the training \"techniques\", which systematically classifies existing\nmultispectral object detection methods, investigates their sensitivity to\nhyper-parameters, and standardizes the core configurations. A comprehensive\nevaluation is conducted across multiple representative multispectral object\ndetection datasets, utilizing various backbone networks and detection\nframeworks. Additionally, we introduce an efficient and easily deployable\nmultispectral object detection framework that can seamlessly optimize\nhigh-performing single-modality models into dual-modality models, integrating\nour advanced training techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multispectral object detection, utilizing RGB and TIR (thermal infrared)\nmodalities, is widely recognized as a challenging task. It requires not only\nthe effective extraction of features from both modalities and robust fusion\nstrategies, but also the ability to address issues such as spectral\ndiscrepancies, spatial misalignment, and environmental dependencies between RGB\nand TIR images. These challenges significantly hinder the generalization of\nmultispectral detection systems across diverse scenarios. Although numerous\nstudies have attempted to overcome these limitations, it remains difficult to\nclearly distinguish the performance gains of multispectral detection systems\nfrom the impact of these \"optimization techniques\". Worse still, despite the\nrapid emergence of high-performing single-modality detection models, there is\nstill a lack of specialized training techniques that can effectively adapt\nthese models for multispectral detection tasks. The absence of a standardized\nbenchmark with fair and consistent experimental setups also poses a significant\nbarrier to evaluating the effectiveness of new approaches. To this end, we\npropose the first fair and reproducible benchmark specifically designed to\nevaluate the training \"techniques\", which systematically classifies existing\nmultispectral object detection methods, investigates their sensitivity to\nhyper-parameters, and standardizes the core configurations. A comprehensive\nevaluation is conducted across multiple representative multispectral object\ndetection datasets, utilizing various backbone networks and detection\nframeworks. Additionally, we introduce an efficient and easily deployable\nmultispectral object detection framework that can seamlessly optimize\nhigh-performing single-modality models into dual-modality models, integrating\nour advanced training techniques."
                },
                "authors": [
                    {
                        "name": "Chen Zhou"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yanyan Xu"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18280v1",
                "updated": "2024-11-27T12:15:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    15,
                    22,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:15:22Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    15,
                    22,
                    2,
                    332,
                    0
                ],
                "title": "Neutralizing Backdoors through Information Conflicts for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing Backdoors through Information Conflicts for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks, from\nunderstanding to reasoning. However, they remain vulnerable to backdoor\nattacks, where models behave normally for standard queries but generate harmful\nresponses or unintended output when specific triggers are activated. Existing\nbackdoor defenses often suffer from drawbacks that they either focus on\ndetection without removal, rely on rigid assumptions about trigger properties,\nor prove to be ineffective against advanced attacks like multi-trigger\nbackdoors. In this paper, we present a novel method to eliminate backdoor\nbehaviors from LLMs through the construction of information conflicts using\nboth internal and external mechanisms. Internally, we leverage a lightweight\ndataset to train a conflict model, which is then merged with the backdoored\nmodel to neutralize malicious behaviors by embedding contradictory information\nwithin the model's parametric memory. Externally, we incorporate convincing\ncontradictory evidence into the prompt to challenge the model's internal\nbackdoor knowledge. Experimental results on classification and conversational\ntasks across 4 widely used LLMs demonstrate that our method outperforms 8\nstate-of-the-art backdoor defense baselines. We can reduce the attack success\nrate of advanced backdoor attacks by up to 98% while maintaining over 90% clean\ndata accuracy. Furthermore, our method has proven to be robust against adaptive\nbackdoor attacks. The code will be open-sourced upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen significant advancements, achieving\nsuperior performance in various Natural Language Processing (NLP) tasks, from\nunderstanding to reasoning. However, they remain vulnerable to backdoor\nattacks, where models behave normally for standard queries but generate harmful\nresponses or unintended output when specific triggers are activated. Existing\nbackdoor defenses often suffer from drawbacks that they either focus on\ndetection without removal, rely on rigid assumptions about trigger properties,\nor prove to be ineffective against advanced attacks like multi-trigger\nbackdoors. In this paper, we present a novel method to eliminate backdoor\nbehaviors from LLMs through the construction of information conflicts using\nboth internal and external mechanisms. Internally, we leverage a lightweight\ndataset to train a conflict model, which is then merged with the backdoored\nmodel to neutralize malicious behaviors by embedding contradictory information\nwithin the model's parametric memory. Externally, we incorporate convincing\ncontradictory evidence into the prompt to challenge the model's internal\nbackdoor knowledge. Experimental results on classification and conversational\ntasks across 4 widely used LLMs demonstrate that our method outperforms 8\nstate-of-the-art backdoor defense baselines. We can reduce the attack success\nrate of advanced backdoor attacks by up to 98% while maintaining over 90% clean\ndata accuracy. Furthermore, our method has proven to be robust against adaptive\nbackdoor attacks. The code will be open-sourced upon publication."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Sun"
                    },
                    {
                        "name": "Xueluan Gong"
                    },
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    }
                ],
                "author_detail": {
                    "name": "Kwok-Yan Lam"
                },
                "author": "Kwok-Yan Lam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v1",
                "updated": "2024-11-27T12:13:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18266v1",
                "updated": "2024-11-27T12:03:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    52,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T12:03:52Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    52,
                    2,
                    332,
                    0
                ],
                "title": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable intelligent throat enables natural speech in stroke patients\n  with dysarthria"
                },
                "summary": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wearable silent speech systems hold significant potential for restoring\ncommunication in patients with speech impairments. However, seamless, coherent\nspeech remains elusive, and clinical efficacy is still unproven. Here, we\npresent an AI-driven intelligent throat (IT) system that integrates throat\nmuscle vibrations and carotid pulse signal sensors with large language model\n(LLM) processing to enable fluent, emotionally expressive communication. The\nsystem utilizes ultrasensitive textile strain sensors to capture high-quality\nsignals from the neck area and supports token-level processing for real-time,\ncontinuous speech decoding, enabling seamless, delay-free communication. In\ntests with five stroke patients with dysarthria, IT's LLM agents intelligently\ncorrected token errors and enriched sentence-level emotional and logical\ncoherence, achieving low error rates (4.2% word error rate, 2.9% sentence error\nrate) and a 55% increase in user satisfaction. This work establishes a\nportable, intuitive communication platform for patients with dysarthria with\nthe potential to be applied broadly across different neurological conditions\nand in multi-language support systems."
                },
                "authors": [
                    {
                        "name": "Chenyu Tang"
                    },
                    {
                        "name": "Shuo Gao"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Wentian Yi"
                    },
                    {
                        "name": "Yuxuan Jin"
                    },
                    {
                        "name": "Xiaoxue Zhai"
                    },
                    {
                        "name": "Sixuan Lei"
                    },
                    {
                        "name": "Hongbei Meng"
                    },
                    {
                        "name": "Zibo Zhang"
                    },
                    {
                        "name": "Muzi Xu"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Xuhang Chen"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Hongyun Yang"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Wenyu Wang"
                    },
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Xiaodong Feng"
                    },
                    {
                        "name": "Peter Smielewski"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Wenhui Song"
                    },
                    {
                        "name": "Martin Birchall"
                    },
                    {
                        "name": "Luigi G. Occhipint"
                    }
                ],
                "author_detail": {
                    "name": "Luigi G. Occhipint"
                },
                "author": "Luigi G. Occhipint",
                "arxiv_comment": "5 figures, 45 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11782v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11782v2",
                "updated": "2024-11-27T12:03:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    3,
                    27,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-15T17:01:21Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks"
                },
                "summary": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Dawei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Cheng"
                },
                "author": "Dawei Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11782v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11782v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18262v1",
                "updated": "2024-11-27T11:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    59,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    59,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Break the ID-Language Barrier: An Adaption Framework for Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Break the ID-Language Barrier: An Adaption Framework for Sequential\n  Recommendation"
                },
                "summary": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent breakthrough of large language models (LLMs) in natural language\nprocessing has sparked exploration in recommendation systems, however, their\nlimited domain-specific knowledge remains a critical bottleneck. Specifically,\nLLMs lack key pieces of information crucial for sequential recommendations,\nsuch as user behavior patterns. To address this critical gap, we propose\nIDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich\nin domain-specific knowledge, into LLMs to improve recommendation accuracy.\nIDLE-Adapter acts as a bridge, transforming sparse user-item interaction data\ninto dense, LLM-compatible representations through a Pre-trained ID Sequential\nModel, Dimensionality Alignment, Layer-wise Embedding Refinement, and\nLayer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates\nremarkable flexibility by seamlessly integrating ID embeddings from diverse\nID-based sequential models and LLM architectures. Extensive experiments across\nvarious datasets demonstrate the superiority of IDLE-Adapter, achieving over\n10\\% and 20\\% improvements in HitRate@5 and NDCG@5 metrics, respectively,\ncompared to state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xiaohan Yu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15115v3",
                "updated": "2024-11-27T11:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    58,
                    50,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-19T13:53:50Z",
                "published_parsed": [
                    2024,
                    10,
                    19,
                    13,
                    53,
                    50,
                    5,
                    293,
                    0
                ],
                "title": "On Designing Effective RL Reward at Training Time for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Designing Effective RL Reward at Training Time for LLM Reasoning"
                },
                "summary": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models have been increasingly critical for improving the reasoning\ncapability of LLMs. Existing research has shown that a well-trained reward\nmodel can substantially improve model performances at inference time via\nsearch. However, the potential of reward models during RL training time still\nremains largely under-explored. It is currently unclear whether these reward\nmodels can provide additional training signals to enhance the reasoning\ncapabilities of LLMs in RL training that uses sparse success rewards, which\nverify the correctness of solutions. In this work, we evaluate popular reward\nmodels for RL training, including the Outcome-supervised Reward Model (ORM) and\nthe Process-supervised Reward Model (PRM), and train a collection of LLMs for\nmath problems using RL by combining these learned rewards with success rewards.\nSurprisingly, even though these learned reward models have strong\ninference-time performances, they may NOT help or even hurt RL training,\nproducing worse performances than LLMs trained with the success reward only.\nOur analysis reveals that an LLM can receive high rewards from some of these\nreward models by repeating correct but unnecessary reasoning steps, leading to\na severe reward hacking issue. Therefore, we introduce two novel reward\nrefinement techniques, including Clipping and Delta. The key idea is to ensure\nthe accumulative reward of any reasoning trajectory is upper-bounded to keep a\nlearned reward model effective without being exploited. We evaluate our\ntechniques with multiple reward models over a set of 1.5B and 7B LLMs on MATH\nand GSM8K benchmarks and demonstrate that with a carefully designed reward\nfunction, RL training without any additional supervised tuning can improve all\nthe evaluated LLMs, including the state-of-the-art 7B LLM\nQwen2.5-Math-7B-Instruct on MATH and GSM8K benchmarks."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wenjie Ye"
                    },
                    {
                        "name": "Weilin Liu"
                    },
                    {
                        "name": "Chuyi He"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Guangju Wang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17175v3",
                "updated": "2024-11-27T11:47:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    47,
                    45,
                    2,
                    332,
                    0
                ],
                "published": "2024-08-30T10:24:07Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    24,
                    7,
                    4,
                    243,
                    0
                ],
                "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio\n  Language Model"
                },
                "summary": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio generation have been significantly propelled by\nthe capabilities of Large Language Models (LLMs). The existing research on\naudio LLM has primarily focused on enhancing the architecture and scale of\naudio language models, as well as leveraging larger datasets, and generally,\nacoustic codecs, such as EnCodec, are used for audio tokenization. However,\nthese codecs were originally designed for audio compression, which may lead to\nsuboptimal performance in the context of audio LLM. Our research aims to\naddress the shortcomings of current audio LLM codecs, particularly their\nchallenges in maintaining semantic integrity in generated audio. For instance,\nexisting methods like VALL-E, which condition acoustic token generation on text\ntranscriptions, often suffer from content inaccuracies and elevated word error\nrates (WER) due to semantic misinterpretations of acoustic tokens, resulting in\nword skipping and errors. To overcome these issues, we propose a\nstraightforward yet effective approach called X-Codec. X-Codec incorporates\nsemantic features from a pre-trained semantic encoder before the Residual\nVector Quantization (RVQ) stage and introduces a semantic reconstruction loss\nafter RVQ. By enhancing the semantic ability of the codec, X-Codec\nsignificantly reduces WER in speech synthesis tasks and extends these benefits\nto non-speech applications, including music and sound generation. Our\nexperiments in text-to-speech, music continuation, and text-to-sound tasks\ndemonstrate that integrating semantic information substantially improves the\noverall performance of language models in audio generation. Our code and demo\nare available (Demo: https://x-codec-audio.github.io Code:\nhttps://github.com/zhenye234/xcodec)"
                },
                "authors": [
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Peiwen Sun"
                    },
                    {
                        "name": "Jiahe Lei"
                    },
                    {
                        "name": "Hongzhan Lin"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Zheqi Dai"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Jiahao Pan"
                    },
                    {
                        "name": "Qifeng Liu"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18247v1",
                "updated": "2024-11-27T11:38:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    38,
                    9,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:38:09Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    38,
                    9,
                    2,
                    332,
                    0
                ],
                "title": "A gentle push funziona benissimo: making instructed models in Italian\n  via contrastive activation steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A gentle push funziona benissimo: making instructed models in Italian\n  via contrastive activation steering"
                },
                "summary": "Adapting models to a language that was only partially present in the\npre-training data requires fine-tuning, which is expensive in terms of both\ndata and computational resources. As an alternative to fine-tuning, we explore\nthe potential of activation steering-based techniques to enhance model\nperformance on Italian tasks. Through our experiments we show that Italian\nsteering (i) can be successfully applied to different models, (ii) achieves\nperformances comparable to, or even better than, fine-tuned models for Italian,\nand (iii) yields higher quality and consistency in Italian generations. We also\ndiscuss the utility of steering and fine-tuning in the contemporary LLM\nlandscape where models are anyway getting high Italian performances even if not\nexplicitly trained in this language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting models to a language that was only partially present in the\npre-training data requires fine-tuning, which is expensive in terms of both\ndata and computational resources. As an alternative to fine-tuning, we explore\nthe potential of activation steering-based techniques to enhance model\nperformance on Italian tasks. Through our experiments we show that Italian\nsteering (i) can be successfully applied to different models, (ii) achieves\nperformances comparable to, or even better than, fine-tuned models for Italian,\nand (iii) yields higher quality and consistency in Italian generations. We also\ndiscuss the utility of steering and fine-tuning in the contemporary LLM\nlandscape where models are anyway getting high Italian performances even if not\nexplicitly trained in this language."
                },
                "authors": [
                    {
                        "name": "Daniel Scalena"
                    },
                    {
                        "name": "Elisabetta Fersini"
                    },
                    {
                        "name": "Malvina Nissim"
                    }
                ],
                "author_detail": {
                    "name": "Malvina Nissim"
                },
                "author": "Malvina Nissim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18242v1",
                "updated": "2024-11-27T11:30:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    30,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    30,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Thai Financial Domain Adaptation of THaLLE -- Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thai Financial Domain Adaptation of THaLLE -- Technical Report"
                },
                "summary": "Large Language Models (LLMs) excel in general tasks but struggle with\ndomain-specific challenges, such as specialized terminology and localized\nregulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack\nsupport for the Thai financial domain. We developed a Thai Financial LLM using\nthe Investment Consultant (IC) exam dataset from the Stock Exchange of\nThailand. To address dataset limitations, we applied data augmentation, ReLoRA\nfor efficient training, Continued Pretraining (CPT) for domain knowledge, and\nRank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)\nsimulated exam scenarios, while Direct Preference Optimization (DPO) refined\nthe model using feedback. The model achieved scores of 72%, 72%, and 84% on IC\nexam levels P1, P2, and P3, respectively, demonstrating its effectiveness in\nThai financial advisory tasks and its potential for specialized applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in general tasks but struggle with\ndomain-specific challenges, such as specialized terminology and localized\nregulations. Existing financial LLMs, like FinGPT and BloombergGPT, lack\nsupport for the Thai financial domain. We developed a Thai Financial LLM using\nthe Investment Consultant (IC) exam dataset from the Stock Exchange of\nThailand. To address dataset limitations, we applied data augmentation, ReLoRA\nfor efficient training, Continued Pretraining (CPT) for domain knowledge, and\nRank-Stabilized LoRA (rsLoRA) for fine-tuning. Supervised Fine-Tuning (SFT)\nsimulated exam scenarios, while Direct Preference Optimization (DPO) refined\nthe model using feedback. The model achieved scores of 72%, 72%, and 84% on IC\nexam levels P1, P2, and P3, respectively, demonstrating its effectiveness in\nThai financial advisory tasks and its potential for specialized applications."
                },
                "authors": [
                    {
                        "name": "KBTG Labs"
                    },
                    {
                        "name": "Atthakorn Petchsod"
                    },
                    {
                        "name": "Pornchanan Balee"
                    },
                    {
                        "name": "Danupat Khamnuansin"
                    },
                    {
                        "name": "Anuruth Lertpiya"
                    },
                    {
                        "name": "Chanatip Saetia"
                    },
                    {
                        "name": "Tawunrat Chalothorn"
                    },
                    {
                        "name": "Thadpong Pongthawornkamol"
                    },
                    {
                        "name": "Monchai Lertsutthiwong"
                    }
                ],
                "author_detail": {
                    "name": "Monchai Lertsutthiwong"
                },
                "author": "Monchai Lertsutthiwong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18241v1",
                "updated": "2024-11-27T11:29:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    29,
                    17,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T11:29:17Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    29,
                    17,
                    2,
                    332,
                    0
                ],
                "title": "Exploration of LLM Multi-Agent Application Implementation Based on\n  LangGraph+CrewAI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration of LLM Multi-Agent Application Implementation Based on\n  LangGraph+CrewAI"
                },
                "summary": "With the rapid development of large model technology, the application of\nagent technology in various fields is becoming increasingly widespread,\nprofoundly changing people's work and lifestyles. In complex and dynamic\nsystems, multi-agents achieve complex tasks that are difficult for a single\nagent to complete through division of labor and collaboration among agents.\nThis paper discusses the integrated application of LangGraph and CrewAI.\nLangGraph improves the efficiency of information transmission through graph\narchitecture, while CrewAI enhances team collaboration capabilities and system\nperformance through intelligent task allocation and resource management. The\nmain research contents of this paper are: (1) designing the architecture of\nagents based on LangGraph for precise control; (2) enhancing the capabilities\nof agents based on CrewAI to complete a variety of tasks. This study aims to\ndelve into the application of LangGraph and CrewAI in multi-agent systems,\nproviding new perspectives for the future development of agent technology, and\npromoting technological progress and application innovation in the field of\nlarge model intelligent agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large model technology, the application of\nagent technology in various fields is becoming increasingly widespread,\nprofoundly changing people's work and lifestyles. In complex and dynamic\nsystems, multi-agents achieve complex tasks that are difficult for a single\nagent to complete through division of labor and collaboration among agents.\nThis paper discusses the integrated application of LangGraph and CrewAI.\nLangGraph improves the efficiency of information transmission through graph\narchitecture, while CrewAI enhances team collaboration capabilities and system\nperformance through intelligent task allocation and resource management. The\nmain research contents of this paper are: (1) designing the architecture of\nagents based on LangGraph for precise control; (2) enhancing the capabilities\nof agents based on CrewAI to complete a variety of tasks. This study aims to\ndelve into the application of LangGraph and CrewAI in multi-agent systems,\nproviding new perspectives for the future development of agent technology, and\npromoting technological progress and application innovation in the field of\nlarge model intelligent agents."
                },
                "authors": [
                    {
                        "name": "Zhihua Duan"
                    },
                    {
                        "name": "Jialin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Wang"
                },
                "author": "Jialin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23558v2",
                "updated": "2024-11-27T11:28:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    28,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-31T01:55:33Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    1,
                    55,
                    33,
                    3,
                    305,
                    0
                ],
                "title": "Transferable Ensemble Black-box Jailbreak Attacks on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transferable Ensemble Black-box Jailbreak Attacks on Large Language\n  Models"
                },
                "summary": "In this report, we propose a novel black-box jailbreak attacking framework\nthat incorporates various LLM-as-Attacker methods to deliver transferable and\npowerful jailbreak attacks. Our method is designed based on three key\nobservations from existing jailbreaking studies and practices. First, we\nconsider an ensemble approach should be more effective in exposing the\nvulnerabilities of an aligned LLM compared to individual attacks. Second,\ndifferent malicious instructions inherently vary in their jailbreaking\ndifficulty, necessitating differentiated treatment to ensure more efficient\nattacks. Finally, the semantic coherence of a malicious instruction is crucial\nfor triggering the defenses of an aligned LLM; therefore, it must be carefully\ndisrupted to manipulate its embedding representation, thereby increasing the\njailbreak success rate. We validated our approach by participating in the\nCompetition for LLM and Agent Safety 2024, where our team achieved top\nperformance in the Jailbreaking Attack Track.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we propose a novel black-box jailbreak attacking framework\nthat incorporates various LLM-as-Attacker methods to deliver transferable and\npowerful jailbreak attacks. Our method is designed based on three key\nobservations from existing jailbreaking studies and practices. First, we\nconsider an ensemble approach should be more effective in exposing the\nvulnerabilities of an aligned LLM compared to individual attacks. Second,\ndifferent malicious instructions inherently vary in their jailbreaking\ndifficulty, necessitating differentiated treatment to ensure more efficient\nattacks. Finally, the semantic coherence of a malicious instruction is crucial\nfor triggering the defenses of an aligned LLM; therefore, it must be carefully\ndisrupted to manipulate its embedding representation, thereby increasing the\njailbreak success rate. We validated our approach by participating in the\nCompetition for LLM and Agent Safety 2024, where our team achieved top\nperformance in the Jailbreaking Attack Track."
                },
                "authors": [
                    {
                        "name": "Yiqi Yang"
                    },
                    {
                        "name": "Hongye Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hongye Fu"
                },
                "author": "Hongye Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.00319v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.00319v4",
                "updated": "2024-11-27T11:23:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    23,
                    55,
                    2,
                    332,
                    0
                ],
                "published": "2023-07-01T12:10:18Z",
                "published_parsed": [
                    2023,
                    7,
                    1,
                    12,
                    10,
                    18,
                    5,
                    182,
                    0
                ],
                "title": "Explainable AI in 6G O-RAN: A Tutorial and Survey on Architecture, Use\n  Cases, Challenges, and Future Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable AI in 6G O-RAN: A Tutorial and Survey on Architecture, Use\n  Cases, Challenges, and Future Research"
                },
                "summary": "The recent O-RAN specifications promote the evolution of RAN architecture by\nfunction disaggregation, adoption of open interfaces, and instantiation of a\nhierarchical closed-loop control architecture managed by RAN Intelligent\nControllers (RICs) entities. This paves the road to novel data-driven network\nmanagement approaches based on programmable logic. Aided by Artificial\nIntelligence (AI) and Machine Learning (ML), novel solutions targeting\ntraditionally unsolved RAN management issues can be devised. Nevertheless, the\nadoption of such smart and autonomous systems is limited by the current\ninability of human operators to understand the decision process of such AI/ML\nsolutions, affecting their trust in such novel tools. eXplainable AI (XAI) aims\nat solving this issue, enabling human users to better understand and\neffectively manage the emerging generation of artificially intelligent schemes,\nreducing the human-to-machine barrier. In this survey, we provide a summary of\nthe XAI methods and metrics before studying their deployment over the O-RAN\nAlliance RAN architecture along with its main building blocks. We then present\nvarious use cases and discuss the automation of XAI pipelines for O-RAN as well\nas the underlying security aspects. We also review some projects/standards that\ntackle this area. Finally, we identify different challenges and research\ndirections that may arise from the heavy adoption of AI/ML decision entities in\nthis context, focusing on how XAI can help to interpret, understand, and\nimprove trust in O-RAN operational networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent O-RAN specifications promote the evolution of RAN architecture by\nfunction disaggregation, adoption of open interfaces, and instantiation of a\nhierarchical closed-loop control architecture managed by RAN Intelligent\nControllers (RICs) entities. This paves the road to novel data-driven network\nmanagement approaches based on programmable logic. Aided by Artificial\nIntelligence (AI) and Machine Learning (ML), novel solutions targeting\ntraditionally unsolved RAN management issues can be devised. Nevertheless, the\nadoption of such smart and autonomous systems is limited by the current\ninability of human operators to understand the decision process of such AI/ML\nsolutions, affecting their trust in such novel tools. eXplainable AI (XAI) aims\nat solving this issue, enabling human users to better understand and\neffectively manage the emerging generation of artificially intelligent schemes,\nreducing the human-to-machine barrier. In this survey, we provide a summary of\nthe XAI methods and metrics before studying their deployment over the O-RAN\nAlliance RAN architecture along with its main building blocks. We then present\nvarious use cases and discuss the automation of XAI pipelines for O-RAN as well\nas the underlying security aspects. We also review some projects/standards that\ntackle this area. Finally, we identify different challenges and research\ndirections that may arise from the heavy adoption of AI/ML decision entities in\nthis context, focusing on how XAI can help to interpret, understand, and\nimprove trust in O-RAN operational networks."
                },
                "authors": [
                    {
                        "name": "Bouziane Brik"
                    },
                    {
                        "name": "Hatim Chergui"
                    },
                    {
                        "name": "Lanfranco Zanzi"
                    },
                    {
                        "name": "Francesco Devoti"
                    },
                    {
                        "name": "Adlen Ksentini"
                    },
                    {
                        "name": "Muhammad Shuaib Siddiqui"
                    },
                    {
                        "name": "Xavier Costa-PÃ©rez"
                    },
                    {
                        "name": "Christos Verikoukis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Verikoukis"
                },
                "author": "Christos Verikoukis",
                "arxiv_comment": "Accepted in IEEE Communications Surveys and Tutorials, 38 pages, 14\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.00319v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.00319v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09870v2",
                "updated": "2024-11-27T11:11:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    11,
                    11,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-13T15:08:49Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    15,
                    8,
                    49,
                    6,
                    287,
                    0
                ],
                "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in\n  Multiple Domains"
                },
                "summary": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply ourChroKnowPrompt,\nan in-depth prompting to elicit chronological knowledge by traversing\nstep-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge\nremains challenging. Existing approaches fall short in addressing the temporal\nadaptability of knowledge, often relying on a fixed time-point view. To\novercome this, we introduce ChroKnowBench, a benchmark dataset designed to\nevaluate chronologically accumulated knowledge across three key aspects:\nmultiple domains, time dependency, temporal state. Our benchmark distinguishes\nbetween knowledge that evolves (e.g., personal history, scientific discoveries,\namended laws) and knowledge that remain constant (e.g., mathematical truths,\ncommonsense facts). Building on this benchmark, we present ChroKnowledge\n(Chronological Categorization of Knowledge), a novel sampling-based framework\nfor evaluating LLMs' non-parametric chronological knowledge. Our evaluation led\nto the following observations: (1) The ability of eliciting temporal knowledge\nvaries depending on the data format that model was trained on. (2) LLMs\npartially recall knowledge or show a cut-off at temporal boundaries rather than\nrecalling all aspects of knowledge correctly. Thus, we apply ourChroKnowPrompt,\nan in-depth prompting to elicit chronological knowledge by traversing\nstep-by-step through the surrounding time spans. We observe that it\nsuccessfully recalls objects across both open-source and proprietary LLMs,\ndemonstrating versatility, though it faces challenges with dynamic datasets and\nunstructured formats."
                },
                "authors": [
                    {
                        "name": "Yein Park"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Jungwoo Park"
                    },
                    {
                        "name": "Donghyeon Lee"
                    },
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18220v1",
                "updated": "2024-11-27T10:57:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:57:06Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    57,
                    6,
                    2,
                    332,
                    0
                ],
                "title": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-MTLLMF: Resilient Multi-Task Large Language Model Fusion at the\n  Wireless Edge"
                },
                "summary": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task large language models (MTLLMs) are important for many applications\nat the wireless edge, where users demand specialized models to handle multiple\ntasks efficiently. However, training MTLLMs is complex and exhaustive,\nparticularly when tasks are subject to change. Recently, the concept of model\nfusion via task vectors has emerged as an efficient approach for combining\nfine-tuning parameters to produce an MTLLM. In this paper, the problem of\nenabling edge users to collaboratively craft such MTTLMs via tasks vectors is\nstudied, under the assumption of worst-case adversarial attacks. To this end,\nfirst the influence of adversarial noise to multi-task model fusion is\ninvestigated and a relationship between the so-called weight disentanglement\nerror and the mean squared error (MSE) is derived. Using hypothesis testing, it\nis directly shown that the MSE increases interference between task vectors,\nthereby rendering model fusion ineffective. Then, a novel resilient MTLLM\nfusion (R-MTLLMF) is proposed, which leverages insights about the LLM\narchitecture and fine-tuning process to safeguard task vector aggregation under\nadversarial noise by realigning the MTLLM. The proposed R-MTLLMF is then\ncompared for both worst-case and ideal transmission scenarios to study the\nimpact of the wireless channel. Extensive model fusion experiments with vision\nLLMs demonstrate R-MTLLMF's effectiveness, achieving close-to-baseline\nperformance across eight different tasks in ideal noise scenarios and\nsignificantly outperforming unprotected model fusion in worst-case scenarios.\nThe results further advocate for additional physical layer protection for a\nholistic approach to resilience, from both a wireless and LLM perspective."
                },
                "authors": [
                    {
                        "name": "Aladin Djuhera"
                    },
                    {
                        "name": "Vlad C. Andrei"
                    },
                    {
                        "name": "Mohsen Pourghasemian"
                    },
                    {
                        "name": "Haris Gacanin"
                    },
                    {
                        "name": "Holger Boche"
                    },
                    {
                        "name": "Walid Saad"
                    }
                ],
                "author_detail": {
                    "name": "Walid Saad"
                },
                "author": "Walid Saad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18216v1",
                "updated": "2024-11-27T10:48:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    48,
                    37,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:48:37Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    48,
                    37,
                    2,
                    332,
                    0
                ],
                "title": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving the Robustness of Security Attack Detectors\n  Generated by LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. However, LLMs struggle to generate accurate code, resulting,\ne.g., in attack detectors that miss well-known attacks when used in practice.\nThis is most likely due to the LLM lacking knowledge about some existing\nattacks and to the generated code being not evaluated in real usage scenarios.\nWe propose a novel approach integrating Retrieval Augmented Generation (RAG)\nand Self-Ranking into the LLM pipeline. RAG enhances the robustness of the\noutput by incorporating external knowledge sources, while the Self-Ranking\ntechnique, inspired to the concept of Self-Consistency, generates multiple\nreasoning paths and creates ranks to select the most robust detector. Our\nextensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance compared to baselines, with an increase of up to 71%pt and 37%pt in\nthe F2-Score for XSS and SQLi detection, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software development to\ngenerate functions, such as attack detectors, that implement security\nrequirements. However, LLMs struggle to generate accurate code, resulting,\ne.g., in attack detectors that miss well-known attacks when used in practice.\nThis is most likely due to the LLM lacking knowledge about some existing\nattacks and to the generated code being not evaluated in real usage scenarios.\nWe propose a novel approach integrating Retrieval Augmented Generation (RAG)\nand Self-Ranking into the LLM pipeline. RAG enhances the robustness of the\noutput by incorporating external knowledge sources, while the Self-Ranking\ntechnique, inspired to the concept of Self-Consistency, generates multiple\nreasoning paths and creates ranks to select the most robust detector. Our\nextensive empirical study targets code generated by LLMs to detect two\nprevalent injection attacks in web security: Cross-Site Scripting (XSS) and SQL\ninjection (SQLi). Results show a significant improvement in detection\nperformance compared to baselines, with an increase of up to 71%pt and 37%pt in\nthe F2-Score for XSS and SQLi detection, respectively."
                },
                "authors": [
                    {
                        "name": "Samuele Pasini"
                    },
                    {
                        "name": "Jinhan Kim"
                    },
                    {
                        "name": "Tommaso Aiello"
                    },
                    {
                        "name": "Rocio Cabrera Lozoya"
                    },
                    {
                        "name": "Antonino Sabetta"
                    },
                    {
                        "name": "Paolo Tonella"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Tonella"
                },
                "author": "Paolo Tonella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18211v1",
                "updated": "2024-11-27T10:45:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    45,
                    40,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:45:40Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    45,
                    40,
                    2,
                    332,
                    0
                ],
                "title": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding\n  with Superior Temporal Localization Ability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TimeMarker: A Versatile Video-LLM for Long and Short Video Understanding\n  with Superior Temporal Localization Ability"
                },
                "summary": "Rapid development of large language models (LLMs) has significantly advanced\nmultimodal large language models (LMMs), particularly in vision-language tasks.\nHowever, existing video-language models often overlook precise temporal\nlocalization and struggle with videos of varying lengths. We introduce\nTimeMarker, a versatile Video-LLM designed for high-quality dialogue based on\nvideo content, emphasizing temporal localization. TimeMarker integrates\nTemporal Separator Tokens to enhance temporal awareness, accurately marking\nspecific moments within videos. It employs the AnyLength mechanism for dynamic\nframe sampling and adaptive token merging, enabling effective handling of both\nshort and long videos. Additionally, TimeMarker utilizes diverse datasets,\nincluding further transformed temporal-related video QA datasets, to bolster\nits temporal understanding capabilities. Image and interleaved data are also\nemployed to further enhance the model's semantic perception ability.\nEvaluations demonstrate that TimeMarker achieves state-of-the-art performance\nacross multiple benchmarks, excelling in both short and long video categories.\nOur project page is at \\url{https://github.com/TimeMarker-LLM/TimeMarker/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid development of large language models (LLMs) has significantly advanced\nmultimodal large language models (LMMs), particularly in vision-language tasks.\nHowever, existing video-language models often overlook precise temporal\nlocalization and struggle with videos of varying lengths. We introduce\nTimeMarker, a versatile Video-LLM designed for high-quality dialogue based on\nvideo content, emphasizing temporal localization. TimeMarker integrates\nTemporal Separator Tokens to enhance temporal awareness, accurately marking\nspecific moments within videos. It employs the AnyLength mechanism for dynamic\nframe sampling and adaptive token merging, enabling effective handling of both\nshort and long videos. Additionally, TimeMarker utilizes diverse datasets,\nincluding further transformed temporal-related video QA datasets, to bolster\nits temporal understanding capabilities. Image and interleaved data are also\nemployed to further enhance the model's semantic perception ability.\nEvaluations demonstrate that TimeMarker achieves state-of-the-art performance\nacross multiple benchmarks, excelling in both short and long video categories.\nOur project page is at \\url{https://github.com/TimeMarker-LLM/TimeMarker/}."
                },
                "authors": [
                    {
                        "name": "Shimin Chen"
                    },
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01658v2",
                "updated": "2024-11-27T10:35:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    35,
                    19,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-03T07:01:37Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    7,
                    1,
                    37,
                    1,
                    247,
                    0
                ],
                "title": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language\n  Models with Pinpoint Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language\n  Models with Pinpoint Tuning"
                },
                "summary": "Large Language Models (LLMs) tend to prioritize adherence to user prompts\nover providing veracious responses, leading to the sycophancy issue. When\nchallenged by users, LLMs tend to admit mistakes and provide inaccurate\nresponses even if they initially provided the correct answer. Recent works\npropose to employ supervised fine-tuning (SFT) to mitigate the sycophancy\nissue, while it typically leads to the degeneration of LLMs' general\ncapability. To address the challenge, we propose a novel supervised pinpoint\ntuning (SPT), where the region-of-interest modules are tuned for a given\nobjective. Specifically, SPT first reveals and verifies a small percentage\n(<5%) of the basic modules, which significantly affect a particular behavior of\nLLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified\nmodules while freezing the rest. To verify the effectiveness of the proposed\nSPT, we conduct comprehensive experiments, demonstrating that SPT significantly\nmitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT\nintroduces limited or even no side effects on the general capability of LLMs.\nOur results shed light on how to precisely, effectively, and efficiently\nexplain and improve the targeted ability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to prioritize adherence to user prompts\nover providing veracious responses, leading to the sycophancy issue. When\nchallenged by users, LLMs tend to admit mistakes and provide inaccurate\nresponses even if they initially provided the correct answer. Recent works\npropose to employ supervised fine-tuning (SFT) to mitigate the sycophancy\nissue, while it typically leads to the degeneration of LLMs' general\ncapability. To address the challenge, we propose a novel supervised pinpoint\ntuning (SPT), where the region-of-interest modules are tuned for a given\nobjective. Specifically, SPT first reveals and verifies a small percentage\n(<5%) of the basic modules, which significantly affect a particular behavior of\nLLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified\nmodules while freezing the rest. To verify the effectiveness of the proposed\nSPT, we conduct comprehensive experiments, demonstrating that SPT significantly\nmitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT\nintroduces limited or even no side effects on the general capability of LLMs.\nOur results shed light on how to precisely, effectively, and efficiently\nexplain and improve the targeted ability of LLMs."
                },
                "authors": [
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Houqiang Li"
                    },
                    {
                        "name": "Le Lu"
                    },
                    {
                        "name": "Xinmei Tian"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yonggang Zhang"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted by ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12361v2",
                "updated": "2024-11-27T10:14:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    54,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-16T08:24:09Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    24,
                    9,
                    2,
                    290,
                    0
                ],
                "title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active\n  Assistance"
                },
                "summary": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by large language models have shown remarkable abilities in\nsolving complex tasks. However, most agent systems remain reactive, limiting\ntheir effectiveness in scenarios requiring foresight and autonomous\ndecision-making. In this paper, we tackle the challenge of developing proactive\nagents capable of anticipating and initiating tasks without explicit human\ninstructions. We propose a novel data-driven approach for this problem.\nFirstly, we collect real-world human activities to generate proactive task\npredictions. These predictions are then labeled by human annotators as either\naccepted or rejected. The labeled data is used to train a reward model that\nsimulates human judgment and serves as an automatic evaluator of the\nproactiveness of LLM agents. Building on this, we develop a comprehensive data\ngeneration pipeline to create a diverse dataset, ProactiveBench, containing\n6,790 events. Finally, we demonstrate that fine-tuning models with the proposed\nProactiveBench can significantly elicit the proactiveness of LLM agents.\nExperimental results show that our fine-tuned model achieves an F1-Score of\n66.47% in proactively offering assistance, outperforming all open-source and\nclose-source models. These results highlight the potential of our method in\ncreating more proactive and effective agent systems, paving the way for future\nadvancements in human-agent collaboration."
                },
                "authors": [
                    {
                        "name": "Yaxi Lu"
                    },
                    {
                        "name": "Shenzhi Yang"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Guirong Chen"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v1",
                "updated": "2024-11-27T10:14:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\nIn this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\nIn this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Qi Guo"
                    },
                    {
                        "name": "Xing Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xing Hu"
                },
                "author": "Xing Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2211.15656v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2211.15656v4",
                "updated": "2024-11-27T09:49:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    49,
                    25,
                    2,
                    332,
                    0
                ],
                "published": "2022-11-28T18:59:02Z",
                "published_parsed": [
                    2022,
                    11,
                    28,
                    18,
                    59,
                    2,
                    0,
                    332,
                    0
                ],
                "title": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperFusion: Multilevel LiDAR-Camera Fusion for Long-Range HD Map\n  Generation"
                },
                "summary": "High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-definition (HD) semantic map generation of the environment is an\nessential component of autonomous driving. Existing methods have achieved good\nperformance in this task by fusing different sensor modalities, such as LiDAR\nand camera. However, current works are based on raw data or network\nfeature-level fusion and only consider short-range HD map generation, limiting\ntheir deployment to realistic autonomous driving applications. In this paper,\nwe focus on the task of building the HD maps in both short ranges, i.e., within\n30 m, and also predicting long-range HD maps up to 90 m, which is required by\ndownstream path planning and control tasks to improve the smoothness and safety\nof autonomous driving. To this end, we propose a novel network named\nSuperFusion, exploiting the fusion of LiDAR and camera data at multiple levels.\nWe use LiDAR depth to improve image depth estimation and use image features to\nguide long-range LiDAR feature prediction. We benchmark our SuperFusion on the\nnuScenes dataset and a self-recorded dataset and show that it outperforms the\nstate-of-the-art baseline methods with large margins on all intervals.\nAdditionally, we apply the generated HD map to a downstream path planning task,\ndemonstrating that the long-range HD maps predicted by our method can lead to\nbetter path planning for autonomous vehicles. Our code has been released at\nhttps://github.com/haomo-ai/SuperFusion."
                },
                "authors": [
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Weihao Gu"
                    },
                    {
                        "name": "Xianjing Zhang"
                    },
                    {
                        "name": "Jintao Xu"
                    },
                    {
                        "name": "Rui Ai"
                    },
                    {
                        "name": "Huimin Lu"
                    },
                    {
                        "name": "Juho Kannala"
                    },
                    {
                        "name": "Xieyuanli Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xieyuanli Chen"
                },
                "author": "Xieyuanli Chen",
                "arxiv_comment": "ICRA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2211.15656v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2211.15656v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14945v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14945v3",
                "updated": "2024-11-27T09:43:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    43,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2023-12-06T15:24:01Z",
                "published_parsed": [
                    2023,
                    12,
                    6,
                    15,
                    24,
                    1,
                    2,
                    340,
                    0
                ],
                "title": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge\n  Base for Industrial Prognostics and Health Management"
                },
                "summary": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prognostics and health management (PHM) is essential for industrial operation\nand maintenance, focusing on predicting, diagnosing, and managing the health\nstatus of industrial systems. The emergence of the ChatGPT-Like large-scale\nlanguage model (LLM) has begun to lead a new round of innovation in the AI\nfield. It has extensively promoted the level of intelligence in various fields.\nTherefore, it is also expected further to change the application paradigm in\nindustrial PHM and promote PHM to become intelligent. Although ChatGPT-Like\nLLMs have rich knowledge reserves and powerful language understanding and\ngeneration capabilities, they lack domain-specific expertise, significantly\nlimiting their practicability in PHM applications. To this end, this study\nexplores the ChatGPT-Like LLM empowered by the local knowledge base (LKB) in\nindustrial PHM to solve the above limitations. In addition, we introduce the\nmethod and steps of combining the LKB with LLMs, including LKB preparation, LKB\nvectorization, prompt engineering, etc. Experimental analysis of real cases\nshows that combining the LKB with ChatGPT-Like LLM can significantly improve\nits performance and make ChatGPT-Like LLMs more accurate, relevant, and able to\nprovide more insightful information. This can promote the development of\nChatGPT-Like LLMs in industrial PHM and promote their efficiency and quality."
                },
                "authors": [
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Yan-Fu Li"
                    },
                    {
                        "name": "Min Xie"
                    }
                ],
                "author_detail": {
                    "name": "Min Xie"
                },
                "author": "Min Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14945v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14945v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18162v1",
                "updated": "2024-11-27T09:18:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    18,
                    26,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T09:18:26Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    18,
                    26,
                    2,
                    332,
                    0
                ],
                "title": "SentiXRL: An advanced large language Model Framework for Multilingual\n  Fine-Grained Emotion Classification in Complex Text Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentiXRL: An advanced large language Model Framework for Multilingual\n  Fine-Grained Emotion Classification in Complex Text Environment"
                },
                "summary": "With strong expressive capabilities in Large Language Models(LLMs),\ngenerative models effectively capture sentiment structures and deep semantics,\nhowever, challenges remain in fine-grained sentiment classification across\nmulti-lingual and complex contexts. To address this, we propose the Sentiment\nCross-Lingual Recognition and Logic Framework (SentiXRL), which incorporates\ntwo modules,an emotion retrieval enhancement module to improve sentiment\nclassification accuracy in complex contexts through historical dialogue and\nlogical reasoning,and a self-circulating analysis negotiation mechanism\n(SANM)to facilitates autonomous decision-making within a single model for\nclassification tasks.We have validated SentiXRL's superiority on multiple\nstandard datasets, outperforming existing models on CPED and CH-SIMS,and\nachieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, we\nunified labels across several fine-grained sentiment annotation datasets and\nconducted category confusion experiments, revealing challenges and impacts of\nclass imbalance in standard datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With strong expressive capabilities in Large Language Models(LLMs),\ngenerative models effectively capture sentiment structures and deep semantics,\nhowever, challenges remain in fine-grained sentiment classification across\nmulti-lingual and complex contexts. To address this, we propose the Sentiment\nCross-Lingual Recognition and Logic Framework (SentiXRL), which incorporates\ntwo modules,an emotion retrieval enhancement module to improve sentiment\nclassification accuracy in complex contexts through historical dialogue and\nlogical reasoning,and a self-circulating analysis negotiation mechanism\n(SANM)to facilitates autonomous decision-making within a single model for\nclassification tasks.We have validated SentiXRL's superiority on multiple\nstandard datasets, outperforming existing models on CPED and CH-SIMS,and\nachieving overall better performance on MELD,Emorynlp and IEMOCAP. Notably, we\nunified labels across several fine-grained sentiment annotation datasets and\nconducted category confusion experiments, revealing challenges and impacts of\nclass imbalance in standard datasets."
                },
                "authors": [
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Zhilin Zhang"
                    },
                    {
                        "name": "Jianhao Zeng"
                    },
                    {
                        "name": "Kaidi Wang"
                    },
                    {
                        "name": "Zhiyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyang Chen"
                },
                "author": "Zhiyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18157v1",
                "updated": "2024-11-27T09:04:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    4,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T09:04:47Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    9,
                    4,
                    47,
                    2,
                    332,
                    0
                ],
                "title": "A survey on cutting-edge relation extraction techniques based on\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A survey on cutting-edge relation extraction techniques based on\n  language models"
                },
                "summary": "This comprehensive survey delves into the latest advancements in Relation\nExtraction (RE), a pivotal task in natural language processing essential for\napplications across biomedical, financial, and legal sectors. This study\nhighlights the evolution and current state of RE techniques by analyzing 137\npapers presented at the Association for Computational Linguistics (ACL)\nconferences over the past four years, focusing on models that leverage language\nmodels. Our findings underscore the dominance of BERT-based methods in\nachieving state-of-the-art results for RE while also noting the promising\ncapabilities of emerging large language models (LLMs) like T5, especially in\nfew-shot relation extraction scenarios where they excel in identifying\npreviously unseen relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive survey delves into the latest advancements in Relation\nExtraction (RE), a pivotal task in natural language processing essential for\napplications across biomedical, financial, and legal sectors. This study\nhighlights the evolution and current state of RE techniques by analyzing 137\npapers presented at the Association for Computational Linguistics (ACL)\nconferences over the past four years, focusing on models that leverage language\nmodels. Our findings underscore the dominance of BERT-based methods in\nachieving state-of-the-art results for RE while also noting the promising\ncapabilities of emerging large language models (LLMs) like T5, especially in\nfew-shot relation extraction scenarios where they excel in identifying\npreviously unseen relations."
                },
                "authors": [
                    {
                        "name": "Jose A. Diaz-Garcia"
                    },
                    {
                        "name": "Julio Amador Diaz Lopez"
                    }
                ],
                "author_detail": {
                    "name": "Julio Amador Diaz Lopez"
                },
                "author": "Julio Amador Diaz Lopez",
                "arxiv_comment": "50 pages, under review in Artificial Intelligence Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18151v1",
                "updated": "2024-11-27T08:59:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    59,
                    34,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:59:34Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    59,
                    34,
                    2,
                    332,
                    0
                ],
                "title": "Howzat? Appealing to Expert Judgement for Evaluating Human and AI\n  Next-Step Hints for Novice Programmers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Howzat? Appealing to Expert Judgement for Evaluating Human and AI\n  Next-Step Hints for Novice Programmers"
                },
                "summary": "Motivation: Students learning to program often reach states where they are\nstuck and can make no forward progress. An automatically generated next-step\nhint can help them make forward progress and support their learning. It is\nimportant to know what makes a good hint or a bad hint, and how to generate\ngood hints automatically in novice programming tools, for example using Large\nLanguage Models (LLMs).\n  Method and participants: We recruited 44 Java educators from around the world\nto participate in an online study. We used a set of real student code states as\nhint-generation scenarios. Participants used a technique known as comparative\njudgement to rank a set of candidate next-step Java hints, which were generated\nby Large Language Models (LLMs) and by five human experienced educators.\nParticipants ranked the hints without being told how they were generated.\n  Findings: We found that LLMs had considerable variation in generating high\nquality next-step hints for programming novices, with GPT-4 outperforming other\nmodels tested. When used with a well-designed prompt, GPT-4 outperformed human\nexperts in generating pedagogically valuable hints. A multi-stage prompt was\nthe most effective LLM prompt. We found that the two most important factors of\na good hint were length (80--160 words being best), and reading level (US grade\n9 or below being best). Offering alternative approaches to solving the problem\nwas considered bad, and we found no effect of sentiment.\n  Conclusions: Automatic generation of these hints is immediately viable, given\nthat LLMs outperformed humans -- even when the students' task is unknown. The\nfact that only the best prompts achieve this outcome suggests that students on\ntheir own are unlikely to be able to produce the same benefit. The prompting\ntask, therefore, should be embedded in an expert-designed tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivation: Students learning to program often reach states where they are\nstuck and can make no forward progress. An automatically generated next-step\nhint can help them make forward progress and support their learning. It is\nimportant to know what makes a good hint or a bad hint, and how to generate\ngood hints automatically in novice programming tools, for example using Large\nLanguage Models (LLMs).\n  Method and participants: We recruited 44 Java educators from around the world\nto participate in an online study. We used a set of real student code states as\nhint-generation scenarios. Participants used a technique known as comparative\njudgement to rank a set of candidate next-step Java hints, which were generated\nby Large Language Models (LLMs) and by five human experienced educators.\nParticipants ranked the hints without being told how they were generated.\n  Findings: We found that LLMs had considerable variation in generating high\nquality next-step hints for programming novices, with GPT-4 outperforming other\nmodels tested. When used with a well-designed prompt, GPT-4 outperformed human\nexperts in generating pedagogically valuable hints. A multi-stage prompt was\nthe most effective LLM prompt. We found that the two most important factors of\na good hint were length (80--160 words being best), and reading level (US grade\n9 or below being best). Offering alternative approaches to solving the problem\nwas considered bad, and we found no effect of sentiment.\n  Conclusions: Automatic generation of these hints is immediately viable, given\nthat LLMs outperformed humans -- even when the students' task is unknown. The\nfact that only the best prompts achieve this outcome suggests that students on\ntheir own are unlikely to be able to produce the same benefit. The prompting\ntask, therefore, should be embedded in an expert-designed tool."
                },
                "authors": [
                    {
                        "name": "Neil C. C. Brown"
                    },
                    {
                        "name": "Pierre Weill-Tessier"
                    },
                    {
                        "name": "Juho Leinonen"
                    },
                    {
                        "name": "Paul Denny"
                    },
                    {
                        "name": "Michael KÃ¶lling"
                    }
                ],
                "author_detail": {
                    "name": "Michael KÃ¶lling"
                },
                "author": "Michael KÃ¶lling",
                "arxiv_comment": "36 pages, 14 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19226v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19226v2",
                "updated": "2024-11-27T08:50:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    50,
                    24,
                    2,
                    332,
                    0
                ],
                "published": "2024-06-27T14:51:07Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    14,
                    51,
                    7,
                    3,
                    179,
                    0
                ],
                "title": "Simulating Classroom Education with LLM-Empowered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Classroom Education with LLM-Empowered Agents"
                },
                "summary": "Large language models (LLMs) have been applied across various intelligent\neducational tasks to assist teaching. While preliminary studies have focused on\ntask-specific, independent LLM-empowered agents, the potential of LLMs within a\nmulti-agent collaborative framework for classroom simulation with real user\nparticipation remains unexplored. In this work, we propose SimClass, a\nmulti-agent classroom simulation teaching framework. We recognize\nrepresentative class roles and introduce a novel class control mechanism for\nautomatic classroom teaching, and conduct user experiments in two real-world\ncourses. Using the Flanders Interactive Analysis System and Community of\nInquiry theoretical frameworks from educational analysis, we demonstrate that\nLLMs can simulate a dynamic learning environment for users with active\nteacher-student and student-student interactions. We also observe group\nbehaviors among agents in SimClass, where agents collaborate to create\nenlivening interactions in classrooms to improve user learning process. We hope\nthis work pioneers the application of LLM-empowered multi-agent systems in\nvirtual classroom teaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been applied across various intelligent\neducational tasks to assist teaching. While preliminary studies have focused on\ntask-specific, independent LLM-empowered agents, the potential of LLMs within a\nmulti-agent collaborative framework for classroom simulation with real user\nparticipation remains unexplored. In this work, we propose SimClass, a\nmulti-agent classroom simulation teaching framework. We recognize\nrepresentative class roles and introduce a novel class control mechanism for\nautomatic classroom teaching, and conduct user experiments in two real-world\ncourses. Using the Flanders Interactive Analysis System and Community of\nInquiry theoretical frameworks from educational analysis, we demonstrate that\nLLMs can simulate a dynamic learning environment for users with active\nteacher-student and student-student interactions. We also observe group\nbehaviors among agents in SimClass, where agents collaborate to create\nenlivening interactions in classrooms to improve user learning process. We hope\nthis work pioneers the application of LLM-empowered multi-agent systems in\nvirtual classroom teaching."
                },
                "authors": [
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Daniel Zhang-Li"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Linlu Gong"
                    },
                    {
                        "name": "Jinchang Zhou"
                    },
                    {
                        "name": "Zhanxin Hao"
                    },
                    {
                        "name": "Jianxiao Jiang"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Huiqin Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19226v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19226v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12959v2",
                "updated": "2024-11-27T08:49:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    49,
                    12,
                    2,
                    332,
                    0
                ],
                "published": "2024-09-19T17:59:45Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    59,
                    45,
                    3,
                    263,
                    0
                ],
                "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal\n  Search Engines"
                },
                "summary": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has paved the way for AI search\nengines, e.g., SearchGPT, showcasing a new paradigm in human-internet\ninteraction. However, most current AI search engines are limited to text-only\nsettings, neglecting the multimodal user queries and the text-image interleaved\nnature of website information. Recently, Large Multimodal Models (LMMs) have\nmade impressive strides. Yet, whether they can function as AI search engines\nremains under-explored, leaving the potential of LMMs in multimodal search an\nopen question. To this end, we first design a delicate pipeline,\nMMSearch-Engine, to empower any LMMs with multimodal search capabilities. On\ntop of this, we introduce MMSearch, a comprehensive evaluation benchmark to\nassess the multimodal search performance of LMMs. The curated dataset contains\n300 manually collected instances spanning 14 subfields, which involves no\noverlap with the current LMMs' training data, ensuring the correct answer can\nonly be obtained within searching. By using MMSearch-Engine, the LMMs are\nevaluated by performing three individual tasks (requery, rerank, and\nsummarization), and one challenging end-to-end task with a complete searching\nprocess. We conduct extensive experiments on closed-source and open-source\nLMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best\nresults, which surpasses the commercial product, Perplexity Pro, in the\nend-to-end task, demonstrating the effectiveness of our proposed pipeline. We\nfurther present error analysis to unveil current LMMs still struggle to fully\ngrasp the multimodal search tasks, and conduct ablation study to indicate the\npotential of scaling test-time computation for AI search engine. We hope\nMMSearch may provide unique insights to guide the future development of\nmultimodal AI search engine. Project Page: https://mmsearch.github.io"
                },
                "authors": [
                    {
                        "name": "Dongzhi Jiang"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Yanmin Wu"
                    },
                    {
                        "name": "Jiayi Lei"
                    },
                    {
                        "name": "Pengshuo Qiu"
                    },
                    {
                        "name": "Pan Lu"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Guanglu Song"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Chunyuan Li"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://mmsearch.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18143v1",
                "updated": "2024-11-27T08:44:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    44,
                    41,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:44:41Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    44,
                    41,
                    2,
                    332,
                    0
                ],
                "title": "Harnessing Large Language Models for Seed Generation in Greybox Fuzzing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Seed Generation in Greybox Fuzzing"
                },
                "summary": "Greybox fuzzing has emerged as a preferred technique for discovering software\nbugs, striking a balance between efficiency and depth of exploration. While\nresearch has focused on improving fuzzing techniques, the importance of\nhigh-quality initial seeds remains critical yet often overlooked. Existing\nmethods for seed generation are limited, especially for programs with\nnon-standard or custom input formats. Large Language Models (LLMs) has\nrevolutionized numerous domains, showcasing unprecedented capabilities in\nunderstanding and generating complex patterns across various fields of\nknowledge. This paper introduces SeedMind, a novel system that leverages LLMs\nto boost greybox fuzzing through intelligent seed generation. Unlike previous\napproaches, SeedMind employs LLMs to create test case generators rather than\ndirectly producing test cases. Our approach implements an iterative,\nfeedback-driven process that guides the LLM to progressively refine test case\ngeneration, aiming for increased code coverage depth and breadth. In developing\nSeedMind, we addressed key challenges including input format limitations,\ncontext window constraints, and ensuring consistent, progress-aware behavior.\nIntensive evaluations with real-world applications show that SeedMind\neffectively harnesses LLMs to generate high-quality test cases and facilitate\nfuzzing in bug finding, presenting utility comparable to human-created seeds\nand significantly outperforming the existing LLM-based solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Greybox fuzzing has emerged as a preferred technique for discovering software\nbugs, striking a balance between efficiency and depth of exploration. While\nresearch has focused on improving fuzzing techniques, the importance of\nhigh-quality initial seeds remains critical yet often overlooked. Existing\nmethods for seed generation are limited, especially for programs with\nnon-standard or custom input formats. Large Language Models (LLMs) has\nrevolutionized numerous domains, showcasing unprecedented capabilities in\nunderstanding and generating complex patterns across various fields of\nknowledge. This paper introduces SeedMind, a novel system that leverages LLMs\nto boost greybox fuzzing through intelligent seed generation. Unlike previous\napproaches, SeedMind employs LLMs to create test case generators rather than\ndirectly producing test cases. Our approach implements an iterative,\nfeedback-driven process that guides the LLM to progressively refine test case\ngeneration, aiming for increased code coverage depth and breadth. In developing\nSeedMind, we addressed key challenges including input format limitations,\ncontext window constraints, and ensuring consistent, progress-aware behavior.\nIntensive evaluations with real-world applications show that SeedMind\neffectively harnesses LLMs to generate high-quality test cases and facilitate\nfuzzing in bug finding, presenting utility comparable to human-created seeds\nand significantly outperforming the existing LLM-based solutions."
                },
                "authors": [
                    {
                        "name": "Wenxuan Shi"
                    },
                    {
                        "name": "Yunhang Zhang"
                    },
                    {
                        "name": "Xinyu Xing"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18138v1",
                "updated": "2024-11-27T08:38:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    38,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:38:57Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    38,
                    57,
                    2,
                    332,
                    0
                ],
                "title": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and\n  Generation"
                },
                "summary": "Full-duplex multimodal large language models (LLMs) provide a unified\nframework for addressing diverse speech understanding and generation tasks,\nenabling more natural and seamless human-machine conversations. Unlike\ntraditional modularised conversational AI systems, which separate speech\nrecognition, understanding, and text-to-speech generation into distinct\ncomponents, multimodal LLMs operate as single end-to-end models. This\nstreamlined design eliminates error propagation across components and fully\nleverages the rich non-verbal information embedded in input speech signals. We\nintroduce SALMONN-omni, a codec-free, full-duplex speech understanding and\ngeneration model capable of simultaneously listening to its own generated\nspeech and background sounds while speaking. To support this capability, we\npropose a novel duplex spoken dialogue framework incorporating a ``thinking''\nmechanism that facilitates asynchronous text and speech generation relying on\nembeddings instead of codecs (quantized speech and audio tokens). Experimental\nresults demonstrate SALMONN-omni's versatility across a broad range of\nstreaming speech tasks, including speech recognition, speech enhancement, and\nspoken question answering. Additionally, SALMONN-omni excels at managing\nturn-taking, barge-in, and echo cancellation scenarios, establishing its\npotential as a robust prototype for full-duplex conversational AI systems. To\nthe best of our knowledge, SALMONN-omni is the first codec-free model of its\nkind. A full technical report along with model checkpoints will be released\nsoon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex multimodal large language models (LLMs) provide a unified\nframework for addressing diverse speech understanding and generation tasks,\nenabling more natural and seamless human-machine conversations. Unlike\ntraditional modularised conversational AI systems, which separate speech\nrecognition, understanding, and text-to-speech generation into distinct\ncomponents, multimodal LLMs operate as single end-to-end models. This\nstreamlined design eliminates error propagation across components and fully\nleverages the rich non-verbal information embedded in input speech signals. We\nintroduce SALMONN-omni, a codec-free, full-duplex speech understanding and\ngeneration model capable of simultaneously listening to its own generated\nspeech and background sounds while speaking. To support this capability, we\npropose a novel duplex spoken dialogue framework incorporating a ``thinking''\nmechanism that facilitates asynchronous text and speech generation relying on\nembeddings instead of codecs (quantized speech and audio tokens). Experimental\nresults demonstrate SALMONN-omni's versatility across a broad range of\nstreaming speech tasks, including speech recognition, speech enhancement, and\nspoken question answering. Additionally, SALMONN-omni excels at managing\nturn-taking, barge-in, and echo cancellation scenarios, establishing its\npotential as a robust prototype for full-duplex conversational AI systems. To\nthe best of our knowledge, SALMONN-omni is the first codec-free model of its\nkind. A full technical report along with model checkpoints will be released\nsoon."
                },
                "authors": [
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Xiaoyu Yang"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v3",
                "updated": "2024-11-27T08:17:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    17,
                    9,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18126v1",
                "updated": "2024-11-27T08:16:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    16,
                    41,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T08:16:41Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    16,
                    41,
                    2,
                    332,
                    0
                ],
                "title": "Curriculum Demonstration Selection for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum Demonstration Selection for In-Context Learning"
                },
                "summary": "Large Language Models (LLMs) have shown strong in-context learning (ICL)\nabilities with a few demonstrations. However, one critical challenge is how to\nselect demonstrations to elicit the full potential of LLMs. In this paper, we\npropose Curriculum Demonstration Selection (CDS), a novel demonstration\nselection method for ICL. Instead of merely using similarity, CDS additionally\npartitions samples by their complexity measurements. Following curriculum\nlearning, CDS then selects demonstrations from easy to difficult. Thus the\nselected demonstrations cover a wide range of difficulty levels, enabling LLMs\nto learn from varied complexities within the training set. Experiments\ndemonstrate that our CDS consistently outperforms baseline methods, achieving\nnotable improvements across nine LLMs on three benchmarks. Moreover, CDS proves\nespecially effective in enhancing LLM performance in solving challenging\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown strong in-context learning (ICL)\nabilities with a few demonstrations. However, one critical challenge is how to\nselect demonstrations to elicit the full potential of LLMs. In this paper, we\npropose Curriculum Demonstration Selection (CDS), a novel demonstration\nselection method for ICL. Instead of merely using similarity, CDS additionally\npartitions samples by their complexity measurements. Following curriculum\nlearning, CDS then selects demonstrations from easy to difficult. Thus the\nselected demonstrations cover a wide range of difficulty levels, enabling LLMs\nto learn from varied complexities within the training set. Experiments\ndemonstrate that our CDS consistently outperforms baseline methods, achieving\nnotable improvements across nine LLMs on three benchmarks. Moreover, CDS proves\nespecially effective in enhancing LLM performance in solving challenging\nproblems."
                },
                "authors": [
                    {
                        "name": "Duc Anh Vu"
                    },
                    {
                        "name": "Nguyen Tran Cong Duy"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Hoang Minh Nhat"
                    },
                    {
                        "name": "Du Mingzhe"
                    },
                    {
                        "name": "Nguyen Thanh Thong"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "arxiv_comment": "Accepted at the 40th ACM/SIGAPP Symposium On Applied Computing (SAC\n  2025), Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08681v2",
                "updated": "2024-11-27T08:09:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    9,
                    56,
                    2,
                    332,
                    0
                ],
                "published": "2024-04-08T07:36:26Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    7,
                    36,
                    26,
                    0,
                    99,
                    0
                ],
                "title": "EFSA: Towards Event-Level Financial Sentiment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EFSA: Towards Event-Level Financial Sentiment Analysis"
                },
                "summary": "In this paper, we extend financial sentiment analysis~(FSA) to event-level\nsince events usually serve as the subject of the sentiment in financial text.\nThough extracting events from the financial text may be conducive to accurate\nsentiment predictions, it has specialized challenges due to the lengthy and\ndiscontinuity of events in a financial text. To this end, we reconceptualize\nthe event extraction as a classification task by designing a categorization\ncomprising coarse-grained and fine-grained event categories. Under this\nsetting, we formulate the \\textbf{E}vent-Level \\textbf{F}inancial\n\\textbf{S}entiment \\textbf{A}nalysis~(\\textbf{EFSA} for short) task that\noutputs quintuples consisting of (company, industry, coarse-grained event,\nfine-grained event, sentiment) from financial text. A large-scale Chinese\ndataset containing $12,160$ news articles and $13,725$ quintuples is publicized\nas a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based\napproach is devised for this task. Systematically investigations are conducted\non our dataset, and the empirical results demonstrate the benchmarking scores\nof existing methods and our proposed method can reach the current\nstate-of-the-art. Our dataset and framework implementation are available at\nhttps://anonymous.4open.science/r/EFSA-645E",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we extend financial sentiment analysis~(FSA) to event-level\nsince events usually serve as the subject of the sentiment in financial text.\nThough extracting events from the financial text may be conducive to accurate\nsentiment predictions, it has specialized challenges due to the lengthy and\ndiscontinuity of events in a financial text. To this end, we reconceptualize\nthe event extraction as a classification task by designing a categorization\ncomprising coarse-grained and fine-grained event categories. Under this\nsetting, we formulate the \\textbf{E}vent-Level \\textbf{F}inancial\n\\textbf{S}entiment \\textbf{A}nalysis~(\\textbf{EFSA} for short) task that\noutputs quintuples consisting of (company, industry, coarse-grained event,\nfine-grained event, sentiment) from financial text. A large-scale Chinese\ndataset containing $12,160$ news articles and $13,725$ quintuples is publicized\nas a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based\napproach is devised for this task. Systematically investigations are conducted\non our dataset, and the empirical results demonstrate the benchmarking scores\nof existing methods and our proposed method can reach the current\nstate-of-the-art. Our dataset and framework implementation are available at\nhttps://anonymous.4open.science/r/EFSA-645E"
                },
                "authors": [
                    {
                        "name": "Tianyu Chen"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Guoxin Yu"
                    },
                    {
                        "name": "Dapeng Zhang"
                    },
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Qing He"
                    },
                    {
                        "name": "Xiang Ao"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Ao"
                },
                "author": "Xiang Ao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18111v1",
                "updated": "2024-11-27T07:45:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    45,
                    25,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T07:45:25Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    45,
                    25,
                    2,
                    332,
                    0
                ],
                "title": "When Large Vision-Language Models Meet Person Re-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Large Vision-Language Models Meet Person Re-Identification"
                },
                "summary": "Large Vision-Language Models (LVLMs) that incorporate visual models and Large\nLanguage Models (LLMs) have achieved impressive results across various\ncross-modal understanding and reasoning tasks. In recent years, person\nre-identification (ReID) has also started to explore cross-modal semantics to\nimprove the accuracy of identity recognition. However, effectively utilizing\nLVLMs for ReID remains an open challenge. While LVLMs operate under a\ngenerative paradigm by predicting the next output word, ReID requires the\nextraction of discriminative identity features to match pedestrians across\ncameras. In this paper, we propose LVLM-ReID, a novel framework that harnesses\nthe strengths of LVLMs to promote ReID. Specifically, we employ instructions to\nguide the LVLM in generating one pedestrian semantic token that encapsulates\nkey appearance semantics from the person image. This token is further refined\nthrough our Semantic-Guided Interaction (SGI) module, establishing a reciprocal\ninteraction between the semantic token and visual tokens. Ultimately, the\nreinforced semantic token serves as the pedestrian identity representation. Our\nframework integrates the semantic understanding and generation capabilities of\nLVLMs into end-to-end ReID training, allowing LVLMs to capture rich semantic\ncues from pedestrian images during both training and inference. Our method\nachieves competitive results on multiple benchmarks without additional\nimage-text annotations, demonstrating the potential of LVLM-generated semantics\nto advance person ReID and offering a promising direction for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) that incorporate visual models and Large\nLanguage Models (LLMs) have achieved impressive results across various\ncross-modal understanding and reasoning tasks. In recent years, person\nre-identification (ReID) has also started to explore cross-modal semantics to\nimprove the accuracy of identity recognition. However, effectively utilizing\nLVLMs for ReID remains an open challenge. While LVLMs operate under a\ngenerative paradigm by predicting the next output word, ReID requires the\nextraction of discriminative identity features to match pedestrians across\ncameras. In this paper, we propose LVLM-ReID, a novel framework that harnesses\nthe strengths of LVLMs to promote ReID. Specifically, we employ instructions to\nguide the LVLM in generating one pedestrian semantic token that encapsulates\nkey appearance semantics from the person image. This token is further refined\nthrough our Semantic-Guided Interaction (SGI) module, establishing a reciprocal\ninteraction between the semantic token and visual tokens. Ultimately, the\nreinforced semantic token serves as the pedestrian identity representation. Our\nframework integrates the semantic understanding and generation capabilities of\nLVLMs into end-to-end ReID training, allowing LVLMs to capture rich semantic\ncues from pedestrian images during both training and inference. Our method\nachieves competitive results on multiple benchmarks without additional\nimage-text annotations, demonstrating the potential of LVLM-generated semantics\nto advance person ReID and offering a promising direction for future research."
                },
                "authors": [
                    {
                        "name": "Qizao Wang"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xiangyang Xue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Xue"
                },
                "author": "Xiangyang Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12762v2",
                "updated": "2024-11-27T07:41:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    41,
                    35,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-16T13:07:13Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    7,
                    13,
                    5,
                    321,
                    0
                ],
                "title": "Playing Language Game with LLMs Leads to Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Playing Language Game with LLMs Leads to Jailbreaking"
                },
                "summary": "The advent of large language models (LLMs) has spurred the development of\nnumerous jailbreak techniques aimed at circumventing their security defenses\nagainst malicious attacks. An effective jailbreak approach is to identify a\ndomain where safety generalization fails, a phenomenon known as mismatched\ngeneralization. In this paper, we introduce two novel jailbreak methods based\non mismatched generalization: natural language games and custom language games,\nboth of which effectively bypass the safety mechanisms of LLMs, with various\nkinds and different variants, making them hard to defend and leading to high\nattack rates. Natural language games involve the use of synthetic linguistic\nconstructs and the actions intertwined with these constructs, such as the Ubbi\nDubbi language. Building on this phenomenon, we propose the custom language\ngames method: by engaging with LLMs using a variety of custom rules, we\nsuccessfully execute jailbreak attacks across multiple LLM platforms. Extensive\nexperiments demonstrate the effectiveness of our methods, achieving success\nrates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet.\nFurthermore, to investigate the generalizability of safety alignments, we\nfine-tuned Llama-3.1-70B with the custom language games to achieve safety\nalignment within our datasets and found that when interacting through other\nlanguage games, the fine-tuned models still failed to identify harmful content.\nThis finding indicates that the safety alignment knowledge embedded in LLMs\nfails to generalize across different linguistic formats, thus opening new\navenues for future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has spurred the development of\nnumerous jailbreak techniques aimed at circumventing their security defenses\nagainst malicious attacks. An effective jailbreak approach is to identify a\ndomain where safety generalization fails, a phenomenon known as mismatched\ngeneralization. In this paper, we introduce two novel jailbreak methods based\non mismatched generalization: natural language games and custom language games,\nboth of which effectively bypass the safety mechanisms of LLMs, with various\nkinds and different variants, making them hard to defend and leading to high\nattack rates. Natural language games involve the use of synthetic linguistic\nconstructs and the actions intertwined with these constructs, such as the Ubbi\nDubbi language. Building on this phenomenon, we propose the custom language\ngames method: by engaging with LLMs using a variety of custom rules, we\nsuccessfully execute jailbreak attacks across multiple LLM platforms. Extensive\nexperiments demonstrate the effectiveness of our methods, achieving success\nrates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet.\nFurthermore, to investigate the generalizability of safety alignments, we\nfine-tuned Llama-3.1-70B with the custom language games to achieve safety\nalignment within our datasets and found that when interacting through other\nlanguage games, the fine-tuned models still failed to identify harmful content.\nThis finding indicates that the safety alignment knowledge embedded in LLMs\nfails to generalize across different linguistic formats, thus opening new\navenues for future research in this area."
                },
                "authors": [
                    {
                        "name": "Yu Peng"
                    },
                    {
                        "name": "Zewen Long"
                    },
                    {
                        "name": "Fangming Dong"
                    },
                    {
                        "name": "Congyi Li"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18104v1",
                "updated": "2024-11-27T07:32:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    32,
                    56,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T07:32:56Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    32,
                    56,
                    2,
                    332,
                    0
                ],
                "title": "Training and Evaluating Language Models with Template-based Data\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and Evaluating Language Models with Template-based Data\n  Generation"
                },
                "summary": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Zhang"
                },
                "author": "Yifan Zhang",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06208v2",
                "updated": "2024-11-27T07:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    29,
                    59,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-09T15:12:43Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    15,
                    12,
                    43,
                    5,
                    314,
                    0
                ],
                "title": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IOPO: Empowering LLMs with Complex Instruction Following via\n  Input-Output Preference Optimization"
                },
                "summary": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the realm of large language models (LLMs), the ability of models to\naccurately follow instructions is paramount as more agents and applications\nleverage LLMs for construction, where the complexity of instructions are\nrapidly increasing. However, on the one hand, there is only a certain amount of\ncomplex instruction evaluation data; on the other hand, there are no dedicated\nalgorithms to improve the ability to follow complex instructions. To this end,\nthis paper introduces TRACE, a benchmark for improving and evaluating the\ncomplex instructionfollowing ability, which consists of 120K training data and\n1K evaluation data. Furthermore, we propose IOPO (Input-Output Preference\nOptimization) alignment method which takes both input and output preference\npairs into consideration, where LLMs not only rapidly align with response\npreferences but also meticulously explore the instruction preferences.\nExtensive experiments on both in-domain and outof-domain datasets confirm the\neffectiveness of IOPO, showing 8.15%, 2.18% improvements on in-domain data and\n6.29%, 3.13% on outof-domain data compared to SFT and DPO respectively."
                },
                "authors": [
                    {
                        "name": "Xinghua Zhang"
                    },
                    {
                        "name": "Haiyang Yu"
                    },
                    {
                        "name": "Cheng Fu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15105v3",
                "updated": "2024-11-27T07:26:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    26,
                    34,
                    2,
                    332,
                    0
                ],
                "published": "2024-02-23T05:30:32Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    5,
                    30,
                    32,
                    4,
                    54,
                    0
                ],
                "title": "A First Look at GPT Apps: Landscape and Vulnerability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look at GPT Apps: Landscape and Vulnerability"
                },
                "summary": "Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the\nlaunch of dedicated LLM app stores. Nevertheless, given its debut, there is a\nlack of sufficient understanding of this new ecosystem. To fill this gap, this\npaper presents a first comprehensive longitudinal (5-month) study of the\nevolution, landscape, and vulnerability of the emerging LLM app ecosystem,\nfocusing on two GPT app stores: \\textit{GPTStore.AI} and the official\n\\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a\nTriLevel configuration extraction strategy to efficiently gather metadata (\\ie\nnames, creators, descriptions, \\etc) and user feedback for all GPT apps across\nthese two stores, as well as configurations (\\ie system prompts, knowledge\nfiles, and APIs) for the top 10,000 popular apps. Our extensive analysis\nreveals: (1) the user enthusiasm for GPT apps consistently rises, whereas\ncreator interest plateaus within three months of GPTs' launch; (2) nearly 90\\%\nsystem prompts can be easily accessed due to widespread failure to secure GPT\napp configurations, leading to considerable plagiarism and duplication among\napps. Our findings highlight the necessity of enhancing the LLM app ecosystem\nby the app stores, creators, and users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following OpenAI's introduction of GPTs, a surge in GPT apps has led to the\nlaunch of dedicated LLM app stores. Nevertheless, given its debut, there is a\nlack of sufficient understanding of this new ecosystem. To fill this gap, this\npaper presents a first comprehensive longitudinal (5-month) study of the\nevolution, landscape, and vulnerability of the emerging LLM app ecosystem,\nfocusing on two GPT app stores: \\textit{GPTStore.AI} and the official\n\\textit{OpenAI GPT Store}. Specifically, we develop two automated tools and a\nTriLevel configuration extraction strategy to efficiently gather metadata (\\ie\nnames, creators, descriptions, \\etc) and user feedback for all GPT apps across\nthese two stores, as well as configurations (\\ie system prompts, knowledge\nfiles, and APIs) for the top 10,000 popular apps. Our extensive analysis\nreveals: (1) the user enthusiasm for GPT apps consistently rises, whereas\ncreator interest plateaus within three months of GPTs' launch; (2) nearly 90\\%\nsystem prompts can be easily accessed due to widespread failure to secure GPT\napp configurations, leading to considerable plagiarism and duplication among\napps. Our findings highlight the necessity of enhancing the LLM app ecosystem\nby the app stores, creators, and users."
                },
                "authors": [
                    {
                        "name": "Zejun Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Anlan Zhang"
                    },
                    {
                        "name": "Mengwei Xu"
                    },
                    {
                        "name": "Feng Qian"
                    }
                ],
                "author_detail": {
                    "name": "Feng Qian"
                },
                "author": "Feng Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06387v3",
                "updated": "2024-11-27T07:25:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    25,
                    2,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-10T08:11:05Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    11,
                    5,
                    6,
                    315,
                    0
                ],
                "title": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Training Meets Consistency: Improving LLMs' Reasoning With\n  Consistency-Driven Rationale Evaluation"
                },
                "summary": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-training approach for large language models (LLMs) improves reasoning\nabilities by training the models on their self-generated rationales. Previous\napproaches have labeled rationales that produce correct answers for a given\nquestion as appropriate for training. However, a single measure risks\nmisjudging rationale quality, leading the models to learn flawed reasoning\npatterns. To address this issue, we propose CREST (Consistency-driven Rationale\nEvaluation for Self-Training), a self-training framework that further evaluates\neach rationale through follow-up questions and leverages this evaluation to\nguide its training. Specifically, we introduce two methods: (1) filtering out\nrationales that frequently result in incorrect answers on follow-up questions\nand (2) preference learning based on mixed preferences from rationale\nevaluation results of both original and follow-up questions. Experiments on\nthree question-answering datasets using open LLMs show that CREST not only\nimproves the logical robustness and correctness of rationales but also improves\nreasoning abilities compared to previous self-training approaches."
                },
                "authors": [
                    {
                        "name": "Jaehyeok Lee"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "JinYeong Bak"
                    }
                ],
                "author_detail": {
                    "name": "JinYeong Bak"
                },
                "author": "JinYeong Bak",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03321v2",
                "updated": "2024-11-27T07:05:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    5,
                    31,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-21T06:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    6,
                    18,
                    53,
                    0,
                    295,
                    0
                ],
                "title": "Towards More Accurate US Presidential Election via Multi-step Reasoning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Accurate US Presidential Election via Multi-step Reasoning\n  with Large Language Models"
                },
                "summary": "Can Large Language Models (LLMs) accurately predict election outcomes? While\nLLMs have demonstrated impressive performance in various domains, including\nhealthcare, legal analysis, and creative tasks, their ability to forecast\nelections remains unknown. Election prediction poses unique challenges, such as\nlimited voter-level data, rapidly changing political landscapes, and the need\nto model complex human behavior. To address these challenges, we introduce a\nmulti-step reasoning framework designed for political analysis. Our approach is\nvalidated on real-world data from the American National Election Studies (ANES)\n2016 and 2020, as well as synthetic personas generated by the leading machine\nlearning framework, offering scalable datasets for voter behavior modeling. To\ncapture temporal dynamics, we incorporate candidates' policy positions and\nbiographical details, ensuring that the model adapts to evolving political\ncontexts. Drawing on Chain of Thought prompting, our multi-step reasoning\npipeline systematically integrates demographic, ideological, and time-dependent\nfactors, enhancing the model's predictive power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models (LLMs) accurately predict election outcomes? While\nLLMs have demonstrated impressive performance in various domains, including\nhealthcare, legal analysis, and creative tasks, their ability to forecast\nelections remains unknown. Election prediction poses unique challenges, such as\nlimited voter-level data, rapidly changing political landscapes, and the need\nto model complex human behavior. To address these challenges, we introduce a\nmulti-step reasoning framework designed for political analysis. Our approach is\nvalidated on real-world data from the American National Election Studies (ANES)\n2016 and 2020, as well as synthetic personas generated by the leading machine\nlearning framework, offering scalable datasets for voter behavior modeling. To\ncapture temporal dynamics, we incorporate candidates' policy positions and\nbiographical details, ensuring that the model adapts to evolving political\ncontexts. Drawing on Chain of Thought prompting, our multi-step reasoning\npipeline systematically integrates demographic, ideological, and time-dependent\nfactors, enhancing the model's predictive power."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Yuangang Li"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "This research is ongoing work. Xiyang Hu and Yue Zhao are the\n  corresponding authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18092v1",
                "updated": "2024-11-27T07:04:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    4,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T07:04:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    7,
                    4,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Training Noise Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Noise Token Pruning"
                },
                "summary": "In the present work we present Training Noise Token (TNT) Pruning for vision\ntransformers. Our method relaxes the discrete token dropping condition to\ncontinuous additive noise, providing smooth optimization in training, while\nretaining discrete dropping computational gains in deployment settings. We\nprovide theoretical connections to Rate-Distortion literature, and empirical\nevaluations on the ImageNet dataset using ViT and DeiT architectures\ndemonstrating TNT's advantages over previous pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the present work we present Training Noise Token (TNT) Pruning for vision\ntransformers. Our method relaxes the discrete token dropping condition to\ncontinuous additive noise, providing smooth optimization in training, while\nretaining discrete dropping computational gains in deployment settings. We\nprovide theoretical connections to Rate-Distortion literature, and empirical\nevaluations on the ImageNet dataset using ViT and DeiT architectures\ndemonstrating TNT's advantages over previous pruning methods."
                },
                "authors": [
                    {
                        "name": "Mingxing Rao"
                    },
                    {
                        "name": "Bohan Jiang"
                    },
                    {
                        "name": "Daniel Moyer"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Moyer"
                },
                "author": "Daniel Moyer",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08603v2",
                "updated": "2024-11-27T06:20:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    20,
                    8,
                    2,
                    332,
                    0
                ],
                "published": "2024-05-14T13:42:05Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    13,
                    42,
                    5,
                    1,
                    135,
                    0
                ],
                "title": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine"
                },
                "summary": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems."
                },
                "authors": [
                    {
                        "name": "Hanguang Xiao"
                    },
                    {
                        "name": "Feizhong Zhou"
                    },
                    {
                        "name": "Xingyue Liu"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Zhipeng Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xiaoxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Huang"
                },
                "author": "Xiaoxuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v1",
                "updated": "2024-11-27T06:10:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV\n  Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18071v1",
                "updated": "2024-11-27T05:48:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    48,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T05:48:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    48,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses\n  about Real-World Entities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses\n  about Real-World Entities"
                },
                "summary": "Do horror writers have worse childhoods than other writers? Though\nbiographical details are known about many writers, quantitatively exploring\nsuch a qualitative hypothesis requires significant human effort, e.g. to sift\nthrough many biographies and interviews of writers and to iteratively search\nfor quantitative features that reflect what is qualitatively of interest. This\npaper explores the potential to quickly prototype these kinds of hypotheses\nthrough (1) applying LLMs to estimate properties of concrete entities like\nspecific people, companies, books, kinds of animals, and countries; (2)\nperforming off-the-shelf analysis methods to reveal possible relationships\namong such properties (e.g. linear regression); and towards further automation,\n(3) applying LLMs to suggest the quantitative properties themselves that could\nhelp ground a particular qualitative hypothesis (e.g. number of adverse\nchildhood events, in the context of the running example). The hope is to allow\nsifting through hypotheses more quickly through collaboration between human and\nmachine. Our experiments highlight that indeed, LLMs can serve as useful\nestimators of tabular data about specific entities across a range of domains,\nand that such estimations improve with model scale. Further, initial\nexperiments demonstrate the potential of LLMs to map a qualitative hypothesis\nof interest to relevant concrete variables that the LLM can then estimate. The\nconclusion is that LLMs offer intriguing potential to help illuminate\nscientifically interesting patterns latent within the internet-scale data they\nare trained upon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do horror writers have worse childhoods than other writers? Though\nbiographical details are known about many writers, quantitatively exploring\nsuch a qualitative hypothesis requires significant human effort, e.g. to sift\nthrough many biographies and interviews of writers and to iteratively search\nfor quantitative features that reflect what is qualitatively of interest. This\npaper explores the potential to quickly prototype these kinds of hypotheses\nthrough (1) applying LLMs to estimate properties of concrete entities like\nspecific people, companies, books, kinds of animals, and countries; (2)\nperforming off-the-shelf analysis methods to reveal possible relationships\namong such properties (e.g. linear regression); and towards further automation,\n(3) applying LLMs to suggest the quantitative properties themselves that could\nhelp ground a particular qualitative hypothesis (e.g. number of adverse\nchildhood events, in the context of the running example). The hope is to allow\nsifting through hypotheses more quickly through collaboration between human and\nmachine. Our experiments highlight that indeed, LLMs can serve as useful\nestimators of tabular data about specific entities across a range of domains,\nand that such estimations improve with model scale. Further, initial\nexperiments demonstrate the potential of LLMs to map a qualitative hypothesis\nof interest to relevant concrete variables that the LLM can then estimate. The\nconclusion is that LLMs offer intriguing potential to help illuminate\nscientifically interesting patterns latent within the internet-scale data they\nare trained upon."
                },
                "authors": [
                    {
                        "name": "Miguel Zabaleta"
                    },
                    {
                        "name": "Joel Lehman"
                    }
                ],
                "author_detail": {
                    "name": "Joel Lehman"
                },
                "author": "Joel Lehman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05160v2",
                "updated": "2024-11-27T05:48:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    48,
                    34,
                    2,
                    332,
                    0
                ],
                "published": "2024-05-08T15:52:50Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    52,
                    50,
                    2,
                    129,
                    0
                ],
                "title": "Selective Classification Under Distribution Shifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Classification Under Distribution Shifts"
                },
                "summary": "In selective classification (SC), a classifier abstains from making\npredictions that are likely to be wrong to avoid excessive errors. To deploy\nimperfect classifiers -- either due to intrinsic statistical noise of data or\nfor robustness issue of the classifier or beyond -- in high-stakes scenarios,\nSC appears to be an attractive and necessary path to follow. Despite decades of\nresearch in SC, most previous SC methods still focus on the ideal statistical\nsetting only, i.e., the data distribution at deployment is the same as that of\ntraining, although practical data can come from the wild. To bridge this gap,\nin this paper, we propose an SC framework that takes into account distribution\nshifts, termed generalized selective classification, that covers label-shifted\n(or out-of-distribution) and covariate-shifted samples, in addition to typical\nin-distribution samples, the first of its kind in the SC literature. We focus\non non-training-based confidence-score functions for generalized SC on deep\nlearning (DL) classifiers, and propose two novel margin-based score functions.\nThrough extensive analysis and experiments, we show that our proposed score\nfunctions are more effective and reliable than the existing ones for\ngeneralized SC on a variety of classification tasks and DL classifiers. Code is\navailable at https://github.com/sun-umn/sc_with_distshift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In selective classification (SC), a classifier abstains from making\npredictions that are likely to be wrong to avoid excessive errors. To deploy\nimperfect classifiers -- either due to intrinsic statistical noise of data or\nfor robustness issue of the classifier or beyond -- in high-stakes scenarios,\nSC appears to be an attractive and necessary path to follow. Despite decades of\nresearch in SC, most previous SC methods still focus on the ideal statistical\nsetting only, i.e., the data distribution at deployment is the same as that of\ntraining, although practical data can come from the wild. To bridge this gap,\nin this paper, we propose an SC framework that takes into account distribution\nshifts, termed generalized selective classification, that covers label-shifted\n(or out-of-distribution) and covariate-shifted samples, in addition to typical\nin-distribution samples, the first of its kind in the SC literature. We focus\non non-training-based confidence-score functions for generalized SC on deep\nlearning (DL) classifiers, and propose two novel margin-based score functions.\nThrough extensive analysis and experiments, we show that our proposed score\nfunctions are more effective and reliable than the existing ones for\ngeneralized SC on a variety of classification tasks and DL classifiers. Code is\navailable at https://github.com/sun-umn/sc_with_distshift."
                },
                "authors": [
                    {
                        "name": "Hengyue Liang"
                    },
                    {
                        "name": "Le Peng"
                    },
                    {
                        "name": "Ju Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ju Sun"
                },
                "author": "Ju Sun",
                "arxiv_comment": "Paper accepted to Transactions on Machine Learning Research (TMLR),\n  issn: 2835-8856,2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18069v1",
                "updated": "2024-11-27T05:43:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    43,
                    0,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T05:43:00Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    43,
                    0,
                    2,
                    332,
                    0
                ],
                "title": "Overview of TREC 2024 Biomedical Generative Retrieval (BioGen) Track",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overview of TREC 2024 Biomedical Generative Retrieval (BioGen) Track"
                },
                "summary": "With the advancement of large language models (LLMs), the biomedical domain\nhas seen significant progress and improvement in multiple tasks such as\nbiomedical question answering, lay language summarization of the biomedical\nliterature, clinical note summarization, etc. However, hallucinations or\nconfabulations remain one of the key challenges when using LLMs in the\nbiomedical and other domains. Inaccuracies may be particularly harmful in\nhigh-risk situations, such as making clinical decisions or appraising\nbiomedical research. Studies on the evaluation of the LLMs' abilities to ground\ngenerated statements in verifiable sources have shown that models perform\nsignificantly worse on lay-user generated questions, and often fail to\nreference relevant sources. This can be problematic when those seeking\ninformation want evidence from studies to back up the claims from LLMs[3].\nUnsupported statements are a major barrier to using LLMs in any applications\nthat may affect health. Methods for grounding generated statements in reliable\nsources along with practical evaluation approaches are needed to overcome this\nbarrier. Towards this, in our pilot task organized at TREC 2024, we introduced\nthe task of reference attribution as a means to mitigate the generation of\nfalse statements by LLMs answering biomedical questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), the biomedical domain\nhas seen significant progress and improvement in multiple tasks such as\nbiomedical question answering, lay language summarization of the biomedical\nliterature, clinical note summarization, etc. However, hallucinations or\nconfabulations remain one of the key challenges when using LLMs in the\nbiomedical and other domains. Inaccuracies may be particularly harmful in\nhigh-risk situations, such as making clinical decisions or appraising\nbiomedical research. Studies on the evaluation of the LLMs' abilities to ground\ngenerated statements in verifiable sources have shown that models perform\nsignificantly worse on lay-user generated questions, and often fail to\nreference relevant sources. This can be problematic when those seeking\ninformation want evidence from studies to back up the claims from LLMs[3].\nUnsupported statements are a major barrier to using LLMs in any applications\nthat may affect health. Methods for grounding generated statements in reliable\nsources along with practical evaluation approaches are needed to overcome this\nbarrier. Towards this, in our pilot task organized at TREC 2024, we introduced\nthe task of reference attribution as a means to mitigate the generation of\nfalse statements by LLMs answering biomedical questions."
                },
                "authors": [
                    {
                        "name": "Deepak Gupta"
                    },
                    {
                        "name": "Dina Demner-Fushman"
                    },
                    {
                        "name": "William Hersh"
                    },
                    {
                        "name": "Steven Bedrick"
                    },
                    {
                        "name": "Kirk Roberts"
                    }
                ],
                "author_detail": {
                    "name": "Kirk Roberts"
                },
                "author": "Kirk Roberts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18065v1",
                "updated": "2024-11-27T05:16:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    16,
                    26,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T05:16:26Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    5,
                    16,
                    26,
                    2,
                    332,
                    0
                ],
                "title": "FlexiBit: Fully Flexible Precision Bit-parallel Accelerator Architecture\n  for Arbitrary Mixed Precision AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiBit: Fully Flexible Precision Bit-parallel Accelerator Architecture\n  for Arbitrary Mixed Precision AI"
                },
                "summary": "Recent research has shown that large language models (LLMs) can utilize\nlow-precision floating point (FP) quantization to deliver high efficiency while\nmaintaining original model accuracy. In particular, recent works have shown the\neffectiveness of non-power-of-two precisions, such as FP6 and FP5, and diverse\nsensitivity to low-precision arithmetic of LLM layers, which motivates mixed\nprecision arithmetic including non-power-of-two precisions in LLMs. Although\nlow-precision algorithmically leads to low computational overheads, such\nbenefits cannot be fully exploited due to hardware constraints that support a\nlimited set of power-of-two precisions (e.g., FP8, 16, 32, and 64 in NVIDIA\nH100 Tensor Core). In addition, the hardware compute units are designed to\nsupport standard formats (e.g., E4M3 and E5M2 for FP8). Such practices require\nre-designing the hardware whenever new precision and format emerge, which leads\nto high hardware replacement costs to exploit the benefits of new precisions\nand formats. Therefore, in this paper, we propose a new accelerator\narchitecture, FlexiBit, which efficiently supports FP and INT arithmetic in\narbitrary precisions and formats. Unlike previous bit-serial designs, which\nalso provide flexibility but at the cost of performance due to its bit-wise\ntemporal processing nature, FlexiBit's architecture enables bit-parallel\nprocessing of any precision and format without compute unit underutilization.\nFlexiBit's new capability to exploit non-power of two precision and format led\nto 1.66x and 1.62x higher performance per area on GPT-3 in FP6 targeting a\ncloud-scale accelerator, compared to a Tensor Core-like architecture and a\nstate-of-the-art bit-parallel flexible precision accelerator, BitFusion,\nrespectively. Also, the bit-parallel nature of FlexiBit's architecture led to\n3.9x higher performance/area compared to a state-of-the-art bit-serial\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that large language models (LLMs) can utilize\nlow-precision floating point (FP) quantization to deliver high efficiency while\nmaintaining original model accuracy. In particular, recent works have shown the\neffectiveness of non-power-of-two precisions, such as FP6 and FP5, and diverse\nsensitivity to low-precision arithmetic of LLM layers, which motivates mixed\nprecision arithmetic including non-power-of-two precisions in LLMs. Although\nlow-precision algorithmically leads to low computational overheads, such\nbenefits cannot be fully exploited due to hardware constraints that support a\nlimited set of power-of-two precisions (e.g., FP8, 16, 32, and 64 in NVIDIA\nH100 Tensor Core). In addition, the hardware compute units are designed to\nsupport standard formats (e.g., E4M3 and E5M2 for FP8). Such practices require\nre-designing the hardware whenever new precision and format emerge, which leads\nto high hardware replacement costs to exploit the benefits of new precisions\nand formats. Therefore, in this paper, we propose a new accelerator\narchitecture, FlexiBit, which efficiently supports FP and INT arithmetic in\narbitrary precisions and formats. Unlike previous bit-serial designs, which\nalso provide flexibility but at the cost of performance due to its bit-wise\ntemporal processing nature, FlexiBit's architecture enables bit-parallel\nprocessing of any precision and format without compute unit underutilization.\nFlexiBit's new capability to exploit non-power of two precision and format led\nto 1.66x and 1.62x higher performance per area on GPT-3 in FP6 targeting a\ncloud-scale accelerator, compared to a Tensor Core-like architecture and a\nstate-of-the-art bit-parallel flexible precision accelerator, BitFusion,\nrespectively. Also, the bit-parallel nature of FlexiBit's architecture led to\n3.9x higher performance/area compared to a state-of-the-art bit-serial\narchitecture."
                },
                "authors": [
                    {
                        "name": "Faraz Tahmasebi"
                    },
                    {
                        "name": "Yian Wang"
                    },
                    {
                        "name": "Benji Y. H. Huang"
                    },
                    {
                        "name": "Hyoukjun Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Hyoukjun Kwon"
                },
                "author": "Hyoukjun Kwon",
                "arxiv_comment": "11 pages, 19 figures, 5 tables, 4 pseudo-codes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.3; C.1.4; C.3; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05295v3",
                "updated": "2024-11-27T04:24:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    4,
                    24,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-03T17:59:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to\n  Jailbreak LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to\n  Jailbreak LLMs"
                },
                "summary": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose AutoDAN-Turbo, a black-box jailbreak method that\ncan automatically discover as many jailbreak strategies as possible from\nscratch, without any human intervention or predefined scopes (e.g., specified\ncandidate strategies), and use them for red-teaming. As a result, AutoDAN-Turbo\ncan significantly outperform baseline methods, achieving a 74.3% higher average\nattack success rate on public benchmarks. Notably, AutoDAN-Turbo achieves an\n88.5 attack success rate on GPT-4-1106-turbo. In addition, AutoDAN-Turbo is a\nunified framework that can incorporate existing human-designed jailbreak\nstrategies in a plug-and-play manner. By integrating human-designed strategies,\nAutoDAN-Turbo can even achieve a higher attack success rate of 93.4 on\nGPT-4-1106-turbo."
                },
                "authors": [
                    {
                        "name": "Xiaogeng Liu"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Edward Suh"
                    },
                    {
                        "name": "Yevgeniy Vorobeychik"
                    },
                    {
                        "name": "Zhuoqing Mao"
                    },
                    {
                        "name": "Somesh Jha"
                    },
                    {
                        "name": "Patrick McDaniel"
                    },
                    {
                        "name": "Huan Sun"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Chaowei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chaowei Xiao"
                },
                "author": "Chaowei Xiao",
                "arxiv_comment": "Pre-print. Project Page: https://autodans.github.io/AutoDAN-Turbo\n  Code: https://github.com/SaFoLab-WISC/AutoDAN-Turbo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18042v1",
                "updated": "2024-11-27T04:24:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    4,
                    24,
                    39,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T04:24:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    4,
                    24,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation"
                },
                "summary": "Multimodal LLMs have advanced vision-language tasks but still struggle with\nunderstanding video scenes. To bridge this gap, Video Scene Graph Generation\n(VidSGG) has emerged to capture multi-object relationships across video frames.\nHowever, prior methods rely on pairwise connections, limiting their ability to\nhandle complex multi-object interactions and reasoning. To this end, we propose\nMultimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about\nmulti-way interactions and higher-order relationships. Our approach uniquely\nintegrates entity scene graphs, which capture spatial relationships between\nobjects, with a procedural graph that models their causal transitions, forming\na unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting\nthis unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene\nGraph Reasoning (VSGR) dataset featuring 1.9M frames from third-person,\negocentric, and drone views and supports five tasks: Scene Graph Generation,\nScene Graph Anticipation, Video Question Answering, Video Captioning, and\nRelation Reasoning. Empirically, HyperGLM consistently outperforms\nstate-of-the-art methods across five tasks, effectively modeling and reasoning\ncomplex relationships in diverse video scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLMs have advanced vision-language tasks but still struggle with\nunderstanding video scenes. To bridge this gap, Video Scene Graph Generation\n(VidSGG) has emerged to capture multi-object relationships across video frames.\nHowever, prior methods rely on pairwise connections, limiting their ability to\nhandle complex multi-object interactions and reasoning. To this end, we propose\nMultimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about\nmulti-way interactions and higher-order relationships. Our approach uniquely\nintegrates entity scene graphs, which capture spatial relationships between\nobjects, with a procedural graph that models their causal transitions, forming\na unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting\nthis unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene\nGraph Reasoning (VSGR) dataset featuring 1.9M frames from third-person,\negocentric, and drone views and supports five tasks: Scene Graph Generation,\nScene Graph Anticipation, Video Question Answering, Video Captioning, and\nRelation Reasoning. Empirically, HyperGLM consistently outperforms\nstate-of-the-art methods across five tasks, effectively modeling and reasoning\ncomplex relationships in diverse video scenes."
                },
                "authors": [
                    {
                        "name": "Trong-Thuan Nguyen"
                    },
                    {
                        "name": "Pha Nguyen"
                    },
                    {
                        "name": "Jackson Cothren"
                    },
                    {
                        "name": "Alper Yilmaz"
                    },
                    {
                        "name": "Khoa Luu"
                    }
                ],
                "author_detail": {
                    "name": "Khoa Luu"
                },
                "author": "Khoa Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18023v1",
                "updated": "2024-11-27T03:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    41,
                    38,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T03:41:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    41,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "Leveraging A New GAN-based Transformer with ECDH Crypto-system for\n  Enhancing Energy Theft Detection in Smart Grid",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging A New GAN-based Transformer with ECDH Crypto-system for\n  Enhancing Energy Theft Detection in Smart Grid"
                },
                "summary": "Detecting energy theft is vital for effectively managing power grids, as it\nensures precise billing and prevents financial losses. Split-learning emerges\nas a promising decentralized machine learning technique for identifying energy\ntheft while preserving user data confidentiality. Nevertheless, traditional\nsplit learning approaches are vulnerable to privacy leakage attacks, which\nsignificantly threaten data confidentiality. To address this challenge, we\npropose a novel GAN-Transformer-based split learning framework in this paper.\nThis framework leverages the strengths of the transformer architecture, which\nis known for its capability to process long-range dependencies in energy\nconsumption data. Thus, it enhances the accuracy of energy theft detection\nwithout compromising user privacy. A distinctive feature of our approach is the\ndeployment of a novel mask-based method, marking a first in its field to\neffectively combat privacy leakage in split learning scenarios targeted at\nAI-enabled adversaries. This method protects sensitive information during the\nmodel's training phase. Our experimental evaluations indicate that the proposed\nframework not only achieves accuracy levels comparable to conventional methods\nbut also significantly enhances privacy protection. The results underscore the\npotential of the GAN-Transformer split learning framework as an effective and\nsecure tool in the domain of energy theft detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting energy theft is vital for effectively managing power grids, as it\nensures precise billing and prevents financial losses. Split-learning emerges\nas a promising decentralized machine learning technique for identifying energy\ntheft while preserving user data confidentiality. Nevertheless, traditional\nsplit learning approaches are vulnerable to privacy leakage attacks, which\nsignificantly threaten data confidentiality. To address this challenge, we\npropose a novel GAN-Transformer-based split learning framework in this paper.\nThis framework leverages the strengths of the transformer architecture, which\nis known for its capability to process long-range dependencies in energy\nconsumption data. Thus, it enhances the accuracy of energy theft detection\nwithout compromising user privacy. A distinctive feature of our approach is the\ndeployment of a novel mask-based method, marking a first in its field to\neffectively combat privacy leakage in split learning scenarios targeted at\nAI-enabled adversaries. This method protects sensitive information during the\nmodel's training phase. Our experimental evaluations indicate that the proposed\nframework not only achieves accuracy levels comparable to conventional methods\nbut also significantly enhances privacy protection. The results underscore the\npotential of the GAN-Transformer split learning framework as an effective and\nsecure tool in the domain of energy theft detection."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Xun Yuan"
                    },
                    {
                        "name": "Arwa Alromih"
                    },
                    {
                        "name": "Aryan Mohammadi Pasikhani"
                    },
                    {
                        "name": "Prosanta Gope"
                    },
                    {
                        "name": "Biplab Sikdar"
                    }
                ],
                "author_detail": {
                    "name": "Biplab Sikdar"
                },
                "author": "Biplab Sikdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13157v2",
                "updated": "2024-11-27T03:25:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    25,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-20T09:46:30Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    46,
                    30,
                    2,
                    325,
                    0
                ],
                "title": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding"
                },
                "summary": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications."
                },
                "authors": [
                    {
                        "name": "Hyun Ryu"
                    },
                    {
                        "name": "Eric Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eric Kim"
                },
                "author": "Eric Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18019v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18019v1",
                "updated": "2024-11-27T03:25:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    25,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T03:25:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    25,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving\n  Capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving\n  Capabilities of Large Language Models"
                },
                "summary": "Automatically resolving software issues is crucial for software development\nin practice, impacting the software quality and user experience. The process of\nresolving real-world issues encompasses tasks such as question-answering (QA),\nfault localization, and code editing. Existing benchmarks such as HumanEval\nfall short in their ability to assess LLMs' proficiency in solving issues\nwithin a codebase. Although benchmarks like SWE-Bench are designed to evaluate\nthe LLMs' capability to handle real-world GitHub issues, the end-to-end\nevaluation method cannot provide granular insights on the performance of\nsubtasks involved in issue solving. To address existing deficiencies in\nbenchmarking LLMs for practical software engineering tasks, we introduce\nFAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe\nsolviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across\nthree distinct tasks: QA, fault localization, and code editing. This benchmark\nis constructed using a dataset curated from 30 well-known GitHub repositories.\nFor each entry, issue and pull request (PR) pairs are meticulously compiled and\nvalidated using cross-referencing and keyword verification methods. FAUN-Eval\nincludes 300 entries and employs both LLM and manual checks to ensure data\nquality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and\nsix open-source models. Our experimental results reveal several key findings.\nWe find that the top-performing LLMs differ across the different tasks.\nAdditionally, features in issues may lead LLMs to generate incorrect\ninformation. Moreover, models may vary in their proficiency with texts of\ndifferent lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically resolving software issues is crucial for software development\nin practice, impacting the software quality and user experience. The process of\nresolving real-world issues encompasses tasks such as question-answering (QA),\nfault localization, and code editing. Existing benchmarks such as HumanEval\nfall short in their ability to assess LLMs' proficiency in solving issues\nwithin a codebase. Although benchmarks like SWE-Bench are designed to evaluate\nthe LLMs' capability to handle real-world GitHub issues, the end-to-end\nevaluation method cannot provide granular insights on the performance of\nsubtasks involved in issue solving. To address existing deficiencies in\nbenchmarking LLMs for practical software engineering tasks, we introduce\nFAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe\nsolviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across\nthree distinct tasks: QA, fault localization, and code editing. This benchmark\nis constructed using a dataset curated from 30 well-known GitHub repositories.\nFor each entry, issue and pull request (PR) pairs are meticulously compiled and\nvalidated using cross-referencing and keyword verification methods. FAUN-Eval\nincludes 300 entries and employs both LLM and manual checks to ensure data\nquality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and\nsix open-source models. Our experimental results reveal several key findings.\nWe find that the top-performing LLMs differ across the different tasks.\nAdditionally, features in issues may lead LLMs to generate incorrect\ninformation. Moreover, models may vary in their proficiency with texts of\ndifferent lengths."
                },
                "authors": [
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Xiangxin Meng"
                    },
                    {
                        "name": "Qinyun Wu"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18019v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18013v1",
                "updated": "2024-11-27T03:14:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    14,
                    16,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T03:14:16Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    14,
                    16,
                    2,
                    332,
                    0
                ],
                "title": "FASIONAD : FAst and Slow FusION Thinking Systems for Human-Like\n  Autonomous Driving with Adaptive Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASIONAD : FAst and Slow FusION Thinking Systems for Human-Like\n  Autonomous Driving with Adaptive Feedback"
                },
                "summary": "Ensuring safe, comfortable, and efficient navigation is a critical goal for\nautonomous driving systems. While end-to-end models trained on large-scale\ndatasets excel in common driving scenarios, they often struggle with rare,\nlong-tail events. Recent progress in large language models (LLMs) has\nintroduced enhanced reasoning capabilities, but their computational demands\npose challenges for real-time decision-making and precise planning. This paper\npresents FASIONAD, a novel dual-system framework inspired by the cognitive\nmodel \"Thinking, Fast and Slow.\" The fast system handles routine navigation\ntasks using rapid, data-driven path planning, while the slow system focuses on\ncomplex reasoning and decision-making in challenging or unfamiliar situations.\nA dynamic switching mechanism based on score distribution and feedback allows\nseamless transitions between the two systems. Visual prompts generated by the\nfast system enable human-like reasoning in the slow system, which provides\nhigh-quality feedback to enhance the fast system's decision-making. To evaluate\nFASIONAD, we introduce a new benchmark derived from the nuScenes dataset,\nspecifically designed to differentiate fast and slow scenarios. FASIONAD\nachieves state-of-the-art performance on this benchmark, establishing a new\nstandard for frameworks integrating fast and slow cognitive processes in\nautonomous driving. This approach paves the way for more adaptive, human-like\nautonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safe, comfortable, and efficient navigation is a critical goal for\nautonomous driving systems. While end-to-end models trained on large-scale\ndatasets excel in common driving scenarios, they often struggle with rare,\nlong-tail events. Recent progress in large language models (LLMs) has\nintroduced enhanced reasoning capabilities, but their computational demands\npose challenges for real-time decision-making and precise planning. This paper\npresents FASIONAD, a novel dual-system framework inspired by the cognitive\nmodel \"Thinking, Fast and Slow.\" The fast system handles routine navigation\ntasks using rapid, data-driven path planning, while the slow system focuses on\ncomplex reasoning and decision-making in challenging or unfamiliar situations.\nA dynamic switching mechanism based on score distribution and feedback allows\nseamless transitions between the two systems. Visual prompts generated by the\nfast system enable human-like reasoning in the slow system, which provides\nhigh-quality feedback to enhance the fast system's decision-making. To evaluate\nFASIONAD, we introduce a new benchmark derived from the nuScenes dataset,\nspecifically designed to differentiate fast and slow scenarios. FASIONAD\nachieves state-of-the-art performance on this benchmark, establishing a new\nstandard for frameworks integrating fast and slow cognitive processes in\nautonomous driving. This approach paves the way for more adaptive, human-like\nautonomous driving systems."
                },
                "authors": [
                    {
                        "name": "Kangan Qian"
                    },
                    {
                        "name": "Zhikun Ma"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Ziang Luo"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Tianze Zhu"
                    },
                    {
                        "name": "Jiayin Li"
                    },
                    {
                        "name": "Jianhui Wang"
                    },
                    {
                        "name": "Ziyu Chen"
                    },
                    {
                        "name": "Xiao He"
                    },
                    {
                        "name": "Yining Shi"
                    },
                    {
                        "name": "Zheng Fu"
                    },
                    {
                        "name": "Xinyu Jiao"
                    },
                    {
                        "name": "Kun Jiang"
                    },
                    {
                        "name": "Diange Yang"
                    },
                    {
                        "name": "Takafumi Matsumaru"
                    }
                ],
                "author_detail": {
                    "name": "Takafumi Matsumaru"
                },
                "author": "Takafumi Matsumaru",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15640v2",
                "updated": "2024-11-27T03:13:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    13,
                    19,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-23T19:43:02Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    19,
                    43,
                    2,
                    5,
                    328,
                    0
                ],
                "title": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AfriMed-QA: A Pan-African, Multi-Specialty, Medical Question-Answering\n  Benchmark Dataset"
                },
                "summary": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model(LLM) performance on medical\nmultiple choice question (MCQ) benchmarks have stimulated interest from\nhealthcare providers and patients globally. Particularly in low-and\nmiddle-income countries (LMICs) facing acute physician shortages and lack of\nspecialists, LLMs offer a potentially scalable pathway to enhance healthcare\naccess and reduce costs. However, their effectiveness in the Global South,\nespecially across the African continent, remains to be established. In this\nwork, we introduce AfriMed-QA, the first large scale Pan-African English\nmulti-specialty medical Question-Answering (QA) dataset, 15,000 questions (open\nand closed-ended) sourced from over 60 medical schools across 16 countries,\ncovering 32 medical specialties. We further evaluate 30 LLMs across multiple\naxes including correctness and demographic bias. Our findings show significant\nperformance variation across specialties and geographies, MCQ performance\nclearly lags USMLE (MedQA). We find that biomedical LLMs underperform general\nmodels and smaller edge-friendly LLMs struggle to achieve a passing score.\nInterestingly, human evaluations show a consistent consumer preference for LLM\nanswers and explanations when compared with clinician answers."
                },
                "authors": [
                    {
                        "name": "Tobi Olatunji"
                    },
                    {
                        "name": "Charles Nimo"
                    },
                    {
                        "name": "Abraham Owodunni"
                    },
                    {
                        "name": "Tassallah Abdullahi"
                    },
                    {
                        "name": "Emmanuel Ayodele"
                    },
                    {
                        "name": "Mardhiyah Sanni"
                    },
                    {
                        "name": "Chinemelu Aka"
                    },
                    {
                        "name": "Folafunmi Omofoye"
                    },
                    {
                        "name": "Foutse Yuehgoh"
                    },
                    {
                        "name": "Timothy Faniran"
                    },
                    {
                        "name": "Bonaventure F. P. Dossou"
                    },
                    {
                        "name": "Moshood Yekini"
                    },
                    {
                        "name": "Jonas Kemp"
                    },
                    {
                        "name": "Katherine Heller"
                    },
                    {
                        "name": "Jude Chidubem Omeke"
                    },
                    {
                        "name": "Chidi Asuzu MD"
                    },
                    {
                        "name": "Naome A. Etori"
                    },
                    {
                        "name": "AimÃ©rou Ndiaye"
                    },
                    {
                        "name": "Ifeoma Okoh"
                    },
                    {
                        "name": "Evans Doe Ocansey"
                    },
                    {
                        "name": "Wendy Kinara"
                    },
                    {
                        "name": "Michael Best"
                    },
                    {
                        "name": "Irfan Essa"
                    },
                    {
                        "name": "Stephen Edward Moore"
                    },
                    {
                        "name": "Chris Fourie"
                    },
                    {
                        "name": "Mercy Nyamewaa Asiedu"
                    }
                ],
                "author_detail": {
                    "name": "Mercy Nyamewaa Asiedu"
                },
                "author": "Mercy Nyamewaa Asiedu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18010v1",
                "updated": "2024-11-27T03:05:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    5,
                    32,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T03:05:32Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    5,
                    32,
                    2,
                    332,
                    0
                ],
                "title": "JPPO: Joint Power and Prompt Optimization for Accelerated Large Language\n  Model Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JPPO: Joint Power and Prompt Optimization for Accelerated Large Language\n  Model Services"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing deployment in wireless networks for\na wide variety of user services. However, the growing longer prompt setting\nhighlights the crucial issue of computational resource demands and huge\ncommunication load. To address this challenge, we propose Joint Power and\nPrompt Optimization (JPPO), a framework that combines Small Language Model\n(SLM)-based prompt compression with wireless power allocation optimization. By\ndeploying SLM at user devices for prompt compression and employing Deep\nReinforcement Learning for joint optimization of compression ratio and\ntransmission power, JPPO effectively balances service quality with resource\nefficiency. Experimental results demonstrate that our framework achieves high\nservice fidelity and low bit error rates while optimizing power usage in\nwireless LLM services. The system reduces response time by about 17%, with the\nimprovement varying based on the length of the original prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks, leading to their increasing deployment in wireless networks for\na wide variety of user services. However, the growing longer prompt setting\nhighlights the crucial issue of computational resource demands and huge\ncommunication load. To address this challenge, we propose Joint Power and\nPrompt Optimization (JPPO), a framework that combines Small Language Model\n(SLM)-based prompt compression with wireless power allocation optimization. By\ndeploying SLM at user devices for prompt compression and employing Deep\nReinforcement Learning for joint optimization of compression ratio and\ntransmission power, JPPO effectively balances service quality with resource\nefficiency. Experimental results demonstrate that our framework achieves high\nservice fidelity and low bit error rates while optimizing power usage in\nwireless LLM services. The system reduces response time by about 17%, with the\nimprovement varying based on the length of the original prompt."
                },
                "authors": [
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Jamalipour"
                },
                "author": "Abbas Jamalipour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17691v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17691v2",
                "updated": "2024-11-27T02:51:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    51,
                    4,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T18:57:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    57,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for\n  Quantized LLMs with 100T Training Tokens"
                },
                "summary": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reveal that low-bit quantization favors undertrained large language models\n(LLMs) by observing that models with larger sizes or fewer training tokens\nexperience less quantization-induced degradation (QiD) when applying low-bit\nquantization, whereas smaller models with extensive training tokens suffer\nsignificant QiD. To gain deeper insights into this trend, we study over 1500\nquantized LLM checkpoints of various sizes and at different training levels\n(undertrained or fully trained) in a controlled setting, deriving scaling laws\nfor understanding the relationship between QiD and factors such as the number\nof training tokens, model size and bit width.\n  With the derived scaling laws, we propose a novel perspective that we can use\nQiD to measure an LLM's training levels and determine the number of training\ntokens required for fully training LLMs of various sizes. Moreover, we use the\nscaling laws to predict the quantization performance of different-sized LLMs\ntrained with 100 trillion tokens. Our projection shows that the low-bit\nquantization performance of future models, which are expected to be trained\nwith over 100 trillion tokens, may NOT be desirable. This poses a potential\nchallenge for low-bit quantization in the future and highlights the need for\nawareness of a model's training level when evaluating low-bit quantization\nresearch. To facilitate future research on this problem, we release all the\n1500+ quantized checkpoints used in this work at\nhttps://huggingface.co/Xu-Ouyang."
                },
                "authors": [
                    {
                        "name": "Xu Ouyang"
                    },
                    {
                        "name": "Tao Ge"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17691v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17691v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09804v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09804v3",
                "updated": "2024-11-27T02:41:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    41,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-10-13T11:15:38Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    11,
                    15,
                    38,
                    6,
                    287,
                    0
                ],
                "title": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models"
                },
                "summary": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable."
                },
                "authors": [
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Victor Shea-Jay Huang"
                    },
                    {
                        "name": "Renmiao Chen"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09804v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09804v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17993v1",
                "updated": "2024-11-27T02:20:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    20,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T02:20:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    20,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "DRS: Deep Question Reformulation With Structured Output",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRS: Deep Question Reformulation With Structured Output"
                },
                "summary": "Question answering is a fundamental capability of large language models\n(LLMs). However, when people encounter completely new knowledge texts, they\noften ask questions that the text cannot answer due to a lack of understanding\nof the knowledge. Recent research shows that large language models identify the\nunanswerability of questions, but they lack the ability to help people\nreformulate their questions. Even powerful models like GPT-3.5 perform poorly\nin this regard. To enhance the ability of LLMs to assist humans in\nreformulating questions to extract relevant knowledge from new documents, we\npropose a zero-shot method called DRS: Deep Question Reformulation With\nStructured Output. Our proposed method leverages large language models and the\nDFS-based algorithm to iteratively search for possible entity combinations and\nconstrain the output with certain entities, effectively improving the\ncapabilities of large language models in this area. Extensive experimental\nresults show that our zero-shot DRS method significantly improves the\nreformulation accuracy of GPT-3.5 from 23.03% to 70.42% and effectively\nimproves the score of open-source large language models, such as Gemma2-9B,\nfrom 26.35% to 56.75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering is a fundamental capability of large language models\n(LLMs). However, when people encounter completely new knowledge texts, they\noften ask questions that the text cannot answer due to a lack of understanding\nof the knowledge. Recent research shows that large language models identify the\nunanswerability of questions, but they lack the ability to help people\nreformulate their questions. Even powerful models like GPT-3.5 perform poorly\nin this regard. To enhance the ability of LLMs to assist humans in\nreformulating questions to extract relevant knowledge from new documents, we\npropose a zero-shot method called DRS: Deep Question Reformulation With\nStructured Output. Our proposed method leverages large language models and the\nDFS-based algorithm to iteratively search for possible entity combinations and\nconstrain the output with certain entities, effectively improving the\ncapabilities of large language models in this area. Extensive experimental\nresults show that our zero-shot DRS method significantly improves the\nreformulation accuracy of GPT-3.5 from 23.03% to 70.42% and effectively\nimproves the score of open-source large language models, such as Gemma2-9B,\nfrom 26.35% to 56.75%."
                },
                "authors": [
                    {
                        "name": "Zhecheng Li"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11248v2",
                "updated": "2024-11-27T02:17:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    2,
                    17,
                    54,
                    2,
                    332,
                    0
                ],
                "published": "2024-06-17T06:19:14Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    6,
                    19,
                    14,
                    0,
                    169,
                    0
                ],
                "title": "Performance Improvement of Language-Queried Audio Source Separation\n  Based on Caption Augmentation From Large Language Models for DCASE Challenge\n  2024 Task 9",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Improvement of Language-Queried Audio Source Separation\n  Based on Caption Augmentation From Large Language Models for DCASE Challenge\n  2024 Task 9"
                },
                "summary": "We present a prompt-engineering-based text-augmentation approach applied to a\nlanguage-queried audio source separation (LASS) task. To enhance the\nperformance of LASS, the proposed approach utilizes large language models\n(LLMs) to generate multiple captions corresponding to each sentence of the\ntraining dataset. To this end, we first perform experiments to identify the\nmost effective prompts for caption augmentation with a smaller number of\ncaptions. A LASS model trained with these augmented captions demonstrates\nimproved performance on the DCASE 2024 Task 9 validation set compared to that\ntrained without augmentation. This study highlights the effectiveness of\nLLM-based caption augmentation in advancing language-queried audio source\nseparation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a prompt-engineering-based text-augmentation approach applied to a\nlanguage-queried audio source separation (LASS) task. To enhance the\nperformance of LASS, the proposed approach utilizes large language models\n(LLMs) to generate multiple captions corresponding to each sentence of the\ntraining dataset. To this end, we first perform experiments to identify the\nmost effective prompts for caption augmentation with a smaller number of\ncaptions. A LASS model trained with these augmented captions demonstrates\nimproved performance on the DCASE 2024 Task 9 validation set compared to that\ntrained without augmentation. This study highlights the effectiveness of\nLLM-based caption augmentation in advancing language-queried audio source\nseparation."
                },
                "authors": [
                    {
                        "name": "Do Hyun Lee"
                    },
                    {
                        "name": "Yoonah Song"
                    },
                    {
                        "name": "Hong Kook Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hong Kook Kim"
                },
                "author": "Hong Kook Kim",
                "arxiv_comment": "DCASE 2024 Challenge Task 9, 4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17989v1",
                "updated": "2024-11-27T01:56:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    1,
                    56,
                    21,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T01:56:21Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    1,
                    56,
                    21,
                    2,
                    332,
                    0
                ],
                "title": "Regularized Multi-LLMs Collaboration for Enhanced Score-based Causal\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regularized Multi-LLMs Collaboration for Enhanced Score-based Causal\n  Discovery"
                },
                "summary": "As the significance of understanding the cause-and-effect relationships among\nvariables increases in the development of modern systems and algorithms,\nlearning causality from observational data has become a preferred and efficient\napproach over conducting randomized control trials. However, purely\nobservational data could be insufficient to reconstruct the true causal graph.\nConsequently, many researchers tried to utilise some form of prior knowledge to\nimprove causal discovery process. In this context, the impressive capabilities\nof large language models (LLMs) have emerged as a promising alternative to the\ncostly acquisition of prior expert knowledge. In this work, we further explore\nthe potential of using LLMs to enhance causal discovery approaches,\nparticularly focusing on score-based methods, and we propose a general\nframework to utilise the capacity of not only one but multiple LLMs to augment\nthe discovery process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the significance of understanding the cause-and-effect relationships among\nvariables increases in the development of modern systems and algorithms,\nlearning causality from observational data has become a preferred and efficient\napproach over conducting randomized control trials. However, purely\nobservational data could be insufficient to reconstruct the true causal graph.\nConsequently, many researchers tried to utilise some form of prior knowledge to\nimprove causal discovery process. In this context, the impressive capabilities\nof large language models (LLMs) have emerged as a promising alternative to the\ncostly acquisition of prior expert knowledge. In this work, we further explore\nthe potential of using LLMs to enhance causal discovery approaches,\nparticularly focusing on score-based methods, and we propose a general\nframework to utilise the capacity of not only one but multiple LLMs to augment\nthe discovery process."
                },
                "authors": [
                    {
                        "name": "Xiaoxuan Li"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17981v1",
                "updated": "2024-11-27T01:30:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    1,
                    30,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T01:30:44Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    1,
                    30,
                    44,
                    2,
                    332,
                    0
                ],
                "title": "Engineering Trustworthy Software: A Mission for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering Trustworthy Software: A Mission for LLMs"
                },
                "summary": "LLMs are transforming software engineering by accelerating development,\nreducing complexity, and cutting costs. When fully integrated into the software\nlifecycle they will drive design, development and deployment while facilitating\nearly bug detection, continuous improvement, and rapid resolution of critical\nissues. However, trustworthy LLM-driven software engineering requires\naddressing multiple challenges such as accuracy, scalability, bias, and\nexplainability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are transforming software engineering by accelerating development,\nreducing complexity, and cutting costs. When fully integrated into the software\nlifecycle they will drive design, development and deployment while facilitating\nearly bug detection, continuous improvement, and rapid resolution of critical\nissues. However, trustworthy LLM-driven software engineering requires\naddressing multiple challenges such as accuracy, scalability, bias, and\nexplainability."
                },
                "authors": [
                    {
                        "name": "Marco Vieira"
                    }
                ],
                "author_detail": {
                    "name": "Marco Vieira"
                },
                "author": "Marco Vieira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17967v1",
                "updated": "2024-11-27T00:52:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    52,
                    21,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T00:52:21Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    52,
                    21,
                    2,
                    332,
                    0
                ],
                "title": "QuaLLM-Health: An Adaptation of an LLM-Based Framework for Quantitative\n  Data Extraction from Online Health Discussions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaLLM-Health: An Adaptation of an LLM-Based Framework for Quantitative\n  Data Extraction from Online Health Discussions"
                },
                "summary": "Health-related discussions on social media like Reddit offer valuable\ninsights, but extracting quantitative data from unstructured text is\nchallenging. In this work, we present an adapted framework from QuaLLM into\nQuaLLM-Health for extracting clinically relevant quantitative data from Reddit\ndiscussions about glucagon-like peptide-1 (GLP-1) receptor agonists using large\nlanguage models (LLMs). We collected 410k posts and comments from five\nGLP-1-related communities using the Reddit API in July 2024. After filtering\nfor cancer-related discussions, 2,059 unique entries remained. We developed\nannotation guidelines to manually extract variables such as cancer\nsurvivorship, family cancer history, cancer types mentioned, risk perceptions,\nand discussions with physicians. Two domain-experts independently annotated a\nrandom sample of 100 entries to create a gold-standard dataset. We then\nemployed iterative prompt engineering with OpenAI's \"GPT-4o-mini\" on the\ngold-standard dataset to build an optimized pipeline that allowed us to extract\nvariables from the large dataset. The optimized LLM achieved accuracies above\n0.85 for all variables, with precision, recall and F1 score macro averaged >\n0.90, indicating balanced performance. Stability testing showed a 95% match\nrate across runs, confirming consistency. Applying the framework to the full\ndataset enabled efficient extraction of variables necessary for downstream\nanalysis, costing under $3 and completing in approximately one hour.\nQuaLLM-Health demonstrates that LLMs can effectively and efficiently extract\nclinically relevant quantitative data from unstructured social media content.\nIncorporating human expertise and iterative prompt refinement ensures accuracy\nand reliability. This methodology can be adapted for large-scale analysis of\npatient-generated data across various health domains, facilitating valuable\ninsights for healthcare research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health-related discussions on social media like Reddit offer valuable\ninsights, but extracting quantitative data from unstructured text is\nchallenging. In this work, we present an adapted framework from QuaLLM into\nQuaLLM-Health for extracting clinically relevant quantitative data from Reddit\ndiscussions about glucagon-like peptide-1 (GLP-1) receptor agonists using large\nlanguage models (LLMs). We collected 410k posts and comments from five\nGLP-1-related communities using the Reddit API in July 2024. After filtering\nfor cancer-related discussions, 2,059 unique entries remained. We developed\nannotation guidelines to manually extract variables such as cancer\nsurvivorship, family cancer history, cancer types mentioned, risk perceptions,\nand discussions with physicians. Two domain-experts independently annotated a\nrandom sample of 100 entries to create a gold-standard dataset. We then\nemployed iterative prompt engineering with OpenAI's \"GPT-4o-mini\" on the\ngold-standard dataset to build an optimized pipeline that allowed us to extract\nvariables from the large dataset. The optimized LLM achieved accuracies above\n0.85 for all variables, with precision, recall and F1 score macro averaged >\n0.90, indicating balanced performance. Stability testing showed a 95% match\nrate across runs, confirming consistency. Applying the framework to the full\ndataset enabled efficient extraction of variables necessary for downstream\nanalysis, costing under $3 and completing in approximately one hour.\nQuaLLM-Health demonstrates that LLMs can effectively and efficiently extract\nclinically relevant quantitative data from unstructured social media content.\nIncorporating human expertise and iterative prompt refinement ensures accuracy\nand reliability. This methodology can be adapted for large-scale analysis of\npatient-generated data across various health domains, facilitating valuable\ninsights for healthcare research."
                },
                "authors": [
                    {
                        "name": "Ramez Kouzy"
                    },
                    {
                        "name": "Roxanna Attar-Olyaee"
                    },
                    {
                        "name": "Michael K. Rooney"
                    },
                    {
                        "name": "Comron J. Hassanzadeh"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    },
                    {
                        "name": "Osama Mohamad"
                    }
                ],
                "author_detail": {
                    "name": "Osama Mohamad"
                },
                "author": "Osama Mohamad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00806v2",
                "updated": "2024-11-27T00:19:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    19,
                    55,
                    2,
                    332,
                    0
                ],
                "published": "2024-03-31T21:43:05Z",
                "published_parsed": [
                    2024,
                    3,
                    31,
                    21,
                    43,
                    5,
                    6,
                    91,
                    0
                ],
                "title": "Algorithmic Collusion by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic Collusion by Large Language Models"
                },
                "summary": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that (1) LLM-based agents are adept at pricing tasks,\n(2) LLM-based pricing agents autonomously collude in oligopoly settings to the\ndetriment of consumers, and (3) variation in seemingly innocuous phrases in LLM\ninstructions (\"prompts\") may increase collusion. Novel off-path analysis\ntechniques uncover price-war concerns as contributing to these phenomena. Our\nresults extend to auction settings. Our findings uncover unique challenges to\nany future regulation of LLM-based pricing agents, and black-box pricing agents\nmore broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of algorithmic pricing raises concerns of algorithmic collusion. We\nconduct experiments with algorithmic pricing agents based on Large Language\nModels (LLMs). We find that (1) LLM-based agents are adept at pricing tasks,\n(2) LLM-based pricing agents autonomously collude in oligopoly settings to the\ndetriment of consumers, and (3) variation in seemingly innocuous phrases in LLM\ninstructions (\"prompts\") may increase collusion. Novel off-path analysis\ntechniques uncover price-war concerns as contributing to these phenomena. Our\nresults extend to auction settings. Our findings uncover unique challenges to\nany future regulation of LLM-based pricing agents, and black-box pricing agents\nmore broadly."
                },
                "authors": [
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Yannai A. Gonczarowski"
                    },
                    {
                        "name": "Ran I. Shorrer"
                    }
                ],
                "author_detail": {
                    "name": "Ran I. Shorrer"
                },
                "author": "Ran I. Shorrer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.10708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.10708v2",
                "updated": "2024-11-27T00:15:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    15,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2023-11-17T18:58:16Z",
                "published_parsed": [
                    2023,
                    11,
                    17,
                    18,
                    58,
                    16,
                    4,
                    321,
                    0
                ],
                "title": "SelfEval: Leveraging the discriminative nature of generative models for\n  evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelfEval: Leveraging the discriminative nature of generative models for\n  evaluation"
                },
                "summary": "We present an automated way to evaluate the text alignment of text-to-image\ngenerative diffusion models using standard image-text recognition datasets. Our\nmethod, called SelfEval, uses the generative model to compute the likelihood of\nreal images given text prompts, and the likelihood can be used to perform\nrecognition tasks with the generative model. We evaluate generative models on\nstandard datasets created for multimodal text-image discriminative learning and\nassess fine-grained aspects of their performance: attribute binding, color\nrecognition, counting, shape recognition, spatial understanding. Existing\nautomated metrics rely on an external pretrained model like CLIP (VLMs) or\nLLMs, and are sensitive to the exact pretrained model and its limitations.\nSelfEval sidesteps these issues, and to the best of our knowledge, is the first\nautomated metric to show a high degree of agreement for measuring\ntext-faithfulness with the gold-standard human evaluations across multiple\ngenerative models, benchmarks and evaluation metrics. SelfEval also reveals\nthat generative models showcase competitive recognition performance on\nchallenging tasks such as Winoground image-score compared to discriminative\nmodels. We hope SelfEval enables easy and reliable automated evaluation for\ndiffusion models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an automated way to evaluate the text alignment of text-to-image\ngenerative diffusion models using standard image-text recognition datasets. Our\nmethod, called SelfEval, uses the generative model to compute the likelihood of\nreal images given text prompts, and the likelihood can be used to perform\nrecognition tasks with the generative model. We evaluate generative models on\nstandard datasets created for multimodal text-image discriminative learning and\nassess fine-grained aspects of their performance: attribute binding, color\nrecognition, counting, shape recognition, spatial understanding. Existing\nautomated metrics rely on an external pretrained model like CLIP (VLMs) or\nLLMs, and are sensitive to the exact pretrained model and its limitations.\nSelfEval sidesteps these issues, and to the best of our knowledge, is the first\nautomated metric to show a high degree of agreement for measuring\ntext-faithfulness with the gold-standard human evaluations across multiple\ngenerative models, benchmarks and evaluation metrics. SelfEval also reveals\nthat generative models showcase competitive recognition performance on\nchallenging tasks such as Winoground image-score compared to discriminative\nmodels. We hope SelfEval enables easy and reliable automated evaluation for\ndiffusion models."
                },
                "authors": [
                    {
                        "name": "Sai Saketh Rambhatla"
                    },
                    {
                        "name": "Ishan Misra"
                    }
                ],
                "author_detail": {
                    "name": "Ishan Misra"
                },
                "author": "Ishan Misra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.10708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.10708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05193v2",
                "updated": "2024-11-27T00:05:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    0,
                    5,
                    44,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-07T21:36:52Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    21,
                    36,
                    52,
                    3,
                    312,
                    0
                ],
                "title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning"
                },
                "summary": "Value-based reinforcement learning (RL) can in principle learn effective\npolicies for a wide range of multi-turn problems, from games to dialogue to\nrobotic control, including via offline RL from static previously collected\ndatasets. However, despite the widespread use of policy gradient methods to\ntrain large language models for single turn tasks (e.g., question answering),\nvalue-based methods for multi-turn RL in an off-policy or offline setting have\nproven particularly challenging to scale to the setting of large language\nmodels. This setting requires effectively leveraging pretraining, scaling to\nlarge architectures with billions of parameters, and training on large\ndatasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses\nthese drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)\nproblem where the probabilities of tokens directly translate to Q-values. In\nthis way we obtain an algorithm that smoothly transitions from maximizing the\nlikelihood of the data during pretraining to learning a near-optimal Q-function\nduring finetuning. Our algorithm has strong theoretical foundations, enjoying\nperformance bounds similar to state-of-the-art Q-learning methods, while in\npractice utilizing an objective that closely resembles SFT. Because of this,\nour approach can enjoy the full benefits of the pretraining of language models,\nwithout the need to reinitialize any weights before RL finetuning, and without\nthe need to initialize new heads for predicting values or advantages.\nEmpirically, we evaluate our method on both pretrained LLMs and VLMs, on a\nvariety of tasks including both natural language dialogue and robotic\nmanipulation and navigation from images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based reinforcement learning (RL) can in principle learn effective\npolicies for a wide range of multi-turn problems, from games to dialogue to\nrobotic control, including via offline RL from static previously collected\ndatasets. However, despite the widespread use of policy gradient methods to\ntrain large language models for single turn tasks (e.g., question answering),\nvalue-based methods for multi-turn RL in an off-policy or offline setting have\nproven particularly challenging to scale to the setting of large language\nmodels. This setting requires effectively leveraging pretraining, scaling to\nlarge architectures with billions of parameters, and training on large\ndatasets, all of which represent major challenges for current value-based RL\nmethods. In this work, we propose a novel offline RL algorithm that addresses\nthese drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT)\nproblem where the probabilities of tokens directly translate to Q-values. In\nthis way we obtain an algorithm that smoothly transitions from maximizing the\nlikelihood of the data during pretraining to learning a near-optimal Q-function\nduring finetuning. Our algorithm has strong theoretical foundations, enjoying\nperformance bounds similar to state-of-the-art Q-learning methods, while in\npractice utilizing an objective that closely resembles SFT. Because of this,\nour approach can enjoy the full benefits of the pretraining of language models,\nwithout the need to reinitialize any weights before RL finetuning, and without\nthe need to initialize new heads for predicting values or advantages.\nEmpirically, we evaluate our method on both pretrained LLMs and VLMs, on a\nvariety of tasks including both natural language dialogue and robotic\nmanipulation and navigation from images."
                },
                "authors": [
                    {
                        "name": "Joey Hong"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "arxiv_comment": "17 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17945v1",
                "updated": "2024-11-26T23:39:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    23,
                    39,
                    43,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T23:39:43Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    23,
                    39,
                    43,
                    1,
                    331,
                    0
                ],
                "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D\n  Content Creation"
                },
                "summary": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity 3D content from text prompts remains a significant\nchallenge in computer vision due to the limited size, diversity, and annotation\ndepth of the existing datasets. To address this, we introduce MARVEL-40M+, an\nextensive dataset with 40 million text annotations for over 8.9 million 3D\nassets aggregated from seven major 3D datasets. Our contribution is a novel\nmulti-stage annotation pipeline that integrates open-source pretrained\nmulti-view VLMs and LLMs to automatically produce multi-level descriptions,\nranging from detailed (150-200 words) to concise semantic tags (10-20 words).\nThis structure supports both fine-grained 3D reconstruction and rapid\nprototyping. Furthermore, we incorporate human metadata from source datasets\ninto our annotation pipeline to add domain-specific information in our\nannotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D,\na two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our\nannotations and use a pretrained image-to-3D network to generate 3D textured\nmeshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly\noutperforms existing datasets in annotation quality and linguistic diversity,\nachieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators."
                },
                "authors": [
                    {
                        "name": "Sankalp Sinha"
                    },
                    {
                        "name": "Mohammad Sadil Khan"
                    },
                    {
                        "name": "Muhammad Usama"
                    },
                    {
                        "name": "Shino Sam"
                    },
                    {
                        "name": "Didier Stricker"
                    },
                    {
                        "name": "Sk Aziz Ali"
                    },
                    {
                        "name": "Muhammad Zeshan Afzal"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Zeshan Afzal"
                },
                "author": "Muhammad Zeshan Afzal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17933v1",
                "updated": "2024-11-26T23:06:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    23,
                    6,
                    9,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T23:06:09Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    23,
                    6,
                    9,
                    1,
                    331,
                    0
                ],
                "title": "Automated Test Transfer Across Android Apps Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Test Transfer Across Android Apps Using Large Language Models"
                },
                "summary": "The pervasiveness of mobile apps in everyday life necessitates robust testing\nstrategies to ensure quality and efficiency, especially through end-to-end\nusage-based tests for mobile apps' user interfaces (UIs). However, manually\ncreating and maintaining such tests can be costly for developers. Since many\napps share similar functionalities beneath diverse UIs, previous works have\nshown the possibility of transferring UI tests across different apps within the\nsame domain, thereby eliminating the need for writing the tests manually.\nHowever, these methods have struggled to accommodate real-world variations,\noften facing limitations in scenarios where source and target apps are not very\nsimilar or fail to accurately transfer test oracles. This paper introduces an\ninnovative technique, LLMigrate, which leverages Large Language Models (LLMs)\nto efficiently transfer usage-based UI tests across mobile apps. Our\nexperimental evaluation shows LLMigrate can achieve a 97.5% success rate in\nautomated test transfer, reducing the manual effort required to write tests\nfrom scratch by 91.1%. This represents an improvement of 9.1% in success rate\nand 38.2% in effort reduction compared to the best-performing prior technique,\nsetting a new benchmark for automated test transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pervasiveness of mobile apps in everyday life necessitates robust testing\nstrategies to ensure quality and efficiency, especially through end-to-end\nusage-based tests for mobile apps' user interfaces (UIs). However, manually\ncreating and maintaining such tests can be costly for developers. Since many\napps share similar functionalities beneath diverse UIs, previous works have\nshown the possibility of transferring UI tests across different apps within the\nsame domain, thereby eliminating the need for writing the tests manually.\nHowever, these methods have struggled to accommodate real-world variations,\noften facing limitations in scenarios where source and target apps are not very\nsimilar or fail to accurately transfer test oracles. This paper introduces an\ninnovative technique, LLMigrate, which leverages Large Language Models (LLMs)\nto efficiently transfer usage-based UI tests across mobile apps. Our\nexperimental evaluation shows LLMigrate can achieve a 97.5% success rate in\nautomated test transfer, reducing the manual effort required to write tests\nfrom scratch by 91.1%. This represents an improvement of 9.1% in success rate\nand 38.2% in effort reduction compared to the best-performing prior technique,\nsetting a new benchmark for automated test transfer."
                },
                "authors": [
                    {
                        "name": "Benyamin Beyzaei"
                    },
                    {
                        "name": "Saghar Talebipour"
                    },
                    {
                        "name": "Ghazal Rafiei"
                    },
                    {
                        "name": "Nenad Medvidovic"
                    },
                    {
                        "name": "Sam Malek"
                    }
                ],
                "author_detail": {
                    "name": "Sam Malek"
                },
                "author": "Sam Malek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17927v1",
                "updated": "2024-11-26T22:48:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T22:48:55Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    48,
                    55,
                    1,
                    331,
                    0
                ],
                "title": "Measuring Emergent Capabilities of LLMs for Software Engineering: How\n  Far Are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Emergent Capabilities of LLMs for Software Engineering: How\n  Far Are We?"
                },
                "summary": "The adoption of Large Language Models (LLMs) across multiple contexts has\nsparked interest in understanding how scaling model size might lead to\nbehavioral changes, as LLMs can exhibit behaviors not observed in their smaller\ncounterparts. Understanding these emergent capabilities is essential for\nadvancing LLM development and improving their interpretability across diverse\ntasks. However, whether LLMs exhibit true emergence in the context of Software\nEngineering remains an unexplored topic, as most research has focused on NLP\ntasks. In this paper, we investigate the emergence of capabilities in the\ncontext of SE. We propose a model-agnostic pipeline for evaluating this\nphenomenon across three SE tasks: bug fixing, code translation, and commit\nmessage generation. More precisely, for each task, we present a case study\ninstantiating our pipeline to analyze the emergence of capabilities in\nCodeGen1-multi across four scales ranging from 350M to 16.1B parameters. Our\nfindings do not not provide evidence to support the idea of emergent\ncapabilities resulting from scaling the model size in the selected set of\ntasks. We hope our results can pave the way to a more nuanced understanding of\nemergent capabilities of LLMs within the SE domain, guiding future research to\nfocus on task-specific evaluations and the identification of alternative\nfactors contributing to this phenomenon. Our work underscores the importance of\ntask diversity in examining model behaviors and highlights potential\nlimitations in transferring prior understandings of and approaches to emergence\nfrom NLP to Software Engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) across multiple contexts has\nsparked interest in understanding how scaling model size might lead to\nbehavioral changes, as LLMs can exhibit behaviors not observed in their smaller\ncounterparts. Understanding these emergent capabilities is essential for\nadvancing LLM development and improving their interpretability across diverse\ntasks. However, whether LLMs exhibit true emergence in the context of Software\nEngineering remains an unexplored topic, as most research has focused on NLP\ntasks. In this paper, we investigate the emergence of capabilities in the\ncontext of SE. We propose a model-agnostic pipeline for evaluating this\nphenomenon across three SE tasks: bug fixing, code translation, and commit\nmessage generation. More precisely, for each task, we present a case study\ninstantiating our pipeline to analyze the emergence of capabilities in\nCodeGen1-multi across four scales ranging from 350M to 16.1B parameters. Our\nfindings do not not provide evidence to support the idea of emergent\ncapabilities resulting from scaling the model size in the selected set of\ntasks. We hope our results can pave the way to a more nuanced understanding of\nemergent capabilities of LLMs within the SE domain, guiding future research to\nfocus on task-specific evaluations and the identification of alternative\nfactors contributing to this phenomenon. Our work underscores the importance of\ntask diversity in examining model behaviors and highlights potential\nlimitations in transferring prior understandings of and approaches to emergence\nfrom NLP to Software Engineering."
                },
                "authors": [
                    {
                        "name": "Conor O'Brien"
                    },
                    {
                        "name": "Daniel Rodriguez-Cardenas"
                    },
                    {
                        "name": "Alejandro Velasco"
                    },
                    {
                        "name": "David N. Palacio"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    }
                ],
                "author_detail": {
                    "name": "Denys Poshyvanyk"
                },
                "author": "Denys Poshyvanyk",
                "arxiv_comment": "Submitted for RENE ICPC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24049v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24049v3",
                "updated": "2024-11-26T22:44:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    44,
                    15,
                    1,
                    331,
                    0
                ],
                "published": "2024-10-31T15:45:23Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    15,
                    45,
                    23,
                    3,
                    305,
                    0
                ],
                "title": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Desert Camels and Oil Sheikhs: Arab-Centric Red Teaming of Frontier LLMs"
                },
                "summary": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but raise ethical concerns due\nto embedded social biases. This study examines LLM biases against Arabs versus\nWesterners across eight domains, including women's rights, terrorism, and\nanti-Semitism and assesses model resistance to perpetuating these biases. To\nthis end, we create two datasets: one to evaluate LLM bias toward Arabs versus\nWesterners and another to test model safety against prompts that exaggerate\nnegative traits (\"jailbreaks\"). We evaluate six LLMs -- GPT-4, GPT-4o, LlaMA\n3.1 (8B & 405B), Mistral 7B, and Claude 3.5 Sonnet. We find 79% of cases\ndisplaying negative biases toward Arabs, with LlaMA 3.1-405B being the most\nbiased. Our jailbreak tests reveal GPT-4o as the most vulnerable, despite being\nan optimized version, followed by LlaMA 3.1-8B and Mistral 7B. All LLMs except\nClaude exhibit attack success rates above 87% in three categories. We also find\nClaude 3.5 Sonnet the safest, but it still displays biases in seven of eight\ncategories. Despite being an optimized version of GPT4, We find GPT-4o to be\nmore prone to biases and jailbreaks, suggesting optimization flaws. Our\nfindings underscore the pressing need for more robust bias mitigation\nstrategies and strengthened security measures in LLMs."
                },
                "authors": [
                    {
                        "name": "Muhammed Saeed"
                    },
                    {
                        "name": "Elgizouli Mohamed"
                    },
                    {
                        "name": "Mukhtar Mohamed"
                    },
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Muhammad Abdul-Mageed"
                    },
                    {
                        "name": "Shady Shehata"
                    }
                ],
                "author_detail": {
                    "name": "Shady Shehata"
                },
                "author": "Shady Shehata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24049v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24049v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17924v1",
                "updated": "2024-11-26T22:39:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    39,
                    11,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T22:39:11Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    39,
                    11,
                    1,
                    331,
                    0
                ],
                "title": "AI2T: Building Trustable AI Tutors by Interactively Teaching a\n  Self-Aware Learning Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI2T: Building Trustable AI Tutors by Interactively Teaching a\n  Self-Aware Learning Agent"
                },
                "summary": "AI2T is an interactively teachable AI for authoring intelligent tutoring\nsystems (ITSs). Authors tutor AI2T by providing a few step-by-step solutions\nand then grading AI2T's own problem-solving attempts. From just 20-30 minutes\nof interactive training, AI2T can induce robust rules for step-by-step solution\ntracking (i.e., model-tracing). As AI2T learns it can accurately estimate its\ncertainty of performing correctly on unseen problem steps using STAND: a\nself-aware precondition learning algorithm that outperforms state-of-the-art\nmethods like XGBoost. Our user study shows that authors can use STAND's\ncertainty heuristic to estimate when AI2T has been trained on enough diverse\nproblems to induce correct and complete model-tracing programs. AI2T-induced\nprograms are more reliable than hallucination-prone LLMs and prior\nauthoring-by-tutoring approaches. With its self-aware induction of hierarchical\nrules, AI2T offers a path toward trustable data-efficient authoring-by-tutoring\nfor complex ITSs that normally require as many as 200-300 hours of programming\nper hour of instruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI2T is an interactively teachable AI for authoring intelligent tutoring\nsystems (ITSs). Authors tutor AI2T by providing a few step-by-step solutions\nand then grading AI2T's own problem-solving attempts. From just 20-30 minutes\nof interactive training, AI2T can induce robust rules for step-by-step solution\ntracking (i.e., model-tracing). As AI2T learns it can accurately estimate its\ncertainty of performing correctly on unseen problem steps using STAND: a\nself-aware precondition learning algorithm that outperforms state-of-the-art\nmethods like XGBoost. Our user study shows that authors can use STAND's\ncertainty heuristic to estimate when AI2T has been trained on enough diverse\nproblems to induce correct and complete model-tracing programs. AI2T-induced\nprograms are more reliable than hallucination-prone LLMs and prior\nauthoring-by-tutoring approaches. With its self-aware induction of hierarchical\nrules, AI2T offers a path toward trustable data-efficient authoring-by-tutoring\nfor complex ITSs that normally require as many as 200-300 hours of programming\nper hour of instruction."
                },
                "authors": [
                    {
                        "name": "Daniel Weitekamp"
                    },
                    {
                        "name": "Erik Harpstead"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Koedinger"
                },
                "author": "Kenneth Koedinger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17912v1",
                "updated": "2024-11-26T22:06:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    6,
                    39,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T22:06:39Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    22,
                    6,
                    39,
                    1,
                    331,
                    0
                ],
                "title": "Can LLMs plan paths in the real world?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs plan paths in the real world?"
                },
                "summary": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly integrate into vehicle\nnavigation systems, understanding their path-planning capability is crucial. We\ntested three LLMs through six real-world path-planning scenarios in various\nsettings and with various difficulties. Our experiments showed that all LLMs\nmade numerous errors in all scenarios, revealing that they are unreliable path\nplanners. We suggest that future work focus on implementing mechanisms for\nreality checks, enhancing model transparency, and developing smaller models."
                },
                "authors": [
                    {
                        "name": "Wanyi Chen"
                    },
                    {
                        "name": "Meng-Wen Su"
                    },
                    {
                        "name": "Nafisa Mehjabin"
                    },
                    {
                        "name": "Mary L. Cummings"
                    }
                ],
                "author_detail": {
                    "name": "Mary L. Cummings"
                },
                "author": "Mary L. Cummings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10513v2",
                "updated": "2024-11-26T21:43:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    21,
                    43,
                    1,
                    1,
                    331,
                    0
                ],
                "published": "2024-04-16T12:37:10Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    12,
                    37,
                    10,
                    1,
                    107,
                    0
                ],
                "title": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\n  Granularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoTAR: Chain-of-Thought Attribution Reasoning with Multi-level\n  Granularity"
                },
                "summary": "State-of-the-art performance in QA tasks is currently achieved by systems\nemploying Large Language Models (LLMs), however these models tend to\nhallucinate information in their responses. One approach focuses on enhancing\nthe generation process by incorporating attribution from the given input to the\noutput. However, the challenge of identifying appropriate attributions and\nverifying their accuracy against a source is a complex task that requires\nsignificant improvements in assessing such systems. We introduce an\nattribution-oriented Chain-of-Thought reasoning method to enhance the accuracy\nof attributions. This approach focuses the reasoning process on generating an\nattribution-centric output. Evaluations on two context-enhanced\nquestion-answering datasets using GPT-4 demonstrate improved accuracy and\ncorrectness of attributions. In addition, the combination of our method with\nfinetuning enhances the response and attribution accuracy of two smaller LLMs,\nshowing their potential to outperform GPT-4 in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art performance in QA tasks is currently achieved by systems\nemploying Large Language Models (LLMs), however these models tend to\nhallucinate information in their responses. One approach focuses on enhancing\nthe generation process by incorporating attribution from the given input to the\noutput. However, the challenge of identifying appropriate attributions and\nverifying their accuracy against a source is a complex task that requires\nsignificant improvements in assessing such systems. We introduce an\nattribution-oriented Chain-of-Thought reasoning method to enhance the accuracy\nof attributions. This approach focuses the reasoning process on generating an\nattribution-centric output. Evaluations on two context-enhanced\nquestion-answering datasets using GPT-4 demonstrate improved accuracy and\ncorrectness of attributions. In addition, the combination of our method with\nfinetuning enhances the response and attribution accuracy of two smaller LLMs,\nshowing their potential to outperform GPT-4 in some cases."
                },
                "authors": [
                    {
                        "name": "Moshe Berchansky"
                    },
                    {
                        "name": "Daniel Fleischer"
                    },
                    {
                        "name": "Moshe Wasserblat"
                    },
                    {
                        "name": "Peter Izsak"
                    }
                ],
                "author_detail": {
                    "name": "Peter Izsak"
                },
                "author": "Peter Izsak",
                "arxiv_comment": "Findings of the Association for Computational Linguistics: EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17900v1",
                "updated": "2024-11-26T21:31:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    21,
                    31,
                    58,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T21:31:58Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    21,
                    31,
                    58,
                    1,
                    331,
                    0
                ],
                "title": "Pretrained LLM Adapted with LoRA as a Decision Transformer for Offline\n  RL in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained LLM Adapted with LoRA as a Decision Transformer for Offline\n  RL in Quantitative Trading"
                },
                "summary": "Developing effective quantitative trading strategies using reinforcement\nlearning (RL) is challenging due to the high risks associated with online\ninteraction with live financial markets. Consequently, offline RL, which\nleverages historical market data without additional exploration, becomes\nessential. However, existing offline RL methods often struggle to capture the\ncomplex temporal dependencies inherent in financial time series and may overfit\nto historical patterns. To address these challenges, we introduce a Decision\nTransformer (DT) initialized with pre-trained GPT-2 weights and fine-tuned\nusing Low-Rank Adaptation (LoRA). This architecture leverages the\ngeneralization capabilities of pre-trained language models and the efficiency\nof LoRA to learn effective trading policies from expert trajectories solely\nfrom historical data. Our model performs competitively with established offline\nRL algorithms, including Conservative Q-Learning (CQL), Implicit Q-Learning\n(IQL), and Behavior Cloning (BC), as well as a baseline Decision Transformer\nwith randomly initialized GPT-2 weights and LoRA. Empirical results demonstrate\nthat our approach effectively learns from expert trajectories and secures\nsuperior rewards in certain trading scenarios, highlighting the effectiveness\nof integrating pre-trained language models and parameter-efficient fine-tuning\nin offline RL for quantitative trading. Replication code for our experiments is\npublicly available at https://github.com/syyunn/finrl-dt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing effective quantitative trading strategies using reinforcement\nlearning (RL) is challenging due to the high risks associated with online\ninteraction with live financial markets. Consequently, offline RL, which\nleverages historical market data without additional exploration, becomes\nessential. However, existing offline RL methods often struggle to capture the\ncomplex temporal dependencies inherent in financial time series and may overfit\nto historical patterns. To address these challenges, we introduce a Decision\nTransformer (DT) initialized with pre-trained GPT-2 weights and fine-tuned\nusing Low-Rank Adaptation (LoRA). This architecture leverages the\ngeneralization capabilities of pre-trained language models and the efficiency\nof LoRA to learn effective trading policies from expert trajectories solely\nfrom historical data. Our model performs competitively with established offline\nRL algorithms, including Conservative Q-Learning (CQL), Implicit Q-Learning\n(IQL), and Behavior Cloning (BC), as well as a baseline Decision Transformer\nwith randomly initialized GPT-2 weights and LoRA. Empirical results demonstrate\nthat our approach effectively learns from expert trajectories and secures\nsuperior rewards in certain trading scenarios, highlighting the effectiveness\nof integrating pre-trained language models and parameter-efficient fine-tuning\nin offline RL for quantitative trading. Replication code for our experiments is\npublicly available at https://github.com/syyunn/finrl-dt"
                },
                "authors": [
                    {
                        "name": "Suyeol Yun"
                    }
                ],
                "author_detail": {
                    "name": "Suyeol Yun"
                },
                "author": "Suyeol Yun",
                "arxiv_comment": "Accepted for presentation at the ICAIF 2024 Workshop on LLMs and\n  Generative AI for Finance (poster session)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17891v1",
                "updated": "2024-11-26T21:21:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    21,
                    21,
                    45,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T21:21:45Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    21,
                    21,
                    45,
                    1,
                    331,
                    0
                ],
                "title": "HOPPR Medical-Grade Platform for Medical Imaging AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOPPR Medical-Grade Platform for Medical Imaging AI"
                },
                "summary": "Technological advances in artificial intelligence (AI) have enabled the\ndevelopment of large vision language models (LVLMs) that are trained on\nmillions of paired image and text samples. Subsequent research efforts have\ndemonstrated great potential of LVLMs to achieve high performance in medical\nimaging use cases (e.g., radiology report generation), but there remain\nbarriers that hinder the ability to deploy these solutions broadly. These\ninclude the cost of extensive computational requirements for developing large\nscale models, expertise in the development of sophisticated AI models, and the\ndifficulty in accessing substantially large, high-quality datasets that\nadequately represent the population in which the LVLM solution is to be\ndeployed. The HOPPR Medical-Grade Platform addresses these barriers by\nproviding powerful computational infrastructure, a suite of foundation models\non top of which developers can fine-tune for their specific use cases, and a\nrobust quality management system that sets a standard for evaluating fine-tuned\nmodels for deployment in clinical settings. The HOPPR Platform has access to\nmillions of imaging studies and text reports sourced from hundreds of imaging\ncenters from diverse populations to pretrain foundation models and enable use\ncase-specific cohorts for fine-tuning. All data are deidentified and securely\nstored for HIPAA compliance. Additionally, developers can securely host models\non the HOPPR platform and access them via an API to make inferences using these\nmodels within established clinical workflows. With the Medical-Grade Platform,\nHOPPR's mission is to expedite the deployment of LVLM solutions for medical\nimaging and ultimately optimize radiologist's workflows and meet the growing\ndemands of the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technological advances in artificial intelligence (AI) have enabled the\ndevelopment of large vision language models (LVLMs) that are trained on\nmillions of paired image and text samples. Subsequent research efforts have\ndemonstrated great potential of LVLMs to achieve high performance in medical\nimaging use cases (e.g., radiology report generation), but there remain\nbarriers that hinder the ability to deploy these solutions broadly. These\ninclude the cost of extensive computational requirements for developing large\nscale models, expertise in the development of sophisticated AI models, and the\ndifficulty in accessing substantially large, high-quality datasets that\nadequately represent the population in which the LVLM solution is to be\ndeployed. The HOPPR Medical-Grade Platform addresses these barriers by\nproviding powerful computational infrastructure, a suite of foundation models\non top of which developers can fine-tune for their specific use cases, and a\nrobust quality management system that sets a standard for evaluating fine-tuned\nmodels for deployment in clinical settings. The HOPPR Platform has access to\nmillions of imaging studies and text reports sourced from hundreds of imaging\ncenters from diverse populations to pretrain foundation models and enable use\ncase-specific cohorts for fine-tuning. All data are deidentified and securely\nstored for HIPAA compliance. Additionally, developers can securely host models\non the HOPPR platform and access them via an API to make inferences using these\nmodels within established clinical workflows. With the Medical-Grade Platform,\nHOPPR's mission is to expedite the deployment of LVLM solutions for medical\nimaging and ultimately optimize radiologist's workflows and meet the growing\ndemands of the field."
                },
                "authors": [
                    {
                        "name": "Kalina P. Slavkova"
                    },
                    {
                        "name": "Melanie Traughber"
                    },
                    {
                        "name": "Oliver Chen"
                    },
                    {
                        "name": "Robert Bakos"
                    },
                    {
                        "name": "Shayna Goldstein"
                    },
                    {
                        "name": "Dan Harms"
                    },
                    {
                        "name": "Bradley J. Erickson"
                    },
                    {
                        "name": "Khan M. Siddiqui"
                    }
                ],
                "author_detail": {
                    "name": "Khan M. Siddiqui"
                },
                "author": "Khan M. Siddiqui",
                "arxiv_comment": "6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17855v1",
                "updated": "2024-11-26T20:11:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    11,
                    46,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T20:11:46Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    11,
                    46,
                    1,
                    331,
                    0
                ],
                "title": "\"Give me the code\" -- Log Analysis of First-Year CS Students'\n  Interactions With GPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Give me the code\" -- Log Analysis of First-Year CS Students'\n  Interactions With GPT"
                },
                "summary": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of Large Language Models (LLMs) like GPT-3, GPT-4, and Bard in\ncomputer science (CS) education is expected to be profound. Students now have\nthe power to generate code solutions for a wide array of programming\nassignments. For first-year students, this may be particularly problematic\nsince the foundational skills are still in development and an over-reliance on\ngenerative AI tools can hinder their ability to grasp essential programming\nconcepts. This paper analyzes the prompts used by 69 freshmen undergraduate\nstudents to solve a certain programming problem within a project assignment,\nwithout giving them prior prompt training. We also present the rules of the\nexercise that motivated the prompts, designed to foster critical thinking\nskills during the interaction. Despite using unsophisticated prompting\ntechniques, our findings suggest that the majority of students successfully\nleveraged GPT, incorporating the suggested solutions into their projects.\nAdditionally, half of the students demonstrated the ability to exercise\njudgment in selecting from multiple GPT-generated solutions, showcasing the\ndevelopment of their critical thinking skills in evaluating AI-generated code."
                },
                "authors": [
                    {
                        "name": "Pedro Alves"
                    },
                    {
                        "name": "Bruno Pereira Cipriano"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Pereira Cipriano"
                },
                "author": "Bruno Pereira Cipriano",
                "arxiv_comment": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17850v1",
                "updated": "2024-11-26T20:07:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    7,
                    16,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T20:07:16Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    7,
                    16,
                    1,
                    331,
                    0
                ],
                "title": "Reliability of deep learning models for anatomical landmark detection:\n  The role of inter-rater variability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of deep learning models for anatomical landmark detection:\n  The role of inter-rater variability"
                },
                "summary": "Automated detection of anatomical landmarks plays a crucial role in many\ndiagnostic and surgical applications. Progresses in deep learning (DL) methods\nhave resulted in significant performance enhancement in tasks related to\nanatomical landmark detection. While current research focuses on accurately\nlocalizing these landmarks in medical scans, the importance of inter-rater\nannotation variability in building DL models is often overlooked. Understanding\nhow inter-rater variability impacts the performance and reliability of the\nresulting DL algorithms, which are crucial for clinical deployment, can inform\nthe improvement of training data construction and boost DL models' outcomes. In\nthis paper, we conducted a thorough study of different annotation-fusion\nstrategies to preserve inter-rater variability in DL models for anatomical\nlandmark detection, aiming to boost the performance and reliability of the\nresulting algorithms. Additionally, we explored the characteristics and\nreliability of four metrics, including a novel Weighted Coordinate Variance\nmetric to quantify landmark detection uncertainty/inter-rater variability. Our\nresearch highlights the crucial connection between inter-rater variability,\nDL-models performances, and uncertainty, revealing how different approaches for\nmulti-rater landmark annotation fusion can influence these factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of anatomical landmarks plays a crucial role in many\ndiagnostic and surgical applications. Progresses in deep learning (DL) methods\nhave resulted in significant performance enhancement in tasks related to\nanatomical landmark detection. While current research focuses on accurately\nlocalizing these landmarks in medical scans, the importance of inter-rater\nannotation variability in building DL models is often overlooked. Understanding\nhow inter-rater variability impacts the performance and reliability of the\nresulting DL algorithms, which are crucial for clinical deployment, can inform\nthe improvement of training data construction and boost DL models' outcomes. In\nthis paper, we conducted a thorough study of different annotation-fusion\nstrategies to preserve inter-rater variability in DL models for anatomical\nlandmark detection, aiming to boost the performance and reliability of the\nresulting algorithms. Additionally, we explored the characteristics and\nreliability of four metrics, including a novel Weighted Coordinate Variance\nmetric to quantify landmark detection uncertainty/inter-rater variability. Our\nresearch highlights the crucial connection between inter-rater variability,\nDL-models performances, and uncertainty, revealing how different approaches for\nmulti-rater landmark annotation fusion can influence these factors."
                },
                "authors": [
                    {
                        "name": "Soorena Salari"
                    },
                    {
                        "name": "Hassan Rivaz"
                    },
                    {
                        "name": "Yiming Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xiao"
                },
                "author": "Yiming Xiao",
                "arxiv_comment": "Accepted to SPIE Medical Imaging 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17847v1",
                "updated": "2024-11-26T20:00:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    0,
                    54,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T20:00:54Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    20,
                    0,
                    54,
                    1,
                    331,
                    0
                ],
                "title": "SoftmAP: Software-Hardware Co-design for Integer-Only Softmax on\n  Associative Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoftmAP: Software-Hardware Co-design for Integer-Only Softmax on\n  Associative Processors"
                },
                "summary": "Recent research efforts focus on reducing the computational and memory\noverheads of Large Language Models (LLMs) to make them feasible on\nresource-constrained devices. Despite advancements in compression techniques,\nnon-linear operators like Softmax and Layernorm remain bottlenecks due to their\nsensitivity to quantization. We propose SoftmAP, a software-hardware co-design\nmethodology that implements an integer-only low-precision Softmax using\nIn-Memory Compute (IMC) hardware. Our method achieves up to three orders of\nmagnitude improvement in the energy-delay product compared to A100 and RTX3090\nGPUs, making LLMs more deployable without compromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research efforts focus on reducing the computational and memory\noverheads of Large Language Models (LLMs) to make them feasible on\nresource-constrained devices. Despite advancements in compression techniques,\nnon-linear operators like Softmax and Layernorm remain bottlenecks due to their\nsensitivity to quantization. We propose SoftmAP, a software-hardware co-design\nmethodology that implements an integer-only low-precision Softmax using\nIn-Memory Compute (IMC) hardware. Our method achieves up to three orders of\nmagnitude improvement in the energy-delay product compared to A100 and RTX3090\nGPUs, making LLMs more deployable without compromising performance."
                },
                "authors": [
                    {
                        "name": "Mariam Rakka"
                    },
                    {
                        "name": "Jinhao Li"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Ahmed Eltawil"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    },
                    {
                        "name": "Fadi Kurdahi"
                    }
                ],
                "author_detail": {
                    "name": "Fadi Kurdahi"
                },
                "author": "Fadi Kurdahi",
                "arxiv_comment": "Accepted in DATE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]