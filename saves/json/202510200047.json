[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14891v1",
                "updated": "2025-10-16T17:10:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:10:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    10,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "A Performance Portable Matrix Free Dense MTTKRP in GenTen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Performance Portable Matrix Free Dense MTTKRP in GenTen"
                },
                "summary": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend the GenTen tensor decomposition package by introducing an\naccelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the\nworkhorse kernel for canonical polyadic (CP) tensor decompositions, that is\nportable and performant on modern CPU and GPU architectures. In contrast to the\nstate-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox,\nTensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a\nmatrix-free element-wise parallelization approach whose memory cost grows with\nthe rank R like the sum of the tensor shape O(R(n+m+k)), compared to\nmatrix-based methods whose memory cost grows like the product of the tensor\nshape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the\nsmaller growth rate yields a matrix-free memory cost of just 2% of the\nmatrix-based methods, a 50x improvement. In practice, the reduced memory impact\nmeans our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a\nsingle NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also\ncompare our optimized matrix-free MTTKRP to baseline matrix-free\nimplementations on different devices, showing a 3x single-device speedup on an\nIntel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical\nresults, we provide fine grained performance models for an ideal multi-level\ncache machine, compare analytical performance predictions to empirical results,\nand provide a motivated heuristic selection for selecting an algorithmic\nhyperparameter."
                },
                "authors": [
                    {
                        "name": "Gabriel Kosmacher"
                    },
                    {
                        "name": "Eric T. Phipps"
                    },
                    {
                        "name": "Sivasankaran Rajamanickam"
                    }
                ],
                "author_detail": {
                    "name": "Sivasankaran Rajamanickam"
                },
                "author": "Sivasankaran Rajamanickam",
                "arxiv_comment": "10 pages, 5 figures, 4 tables, for implementation see\n  https://github.com/sandialabs/GenTen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14622v1",
                "updated": "2025-10-16T12:32:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:32:51Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    32,
                    51,
                    3,
                    289,
                    0
                ],
                "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC\n  Systems"
                },
                "summary": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPI implementations commonly rely on explicit memory-copy operations,\nincurring overhead from redundant data movement and buffer management. This\noverhead notably impacts HPC workloads involving intensive inter-processor\ncommunication. In response, we introduce MPI-over-CXL, a novel MPI\ncommunication paradigm leveraging CXL, which provides cache-coherent shared\nmemory across multiple hosts. MPI-over-CXL replaces traditional data-copy\nmethods with direct shared memory access, significantly reducing communication\nlatency and memory bandwidth usage. By mapping shared memory regions directly\ninto the virtual address spaces of MPI processes, our design enables efficient\npointer-based communication, eliminating redundant copying operations. To\nvalidate this approach, we implement a comprehensive hardware and software\nenvironment, including a custom CXL 3.2 controller, FPGA-based multi-host\nemulation, and dedicated software stack. Our evaluations using representative\nbenchmarks demonstrate substantial performance improvements over conventional\nMPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and\nscalability in large-scale HPC environments."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Hyein Woo"
                    },
                    {
                        "name": "Junhee Kim"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Kyungkuk Nam"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Jiseon Kim"
                    },
                    {
                        "name": "Hanyeoreum Bae"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14531v1",
                "updated": "2025-10-16T10:21:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T10:21:35Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    10,
                    21,
                    35,
                    3,
                    289,
                    0
                ],
                "title": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and simulation of a 4H-SiC low gain avalanche diode with\n  trench-isolation"
                },
                "summary": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and simulation of a 30 $\\mathrm{\\mu m}$ thick 4H-SiC\nLow Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4\n$\\mathrm{\\mu m}$ thick epitaxially grown gain layer enables controlled internal\namplification up to 1 kV reverse bias, while maintaining full depletion below\n500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were\nsimulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a\nquasi-1D geometry and verified across process-related variations in gain layer\nparameters. To ensure high-voltage stability and proper edge termination, a\nguard structure combining deep etched trenches and deep $p^+$ junction\ntermination extension (JTE) implants was designed. TCAD simulations varying the\nguard structure dimensions yielded an optimized design with a breakdown voltage\nabove 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM,\nBarcelona."
                },
                "authors": [
                    {
                        "name": "Sebastian Onder"
                    },
                    {
                        "name": "Philipp Gaggl"
                    },
                    {
                        "name": "JÃ¼rgen Burin"
                    },
                    {
                        "name": "Andreas Gsponer"
                    },
                    {
                        "name": "Matthias Knopf"
                    },
                    {
                        "name": "Simon Waid"
                    },
                    {
                        "name": "Neil Moffat"
                    },
                    {
                        "name": "Giulio Pellegrini"
                    },
                    {
                        "name": "Thomas Bergauer"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Bergauer"
                },
                "author": "Thomas Bergauer",
                "arxiv_doi": "10.1016/j.nima.2025.170740",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nima.2025.170740",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14126v1",
                "updated": "2025-10-15T21:49:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T21:49:38Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    21,
                    49,
                    38,
                    2,
                    288,
                    0
                ],
                "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic\n  Serving"
                },
                "summary": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\"",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Cortex, a prototype workflow-aware serving platform designed for\nagentic workloads. The core principle of Cortex is stage isolation: it\nprovisions dedicated resource pools for each distinct stage of an agentic\nworkflow. This simple yet powerful strategy mitigates inter-stage interference\nin compute and memory, leading to better KV cache utilization, higher\nthroughput, and more predictable performance. By customizing resource\nallocation and scheduling within each distinct stage of agentic workflows,\nCortex lays the groundwork for more advanced, agent-native serving paradigms,\nincluding malleable resource management, speculative execution of workflow\nbranches, and a shared, multi-tiered cache for \"agentic state.\""
                },
                "authors": [
                    {
                        "name": "Nikos Pagonas"
                    },
                    {
                        "name": "Yeounoh Chung"
                    },
                    {
                        "name": "Kostis Kaffes"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13940v1",
                "updated": "2025-10-15T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is More: Improving LLM Reasoning with Minimal Test-Time\n  Intervention"
                },
                "summary": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has focused on test-time\nscaling to improve reasoning via increased inference computation, but often at\nthe cost of efficiency. We revisit test-time behavior and uncover a simple yet\nunderexplored phenomenon: reasoning uncertainty is highly localized-only a\nsmall subset of high-entropy tokens dominantly affects output correctness.\nMotivated by this, we propose Minimal Test-Time Intervention (MTI), a\ntraining-free framework that enhances reasoning accuracy and stability with\nminimal overhead. MTI includes: (i) Selective CFG intervention, applying\nclassifier-free guidance only at uncertain positions; and (ii) Lightweight\nnegative-prompt guidance, reusing the main model's KV cache to approximate\nunconditional decoding efficiently. MTI yields consistent gains across general,\ncoding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for\nQwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining\nhighly efficient."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Mingyang Zhang"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Ganggui Ding"
                    },
                    {
                        "name": "Liang Hou"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Ying-Cong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Cong Chen"
                },
                "author": "Ying-Cong Chen",
                "arxiv_comment": "Code: https://github.com/EnVision-Research/MTI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "KiantÃ© Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "JosÃ© Cano"
                    }
                ],
                "author_detail": {
                    "name": "JosÃ© Cano"
                },
                "author": "JosÃ© Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13334v1",
                "updated": "2025-10-15T09:18:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:18:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Fragility of KV Cache Eviction in LLM Inference"
                },
                "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Haoyu Guo"
                    },
                    {
                        "name": "JunLin Lv"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Xike Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xike Xie"
                },
                "author": "Xike Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13279v1",
                "updated": "2025-10-15T08:25:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T08:25:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time"
                },
                "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets."
                },
                "authors": [
                    {
                        "name": "Fuma Omori"
                    },
                    {
                        "name": "Atsushi Yano"
                    },
                    {
                        "name": "Takuya Azumi"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Azumi"
                },
                "author": "Takuya Azumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13223v1",
                "updated": "2025-10-15T07:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T07:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13084v1",
                "updated": "2025-10-15T01:55:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T01:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation"
                },
                "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Yi Zuo"
                    },
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12889v1",
                "updated": "2025-10-14T18:04:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:04:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters"
                },
                "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "single column,20 pages and 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v1",
                "updated": "2025-10-14T18:00:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v2",
                "updated": "2025-10-13T20:40:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    20,
                    40,
                    32,
                    0,
                    286,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v2",
                "updated": "2025-10-13T10:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    39,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v2",
                "updated": "2025-10-13T08:26:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    26,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13860v1",
                "updated": "2025-10-13T04:04:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T04:04:54Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    4,
                    4,
                    54,
                    0,
                    286,
                    0
                ],
                "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP\n  Architecture and Paired Weight Sharing"
                },
                "summary": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the transformer architecture has achieved state-of-the-art performance\non natural language processing tasks, these models impose substantial memory\nand computational overhead. Recent research has identified significant\narchitectural redundancies within these models, presenting opportunities for\noptimization without compromising performance. Taking insights from research in\nAI interpretability and inference-time layer pruning, we introduce an efficient\nlanguage model architecture, referred to as ShishuLM, which reduces both the\nparameter count and Key-Value (KV) cache requirements. Given the increasing\nimportance of Small Language Models (SLMs) in agentic AI systems, we evaluate\nour approach on two SLMs of different scales. Our analysis reveals that for\nmoderate-context scenarios, normalization coupled with attention computation is\nroughly linear with the input, enabling entire transformer blocks to be\napproximated through Multi-Layer Perceptrons (MLPs). Our results show that\nShishuLM provides up to 25% reduction in memory requirements and up to 40%\nimprovement in latency during both training and inference, compared to parent\nmodels. Our experimental and analytical findings provide insights towards\nbuilding more efficient SLM architectures from a pre-training standpoint."
                },
                "authors": [
                    {
                        "name": "Shivanshu Kumar"
                    },
                    {
                        "name": "Gopalakrishnan Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Gopalakrishnan Srinivasan"
                },
                "author": "Gopalakrishnan Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v2",
                "updated": "2025-10-10T16:56:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    56,
                    23,
                    4,
                    283,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_doi": "10.1145/3769780",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769780",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v2",
                "updated": "2025-10-10T16:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    8,
                    26,
                    4,
                    283,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. KÃ¼hn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. KÃ¼hn"
                },
                "author": "Martin J. KÃ¼hn",
                "arxiv_comment": "29 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09477v1",
                "updated": "2025-10-10T15:32:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T15:32:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Autoregressive Inference for Transformer Probabilistic Models"
                },
                "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."
                },
                "authors": [
                    {
                        "name": "Conor Hassan"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09409v1",
                "updated": "2025-10-10T14:03:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T14:03:42Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "title": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network"
                },
                "summary": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC."
                },
                "authors": [
                    {
                        "name": "Chongxiao Cai"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Min Sheng"
                    },
                    {
                        "name": "Jiandong Li"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Di Zhou"
                    },
                    {
                        "name": "Ziwen Xie"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v2",
                "updated": "2025-10-10T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    15,
                    40,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "CÃ©drick Austa"
                    },
                    {
                        "name": "Jan Tobias MÃ¼hlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v2",
                "updated": "2025-10-10T13:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    8,
                    39,
                    4,
                    283,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The paper is currently under investigation regarding concerns of\n  potential academic misconduct. While the investigation is ongoing, the\n  authors have voluntarily requested to withdraw the manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09309v1",
                "updated": "2025-10-10T12:01:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T12:01:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference"
                },
                "summary": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV"
                },
                "authors": [
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09182v1",
                "updated": "2025-10-10T09:24:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T09:24:53Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption"
                },
                "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware."
                },
                "authors": [
                    {
                        "name": "Johann-Friedrich Feiden"
                    },
                    {
                        "name": "Tim KÃ¼chler"
                    },
                    {
                        "name": "Denis Zavadski"
                    },
                    {
                        "name": "Bogdan Savchynskyy"
                    },
                    {
                        "name": "Carsten Rother"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rother"
                },
                "author": "Carsten Rother",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09154v1",
                "updated": "2025-10-10T08:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T08:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications"
                },
                "summary": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications."
                },
                "authors": [
                    {
                        "name": "Tanjim Rahman"
                    },
                    {
                        "name": "Trupti Ranjan Lenka"
                    }
                ],
                "author_detail": {
                    "name": "Trupti Ranjan Lenka"
                },
                "author": "Trupti Ranjan Lenka",
                "arxiv_comment": "13 pages, 13 figures including DC, RF, and breakdown analysis of\n  field-plated AlGaN/GaN HEMT using TCAD simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v3",
                "updated": "2025-10-09T20:37:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    37,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08803v1",
                "updated": "2025-10-09T20:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T20:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Man-Made Heuristics Are Dead. Long Live Code Generators!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Man-Made Heuristics Are Dead. Long Live Code Generators!"
                },
                "summary": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    },
                    {
                        "name": "Daehyeok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daehyeok Kim"
                },
                "author": "Daehyeok Kim",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. To be presented at HotNets 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08774v1",
                "updated": "2025-10-09T19:45:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T19:45:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings"
                },
                "summary": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models."
                },
                "authors": [
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08669v1",
                "updated": "2025-10-09T17:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching"
                },
                "summary": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Deyang Kong"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Yupei Pan"
                    },
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08351v1",
                "updated": "2025-10-09T15:38:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "FMCache: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMCache: File-System Metadata Caching in Programmable Switches"
                },
                "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "arxiv_affiliation": "The Chinese University of Hong Kong",
                "author": "Patrick P. C. Lee",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08180v1",
                "updated": "2025-10-09T13:06:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:06:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Energy-Efficient Serverless Computing with Hardware Isolation"
                },
                "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW."
                },
                "authors": [
                    {
                        "name": "Natalie Carl"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v4",
                "updated": "2025-10-09T12:05:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    5,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v2",
                "updated": "2025-10-09T12:01:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    1,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v4",
                "updated": "2025-10-09T09:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    33,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v2",
                "updated": "2025-10-09T09:14:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    14,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v4",
                "updated": "2025-10-09T02:37:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    37,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1145/3771283",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771283",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v2",
                "updated": "2025-10-09T01:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    43,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07667v1",
                "updated": "2025-10-09T01:40:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T01:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies"
                },
                "summary": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators."
                },
                "authors": [
                    {
                        "name": "Binzhe Yuan"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yuefeng Zhang"
                    },
                    {
                        "name": "Haochuan Wan"
                    },
                    {
                        "name": "Zhechen Yuan"
                    },
                    {
                        "name": "Junsheng Chen"
                    },
                    {
                        "name": "Yunxiang He"
                    },
                    {
                        "name": "Junran Ding"
                    },
                    {
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "name": "Chaolin Rao"
                    },
                    {
                        "name": "Wenyan Su"
                    },
                    {
                        "name": "Pingqiang Zhou"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xin Lou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lou"
                },
                "author": "Xin Lou",
                "arxiv_comment": "11 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07651v1",
                "updated": "2025-10-09T00:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T00:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference"
                },
                "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Xiyu Liang"
                    },
                    {
                        "name": "Jiaojiao Zhao"
                    },
                    {
                        "name": "Enmao Diao"
                    }
                ],
                "author_detail": {
                    "name": "Enmao Diao"
                },
                "author": "Enmao Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07499v1",
                "updated": "2025-10-08T19:52:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:52:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"
                },
                "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL)."
                },
                "authors": [
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Taehee Jung"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07486v1",
                "updated": "2025-10-08T19:36:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:36:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding"
                },
                "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500)."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Yilin Guan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "14 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v2",
                "updated": "2025-10-08T18:16:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    18,
                    16,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07297v1",
                "updated": "2025-10-08T17:51:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:51:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Agentic generative AI for media content discovery at the national\n  football league",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic generative AI for media content discovery at the national\n  football league"
                },
                "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."
                },
                "authors": [
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Jake Lee"
                    },
                    {
                        "name": "Ross Claytor"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Michael Chi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chi"
                },
                "author": "Michael Chi",
                "arxiv_comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09665v1",
                "updated": "2025-10-08T00:15:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v2",
                "updated": "2025-10-08T00:06:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    6,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025 Workshop on Multi-Turn Interactions in Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v3",
                "updated": "2025-10-07T22:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    22,
                    7,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference"
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06415v1",
                "updated": "2025-10-07T19:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Enhanced Breakdown Voltage in $Î²$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown Voltage in $Î²$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing"
                },
                "summary": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments."
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "JosÃ© GÃ³mez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime GalÃ¡n-JimÃ©nez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.14981v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14981v1",
                "updated": "2025-10-16T17:59:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    59,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:59Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    59,
                    3,
                    289,
                    0
                ],
                "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing"
                },
                "summary": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing."
                },
                "authors": [
                    {
                        "name": "Hadi Alzayer"
                    },
                    {
                        "name": "Yunzhi Zhang"
                    },
                    {
                        "name": "Chen Geng"
                    },
                    {
                        "name": "Jia-Bin Huang"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "Project page: https://coupled-diffusion.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14981v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14981v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14980v1",
                "updated": "2025-10-16T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "Agentic Design of Compositional Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Design of Compositional Machines"
                },
                "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning."
                },
                "authors": [
                    {
                        "name": "Wenqian Zhang"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Zhen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Liu"
                },
                "author": "Zhen Liu",
                "arxiv_comment": "75 pages, 31 figures, Project Page: https://besiegefield.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14976v1",
                "updated": "2025-10-16T17:59:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    56,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:56Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    56,
                    3,
                    289,
                    0
                ],
                "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation"
                },
                "summary": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework."
                },
                "authors": [
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Bing Zhou"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://stevenlsw.github.io/ponimator/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14972v1",
                "updated": "2025-10-16T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    45,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    45,
                    3,
                    289,
                    0
                ],
                "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar"
                },
                "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs."
                },
                "authors": [
                    {
                        "name": "Yinxi Li"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Pengyu Nie"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Nie"
                },
                "author": "Pengyu Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14969v1",
                "updated": "2025-10-16T17:59:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:38Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training"
                },
                "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Yuedong Cui"
                    },
                    {
                        "name": "Ruichen Zheng"
                    },
                    {
                        "name": "Zhiqian Li"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xueqing Wu"
                    },
                    {
                        "name": "Chenchen Ye"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "Preprint. Project page:\n  https://ui-simulator.notion.site/llms-as-scalable-digital-world-simulator;\n  Code and data: https://github.com/WadeYin9712/UI-Simulator",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14970v1",
                "updated": "2025-10-16T17:59:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:38Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "title": "Biology-informed neural networks learn nonlinear representations from\n  omics data to improve genomic prediction and interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biology-informed neural networks learn nonlinear representations from\n  omics data to improve genomic prediction and interpretability"
                },
                "summary": "We extend biologically-informed neural networks (BINNs) for genomic\nprediction (GP) and selection (GS) in crops by integrating thousands of\nsingle-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior\nbiological knowledge. Traditional genotype-to-phenotype (G2P) models depend\nheavily on direct mappings that achieve only modest accuracy, forcing breeders\nto conduct large, costly field trials to maintain or marginally improve genetic\ngain. Models that incorporate intermediate molecular phenotypes such as gene\nexpression can achieve higher predictive fit, but they remain impractical for\nGS since such data are unavailable at deployment or design time. BINNs overcome\nthis limitation by encoding pathway-level inductive biases and leveraging\nmulti-omics data only during training, while using genotype data alone during\ninference. Applied to maize gene-expression and multi-environment field-trial\ndata, BINN improves rank-correlation accuracy by up to 56% within and across\nsubpopulations under sparse-data conditions and nonlinearly identifies genes\nthat GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic\nmetabolomics benchmark, BINN reduces prediction error by 75% relative to\nconventional neural nets and correctly identifies the most important nonlinear\npathway. Importantly, both cases show highly sensitive BINN latent variables\ncorrelate with the experimental quantities they represent, despite not being\ntrained on them. This suggests BINNs learn biologically-relevant\nrepresentations, nonlinear or linear, from genotype to phenotype. Together,\nBINNs establish a framework that leverages intermediate domain information to\nimprove genomic prediction accuracy and reveal nonlinear biological\nrelationships that can guide genomic selection, candidate gene selection,\npathway enrichment, and gene-editing prioritization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend biologically-informed neural networks (BINNs) for genomic\nprediction (GP) and selection (GS) in crops by integrating thousands of\nsingle-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior\nbiological knowledge. Traditional genotype-to-phenotype (G2P) models depend\nheavily on direct mappings that achieve only modest accuracy, forcing breeders\nto conduct large, costly field trials to maintain or marginally improve genetic\ngain. Models that incorporate intermediate molecular phenotypes such as gene\nexpression can achieve higher predictive fit, but they remain impractical for\nGS since such data are unavailable at deployment or design time. BINNs overcome\nthis limitation by encoding pathway-level inductive biases and leveraging\nmulti-omics data only during training, while using genotype data alone during\ninference. Applied to maize gene-expression and multi-environment field-trial\ndata, BINN improves rank-correlation accuracy by up to 56% within and across\nsubpopulations under sparse-data conditions and nonlinearly identifies genes\nthat GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic\nmetabolomics benchmark, BINN reduces prediction error by 75% relative to\nconventional neural nets and correctly identifies the most important nonlinear\npathway. Importantly, both cases show highly sensitive BINN latent variables\ncorrelate with the experimental quantities they represent, despite not being\ntrained on them. This suggests BINNs learn biologically-relevant\nrepresentations, nonlinear or linear, from genotype to phenotype. Together,\nBINNs establish a framework that leverages intermediate domain information to\nimprove genomic prediction accuracy and reveal nonlinear biological\nrelationships that can guide genomic selection, candidate gene selection,\npathway enrichment, and gene-editing prioritization."
                },
                "authors": [
                    {
                        "name": "Katiana Kontolati"
                    },
                    {
                        "name": "Rini Jasmine Gladstone"
                    },
                    {
                        "name": "Ian Davis"
                    },
                    {
                        "name": "Ethan Pickering"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Pickering"
                },
                "author": "Ethan Pickering",
                "arxiv_comment": "35 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14967v1",
                "updated": "2025-10-16T17:59:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    32,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:32Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    32,
                    3,
                    289,
                    0
                ],
                "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents"
                },
                "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency."
                },
                "authors": [
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Guangze Ye"
                    },
                    {
                        "name": "Zeyu Gan"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhe Ying"
                },
                "author": "Zhenzhe Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14966v1",
                "updated": "2025-10-16T17:59:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    25,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:25Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    25,
                    3,
                    289,
                    0
                ],
                "title": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity\n  in TVD-MI Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity\n  in TVD-MI Scores"
                },
                "summary": "Pairwise comparisons of large language models using total variation distance\nmutual information (TVD-MI) produce binary critic decisions per pair. We show\nthat averaging TVD-MI's binary trials yields centered-probability scores with\nadditive structure suitable for item-response theory (IRT) without nonlinear\nlink functions. Maximum-likelihood approaches to IRT use logistic links, but we\nfind empirically that these transformations introduce curvature that breaks\nadditivity: across three domains, the identity link yields median curl on raw\ndata of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce\nsubstantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We\nderive this clipped-linear model from Gini entropy maximization, yielding a\nbox-constrained least-squares formulation that handles boundary saturation. At\n33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent\nrankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations\nthan full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows\nstrong agreement in agent rankings ($\\rho = 0.872$) and consistent\nidentity-link advantage. TVD-MI's geometry is best preserved by identity\nmapping for efficient LLM evaluation, applicable to other bounded-response\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise comparisons of large language models using total variation distance\nmutual information (TVD-MI) produce binary critic decisions per pair. We show\nthat averaging TVD-MI's binary trials yields centered-probability scores with\nadditive structure suitable for item-response theory (IRT) without nonlinear\nlink functions. Maximum-likelihood approaches to IRT use logistic links, but we\nfind empirically that these transformations introduce curvature that breaks\nadditivity: across three domains, the identity link yields median curl on raw\ndata of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce\nsubstantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We\nderive this clipped-linear model from Gini entropy maximization, yielding a\nbox-constrained least-squares formulation that handles boundary saturation. At\n33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent\nrankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations\nthan full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows\nstrong agreement in agent rankings ($\\rho = 0.872$) and consistent\nidentity-link advantage. TVD-MI's geometry is best preserved by identity\nmapping for efficient LLM evaluation, applicable to other bounded-response\ndomains."
                },
                "authors": [
                    {
                        "name": "Zachary Robertson"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Robertson"
                },
                "author": "Zachary Robertson",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14961v1",
                "updated": "2025-10-16T17:59:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:07Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    7,
                    3,
                    289,
                    0
                ],
                "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models"
                },
                "summary": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models."
                },
                "authors": [
                    {
                        "name": "Jonas Geiping"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Guinan Su"
                    }
                ],
                "author_detail": {
                    "name": "Guinan Su"
                },
                "author": "Guinan Su",
                "arxiv_comment": "Code can be found at https://github.com/seal-rg/recurrent-pretraining",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14958v1",
                "updated": "2025-10-16T17:58:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    58,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:58:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    58,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning"
                },
                "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/"
                },
                "authors": [
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Aldrich Yu"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Xinyu Fu"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Linjiang Huang"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://mathcanvas.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14946v1",
                "updated": "2025-10-16T17:55:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    56,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:55:56Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    56,
                    3,
                    289,
                    0
                ],
                "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge\n  Devices"
                },
                "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity."
                },
                "authors": [
                    {
                        "name": "Romina Aalishah"
                    },
                    {
                        "name": "Mozhgan Navardi"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "The 11th IEEE International Conference on Edge Computing and Scalable\n  Cloud (IEEE EdgeCom 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14944v1",
                "updated": "2025-10-16T17:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    14,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:55:14Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    14,
                    3,
                    289,
                    0
                ],
                "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch."
                },
                "authors": [
                    {
                        "name": "Yuxing Lu"
                    },
                    {
                        "name": "Xukai Zhao"
                    },
                    {
                        "name": "J. Ben Tamo"
                    },
                    {
                        "name": "Micky C. Nnamdi"
                    },
                    {
                        "name": "Rui Peng"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Xingyu Hu"
                    },
                    {
                        "name": "Jinzhuo Wang"
                    },
                    {
                        "name": "May D. Wang"
                    }
                ],
                "author_detail": {
                    "name": "May D. Wang"
                },
                "author": "May D. Wang",
                "arxiv_comment": "22 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14943v1",
                "updated": "2025-10-16T17:55:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    11,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:55:11Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    11,
                    3,
                    289,
                    0
                ],
                "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Yiju Guo"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Yankai Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yankai Lin"
                },
                "author": "Yankai Lin",
                "arxiv_comment": "Work in progress. Github repo: https://github.com/RUCBM/LaSeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14942v1",
                "updated": "2025-10-16T17:54:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    54,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:54:07Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    54,
                    7,
                    3,
                    289,
                    0
                ],
                "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning"
                },
                "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning."
                },
                "authors": [
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Weiguo Li"
                    },
                    {
                        "name": "Haokun Chen"
                    },
                    {
                        "name": "Jingpei Wu"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06371v2",
                "updated": "2025-10-16T17:51:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    51,
                    15,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-09T18:27:32Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    18,
                    27,
                    32,
                    4,
                    129,
                    0
                ],
                "title": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement\n  and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement\n  and Optimization"
                },
                "summary": "As the adoption of Generative AI in real-world services grow explosively,\nenergy has emerged as a critical bottleneck resource. However, energy remains a\nmetric that is often overlooked, under-explored, or poorly understood in the\ncontext of building ML systems. We present the ML$.$ENERGY Benchmark, a\nbenchmark suite and tool for measuring inference energy consumption under\nrealistic service environments, and the corresponding ML$.$ENERGY Leaderboard,\nwhich have served as a valuable resource for those hoping to understand and\noptimize the energy consumption of their generative AI services. In this paper,\nwe explain four key design principles for benchmarking ML energy we have\nacquired over time, and then describe how they are implemented in the\nML$.$ENERGY Benchmark. We then highlight results from the early 2025 iteration\nof the benchmark, including energy measurements of 40 widely used model\narchitectures across 6 different tasks, case studies of how ML design choices\nimpact energy consumption, and how automated optimization recommendations can\nlead to significant (sometimes more than 40%) energy savings without changing\nwhat is being computed by the model. The ML$.$ENERGY Benchmark is open-source\nand can be easily extended to various customized models and application\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the adoption of Generative AI in real-world services grow explosively,\nenergy has emerged as a critical bottleneck resource. However, energy remains a\nmetric that is often overlooked, under-explored, or poorly understood in the\ncontext of building ML systems. We present the ML$.$ENERGY Benchmark, a\nbenchmark suite and tool for measuring inference energy consumption under\nrealistic service environments, and the corresponding ML$.$ENERGY Leaderboard,\nwhich have served as a valuable resource for those hoping to understand and\noptimize the energy consumption of their generative AI services. In this paper,\nwe explain four key design principles for benchmarking ML energy we have\nacquired over time, and then describe how they are implemented in the\nML$.$ENERGY Benchmark. We then highlight results from the early 2025 iteration\nof the benchmark, including energy measurements of 40 widely used model\narchitectures across 6 different tasks, case studies of how ML design choices\nimpact energy consumption, and how automated optimization recommendations can\nlead to significant (sometimes more than 40%) energy savings without changing\nwhat is being computed by the model. The ML$.$ENERGY Benchmark is open-source\nand can be easily extended to various customized models and application\nscenarios."
                },
                "authors": [
                    {
                        "name": "Jae-Won Chung"
                    },
                    {
                        "name": "Jeff J. Ma"
                    },
                    {
                        "name": "Ruofan Wu"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Oh Jun Kweon"
                    },
                    {
                        "name": "Yuxuan Xia"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "arxiv_comment": "NeurIPS D&B 2025 (Spotlight) | Benchmark:\n  https://github.com/ml-energy/benchmark | Leaderboard:\n  https://ml.energy/leaderboard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14937v1",
                "updated": "2025-10-16T17:50:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    50,
                    4,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:50:04Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    50,
                    4,
                    3,
                    289,
                    0
                ],
                "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World\n  Clinical Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World\n  Clinical Conversations"
                },
                "summary": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Julina Maharjan"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Karin G. Coifman"
                    },
                    {
                        "name": "Ruoming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ruoming Jin"
                },
                "author": "Ruoming Jin",
                "arxiv_comment": "7 pages 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14936v1",
                "updated": "2025-10-16T17:49:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    49,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:49:41Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    49,
                    41,
                    3,
                    289,
                    0
                ],
                "title": "Circuit Insights: Towards Interpretability Beyond Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit Insights: Towards Interpretability Beyond Activations"
                },
                "summary": "The fields of explainable AI and mechanistic interpretability aim to uncover\nthe internal structure of neural networks, with circuit discovery as a central\ntool for understanding model computations. Existing approaches, however, rely\non manual inspection and remain limited to toy tasks. Automated\ninterpretability offers scalability by analyzing isolated features and their\nactivations, but it often misses interactions between features and depends\nstrongly on external LLMs and dataset quality. Transcoders have recently made\nit possible to separate feature attributions into input-dependent and\ninput-invariant components, providing a foundation for more systematic circuit\nanalysis. Building on this, we propose WeightLens and CircuitLens, two\ncomplementary methods that go beyond activation-based analysis. WeightLens\ninterprets features directly from their learned weights, removing the need for\nexplainer models or datasets while matching or exceeding the performance of\nexisting methods on context-independent features. CircuitLens captures how\nfeature activations arise from interactions between components, revealing\ncircuit-level dynamics that activation-only approaches cannot identify.\nTogether, these methods increase interpretability robustness and enhance\nscalable mechanistic analysis of circuits while maintaining efficiency and\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fields of explainable AI and mechanistic interpretability aim to uncover\nthe internal structure of neural networks, with circuit discovery as a central\ntool for understanding model computations. Existing approaches, however, rely\non manual inspection and remain limited to toy tasks. Automated\ninterpretability offers scalability by analyzing isolated features and their\nactivations, but it often misses interactions between features and depends\nstrongly on external LLMs and dataset quality. Transcoders have recently made\nit possible to separate feature attributions into input-dependent and\ninput-invariant components, providing a foundation for more systematic circuit\nanalysis. Building on this, we propose WeightLens and CircuitLens, two\ncomplementary methods that go beyond activation-based analysis. WeightLens\ninterprets features directly from their learned weights, removing the need for\nexplainer models or datasets while matching or exceeding the performance of\nexisting methods on context-independent features. CircuitLens captures how\nfeature activations arise from interactions between components, revealing\ncircuit-level dynamics that activation-only approaches cannot identify.\nTogether, these methods increase interpretability robustness and enhance\nscalable mechanistic analysis of circuits while maintaining efficiency and\nquality."
                },
                "authors": [
                    {
                        "name": "Elena Golimblevskaia"
                    },
                    {
                        "name": "Aakriti Jain"
                    },
                    {
                        "name": "Bruno Puri"
                    },
                    {
                        "name": "Ammar Ibrahim"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07670v2",
                "updated": "2025-10-16T17:48:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    48,
                    29,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-09T01:48:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    48,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Ctrl-VI: Controllable Video Synthesis via Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ctrl-VI: Controllable Video Synthesis via Variational Inference"
                },
                "summary": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop Ctrl-VI, a video synthesis method that addresses this\nneed and generates samples with high controllability for specified elements\nwhile maintaining diversity for under-specified ones. We cast the task as\nvariational inference to approximate a composed distribution, leveraging\nmultiple video generation backbones to account for all task constraints\ncollectively. To address the optimization challenge, we break down the problem\ninto step-wise KL divergence minimization over an annealed sequence of\ndistributions, and further propose a context-conditioned factorization\ntechnique that reduces modes in the solution space to circumvent local optima.\nExperiments suggest that our method produces samples with improved\ncontrollability, diversity, and 3D consistency compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many video workflows benefit from a mixture of user controls with varying\ngranularity, from exact 4D object trajectories and camera paths to coarse text\nprompts, while existing video generative models are typically trained for fixed\ninput formats. We develop Ctrl-VI, a video synthesis method that addresses this\nneed and generates samples with high controllability for specified elements\nwhile maintaining diversity for under-specified ones. We cast the task as\nvariational inference to approximate a composed distribution, leveraging\nmultiple video generation backbones to account for all task constraints\ncollectively. To address the optimization challenge, we break down the problem\ninto step-wise KL divergence minimization over an annealed sequence of\ndistributions, and further propose a context-conditioned factorization\ntechnique that reduces modes in the solution space to circumvent local optima.\nExperiments suggest that our method produces samples with improved\ncontrollability, diversity, and 3D consistency compared to prior works."
                },
                "authors": [
                    {
                        "name": "Haoyi Duan"
                    },
                    {
                        "name": "Yunzhi Zhang"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Jiajun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jiajun Wu"
                },
                "author": "Jiajun Wu",
                "arxiv_comment": "Project page: https://video-synthesis-variational.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07981v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07981v2",
                "updated": "2025-10-16T17:45:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    45,
                    44,
                    3,
                    289,
                    0
                ],
                "published": "2025-07-10T17:55:05Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    55,
                    5,
                    3,
                    191,
                    0
                ],
                "title": "Why is Your Language Model a Poor Implicit Reward Model?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why is Your Language Model a Poor Implicit Reward Model?"
                },
                "summary": "Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Toward a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models are key to language model post-training and inference\npipelines. Conveniently, recent work showed that every language model defines\nan implicit reward model (IM-RM), without requiring any architectural changes.\nHowever, such IM-RMs tend to generalize worse, especially out-of-distribution,\ncompared to explicit reward models (EX-RMs) that apply a dedicated linear head\nover the hidden representations of a language model. The existence of a\ngeneralization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They\ncan be trained using the same data, loss function, and language model, and\ndiffer only in how the reward is computed. Toward a fundamental understanding\nof the implicit biases underlying different reward model types, we investigate\nthe root cause of this gap. Our main finding, backed by theory and experiments,\nis that IM-RMs rely more heavily on superficial token-level cues. Consequently,\nthey often generalize worse than EX-RMs under token-level distribution shifts,\nas well as in-distribution. Furthermore, we provide evidence against\nalternative hypotheses for the generalization gap. Most notably, we challenge\nthe intuitive claim that IM-RMs struggle in tasks where generation is harder\nthan verification because they can operate both as a verifier and a generator.\nTaken together, our results highlight that seemingly minor design choices can\nsubstantially impact the generalization behavior of reward models."
                },
                "authors": [
                    {
                        "name": "Noam Razin"
                    },
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Jiarui Yao"
                    },
                    {
                        "name": "Sanjeev Arora"
                    }
                ],
                "author_detail": {
                    "name": "Sanjeev Arora"
                },
                "author": "Sanjeev Arora",
                "arxiv_comment": "Code available at https://github.com/princeton-pli/exrm-vs-imrm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07981v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07981v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23339v2",
                "updated": "2025-10-16T17:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    43,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-06-29T17:17:04Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    17,
                    17,
                    4,
                    6,
                    180,
                    0
                ],
                "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular\n  Design"
                },
                "summary": "Large Language Models demonstrate substantial promise for advancing\nscientific discovery, yet their deployment in disciplines demanding factual\nprecision and specialized domain constraints presents significant challenges.\nWithin molecular design for pharmaceutical development, these models can\npropose innovative molecular modifications but frequently generate chemically\ninfeasible structures. We introduce VALID-Mol, a comprehensive framework that\nintegrates chemical validation with LLM-driven molecular design, achieving an\nimprovement in valid chemical structure generation from 3% to 83%. Our\nmethodology synthesizes systematic prompt optimization, automated chemical\nverification, and domain-adapted fine-tuning to ensure dependable generation of\nsynthesizable molecules with enhanced properties. Our contribution extends\nbeyond implementation details to provide a transferable methodology for\nscientifically-constrained LLM applications with measurable reliability\nenhancements. Computational analyses indicate our framework generates promising\nsynthesis candidates with up to 17-fold predicted improvements in target\nbinding affinity while preserving synthetic feasibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models demonstrate substantial promise for advancing\nscientific discovery, yet their deployment in disciplines demanding factual\nprecision and specialized domain constraints presents significant challenges.\nWithin molecular design for pharmaceutical development, these models can\npropose innovative molecular modifications but frequently generate chemically\ninfeasible structures. We introduce VALID-Mol, a comprehensive framework that\nintegrates chemical validation with LLM-driven molecular design, achieving an\nimprovement in valid chemical structure generation from 3% to 83%. Our\nmethodology synthesizes systematic prompt optimization, automated chemical\nverification, and domain-adapted fine-tuning to ensure dependable generation of\nsynthesizable molecules with enhanced properties. Our contribution extends\nbeyond implementation details to provide a transferable methodology for\nscientifically-constrained LLM applications with measurable reliability\nenhancements. Computational analyses indicate our framework generates promising\nsynthesis candidates with up to 17-fold predicted improvements in target\nbinding affinity while preserving synthetic feasibility."
                },
                "authors": [
                    {
                        "name": "Malikussaid"
                    },
                    {
                        "name": "Hilal Hudan Nuha"
                    },
                    {
                        "name": "Isman Kurniawan"
                    }
                ],
                "author_detail": {
                    "name": "Isman Kurniawan"
                },
                "author": "Isman Kurniawan",
                "arxiv_comment": "6 pages, 1 figure, 1 algorithm, 5 tables, to be published in ISPACS\n  2025, unabridged version exists as arXiv:2506.23339v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 92E10, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; I.2.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14927v1",
                "updated": "2025-10-16T17:40:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    55,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:40:55Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    55,
                    3,
                    289,
                    0
                ],
                "title": "NIRPS and TESS reveal a peculiar system around the M dwarf TOI-756: A\n  transiting sub-Neptune and a cold eccentric giant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NIRPS and TESS reveal a peculiar system around the M dwarf TOI-756: A\n  transiting sub-Neptune and a cold eccentric giant"
                },
                "summary": "The Near InfraRed Planet Searcher (NIRPS) joined HARPS on the 3.6-m ESO\ntelescope at La Silla Observatory in April 2023, dedicating part of its\nGuaranteed Time Observations (GTO) program to the radial velocity follow-up of\nTESS planet candidates to confirm and characterize transiting planets around M\ndwarfs. We report the first results of this program with the characterization\nof the TOI-756 system, which consists of TOI-756 b, a transiting sub-Neptune\ncandidate detected by TESS, as well as TOI-756 c, an additional non-transiting\nplanet discovered by NIRPS and HARPS. TOI-756 b is a 1.24-day period\nsub-Neptune with a radius of 2.81 $\\pm$ 0.10 $R_\\oplus$ and a mass of\n9.8$^{+1.8}_{-1.6}$ $M_\\oplus$. TOI-756 c is a cold eccentric (e$_c$ = 0.45\n$\\pm$ 0.01) giant planet orbiting with a period of 149.6 days around its star\nwith a minimum mass of 4.05 $\\pm$ 0.11 $M_\\mathrm{jup}$. Additionally, a linear\ntrend of 146$~\\mathrm{m\\,s}^{-1}\\,\\mathrm{yr}^{-1}$ is visible in the radial\nvelocities, hinting at a third component, possibly in the planetary or brown\ndwarf regime. This system is unique in the exoplanet landscape, standing as the\nfirst confirmed example of such a planetary architecture around an M dwarf.\nWith a density of 2.42 $\\pm$ 0.49 g cm$^{-3}$, the inner planet, TOI-756 b, is\na volatile-rich sub-Neptune. Assuming a pure H/He envelope, we inferred an\natmospheric mass fraction of 0.023 and a core mass fraction of 0.27, which is\nwell constrained by stellar refractory abundances derived from NIRPS spectra.\nIt falls within the still poorly explored radius cliff and at the lower\nboundary of the Neptune desert, making it a prime target for a future\natmospheric characterization with JWST to improve our understanding of this\npopulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Near InfraRed Planet Searcher (NIRPS) joined HARPS on the 3.6-m ESO\ntelescope at La Silla Observatory in April 2023, dedicating part of its\nGuaranteed Time Observations (GTO) program to the radial velocity follow-up of\nTESS planet candidates to confirm and characterize transiting planets around M\ndwarfs. We report the first results of this program with the characterization\nof the TOI-756 system, which consists of TOI-756 b, a transiting sub-Neptune\ncandidate detected by TESS, as well as TOI-756 c, an additional non-transiting\nplanet discovered by NIRPS and HARPS. TOI-756 b is a 1.24-day period\nsub-Neptune with a radius of 2.81 $\\pm$ 0.10 $R_\\oplus$ and a mass of\n9.8$^{+1.8}_{-1.6}$ $M_\\oplus$. TOI-756 c is a cold eccentric (e$_c$ = 0.45\n$\\pm$ 0.01) giant planet orbiting with a period of 149.6 days around its star\nwith a minimum mass of 4.05 $\\pm$ 0.11 $M_\\mathrm{jup}$. Additionally, a linear\ntrend of 146$~\\mathrm{m\\,s}^{-1}\\,\\mathrm{yr}^{-1}$ is visible in the radial\nvelocities, hinting at a third component, possibly in the planetary or brown\ndwarf regime. This system is unique in the exoplanet landscape, standing as the\nfirst confirmed example of such a planetary architecture around an M dwarf.\nWith a density of 2.42 $\\pm$ 0.49 g cm$^{-3}$, the inner planet, TOI-756 b, is\na volatile-rich sub-Neptune. Assuming a pure H/He envelope, we inferred an\natmospheric mass fraction of 0.023 and a core mass fraction of 0.27, which is\nwell constrained by stellar refractory abundances derived from NIRPS spectra.\nIt falls within the still poorly explored radius cliff and at the lower\nboundary of the Neptune desert, making it a prime target for a future\natmospheric characterization with JWST to improve our understanding of this\npopulation."
                },
                "authors": [
                    {
                        "name": "LÃ©na Parc"
                    },
                    {
                        "name": "FranÃ§ois Bouchy"
                    },
                    {
                        "name": "Neil J. Cook"
                    },
                    {
                        "name": "Nolan Grieves"
                    },
                    {
                        "name": "Ãtienne Artigau"
                    },
                    {
                        "name": "Alexandrine L'Heureux"
                    },
                    {
                        "name": "RenÃ© Doyon"
                    },
                    {
                        "name": "Yuri S. Messias"
                    },
                    {
                        "name": "FrÃ©dÃ©rique Baron"
                    },
                    {
                        "name": "Susana C. C. Barros"
                    },
                    {
                        "name": "BjÃ¶rn Benneke"
                    },
                    {
                        "name": "Xavier Bonfils"
                    },
                    {
                        "name": "Marta Bryan"
                    },
                    {
                        "name": "Bruno L. Canto Martins"
                    },
                    {
                        "name": "Ryan Cloutier"
                    },
                    {
                        "name": "Nicolas B. Cowan"
                    },
                    {
                        "name": "Daniel Brito de Freitas"
                    },
                    {
                        "name": "Jose Renan De Medeiros"
                    },
                    {
                        "name": "Xavier Delfosse"
                    },
                    {
                        "name": "Elisa Delgado-Mena"
                    },
                    {
                        "name": "Xavier Dumusque"
                    },
                    {
                        "name": "David Ehrenreich"
                    },
                    {
                        "name": "Pedro Figueira"
                    },
                    {
                        "name": "Jonay I. GonzÃ¡lez HernÃ¡ndez"
                    },
                    {
                        "name": "David LafreniÃ¨re"
                    },
                    {
                        "name": "Izan de Castro LeÃ£o"
                    },
                    {
                        "name": "Christophe Lovis"
                    },
                    {
                        "name": "Lison Malo"
                    },
                    {
                        "name": "Claudio Melo"
                    },
                    {
                        "name": "Lucile Mignon"
                    },
                    {
                        "name": "Christoph Mordasini"
                    },
                    {
                        "name": "Francesco Pepe"
                    },
                    {
                        "name": "Rafael Rebolo"
                    },
                    {
                        "name": "Jason Rowe"
                    },
                    {
                        "name": "Nuno C. Santos"
                    },
                    {
                        "name": "Damien SÃ©gransan"
                    },
                    {
                        "name": "Alejandro SuÃ¡rez MascareÃ±o"
                    },
                    {
                        "name": "StÃ©phane Udry"
                    },
                    {
                        "name": "Diana Valencia"
                    },
                    {
                        "name": "Gregg Wade"
                    },
                    {
                        "name": "Manuel Abreu"
                    },
                    {
                        "name": "JosÃ© L. A. Aguiar"
                    },
                    {
                        "name": "Khaled Al Moulla"
                    },
                    {
                        "name": "Guillaume Allain"
                    },
                    {
                        "name": "Romain Allart"
                    },
                    {
                        "name": "Jose Manuel Almenara"
                    },
                    {
                        "name": "Tomy Arial"
                    },
                    {
                        "name": "Hugues Auger"
                    },
                    {
                        "name": "Luc Bazinet"
                    },
                    {
                        "name": "Nicolas Blind"
                    },
                    {
                        "name": "David Bohlender"
                    },
                    {
                        "name": "Isabelle Boisse"
                    },
                    {
                        "name": "Anne Boucher"
                    },
                    {
                        "name": "Vincent Bourrier"
                    },
                    {
                        "name": "SÃ©bastien Bovay"
                    },
                    {
                        "name": "Pedro Branco"
                    },
                    {
                        "name": "Christopher Broeg"
                    },
                    {
                        "name": "Denis Brousseau"
                    },
                    {
                        "name": "Alexandre Cabral"
                    },
                    {
                        "name": "Charles Cadieux"
                    },
                    {
                        "name": "Andres Carmona"
                    },
                    {
                        "name": "Yann Carteret"
                    },
                    {
                        "name": "Zalpha Challita"
                    },
                    {
                        "name": "David Charbonneau"
                    },
                    {
                        "name": "Bruno Chazelas"
                    },
                    {
                        "name": "Catherine A. Clark"
                    },
                    {
                        "name": "JoÃ£o Coelho"
                    },
                    {
                        "name": "Marion Cointepas"
                    },
                    {
                        "name": "Karen A. Collins"
                    },
                    {
                        "name": "Kevin I. Collins"
                    },
                    {
                        "name": "Uriel Conod"
                    },
                    {
                        "name": "Eduardo Cristo"
                    },
                    {
                        "name": "Ana Rita Costa Silva"
                    },
                    {
                        "name": "Antoine Darveau-Bernier"
                    },
                    {
                        "name": "Laurie Dauplaise"
                    },
                    {
                        "name": "Jean-Baptiste Delisle"
                    },
                    {
                        "name": "Roseane de Lima Gomes"
                    },
                    {
                        "name": "JoÃ£o Faria"
                    },
                    {
                        "name": "Dasaev O. Fontinele"
                    },
                    {
                        "name": "Thierry Forveille"
                    },
                    {
                        "name": "Yolanda G. C. Frensch"
                    },
                    {
                        "name": "Jonathan GagnÃ©"
                    },
                    {
                        "name": "FrÃ©dÃ©ric Genest"
                    },
                    {
                        "name": "Ludovic Genolet"
                    },
                    {
                        "name": "JoÃ£o Gomes da Silva"
                    },
                    {
                        "name": "FÃ©lix Gracia TÃ©mich"
                    },
                    {
                        "name": "Nicole Gromek"
                    },
                    {
                        "name": "Olivier Hernandez"
                    },
                    {
                        "name": "Melissa J. Hobson"
                    },
                    {
                        "name": "Jens H. Hoeijmakers"
                    },
                    {
                        "name": "Norbert Hubin"
                    },
                    {
                        "name": "Marziye Jafariyazani"
                    },
                    {
                        "name": "Farbod Jahandar"
                    },
                    {
                        "name": "Ray Jayawardhana"
                    },
                    {
                        "name": "Hans-Ulrich KÃ¤ufl"
                    },
                    {
                        "name": "Dan Kerley"
                    },
                    {
                        "name": "Johann Kolb"
                    },
                    {
                        "name": "Vigneshwaran Krishnamurthy"
                    },
                    {
                        "name": "Benjamin Kung"
                    },
                    {
                        "name": "Pierrot Lamontagne"
                    },
                    {
                        "name": "Pierre Larue"
                    },
                    {
                        "name": "Henry Leath"
                    },
                    {
                        "name": "Olivia Lim"
                    },
                    {
                        "name": "Gaspare Lo Curto"
                    },
                    {
                        "name": "Allan M. Martins"
                    },
                    {
                        "name": "Elisabeth C. Matthews"
                    },
                    {
                        "name": "Jaymie Matthews"
                    },
                    {
                        "name": "Jean-SÃ©bastien Mayer"
                    },
                    {
                        "name": "Stan Metchev"
                    },
                    {
                        "name": "Lina Messamah"
                    },
                    {
                        "name": "Leslie Moranta"
                    },
                    {
                        "name": "Dany Mounzer"
                    },
                    {
                        "name": "Nicola Nari"
                    },
                    {
                        "name": "Louise D. Nielsen"
                    },
                    {
                        "name": "Ares Osborn"
                    },
                    {
                        "name": "Mathieu Ouellet"
                    },
                    {
                        "name": "Jon Otegi"
                    },
                    {
                        "name": "Luca Pasquini"
                    },
                    {
                        "name": "Vera M. Passegger"
                    },
                    {
                        "name": "Stefan Pelletier"
                    },
                    {
                        "name": "CÃ©line Peroux"
                    },
                    {
                        "name": "Caroline Piaulet-Ghorayeb"
                    },
                    {
                        "name": "Mykhaylo Plotnykov"
                    },
                    {
                        "name": "Emanuela Pompei"
                    },
                    {
                        "name": "Anne-Sophie Poulin-Girard"
                    },
                    {
                        "name": "JosÃ© Luis Rasilla"
                    },
                    {
                        "name": "Vladimir Reshetov"
                    },
                    {
                        "name": "Jonathan Saint-Antoine"
                    },
                    {
                        "name": "Mirsad Sarajlic"
                    },
                    {
                        "name": "Ivo Saviane"
                    },
                    {
                        "name": "Robin Schnell"
                    },
                    {
                        "name": "Alex Segovia"
                    },
                    {
                        "name": "Julia Seidel"
                    },
                    {
                        "name": "Armin Silber"
                    },
                    {
                        "name": "Peter Sinclair"
                    },
                    {
                        "name": "Michael Sordet"
                    },
                    {
                        "name": "Danuta Sosnowska"
                    },
                    {
                        "name": "Avidaan Srivastava"
                    },
                    {
                        "name": "Atanas K. Stefanov"
                    },
                    {
                        "name": "MÃ¡rcio A. Teixeira"
                    },
                    {
                        "name": "Simon Thibault"
                    },
                    {
                        "name": "Philippe VallÃ©e"
                    },
                    {
                        "name": "Thomas Vandal"
                    },
                    {
                        "name": "Valentina Vaulato"
                    },
                    {
                        "name": "Joost P. Wardenier"
                    },
                    {
                        "name": "Bachar Wehbe"
                    },
                    {
                        "name": "Drew Weisserman"
                    },
                    {
                        "name": "Ivan Wevers"
                    },
                    {
                        "name": "FranÃ§ois Wildi"
                    },
                    {
                        "name": "Vincent Yariv"
                    },
                    {
                        "name": "GÃ©rard Zins"
                    }
                ],
                "author_detail": {
                    "name": "GÃ©rard Zins"
                },
                "author": "GÃ©rard Zins",
                "arxiv_doi": "10.1051/0004-6361/202555684",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202555684",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 15 figures, published in A&A, 2025, 702, A138. Full\n  abstract in the article. All data used to produce the results presented in\n  this article are publicly available at the following link:\n  https://dace.unige.ch/openData/?record=10.82180/dace-voj8hff0",
                "arxiv_journal_ref": "A&A, 702, A138 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14925v1",
                "updated": "2025-10-16T17:40:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:40:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models"
                },
                "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision."
                },
                "authors": [
                    {
                        "name": "Akira Okutomi"
                    }
                ],
                "author_detail": {
                    "name": "Akira Okutomi"
                },
                "author": "Akira Okutomi",
                "arxiv_comment": "19 pages, 2 figures, preliminary version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13091v2",
                "updated": "2025-10-16T17:39:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    39,
                    20,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T02:17:19Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    2,
                    17,
                    19,
                    2,
                    288,
                    0
                ],
                "title": "Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments\n  on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments\n  on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents"
                },
                "summary": "Online freelance marketplaces, a rapidly growing part of the global labor\nmarket, are creating a fair environment where professional skills are the main\nfactor for hiring. While these platforms can reduce bias from traditional\nhiring, the personal information in user profiles raises concerns about ongoing\ndiscrimination. Past studies on this topic have mostly used existing data,\nwhich makes it hard to control for other factors and clearly see the effect of\nthings like gender or race. To solve these problems, this paper presents a new\nmethod that uses Retrieval-Augmented Generation (RAG) with a Large Language\nModel (LLM) to create realistic, artificial freelancer profiles for controlled\nexperiments. This approach effectively separates individual factors, enabling a\nclearer statistical analysis of how different variables influence the\nfreelancer project process. In addition to analyzing extracted data with\ntraditional statistical methods for post-project stage analysis, our research\nutilizes a dataset with highly controlled variables, generated by an RAG-LLM,\nto conduct a simulated hiring experiment for pre-project stage analysis. The\nresults of our experiments show that, regarding gender, while no significant\npreference emerged in initial hiring decisions, female freelancers are\nsubstantially more likely to receive imperfect ratings post-project stage.\nRegarding regional bias, a strong and consistent preference favoring US-based\nfreelancers shows that people are more likely to be selected in the simulated\nexperiments, perceived as more leader-like, and receive higher ratings on the\nlive platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online freelance marketplaces, a rapidly growing part of the global labor\nmarket, are creating a fair environment where professional skills are the main\nfactor for hiring. While these platforms can reduce bias from traditional\nhiring, the personal information in user profiles raises concerns about ongoing\ndiscrimination. Past studies on this topic have mostly used existing data,\nwhich makes it hard to control for other factors and clearly see the effect of\nthings like gender or race. To solve these problems, this paper presents a new\nmethod that uses Retrieval-Augmented Generation (RAG) with a Large Language\nModel (LLM) to create realistic, artificial freelancer profiles for controlled\nexperiments. This approach effectively separates individual factors, enabling a\nclearer statistical analysis of how different variables influence the\nfreelancer project process. In addition to analyzing extracted data with\ntraditional statistical methods for post-project stage analysis, our research\nutilizes a dataset with highly controlled variables, generated by an RAG-LLM,\nto conduct a simulated hiring experiment for pre-project stage analysis. The\nresults of our experiments show that, regarding gender, while no significant\npreference emerged in initial hiring decisions, female freelancers are\nsubstantially more likely to receive imperfect ratings post-project stage.\nRegarding regional bias, a strong and consistent preference favoring US-based\nfreelancers shows that people are more likely to be selected in the simulated\nexperiments, perceived as more leader-like, and receive higher ratings on the\nlive platform."
                },
                "authors": [
                    {
                        "name": "Wugeng Zheng"
                    },
                    {
                        "name": "Guohou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Guohou Shan"
                },
                "author": "Guohou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12706v2",
                "updated": "2025-10-16T17:38:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    38,
                    53,
                    3,
                    289,
                    0
                ],
                "published": "2025-01-22T08:23:10Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    8,
                    23,
                    10,
                    2,
                    22,
                    0
                ],
                "title": "REX: Causal discovery based on machine learning and explainability\n  techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REX: Causal discovery based on machine learning and explainability\n  techniques"
                },
                "summary": "Explainable Artificial Intelligence (XAI) techniques hold significant\npotential for enhancing the causal discovery process, which is crucial for\nunderstanding complex systems in areas like healthcare, economics, and\nartificial intelligence. However, no causal discovery methods currently\nincorporate explainability into their models to derive the causal graphs. Thus,\nin this paper we explore this innovative approach, as it offers substantial\npotential and represents a promising new direction worth investigating.\nSpecifically, we introduce ReX, a causal discovery method that leverages\nmachine learning (ML) models coupled with explainability techniques,\nspecifically Shapley values, to identify and interpret significant causal\nrelationships among variables. Comparative evaluations on synthetic datasets\ncomprising continuous tabular data reveal that ReX outperforms state-of-the-art\ncausal discovery methods across diverse data generation processes, including\nnon-linear and additive noise models. Moreover, ReX was tested on the Sachs\nsingle-cell protein-signaling dataset, achieving a precision of 0.952 and\nrecovering key causal relationships with no incorrect edges. Taking together,\nthese results showcase ReX's effectiveness in accurately recovering true causal\nstructures while minimizing false positive predictions, its robustness across\ndiverse datasets, and its applicability to real-world problems. By combining ML\nand explainability techniques with causal discovery, ReX bridges the gap\nbetween predictive modeling and causal inference, offering an effective tool\nfor understanding complex causal structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable Artificial Intelligence (XAI) techniques hold significant\npotential for enhancing the causal discovery process, which is crucial for\nunderstanding complex systems in areas like healthcare, economics, and\nartificial intelligence. However, no causal discovery methods currently\nincorporate explainability into their models to derive the causal graphs. Thus,\nin this paper we explore this innovative approach, as it offers substantial\npotential and represents a promising new direction worth investigating.\nSpecifically, we introduce ReX, a causal discovery method that leverages\nmachine learning (ML) models coupled with explainability techniques,\nspecifically Shapley values, to identify and interpret significant causal\nrelationships among variables. Comparative evaluations on synthetic datasets\ncomprising continuous tabular data reveal that ReX outperforms state-of-the-art\ncausal discovery methods across diverse data generation processes, including\nnon-linear and additive noise models. Moreover, ReX was tested on the Sachs\nsingle-cell protein-signaling dataset, achieving a precision of 0.952 and\nrecovering key causal relationships with no incorrect edges. Taking together,\nthese results showcase ReX's effectiveness in accurately recovering true causal\nstructures while minimizing false positive predictions, its robustness across\ndiverse datasets, and its applicability to real-world problems. By combining ML\nand explainability techniques with causal discovery, ReX bridges the gap\nbetween predictive modeling and causal inference, offering an effective tool\nfor understanding complex causal structures."
                },
                "authors": [
                    {
                        "name": "Jesus Renero"
                    },
                    {
                        "name": "Idoia Ochoa"
                    },
                    {
                        "name": "Roberto Maestre"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Maestre"
                },
                "author": "Roberto Maestre",
                "arxiv_doi": "10.1016/j.patcog.2025.112491",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.patcog.2025.112491",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 30 figures, Published in Elsevier's Pattern Recognition",
                "arxiv_journal_ref": "Volume 172, year 2026, pages 112491",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.25822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.25822v3",
                "updated": "2025-10-16T17:37:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    37,
                    59,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-30T05:56:00Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    5,
                    56,
                    0,
                    1,
                    273,
                    0
                ],
                "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for\n  Adaptive Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for\n  Adaptive Policies"
                },
                "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP-AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle-consistent contrastive loss\nthat organizes the gradient flow of the noise predictor into a coherent\nperception-action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP-AG\nsignificantly outperforms state-of-the-art methods across simulation benchmarks\nand real-world UR5 manipulation tasks. As a result, our DP-AG offers a\npromising step toward bridging biological adaptability and artificial policy\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP-AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle-consistent contrastive loss\nthat organizes the gradient flow of the noise predictor into a coherent\nperception-action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP-AG\nsignificantly outperforms state-of-the-art methods across simulation benchmarks\nand real-world UR5 manipulation tasks. As a result, our DP-AG offers a\npromising step toward bridging biological adaptability and artificial policy\nlearning."
                },
                "authors": [
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Weiting Peng"
                    },
                    {
                        "name": "Jing Tang"
                    },
                    {
                        "name": "Zeyu Gong"
                    },
                    {
                        "name": "Xihua Wang"
                    },
                    {
                        "name": "Bo Tao"
                    },
                    {
                        "name": "Li Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Li Cheng"
                },
                "author": "Li Cheng",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.25822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.25822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14919v1",
                "updated": "2025-10-16T17:35:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    35,
                    18,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:35:18Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    35,
                    18,
                    3,
                    289,
                    0
                ],
                "title": "Predicting Task Performance with Context-aware Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Task Performance with Context-aware Scaling Laws"
                },
                "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling."
                },
                "authors": [
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "David Park"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Michael Bendersky"
                    },
                    {
                        "name": "Beliz Gunel"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Chenguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Wang"
                },
                "author": "Chenguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14915v1",
                "updated": "2025-10-16T17:30:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    30,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:30:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    30,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models\n(LLMs) to generate accurate and reliable responses that are grounded in\nretrieved context. However, LLMs often generate inconsistent outputs for\nsemantically equivalent inputs, a problem compounded by the scarcity of\nconsistency-focused training data and the limitations of current fine-tuning\ntechniques in enhancing output consistency. We propose a new approach combining\nsystematic synthetic data generation, triplet loss for better embeddings, and a\nnovel layer-wise model merging approach. Using consistency-aware weights\nderived from intermediate layer activations, our method effectively integrates\nknowledge from specialized models. Experimental results how that our merged\nmodel significantly enhances output consistency, achieving a ~47.5\\%\nimprovement in response similarity over the baseline, thus offering a practical\nsolution for increasing the reliability of an industrial RAG system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models\n(LLMs) to generate accurate and reliable responses that are grounded in\nretrieved context. However, LLMs often generate inconsistent outputs for\nsemantically equivalent inputs, a problem compounded by the scarcity of\nconsistency-focused training data and the limitations of current fine-tuning\ntechniques in enhancing output consistency. We propose a new approach combining\nsystematic synthetic data generation, triplet loss for better embeddings, and a\nnovel layer-wise model merging approach. Using consistency-aware weights\nderived from intermediate layer activations, our method effectively integrates\nknowledge from specialized models. Experimental results how that our merged\nmodel significantly enhances output consistency, achieving a ~47.5\\%\nimprovement in response similarity over the baseline, thus offering a practical\nsolution for increasing the reliability of an industrial RAG system."
                },
                "authors": [
                    {
                        "name": "Xujun Peng"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Jingyu Wu"
                    },
                    {
                        "name": "Parker Glenn"
                    },
                    {
                        "name": "Daben Liu"
                    }
                ],
                "author_detail": {
                    "name": "Daben Liu"
                },
                "author": "Daben Liu",
                "arxiv_comment": "EMNLP 2025 Industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06948v2",
                "updated": "2025-10-16T17:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    30,
                    21,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-08T17:58:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach suffers from catastrophic forgetting: second-stage\nRL gradually loses SFT-acquired behaviors and inefficiently explores new\npatterns. This study introduces a novel method for learning reasoning models\nthat employs bilevel optimization to facilitate better cooperation between\nthese training paradigms. By conditioning the SFT objective on the optimal RL\npolicy, our approach enables SFT to meta-learn how to guide RL's optimization\nprocess. During training, the lower level performs RL updates while\nsimultaneously receiving SFT supervision, and the upper level explicitly\nmaximizes the cooperative gain-the performance advantage of joint SFT-RL\ntraining over RL alone. Empirical evaluations on five reasoning benchmarks\ndemonstrate that our method consistently outperforms baselines and achieves a\nbetter balance between effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach suffers from catastrophic forgetting: second-stage\nRL gradually loses SFT-acquired behaviors and inefficiently explores new\npatterns. This study introduces a novel method for learning reasoning models\nthat employs bilevel optimization to facilitate better cooperation between\nthese training paradigms. By conditioning the SFT objective on the optimal RL\npolicy, our approach enables SFT to meta-learn how to guide RL's optimization\nprocess. During training, the lower level performs RL updates while\nsimultaneously receiving SFT supervision, and the upper level explicitly\nmaximizes the cooperative gain-the performance advantage of joint SFT-RL\ntraining over RL alone. Empirical evaluations on five reasoning benchmarks\ndemonstrate that our method consistently outperforms baselines and achieves a\nbetter balance between effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10315v3",
                "updated": "2025-10-16T17:26:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    26,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-11T18:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    18,
                    47,
                    14,
                    5,
                    284,
                    0
                ],
                "title": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web"
                },
                "summary": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices."
                },
                "authors": [
                    {
                        "name": "Nicolas Steinacker-Olsztyn"
                    },
                    {
                        "name": "Devashish Gosain"
                    },
                    {
                        "name": "Ha Dao"
                    }
                ],
                "author_detail": {
                    "name": "Ha Dao"
                },
                "author": "Ha Dao",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14901v1",
                "updated": "2025-10-16T17:18:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    18,
                    11,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:18:11Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    18,
                    11,
                    3,
                    289,
                    0
                ],
                "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with Sampling: Your Base Model is Smarter Than You Think"
                },
                "summary": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains."
                },
                "authors": [
                    {
                        "name": "Aayush Karan"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14900v1",
                "updated": "2025-10-16T17:17:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    17,
                    0,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:17:00Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    17,
                    0,
                    3,
                    289,
                    0
                ],
                "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent\n  That Improves Without Labels or Model Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent\n  That Improves Without Labels or Model Updates"
                },
                "summary": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions."
                },
                "authors": [
                    {
                        "name": "Wen-Kwang Tsao"
                    },
                    {
                        "name": "Yao-Ching Yu"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14896v1",
                "updated": "2025-10-16T17:13:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    13,
                    33,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:13:33Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    13,
                    33,
                    3,
                    289,
                    0
                ],
                "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection"
                },
                "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies."
                },
                "authors": [
                    {
                        "name": "Furkan Mumcu"
                    },
                    {
                        "name": "Michael J. Jones"
                    },
                    {
                        "name": "Anoop Cherian"
                    },
                    {
                        "name": "Yasin Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Yasin Yilmaz"
                },
                "author": "Yasin Yilmaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14885v1",
                "updated": "2025-10-16T17:04:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    4,
                    25,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:04:25Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    4,
                    25,
                    3,
                    289,
                    0
                ],
                "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition\n  Capabilities of Multimodal Large Language Models with Answer Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You May Speak Freely: Improving the Fine-Grained Visual Recognition\n  Capabilities of Multimodal Large Language Models with Answer Extraction"
                },
                "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage."
                },
                "authors": [
                    {
                        "name": "Logan Lawrence"
                    },
                    {
                        "name": "Oindrila Saha"
                    },
                    {
                        "name": "Megan Wei"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Grant Van Horn"
                    }
                ],
                "author_detail": {
                    "name": "Grant Van Horn"
                },
                "author": "Grant Van Horn",
                "arxiv_comment": "Accepted to WACV26. 12 pages, 8 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14882v1",
                "updated": "2025-10-16T17:00:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    59,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:00:59Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    59,
                    3,
                    289,
                    0
                ],
                "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with\n  Multi-Scale Reference Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with\n  Multi-Scale Reference Attention"
                },
                "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released."
                },
                "authors": [
                    {
                        "name": "Keli Liu"
                    },
                    {
                        "name": "Zhendong Wang"
                    },
                    {
                        "name": "Wengang Zhou"
                    },
                    {
                        "name": "Shaodong Xu"
                    },
                    {
                        "name": "Ruixiao Dong"
                    },
                    {
                        "name": "Houqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Houqiang Li"
                },
                "author": "Houqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14881v1",
                "updated": "2025-10-16T17:00:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:00:42Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    42,
                    3,
                    289,
                    0
                ],
                "title": "The Gatekeeper Knows Enough",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gatekeeper Knows Enough"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain."
                },
                "authors": [
                    {
                        "name": "Fikresilase Wondmeneh Abebayew"
                    }
                ],
                "author_detail": {
                    "name": "Fikresilase Wondmeneh Abebayew"
                },
                "author": "Fikresilase Wondmeneh Abebayew",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14879v1",
                "updated": "2025-10-16T17:00:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    2,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:00:02Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    2,
                    3,
                    289,
                    0
                ],
                "title": "Multi-wavelength analysis of the progenitor of GRB 230307A via Bayesian\n  model comparison",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-wavelength analysis of the progenitor of GRB 230307A via Bayesian\n  model comparison"
                },
                "summary": "GRB 230307A is one of the brightest long-duration gamma-ray bursts (GRBs)\never detected, yet its progenitor remains uncertain due to the variety of\nplausible astrophysical scenarios. In this work, we investigate four possible\nprogenitors for GRB 230307A: a binary neutron star (BNS), a neutron star--white\ndwarf (NS--WD) system, a neutron star--black hole (NS--BH) merger, and a tidal\ndisruption event (TDE) involving a white dwarf and a supermassive black hole.\nAdditionally, we explore three distinct central engine models powering the\nkilonova associated with the BNS: radioactive decay of $r$-process nuclei in a\ntwo-component ejecta model, a magnetar-driven model including magnetic dipole\nspin-down, and a combined model of magnetar spin-down with ${}^{56}$Ni\nradioactive decay. We perform Bayesian multi-wavelength light-curve analyses\nusing physically motivated models and priors, and evaluate model performance\nthrough Bayes factors and leave-one-out cross-validation (LOO) scores. Our\nresults show a statistical preference for a BNS or NS--WD progenitor producing\na kilonova powered by a magnetar and ${}^{56}$Ni decay, characterized by a\n${}^{56}$Ni mass of $\\sim4\\times10^{-4}\\,M_{\\odot}$ and an ejecta mass of\n$0.06\\,M_{\\odot}$. Furthermore, under the assumption of a BNS origin within\nthis model, we infer binary component masses of $m_{1} =\n1.81^{+0.46}_{-0.61}\\,M_{\\odot}$ and $m_{2} = 1.61^{+0.65}_{-0.41}\\,M_{\\odot}$,\nwith a dimensionless tidal deformability of $\\tilde{\\Lambda} =\n471^{+318}_{-395}$. From the component mass posteriors, we infer that the\nobserved offset can be explained by a natal kick as long as the systemic\nvelocity is nearly aligned with the pre-kick orbital motion. In this case, the\nrequired kick velocity (co-moving frame) and binary separation range within\n$v'_{\\mathrm{k}}\\sim100$--$150~\\mathrm{km\\,s^{-1}}$, and\n$a_0\\sim2$--$3~R_{\\odot}$, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRB 230307A is one of the brightest long-duration gamma-ray bursts (GRBs)\never detected, yet its progenitor remains uncertain due to the variety of\nplausible astrophysical scenarios. In this work, we investigate four possible\nprogenitors for GRB 230307A: a binary neutron star (BNS), a neutron star--white\ndwarf (NS--WD) system, a neutron star--black hole (NS--BH) merger, and a tidal\ndisruption event (TDE) involving a white dwarf and a supermassive black hole.\nAdditionally, we explore three distinct central engine models powering the\nkilonova associated with the BNS: radioactive decay of $r$-process nuclei in a\ntwo-component ejecta model, a magnetar-driven model including magnetic dipole\nspin-down, and a combined model of magnetar spin-down with ${}^{56}$Ni\nradioactive decay. We perform Bayesian multi-wavelength light-curve analyses\nusing physically motivated models and priors, and evaluate model performance\nthrough Bayes factors and leave-one-out cross-validation (LOO) scores. Our\nresults show a statistical preference for a BNS or NS--WD progenitor producing\na kilonova powered by a magnetar and ${}^{56}$Ni decay, characterized by a\n${}^{56}$Ni mass of $\\sim4\\times10^{-4}\\,M_{\\odot}$ and an ejecta mass of\n$0.06\\,M_{\\odot}$. Furthermore, under the assumption of a BNS origin within\nthis model, we infer binary component masses of $m_{1} =\n1.81^{+0.46}_{-0.61}\\,M_{\\odot}$ and $m_{2} = 1.61^{+0.65}_{-0.41}\\,M_{\\odot}$,\nwith a dimensionless tidal deformability of $\\tilde{\\Lambda} =\n471^{+318}_{-395}$. From the component mass posteriors, we infer that the\nobserved offset can be explained by a natal kick as long as the systemic\nvelocity is nearly aligned with the pre-kick orbital motion. In this case, the\nrequired kick velocity (co-moving frame) and binary separation range within\n$v'_{\\mathrm{k}}\\sim100$--$150~\\mathrm{km\\,s^{-1}}$, and\n$a_0\\sim2$--$3~R_{\\odot}$, respectively."
                },
                "authors": [
                    {
                        "name": "V. Alfradique"
                    },
                    {
                        "name": "R. da Mata"
                    },
                    {
                        "name": "J. C. RodrÃ­guez-RamÃ­rez"
                    },
                    {
                        "name": "C. R. Bom"
                    }
                ],
                "author_detail": {
                    "name": "C. R. Bom"
                },
                "author": "C. R. Bom",
                "arxiv_comment": "14 pages, 3 tables, 5 figures. Submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14875v1",
                "updated": "2025-10-16T16:54:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    54,
                    29,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:54:29Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    54,
                    29,
                    3,
                    289,
                    0
                ],
                "title": "AREPO-RSG: Aspherical Circumstellar Material and Winds from Pulsating\n  Dusty Red Supergiants in Global 3D Radiation Hydrodynamic Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AREPO-RSG: Aspherical Circumstellar Material and Winds from Pulsating\n  Dusty Red Supergiants in Global 3D Radiation Hydrodynamic Simulations"
                },
                "summary": "Recent observations have revealed a surprisingly large fraction of\nhydrogen-rich supernovae (SNe) interacting with dense confined circumstellar\nmaterial (CSM), whose origin is heavily debated. Exploiting our recent\nimplementation of a sophisticated radiation transport scheme in the moving-mesh\ncode AREPO, we perform full-sphere 3D radiation hydrodynamic simulations of red\nsupergiant envelopes. For $10\\, M_\\odot$ and $20\\, M_\\odot$ core-carbon-burning\nstars, we find that large-amplitude radial pulsations lift the surface material\nof density $10^{-14}$-$10^{-12}\\; \\mathrm{g\\; cm^{-3}}$ to the circumstellar\nenvironment up to $3\\times10^{14}$ cm, consistent with the inferred density for\nthe interacting SN 2013fs. There, radiation acts on dust to drive highly\nanisotropic outflows of $10^{-6}$-$10^{-5}\\, M_\\odot\\, \\mathrm{yr^{-1}}$. The\ntotal CSM masses for both simulations are $\\sim 0.01\\, M_\\odot$. Due to\nconvection, the CSM density structure has order-of-magnitude angular\nvariations, dominated by large-scale asymmetries. We suggest that (1) the CSM\naround the progenitor is bound material instead of a widely-assumed steady\nwind, (2) highly aspherical CSM is common and can be created by surface\nconvection rather than only from binary interactions, and (3) 3D effects need\nto be incorporated in 1D SN modeling, potentially via effective clumping. Based\non our simulations, we propose a 1D analytical CSM model to be directly used\nfor SN observable modeling. We predict that progenitor pulsations (seen in SN\n2023ixf) and highly-confined CSM (seen in SN 2013fs) should be common among\nmost hydrogen-rich SNe. This can be tested with progenitor monitoring using\nRubin Observatory and near-future high-cadence surveys such as ULTRASAT and\nUVEX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent observations have revealed a surprisingly large fraction of\nhydrogen-rich supernovae (SNe) interacting with dense confined circumstellar\nmaterial (CSM), whose origin is heavily debated. Exploiting our recent\nimplementation of a sophisticated radiation transport scheme in the moving-mesh\ncode AREPO, we perform full-sphere 3D radiation hydrodynamic simulations of red\nsupergiant envelopes. For $10\\, M_\\odot$ and $20\\, M_\\odot$ core-carbon-burning\nstars, we find that large-amplitude radial pulsations lift the surface material\nof density $10^{-14}$-$10^{-12}\\; \\mathrm{g\\; cm^{-3}}$ to the circumstellar\nenvironment up to $3\\times10^{14}$ cm, consistent with the inferred density for\nthe interacting SN 2013fs. There, radiation acts on dust to drive highly\nanisotropic outflows of $10^{-6}$-$10^{-5}\\, M_\\odot\\, \\mathrm{yr^{-1}}$. The\ntotal CSM masses for both simulations are $\\sim 0.01\\, M_\\odot$. Due to\nconvection, the CSM density structure has order-of-magnitude angular\nvariations, dominated by large-scale asymmetries. We suggest that (1) the CSM\naround the progenitor is bound material instead of a widely-assumed steady\nwind, (2) highly aspherical CSM is common and can be created by surface\nconvection rather than only from binary interactions, and (3) 3D effects need\nto be incorporated in 1D SN modeling, potentially via effective clumping. Based\non our simulations, we propose a 1D analytical CSM model to be directly used\nfor SN observable modeling. We predict that progenitor pulsations (seen in SN\n2023ixf) and highly-confined CSM (seen in SN 2013fs) should be common among\nmost hydrogen-rich SNe. This can be tested with progenitor monitoring using\nRubin Observatory and near-future high-cadence surveys such as ULTRASAT and\nUVEX."
                },
                "authors": [
                    {
                        "name": "Jing-Ze Ma"
                    },
                    {
                        "name": "Stephen Justham"
                    },
                    {
                        "name": "Ruediger Pakmor"
                    },
                    {
                        "name": "Andrea Chiavassa"
                    },
                    {
                        "name": "Taeho Ryu"
                    },
                    {
                        "name": "Selma de Mink"
                    }
                ],
                "author_detail": {
                    "name": "Selma de Mink"
                },
                "author": "Selma de Mink",
                "arxiv_comment": "10+17 pages, 5+8 figures, 1 table. Submitted to ApJL. Comments\n  welcome! For an interactive 3D visualization of the circumstellar material\n  around the star (works both on phones and laptops), see\n  https://jingzema.com/AREPO-RSG/arepo_rsg_csm_fast",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13731v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13731v3",
                "updated": "2025-10-16T16:51:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    51,
                    12,
                    3,
                    289,
                    0
                ],
                "published": "2025-02-19T13:56:20Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    56,
                    20,
                    2,
                    50,
                    0
                ],
                "title": "Robust Counterfactual Inference in Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Counterfactual Inference in Markov Decision Processes"
                },
                "summary": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Jessica Lally"
                    },
                    {
                        "name": "Milad Kazemi"
                    },
                    {
                        "name": "Nicola Paoletti"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Paoletti"
                },
                "author": "Nicola Paoletti",
                "arxiv_comment": "Updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13731v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13731v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18355v2",
                "updated": "2025-10-16T16:44:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    44,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-22T19:31:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    31,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Chiplet-Based RISC-V SoC with Modular AI Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chiplet-Based RISC-V SoC with Modular AI Acceleration"
                },
                "summary": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications."
                },
                "authors": [
                    {
                        "name": "P. Ramkumar"
                    },
                    {
                        "name": "S. S. Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "S. S. Bharadwaj"
                },
                "author": "S. S. Bharadwaj",
                "arxiv_comment": "3 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13450v2",
                "updated": "2025-10-16T16:44:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    44,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-16T18:36:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    18,
                    36,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "SteeringSafety: A Systematic Safety Evaluation Framework of\n  Representation Steering in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SteeringSafety: A Systematic Safety Evaluation Framework of\n  Representation Steering in LLMs"
                },
                "summary": "We introduce SteeringSafety, a systematic framework for evaluating\nrepresentation steering methods across seven safety perspectives spanning 17\ndatasets. While prior work highlights general capabilities of representation\nsteering, we systematically explore safety perspectives including bias,\nharmfulness, hallucination, social behaviors, reasoning, epistemic integrity,\nand normative judgment. Our framework provides modularized building blocks for\nstate-of-the-art steering methods, enabling unified implementation of DIM, ACE,\nCAA, PCA, and LAT with recent enhancements like conditional steering. Results\non Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering\nperformance depends critically on pairing of method, model, and specific\nperspective. DIM shows consistent effectiveness, but all methods exhibit\nsubstantial entanglement: social behaviors show highest vulnerability (reaching\ndegradation as high as 76%), jailbreaking often compromises normative judgment,\nand hallucination steering unpredictably shifts political views. Our findings\nunderscore the critical need for holistic safety evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SteeringSafety, a systematic framework for evaluating\nrepresentation steering methods across seven safety perspectives spanning 17\ndatasets. While prior work highlights general capabilities of representation\nsteering, we systematically explore safety perspectives including bias,\nharmfulness, hallucination, social behaviors, reasoning, epistemic integrity,\nand normative judgment. Our framework provides modularized building blocks for\nstate-of-the-art steering methods, enabling unified implementation of DIM, ACE,\nCAA, PCA, and LAT with recent enhancements like conditional steering. Results\non Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering\nperformance depends critically on pairing of method, model, and specific\nperspective. DIM shows consistent effectiveness, but all methods exhibit\nsubstantial entanglement: social behaviors show highest vulnerability (reaching\ndegradation as high as 76%), jailbreaking often compromises normative judgment,\nand hallucination steering unpredictably shifts political views. Our findings\nunderscore the critical need for holistic safety evaluations."
                },
                "authors": [
                    {
                        "name": "Vincent Siu"
                    },
                    {
                        "name": "Nicholas Crispino"
                    },
                    {
                        "name": "David Park"
                    },
                    {
                        "name": "Nathan W. Henry"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Chenguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Wang"
                },
                "author": "Chenguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03567v3",
                "updated": "2025-10-16T16:42:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    42,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-03T23:32:21Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    23,
                    32,
                    21,
                    4,
                    276,
                    0
                ],
                "title": "Machine Unlearning Meets Adversarial Robustness via Constrained\n  Interventions on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning Meets Adversarial Robustness via Constrained\n  Interventions on LLMs"
                },
                "summary": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Fatmazohra Rezkellah"
                    },
                    {
                        "name": "Ramzi Dakhmouche"
                    }
                ],
                "author_detail": {
                    "name": "Ramzi Dakhmouche"
                },
                "author": "Ramzi Dakhmouche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12052v2",
                "updated": "2025-10-16T16:37:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    37,
                    59,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-15T15:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    2,
                    0,
                    258,
                    0
                ],
                "title": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided\n  Autoregressive Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided\n  Autoregressive Perspective"
                },
                "summary": "Talking-head animation focuses on generating realistic facial videos from\naudio input. Following Generative Adversarial Networks (GANs), diffusion models\nhave become the mainstream, owing to their robust generative capacities.\nHowever, inherent limitations of the diffusion process often lead to\ninter-frame flicker and slow inference, restricting their practical deployment.\nTo address this, we introduce AvatarSync, an autoregressive framework on\nphoneme representations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly by text or audio\ninput. To mitigate flicker and ensure continuity, AvatarSync leverages an\nautoregressive pipeline that enhances temporal modeling. In addition, to ensure\ncontrollability, we introduce phonemes, which are the basic units of speech\nsounds, and construct a many-to-one mapping from text/audio to phonemes,\nenabling precise phoneme-to-visual alignment. Additionally, to further\naccelerate inference, we adopt a two-stage generation strategy that decouples\nsemantic modeling from visual dynamics, and incorporate a customized\nPhoneme-Frame Causal Attention Mask to support multi-step parallel\nacceleration. Extensive experiments conducted on both Chinese (CMLR) and\nEnglish (HDTF) datasets demonstrate that AvatarSync outperforms existing\ntalking-head animation methods in visual fidelity, temporal consistency, and\ncomputational efficiency, providing a scalable and controllable solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking-head animation focuses on generating realistic facial videos from\naudio input. Following Generative Adversarial Networks (GANs), diffusion models\nhave become the mainstream, owing to their robust generative capacities.\nHowever, inherent limitations of the diffusion process often lead to\ninter-frame flicker and slow inference, restricting their practical deployment.\nTo address this, we introduce AvatarSync, an autoregressive framework on\nphoneme representations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly by text or audio\ninput. To mitigate flicker and ensure continuity, AvatarSync leverages an\nautoregressive pipeline that enhances temporal modeling. In addition, to ensure\ncontrollability, we introduce phonemes, which are the basic units of speech\nsounds, and construct a many-to-one mapping from text/audio to phonemes,\nenabling precise phoneme-to-visual alignment. Additionally, to further\naccelerate inference, we adopt a two-stage generation strategy that decouples\nsemantic modeling from visual dynamics, and incorporate a customized\nPhoneme-Frame Causal Attention Mask to support multi-step parallel\nacceleration. Extensive experiments conducted on both Chinese (CMLR) and\nEnglish (HDTF) datasets demonstrate that AvatarSync outperforms existing\ntalking-head animation methods in visual fidelity, temporal consistency, and\ncomputational efficiency, providing a scalable and controllable solution."
                },
                "authors": [
                    {
                        "name": "Yuchen Deng"
                    },
                    {
                        "name": "Xiuyang Wu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Suiyang Zhang"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14859v1",
                "updated": "2025-10-16T16:33:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    33,
                    34,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:33:34Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    33,
                    34,
                    3,
                    289,
                    0
                ],
                "title": "Deuterated water ice on the satellites of Saturn",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deuterated water ice on the satellites of Saturn"
                },
                "summary": "The deuterium to hydrogen ratio in water ice in a planetary body carries\nimportant information on the history of water processing and delivery in the\nprotostellar nebula. For a giant planet satellite, the D/H ratio is also\naffected by the processes and temperatures of the circumplanetary or\ncircumstellar environment in which the satellites formed. Here we present\nrobust JWST spectroscopic detections of the 4.14 $\\mu$m O-D stretch absorption\nline (analogous to the 3 $\\mu$m water O-H stretch) on the mid-sized Saturnian\nsatellites and use these detections to infer a D/H ratio on each satellite.\nWithin the limitations of the technique, we find that all of the satellites are\nconsistent with having a D/H ratio of about $1.5 \\times$ Vienna Standard Mean\nOcean Water (VSMOW), which is about an order of magnitude higher than the value\nof the atmosphere of Saturn. A much higher previously reported D/H ratio for\nPhoebe is ruled out at the 10$\\sigma$ level, and a 3$\\sigma$ upper limit of 2.3\n$\\times$ VSMOW is obtained. The elevated D/H ratios demonstrate that the solid\nplanetesimals and pebbles that built the satellites never sublimed and\nre-equilibrated with the gaseous circumplanetary disk. The similarity of the\nD/H measurements across all satellites suggest that the D/H ratio of water ice\nin the vicinity of Saturn at the time of satellite formation was also\napproximately 1.5 $\\times$ VSMOW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deuterium to hydrogen ratio in water ice in a planetary body carries\nimportant information on the history of water processing and delivery in the\nprotostellar nebula. For a giant planet satellite, the D/H ratio is also\naffected by the processes and temperatures of the circumplanetary or\ncircumstellar environment in which the satellites formed. Here we present\nrobust JWST spectroscopic detections of the 4.14 $\\mu$m O-D stretch absorption\nline (analogous to the 3 $\\mu$m water O-H stretch) on the mid-sized Saturnian\nsatellites and use these detections to infer a D/H ratio on each satellite.\nWithin the limitations of the technique, we find that all of the satellites are\nconsistent with having a D/H ratio of about $1.5 \\times$ Vienna Standard Mean\nOcean Water (VSMOW), which is about an order of magnitude higher than the value\nof the atmosphere of Saturn. A much higher previously reported D/H ratio for\nPhoebe is ruled out at the 10$\\sigma$ level, and a 3$\\sigma$ upper limit of 2.3\n$\\times$ VSMOW is obtained. The elevated D/H ratios demonstrate that the solid\nplanetesimals and pebbles that built the satellites never sublimed and\nre-equilibrated with the gaseous circumplanetary disk. The similarity of the\nD/H measurements across all satellites suggest that the D/H ratio of water ice\nin the vicinity of Saturn at the time of satellite formation was also\napproximately 1.5 $\\times$ VSMOW."
                },
                "authors": [
                    {
                        "name": "Michael E. Brown"
                    },
                    {
                        "name": "Samantha K. Trumbo"
                    },
                    {
                        "name": "M. Ryleigh Davis"
                    },
                    {
                        "name": "Swaroop Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Chandra"
                },
                "author": "Swaroop Chandra",
                "arxiv_doi": "10.3847/PSJ/adfbf5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/PSJ/adfbf5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.14859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Michael E. Brown et al 2025 Planet. Sci. J. 6 229",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08808v3",
                "updated": "2025-10-16T16:29:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    29,
                    36,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-09T20:47:07Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    47,
                    7,
                    3,
                    282,
                    0
                ],
                "title": "TinyGraphEstimator: Adapting Lightweight Language Models for Graph\n  Structure Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyGraphEstimator: Adapting Lightweight Language Models for Graph\n  Structure Inference"
                },
                "summary": "Graphs provide a universal framework for representing complex relational\nsystems, and inferring their structural properties is a core challenge in graph\nanalysis and reasoning. While large language models have recently demonstrated\nemerging abilities to perform symbolic and numerical reasoning, the potential\nof smaller, resource-efficient models in this context remains largely\nunexplored. This paper investigates whether compact transformer-based language\nmodels can infer graph-theoretic parameters directly from graph\nrepresentations. To enable systematic evaluation, we introduce the\nTinyGraphEstimator dataset - a balanced collection of connected graphs\ngenerated from multiple random graph models and annotated with detailed\nstructural metadata. We evaluate several small open models on their ability to\npredict key graph parameters such as density, clustering, and chromatic number.\nFurthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation\n(LoRA) technique, achieving consistent improvements across all evaluated\nmetrics. The results demonstrate that small language models possess non-trivial\nreasoning capacity over graph-structured data and can be effectively adapted\nfor structural inference tasks through efficient parameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs provide a universal framework for representing complex relational\nsystems, and inferring their structural properties is a core challenge in graph\nanalysis and reasoning. While large language models have recently demonstrated\nemerging abilities to perform symbolic and numerical reasoning, the potential\nof smaller, resource-efficient models in this context remains largely\nunexplored. This paper investigates whether compact transformer-based language\nmodels can infer graph-theoretic parameters directly from graph\nrepresentations. To enable systematic evaluation, we introduce the\nTinyGraphEstimator dataset - a balanced collection of connected graphs\ngenerated from multiple random graph models and annotated with detailed\nstructural metadata. We evaluate several small open models on their ability to\npredict key graph parameters such as density, clustering, and chromatic number.\nFurthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation\n(LoRA) technique, achieving consistent improvements across all evaluated\nmetrics. The results demonstrate that small language models possess non-trivial\nreasoning capacity over graph-structured data and can be effectively adapted\nfor structural inference tasks through efficient parameter tuning."
                },
                "authors": [
                    {
                        "name": "Michal Podstawski"
                    }
                ],
                "author_detail": {
                    "name": "Michal Podstawski"
                },
                "author": "Michal Podstawski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21097v2",
                "updated": "2025-10-16T16:20:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    20,
                    17,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-27T12:22:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    22,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "Thinker: Learning to Think Fast and Slow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinker: Learning to Think Fast and Slow"
                },
                "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training. Additionally, we have open-sourced both the trained models\nand the source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training. Additionally, we have open-sourced both the trained models\nand the source code."
                },
                "authors": [
                    {
                        "name": "Stephen Chung"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14847v1",
                "updated": "2025-10-16T16:19:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    19,
                    13,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:19:13Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    19,
                    13,
                    3,
                    289,
                    0
                ],
                "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond\n  Semantic Dependency Constraints"
                },
                "summary": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have achieved remarkable progress, particularly\nexcelling in realistic scenarios; however, their performance degrades notably\nin imaginative scenarios. These prompts often involve rarely co-occurring\nconcepts with long-distance semantic relationships, falling outside training\ndistributions. Existing methods typically apply test-time scaling for improving\nvideo quality, but their fixed search spaces and static reward designs limit\nadaptability to imaginative scenarios. To fill this gap, we propose\nImagerySearch, a prompt-guided adaptive test-time search strategy that\ndynamically adjusts both the inference search space and reward function\naccording to semantic relationships in the prompt. This enables more coherent\nand visually plausible videos in challenging imaginative settings. To evaluate\nprogress in this direction, we introduce LDT-Bench, the first dedicated\nbenchmark for long-distance semantic prompts, consisting of 2,839 diverse\nconcept pairs and an automated protocol for assessing creative generation\ncapabilities. Extensive experiments show that ImagerySearch consistently\noutperforms strong video generation baselines and existing test-time scaling\napproaches on LDT-Bench, and achieves competitive improvements on VBench,\ndemonstrating its effectiveness across diverse prompt types. We will release\nLDT-Bench and code to facilitate future research on imaginative video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jiashu Zhu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Chubin Chen"
                    },
                    {
                        "name": "Chen Zhu"
                    },
                    {
                        "name": "Bingze Song"
                    },
                    {
                        "name": "Fangyuan Mao"
                    },
                    {
                        "name": "Jiahong Wu"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14846v1",
                "updated": "2025-10-16T16:18:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    18,
                    37,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:18:37Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    18,
                    37,
                    3,
                    289,
                    0
                ],
                "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents"
                },
                "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhuo-Yang Song"
                },
                "author": "Zhuo-Yang Song",
                "arxiv_comment": "10 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14842v1",
                "updated": "2025-10-16T16:15:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    15,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:15:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    15,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "Boosting Instruction Following at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Instruction Following at Scale"
                },
                "summary": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance."
                },
                "authors": [
                    {
                        "name": "Ben Elder"
                    },
                    {
                        "name": "Evelyn Duesterwald"
                    },
                    {
                        "name": "Vinod Muthusamy"
                    }
                ],
                "author_detail": {
                    "name": "Vinod Muthusamy"
                },
                "author": "Vinod Muthusamy",
                "arxiv_comment": "6+4 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14825v1",
                "updated": "2025-10-16T16:02:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:02:42Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    42,
                    3,
                    289,
                    0
                ],
                "title": "Programmatic Representation Learning with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmatic Representation Learning with Language Models"
                },
                "summary": "Classical models for supervised machine learning, such as decision trees, are\nefficient and interpretable predictors, but their quality is highly dependent\non the particular choice of input features. Although neural networks can learn\nuseful representations directly from raw data (e.g., images or text), this\ncomes at the expense of interpretability and the need for specialized hardware\nto run them efficiently. In this paper, we explore a hypothesis class we call\nLearned Programmatic Representations (LeaPR) models, which stack arbitrary\nfeatures represented as code (functions from data points to scalars) and\ndecision tree predictors. We synthesize feature functions using Large Language\nModels (LLMs), which have rich prior knowledge in a wide range of domains and a\nremarkable ability to write code using existing domain-specific libraries. We\npropose two algorithms to learn LeaPR models from supervised data. First, we\ndesign an adaptation of FunSearch to learn features rather than directly\ngenerate predictors. Then, we develop a novel variant of the classical ID3\nalgorithm for decision tree learning, where new features are generated on\ndemand when splitting leaf nodes. In experiments from chess position evaluation\nto image and text classification, our methods learn high-quality, neural\nnetwork-free predictors often competitive with neural networks. Our work\nsuggests a flexible paradigm for learning interpretable representations\nend-to-end where features and predictions can be readily inspected and\nunderstood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical models for supervised machine learning, such as decision trees, are\nefficient and interpretable predictors, but their quality is highly dependent\non the particular choice of input features. Although neural networks can learn\nuseful representations directly from raw data (e.g., images or text), this\ncomes at the expense of interpretability and the need for specialized hardware\nto run them efficiently. In this paper, we explore a hypothesis class we call\nLearned Programmatic Representations (LeaPR) models, which stack arbitrary\nfeatures represented as code (functions from data points to scalars) and\ndecision tree predictors. We synthesize feature functions using Large Language\nModels (LLMs), which have rich prior knowledge in a wide range of domains and a\nremarkable ability to write code using existing domain-specific libraries. We\npropose two algorithms to learn LeaPR models from supervised data. First, we\ndesign an adaptation of FunSearch to learn features rather than directly\ngenerate predictors. Then, we develop a novel variant of the classical ID3\nalgorithm for decision tree learning, where new features are generated on\ndemand when splitting leaf nodes. In experiments from chess position evaluation\nto image and text classification, our methods learn high-quality, neural\nnetwork-free predictors often competitive with neural networks. Our work\nsuggests a flexible paradigm for learning interpretable representations\nend-to-end where features and predictions can be readily inspected and\nunderstood."
                },
                "authors": [
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Georgia Gabriela Sampaio"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gabriela Sampaio"
                },
                "author": "Georgia Gabriela Sampaio",
                "arxiv_comment": "Code available at https://github.com/gpoesia/leapr/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14824v1",
                "updated": "2025-10-16T16:02:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    27,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:02:27Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    27,
                    3,
                    289,
                    0
                ],
                "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking"
                },
                "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area."
                },
                "authors": [
                    {
                        "name": "Ziqi Dai"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14820v1",
                "updated": "2025-10-16T15:55:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    55,
                    54,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:55:54Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    55,
                    54,
                    3,
                    289,
                    0
                ],
                "title": "Exploring a cosmic ray inverse-Compton origin to the SZ-to-X-ray\n  pressure deficit in the cool core cluster ZwCl 3146",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring a cosmic ray inverse-Compton origin to the SZ-to-X-ray\n  pressure deficit in the cool core cluster ZwCl 3146"
                },
                "summary": "We explore the possibility that inverse-Compton (IC) scattering of cosmic\nmicrowave background photons by $\\sim$GeV cosmic rays (CRs) injected by the\ncentral active galactic nucleus (AGN) in cool core (CC) clusters produces a\nnon-negligible continuum-like X-ray signal that is easily misinterpreted as\nintracluster medium (ICM) thermal bremsstrahlung continuum. This is\nparticularly relevant to the cooling flow problem--the lack of star formation\nrelative to X-ray-inferred ICM cooling rates. Using ZwCl 3146, a relaxed CC\nsystem at $z = 0.291$, we compare pressure profiles derived via X-rays and the\nthermal Sunyaev-Zel'dovich (SZ) effect. While SZ measurements probe only\nthermal ICM electrons, additional CR-IC emission would appear to boost the\nX-ray-inferred pressure. Relative to unity, we measure a $\\simeq30\\%$ decrement\nin $P_{SZ}/P_X$ within 100 kpc of the ZwCl 3146 center at a statistical\nsignificance of $\\simeq 3.3\\sigma$, consistent with predicted deficits from\nCR-IC contamination in reasonable models of central AGN-driven CR injection.\nX-ray spectral fits of a two-component model with thermal ICM and CR-IC\nemission are consistent with CR-IC as the cause of this deficit. We test\nalternative explanations and systematics that could drive such a decrement,\nwith the leading order systematics associated with halo triaxiality.\nCollectively, these systematics are unlikely to produce a $P_{SZ}/P_X$\ndecrement $\\gtrsim10\\%$. While our results establish that non-negligible CR-IC\nemission is plausible in ZwCl 3146, we stress that more detailed studies of\nlarger cluster samples are required to robustly assess whether CR-IC is\nrelevant to the cooling flow problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the possibility that inverse-Compton (IC) scattering of cosmic\nmicrowave background photons by $\\sim$GeV cosmic rays (CRs) injected by the\ncentral active galactic nucleus (AGN) in cool core (CC) clusters produces a\nnon-negligible continuum-like X-ray signal that is easily misinterpreted as\nintracluster medium (ICM) thermal bremsstrahlung continuum. This is\nparticularly relevant to the cooling flow problem--the lack of star formation\nrelative to X-ray-inferred ICM cooling rates. Using ZwCl 3146, a relaxed CC\nsystem at $z = 0.291$, we compare pressure profiles derived via X-rays and the\nthermal Sunyaev-Zel'dovich (SZ) effect. While SZ measurements probe only\nthermal ICM electrons, additional CR-IC emission would appear to boost the\nX-ray-inferred pressure. Relative to unity, we measure a $\\simeq30\\%$ decrement\nin $P_{SZ}/P_X$ within 100 kpc of the ZwCl 3146 center at a statistical\nsignificance of $\\simeq 3.3\\sigma$, consistent with predicted deficits from\nCR-IC contamination in reasonable models of central AGN-driven CR injection.\nX-ray spectral fits of a two-component model with thermal ICM and CR-IC\nemission are consistent with CR-IC as the cause of this deficit. We test\nalternative explanations and systematics that could drive such a decrement,\nwith the leading order systematics associated with halo triaxiality.\nCollectively, these systematics are unlikely to produce a $P_{SZ}/P_X$\ndecrement $\\gtrsim10\\%$. While our results establish that non-negligible CR-IC\nemission is plausible in ZwCl 3146, we stress that more detailed studies of\nlarger cluster samples are required to robustly assess whether CR-IC is\nrelevant to the cooling flow problem."
                },
                "authors": [
                    {
                        "name": "Emily M. Silich"
                    },
                    {
                        "name": "Jack Sayers"
                    },
                    {
                        "name": "Philip F. Hopkins"
                    },
                    {
                        "name": "Charles Romero"
                    },
                    {
                        "name": "Brian Mason"
                    },
                    {
                        "name": "John Orlowski-Scherer"
                    },
                    {
                        "name": "Craig L. Sarazin"
                    }
                ],
                "author_detail": {
                    "name": "Craig L. Sarazin"
                },
                "author": "Craig L. Sarazin",
                "arxiv_comment": "15 pages, 3 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14812v1",
                "updated": "2025-10-16T15:48:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    48,
                    17,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:48:17Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    48,
                    17,
                    3,
                    289,
                    0
                ],
                "title": "Efficient Dynamic Structured Sparse Training with Learned Shuffles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Structured Sparse Training with Learned Shuffles"
                },
                "summary": "Structured sparsity accelerates training and inference on modern GPUs, yet it\nstill trails unstructured dynamic sparse training (DST) in accuracy. The\nshortfall stems from a loss of expressivity: whereas a dense layer can realize\nevery possible mask obtained by choosing any $w$ active weights out of $n$, a\nfixed block or N:M layout explores only a subset of those possibilities. We\npropose to close this gap by learning, for each layer, a single permutation\nmatrix jointly with the structured weight matrix. Applied to three canonical\nstructures -- block, N:M, and diagonals -- we show that permutation-augmented\nDST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\\% sparsity on\nImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\\times$\nand infers up to $2.9\\times$ faster. The results position structure + learned\npermutation as a sweet spot between accuracy and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured sparsity accelerates training and inference on modern GPUs, yet it\nstill trails unstructured dynamic sparse training (DST) in accuracy. The\nshortfall stems from a loss of expressivity: whereas a dense layer can realize\nevery possible mask obtained by choosing any $w$ active weights out of $n$, a\nfixed block or N:M layout explores only a subset of those possibilities. We\npropose to close this gap by learning, for each layer, a single permutation\nmatrix jointly with the structured weight matrix. Applied to three canonical\nstructures -- block, N:M, and diagonals -- we show that permutation-augmented\nDST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\\% sparsity on\nImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\\times$\nand infers up to $2.9\\times$ faster. The results position structure + learned\npermutation as a sweet spot between accuracy and efficiency."
                },
                "authors": [
                    {
                        "name": "Abhishek Tyagi"
                    },
                    {
                        "name": "Arjun Iyer"
                    },
                    {
                        "name": "Liam Young"
                    },
                    {
                        "name": "William H Renninger"
                    },
                    {
                        "name": "Christopher Kanan"
                    },
                    {
                        "name": "Yuhao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Zhu"
                },
                "author": "Yuhao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14808v1",
                "updated": "2025-10-16T15:42:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    42,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:42:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    42,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Agentic NL2SQL to Reduce Computational Costs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic NL2SQL to Reduce Computational Costs"
                },
                "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Dominik Jehle"
                    },
                    {
                        "name": "Lennart Purucker"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "Accepted at the NeurIPS 2025 Workshop on Efficient Reasoning. 10\n  pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14807v1",
                "updated": "2025-10-16T15:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    40,
                    49,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:40:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    40,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "SimKO: Simple Pass@K Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimKO: Simple Pass@K Policy Optimization"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration."
                },
                "authors": [
                    {
                        "name": "Ruotian Peng"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Yandong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yandong Wen"
                },
                "author": "Yandong Wen",
                "arxiv_comment": "Technical report (20 pages, 10 figures, project page:\n  https://spherelab.ai/simko/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14801v1",
                "updated": "2025-10-16T15:33:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    33,
                    20,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:33:20Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    33,
                    20,
                    3,
                    289,
                    0
                ],
                "title": "A new photometric ephemeris for the 2M1510 AB double brown dwarf\n  eclipsing binary system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new photometric ephemeris for the 2M1510 AB double brown dwarf\n  eclipsing binary system"
                },
                "summary": "Eclipsing brown dwarfs are important calibrators of sub-stellar evolution\nmodels used to infer the characteristics of directly imaged brown dwarfs and\ngiant exoplanets. Only two double brown dwarf eclipsing binary systems are\nknown, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a\npoorly constrained orbital period. Here we analyse TESS full-frame image (FFI)\nphotometry of this faint ($T=15.9$) binary and detect a significant\n(${>}10\\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with\nprevious data. We refine the orbital period to $20.897782 \\pm 0.000036$ d,\nreducing its present-day uncertainty from 18 h to 8 min. Our work is crucial\nfor scheduling follow-up observations of this system for detailed study with\nother photometric facilities. We also find that a recent orbital solution from\nDoppler data is inconsistent with existing photometry. A timing offset in the\nDoppler data may have produced a spurious signal mimicking retrograde apsidal\nprecession, from which the claimed circumbinary planet 2M1510 ABb was inferred.\nFrom our best attempt at correcting the data we were unable to reconcile the\nradial velocity data with the photometry, suggesting that the radial velocity\nuncertainties are underestimated, and that the circumbinary planet 2M1510 ABb\nmay be a false positive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eclipsing brown dwarfs are important calibrators of sub-stellar evolution\nmodels used to infer the characteristics of directly imaged brown dwarfs and\ngiant exoplanets. Only two double brown dwarf eclipsing binary systems are\nknown, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a\npoorly constrained orbital period. Here we analyse TESS full-frame image (FFI)\nphotometry of this faint ($T=15.9$) binary and detect a significant\n(${>}10\\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with\nprevious data. We refine the orbital period to $20.897782 \\pm 0.000036$ d,\nreducing its present-day uncertainty from 18 h to 8 min. Our work is crucial\nfor scheduling follow-up observations of this system for detailed study with\nother photometric facilities. We also find that a recent orbital solution from\nDoppler data is inconsistent with existing photometry. A timing offset in the\nDoppler data may have produced a spurious signal mimicking retrograde apsidal\nprecession, from which the claimed circumbinary planet 2M1510 ABb was inferred.\nFrom our best attempt at correcting the data we were unable to reconcile the\nradial velocity data with the photometry, suggesting that the radial velocity\nuncertainties are underestimated, and that the circumbinary planet 2M1510 ABb\nmay be a false positive."
                },
                "authors": [
                    {
                        "name": "Seb T. Millward"
                    },
                    {
                        "name": "Vedad Kunovac"
                    }
                ],
                "author_detail": {
                    "name": "Vedad Kunovac"
                },
                "author": "Vedad Kunovac",
                "arxiv_comment": "Revised version submitted to MNRAS following referee comments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20921v2",
                "updated": "2025-10-16T15:31:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    31,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-06-26T01:03:44Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    3,
                    44,
                    3,
                    177,
                    0
                ],
                "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach"
                },
                "summary": "Chemical process optimization maximizes production efficiency and economic\nperformance, but optimization algorithms, including gradient-based solvers,\nnumerical methods, and parameter grid searches, become impractical when\noperating constraints are ill-defined or unavailable. We present a multi-agent\nLLM framework that autonomously infers operating constraints from minimal\nprocess descriptions, then collaboratively guides optimization. Our\nAutoGen-based framework employs OpenAI's o3 model with specialized agents for\nconstraint generation, parameter validation, simulation, and optimization\nguidance. Through autonomous constraint generation and iterative multi-agent\noptimization, the framework eliminates the need for predefined operational\nbounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost\nratio metrics, the framework achieved competitive performance with conventional\nmethods while reducing wall-time 31-fold relative to grid search, converging in\nunder 20 minutes. The reasoning-guided search demonstrates sophisticated\nprocess understanding, correctly identifying utility trade-offs and applying\ndomain-informed heuristics. Unlike conventional methods requiring predefined\nconstraints, our approach uniquely combines autonomous constraint generation\nwith interpretable parameter exploration. Model comparison reveals\nreasoning-capable architectures (o3, o1) are essential for successful\noptimization, while standard models fail to converge. This approach is\nparticularly valuable for emerging processes and retrofit applications where\noperational constraints are poorly characterized or unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical process optimization maximizes production efficiency and economic\nperformance, but optimization algorithms, including gradient-based solvers,\nnumerical methods, and parameter grid searches, become impractical when\noperating constraints are ill-defined or unavailable. We present a multi-agent\nLLM framework that autonomously infers operating constraints from minimal\nprocess descriptions, then collaboratively guides optimization. Our\nAutoGen-based framework employs OpenAI's o3 model with specialized agents for\nconstraint generation, parameter validation, simulation, and optimization\nguidance. Through autonomous constraint generation and iterative multi-agent\noptimization, the framework eliminates the need for predefined operational\nbounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost\nratio metrics, the framework achieved competitive performance with conventional\nmethods while reducing wall-time 31-fold relative to grid search, converging in\nunder 20 minutes. The reasoning-guided search demonstrates sophisticated\nprocess understanding, correctly identifying utility trade-offs and applying\ndomain-informed heuristics. Unlike conventional methods requiring predefined\nconstraints, our approach uniquely combines autonomous constraint generation\nwith interpretable parameter exploration. Model comparison reveals\nreasoning-capable architectures (o3, o1) are essential for successful\noptimization, while standard models fail to converge. This approach is\nparticularly valuable for emerging processes and retrofit applications where\noperational constraints are poorly characterized or unavailable."
                },
                "authors": [
                    {
                        "name": "Tong Zeng"
                    },
                    {
                        "name": "Srivathsan Badrinarayanan"
                    },
                    {
                        "name": "Janghoon Ock"
                    },
                    {
                        "name": "Cheng-Kai Lai"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "16 pages (main manuscript without references), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14788v1",
                "updated": "2025-10-16T15:20:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    20,
                    49,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:20:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    20,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Scenario Unified Modeling of User Interests at Billion Scale"
                },
                "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms."
                },
                "authors": [
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Zejian Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "arxiv_comment": "The dataset, code, and models will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14779v1",
                "updated": "2025-10-16T15:15:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    15,
                    6,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:15:06Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    15,
                    6,
                    3,
                    289,
                    0
                ],
                "title": "The dark side of early galaxies: $\\texttt{geko}$ uncovers dark-matter\n  fractions at $z\\sim4-6$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dark side of early galaxies: $\\texttt{geko}$ uncovers dark-matter\n  fractions at $z\\sim4-6$"
                },
                "summary": "JWST/NIRCam slitless spectroscopy enables dynamical mass measurements for\ntypical star-forming galaxies only a billion years after the Big Bang. We model\nthe H$\\alpha$ morpho-kinematics of 163 galaxies at redshift $z\\approx4$-6 from\nFRESCO and CONGRESS (with JADES imaging), using the $\\texttt{geko}$ code, and\ninfer rotational velocities and dispersions within $r_{\\rm e}$. Our sample\nspans $\\log M_{\\star}\\approx7$-10 and $\\log M_{\\rm dyn}\\approx9$-11. Gas masses\nare estimated via scaling relations, yielding baryonic masses and dark-matter\n(DM) fractions $f_{\\rm DM}(r<r_{\\rm e})$ within the H$\\alpha$ half-light\nradius. We find high median fractions of $\\langle f_{\\rm gas}\\rangle=0.77$ and\n$\\langle f_{\\rm DM}\\rangle=0.73$, where $f_{\\rm gas}$ is measured with respect\nto the baryonic mass and $f_{\\rm DM}$ with respect to the DM+baryonic mass.\nAbout two-thirds of systems are DM-dominated within $r_{\\rm e}\\sim0.5-1$ kpc.\nBoth $f_{\\rm gas}$ and $f_{\\rm DM}$ decrease with stellar mass, consistent with\nsimulations. The stellar Tully-Fisher relation shows a tentative offset to\nhigher $v_{\\rm circ}$ at fixed $M_{\\star}$ and substantial intrinsic scatter,\nsuggesting that the relation is only beginning to emerge at $z\\sim5$. We\nmeasure a negative correlation between $f_{\\rm DM}$ and baryonic surface\ndensity $\\Sigma_{\\rm bar}$, weaker but broadly consistent with trends at cosmic\nnoon and at $z\\sim0$. Qualitatively comparing with modified NFW profiles\ncoupled to an empirical stellar-to-halo mass relation suggests that the lowest\n$f_{\\rm DM}$ ($\\lesssim0.4$) require cored inner DM profiles, while the highest\nfractions favour cuspier profiles, potentially reflecting adiabatic\ncontraction. Overall, the elevated $f_{\\rm gas}$ and $f_{\\rm DM}$ at\n$z\\gtrsim4$ are compatible with progenitors of baryon-dominated systems at\n$z\\sim2$ and naturally anticipate overmassive black holes at fixed $M_{\\star}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JWST/NIRCam slitless spectroscopy enables dynamical mass measurements for\ntypical star-forming galaxies only a billion years after the Big Bang. We model\nthe H$\\alpha$ morpho-kinematics of 163 galaxies at redshift $z\\approx4$-6 from\nFRESCO and CONGRESS (with JADES imaging), using the $\\texttt{geko}$ code, and\ninfer rotational velocities and dispersions within $r_{\\rm e}$. Our sample\nspans $\\log M_{\\star}\\approx7$-10 and $\\log M_{\\rm dyn}\\approx9$-11. Gas masses\nare estimated via scaling relations, yielding baryonic masses and dark-matter\n(DM) fractions $f_{\\rm DM}(r<r_{\\rm e})$ within the H$\\alpha$ half-light\nradius. We find high median fractions of $\\langle f_{\\rm gas}\\rangle=0.77$ and\n$\\langle f_{\\rm DM}\\rangle=0.73$, where $f_{\\rm gas}$ is measured with respect\nto the baryonic mass and $f_{\\rm DM}$ with respect to the DM+baryonic mass.\nAbout two-thirds of systems are DM-dominated within $r_{\\rm e}\\sim0.5-1$ kpc.\nBoth $f_{\\rm gas}$ and $f_{\\rm DM}$ decrease with stellar mass, consistent with\nsimulations. The stellar Tully-Fisher relation shows a tentative offset to\nhigher $v_{\\rm circ}$ at fixed $M_{\\star}$ and substantial intrinsic scatter,\nsuggesting that the relation is only beginning to emerge at $z\\sim5$. We\nmeasure a negative correlation between $f_{\\rm DM}$ and baryonic surface\ndensity $\\Sigma_{\\rm bar}$, weaker but broadly consistent with trends at cosmic\nnoon and at $z\\sim0$. Qualitatively comparing with modified NFW profiles\ncoupled to an empirical stellar-to-halo mass relation suggests that the lowest\n$f_{\\rm DM}$ ($\\lesssim0.4$) require cored inner DM profiles, while the highest\nfractions favour cuspier profiles, potentially reflecting adiabatic\ncontraction. Overall, the elevated $f_{\\rm gas}$ and $f_{\\rm DM}$ at\n$z\\gtrsim4$ are compatible with progenitors of baryon-dominated systems at\n$z\\sim2$ and naturally anticipate overmassive black holes at fixed $M_{\\star}$."
                },
                "authors": [
                    {
                        "name": "A. Lola Danhaive"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Emma Curtis-Lake"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Qiao Duan"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Daniel J. Eisenstein"
                    },
                    {
                        "name": "Benjamin D. Johnson"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "William McClymont"
                    },
                    {
                        "name": "Marcia Rieke"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Fengwu Sun"
                    },
                    {
                        "name": "Christopher N. A. Willmer"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Yongda Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yongda Zhu"
                },
                "author": "Yongda Zhu",
                "arxiv_comment": "14 pages, 7 figures, 2 tables. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14773v1",
                "updated": "2025-10-16T15:09:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    9,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:09:22Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    9,
                    22,
                    3,
                    289,
                    0
                ],
                "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large\n  Language Models with Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Answers in Thought Matters: Revisiting Evaluation on Large\n  Language Models with Reasoning"
                },
                "summary": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation."
                },
                "authors": [
                    {
                        "name": "Hwiyeol Jo"
                    },
                    {
                        "name": "Joosung Lee"
                    },
                    {
                        "name": "Jaehone Lee"
                    },
                    {
                        "name": "Sang-Woo Lee"
                    },
                    {
                        "name": "Joonsuk Park"
                    },
                    {
                        "name": "Kang Min Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Kang Min Yoo"
                },
                "author": "Kang Min Yoo",
                "arxiv_comment": "ARR Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20934v2",
                "updated": "2025-10-16T15:08:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    8,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-03-26T19:05:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    19,
                    5,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method\n  Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method\n  Refactoring"
                },
                "summary": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful."
                },
                "authors": [
                    {
                        "name": "Abhiram Bellur"
                    },
                    {
                        "name": "Fraol Batole"
                    },
                    {
                        "name": "Mohammed Raihan Ullah"
                    },
                    {
                        "name": "Malinda Dilhara"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    },
                    {
                        "name": "Timofey Bryksin"
                    },
                    {
                        "name": "Kai Ishikawa"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Masaharu Morimoto"
                    },
                    {
                        "name": "Shota Motoura"
                    },
                    {
                        "name": "Takeo Hosomi"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Hridesh Rajan"
                    },
                    {
                        "name": "Nikolaos Tsantalis"
                    },
                    {
                        "name": "Danny Dig"
                    }
                ],
                "author_detail": {
                    "name": "Danny Dig"
                },
                "author": "Danny Dig",
                "arxiv_comment": "Published at the International Conference on Software Maintenance and\n  Evolution (ICSME'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14766v1",
                "updated": "2025-10-16T15:02:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    2,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:02:42Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    2,
                    42,
                    3,
                    289,
                    0
                ],
                "title": "Predicting the Subhalo Mass Functions in Simulations from Galaxy Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the Subhalo Mass Functions in Simulations from Galaxy Images"
                },
                "summary": "Strong gravitational lensing provides a powerful tool to directly infer the\ndark matter (DM) subhalo mass function (SHMF) in lens galaxies. However,\ncomparing observationally inferred SHMFs to theoretical predictions remains\nchallenging, as the predicted SHMF can vary significantly between galaxies -\neven within the same cosmological model - due to differences in the properties\nand environment of individual galaxies. We present a machine learning framework\nto infer the galaxy-specific predicted SHMF from galaxy images, conditioned on\nthe assumed inverse warm DM particle mass $M^{-1}_{\\rm DM}$. To train the\nmodel, we use 1024 high-resolution hydrodynamical zoom-in simulations from the\nDREAMS suite. Mock observations are generated using Synthesizer, excluding gas\nparticle contributions, and SHMFs are computed with the Rockstar halo finder.\nOur neural network takes as input both the galaxy images and the inverse DM\nmass. This method enables scalable, image-based predictions for the theoretical\nDM SHMFs of individual galaxies, facilitating direct comparisons with\nobservational measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong gravitational lensing provides a powerful tool to directly infer the\ndark matter (DM) subhalo mass function (SHMF) in lens galaxies. However,\ncomparing observationally inferred SHMFs to theoretical predictions remains\nchallenging, as the predicted SHMF can vary significantly between galaxies -\neven within the same cosmological model - due to differences in the properties\nand environment of individual galaxies. We present a machine learning framework\nto infer the galaxy-specific predicted SHMF from galaxy images, conditioned on\nthe assumed inverse warm DM particle mass $M^{-1}_{\\rm DM}$. To train the\nmodel, we use 1024 high-resolution hydrodynamical zoom-in simulations from the\nDREAMS suite. Mock observations are generated using Synthesizer, excluding gas\nparticle contributions, and SHMFs are computed with the Rockstar halo finder.\nOur neural network takes as input both the galaxy images and the inverse DM\nmass. This method enables scalable, image-based predictions for the theoretical\nDM SHMFs of individual galaxies, facilitating direct comparisons with\nobservational measurements."
                },
                "authors": [
                    {
                        "name": "Andreas Filipp"
                    },
                    {
                        "name": "Tri Nguyen"
                    },
                    {
                        "name": "Laurence Perreault-Levasseur"
                    },
                    {
                        "name": "Jonah Rose"
                    },
                    {
                        "name": "Chris Lovell"
                    },
                    {
                        "name": "Nicolas Payot"
                    },
                    {
                        "name": "Francisco Villaescusa-Navarro"
                    },
                    {
                        "name": "Yashar Hezaveh"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Hezaveh"
                },
                "author": "Yashar Hezaveh",
                "arxiv_comment": "Published as a workshop paper at the ML4Astro Workshop at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07887v2",
                "updated": "2025-10-16T14:59:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    59,
                    50,
                    3,
                    289,
                    0
                ],
                "published": "2025-04-10T16:00:59Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    0,
                    59,
                    3,
                    100,
                    0
                ],
                "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge"
                },
                "summary": "The growing integration of Large Language Models (LLMs) into critical\nsocietal domains has raised concerns about embedded biases that can perpetuate\nstereotypes and undermine fairness. Such biases may stem from historical\ninequalities in training data, linguistic imbalances, or adversarial\nmanipulation. Despite mitigation efforts, recent studies show that LLMs remain\nvulnerable to adversarial attacks that elicit biased outputs. This work\nproposes a scalable benchmarking framework to assess LLM robustness to\nadversarial bias elicitation. Our methodology involves: (i) systematically\nprobing models across multiple tasks targeting diverse sociocultural biases,\n(ii) quantifying robustness through safety scores using an LLM-as-a-Judge\napproach, and (iii) employing jailbreak techniques to reveal safety\nvulnerabilities. To facilitate systematic benchmarking, we release a curated\ndataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying\nDeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is\nuneven, with age, disability, and intersectional biases among the most\nprominent. Some small models outperform larger ones in safety, suggesting that\ntraining and architecture may matter more than scale. However, no model is\nfully robust to adversarial elicitation, with jailbreak attacks using\nlow-resource languages or refusal suppression proving effective across model\nfamilies. We also find that successive LLM generations exhibit slight safety\ngains, while models fine-tuned for the medical domain tend to be less safe than\ntheir general-purpose counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing integration of Large Language Models (LLMs) into critical\nsocietal domains has raised concerns about embedded biases that can perpetuate\nstereotypes and undermine fairness. Such biases may stem from historical\ninequalities in training data, linguistic imbalances, or adversarial\nmanipulation. Despite mitigation efforts, recent studies show that LLMs remain\nvulnerable to adversarial attacks that elicit biased outputs. This work\nproposes a scalable benchmarking framework to assess LLM robustness to\nadversarial bias elicitation. Our methodology involves: (i) systematically\nprobing models across multiple tasks targeting diverse sociocultural biases,\n(ii) quantifying robustness through safety scores using an LLM-as-a-Judge\napproach, and (iii) employing jailbreak techniques to reveal safety\nvulnerabilities. To facilitate systematic benchmarking, we release a curated\ndataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying\nDeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is\nuneven, with age, disability, and intersectional biases among the most\nprominent. Some small models outperform larger ones in safety, suggesting that\ntraining and architecture may matter more than scale. However, no model is\nfully robust to adversarial elicitation, with jailbreak attacks using\nlow-resource languages or refusal suppression proving effective across model\nfamilies. We also find that successive LLM generations exhibit slight safety\ngains, while models fine-tuned for the medical domain tend to be less safe than\ntheir general-purpose counterparts."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Massimo Ruggiero"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "arxiv_doi": "10.1007/s10994-025-06862-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10994-025-06862-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Cantini, R., Orsino, A., Ruggiero, M., Talia, D. Benchmarking\n  adversarial robustness to bias elicitation in large language models: scalable\n  automated assessment with LLM-as-a-judge. Mach Learn 114, 249 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14756v1",
                "updated": "2025-10-16T14:57:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    57,
                    1,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:57:01Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    57,
                    1,
                    3,
                    289,
                    0
                ],
                "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware\n  Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware\n  Code"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to automate hardware\ndesign tasks, including the generation of Verilog code. While early benchmarks\nfocus primarily on functional correctness, efficient hardware design demands\nadditional optimization for synthesis metrics such as area, delay, and power.\nExisting benchmarks fall short in evaluating these aspects comprehensively:\nthey often lack optimized baselines or testbenches for verification. To address\nthese gaps, we present Pluto, a benchmark and evaluation framework designed to\nassess the efficiency of LLM-generated Verilog designs. Pluto presents a\ncomprehensive evaluation set of 114 problems with self-checking testbenches and\nmultiple Pareto-optimal reference implementations. Experimental results show\nthat state-of-the-art LLMs can achieve high functional correctness, reaching\n78.3\\% at pass@1, but their synthesis efficiency still lags behind\nexpert-crafted implementations, with area efficiency of 63.8\\%, delay\nefficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights\nthe need for efficiency-aware evaluation frameworks such as Pluto to drive\nprogress in hardware-focused LLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to automate hardware\ndesign tasks, including the generation of Verilog code. While early benchmarks\nfocus primarily on functional correctness, efficient hardware design demands\nadditional optimization for synthesis metrics such as area, delay, and power.\nExisting benchmarks fall short in evaluating these aspects comprehensively:\nthey often lack optimized baselines or testbenches for verification. To address\nthese gaps, we present Pluto, a benchmark and evaluation framework designed to\nassess the efficiency of LLM-generated Verilog designs. Pluto presents a\ncomprehensive evaluation set of 114 problems with self-checking testbenches and\nmultiple Pareto-optimal reference implementations. Experimental results show\nthat state-of-the-art LLMs can achieve high functional correctness, reaching\n78.3\\% at pass@1, but their synthesis efficiency still lags behind\nexpert-crafted implementations, with area efficiency of 63.8\\%, delay\nefficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights\nthe need for efficiency-aware evaluation frameworks such as Pluto to drive\nprogress in hardware-focused LLM research."
                },
                "authors": [
                    {
                        "name": "Manar Abdelatty"
                    },
                    {
                        "name": "Maryam Nouh"
                    },
                    {
                        "name": "Jacob K. Rosenstein"
                    },
                    {
                        "name": "Sherief Reda"
                    }
                ],
                "author_detail": {
                    "name": "Sherief Reda"
                },
                "author": "Sherief Reda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08403v2",
                "updated": "2025-10-16T14:53:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    53,
                    5,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-13T09:58:23Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    58,
                    23,
                    1,
                    133,
                    0
                ],
                "title": "ConDiSim: Conditional Diffusion Models for Simulation Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConDiSim: Conditional Diffusion Models for Simulation Based Inference"
                },
                "summary": "We present a conditional diffusion model - ConDiSim, for simulation-based\ninference of complex systems with intractable likelihoods. ConDiSim leverages\ndenoising diffusion probabilistic models to approximate posterior\ndistributions, consisting of a forward process that adds Gaussian noise to\nparameters, and a reverse process learning to denoise, conditioned on observed\ndata. This approach effectively captures complex dependencies and\nmulti-modalities within posteriors. ConDiSim is evaluated across ten benchmark\nproblems and two real-world test problems, where it demonstrates effective\nposterior approximation accuracy while maintaining computational efficiency and\nstability in model training. ConDiSim offers a robust and extensible framework\nfor simulation-based inference, particularly suitable for parameter inference\nworkflows requiring fast inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a conditional diffusion model - ConDiSim, for simulation-based\ninference of complex systems with intractable likelihoods. ConDiSim leverages\ndenoising diffusion probabilistic models to approximate posterior\ndistributions, consisting of a forward process that adds Gaussian noise to\nparameters, and a reverse process learning to denoise, conditioned on observed\ndata. This approach effectively captures complex dependencies and\nmulti-modalities within posteriors. ConDiSim is evaluated across ten benchmark\nproblems and two real-world test problems, where it demonstrates effective\nposterior approximation accuracy while maintaining computational efficiency and\nstability in model training. ConDiSim offers a robust and extensible framework\nfor simulation-based inference, particularly suitable for parameter inference\nworkflows requiring fast inference methods."
                },
                "authors": [
                    {
                        "name": "Mayank Nautiyal"
                    },
                    {
                        "name": "Andreas Hellander"
                    },
                    {
                        "name": "Prashant Singh"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Singh"
                },
                "author": "Prashant Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14751v1",
                "updated": "2025-10-16T14:52:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    52,
                    52,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:52:52Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    52,
                    52,
                    3,
                    289,
                    0
                ],
                "title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries"
                },
                "summary": "Next-token prediction (NTP) has driven the success of large language models\n(LLMs), but it struggles with long-horizon reasoning, planning, and creative\nwriting, with these limitations largely attributed to teacher-forced training.\nMulti-token prediction (MTP) partially mitigates these issues by predicting\nseveral future tokens at once, but it mostly captures short-range dependencies\nand offers limited improvement. We propose future summary prediction (FSP),\nwhich trains an auxiliary head to predict a compact representation of the\nlong-term future, preserving information relevant for long-form generations. We\nexplore two variants of FSP: handcrafted summaries, for example, a bag of words\nsummary of the future of the sequence, and learned summaries, which use\nembeddings produced by a reverse language model trained from right to left.\nLarge-scale pretraining experiments (3B and 8B-parameter models) demonstrate\nthat FSP provides improvements over both NTP and MTP across math, reasoning,\nand coding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-token prediction (NTP) has driven the success of large language models\n(LLMs), but it struggles with long-horizon reasoning, planning, and creative\nwriting, with these limitations largely attributed to teacher-forced training.\nMulti-token prediction (MTP) partially mitigates these issues by predicting\nseveral future tokens at once, but it mostly captures short-range dependencies\nand offers limited improvement. We propose future summary prediction (FSP),\nwhich trains an auxiliary head to predict a compact representation of the\nlong-term future, preserving information relevant for long-form generations. We\nexplore two variants of FSP: handcrafted summaries, for example, a bag of words\nsummary of the future of the sequence, and learned summaries, which use\nembeddings produced by a reverse language model trained from right to left.\nLarge-scale pretraining experiments (3B and 8B-parameter models) demonstrate\nthat FSP provides improvements over both NTP and MTP across math, reasoning,\nand coding benchmarks."
                },
                "authors": [
                    {
                        "name": "Divyat Mahajan"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Badr Youbi Idrissi"
                    },
                    {
                        "name": "Mohammad Pezeshki"
                    },
                    {
                        "name": "Ioannis Mitliagkas"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Kartik Ahuja"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Ahuja"
                },
                "author": "Kartik Ahuja",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26238v2",
                "updated": "2025-10-16T14:51:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    51,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-30T13:32:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    32,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models"
                },
                "summary": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc."
                },
                "authors": [
                    {
                        "name": "James Oldfield"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Ioannis Patras"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Project page: http://james-oldfield.github.io/tpc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20149v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20149v2",
                "updated": "2025-10-16T14:45:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    45,
                    36,
                    3,
                    289,
                    0
                ],
                "published": "2025-03-26T01:56:52Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    56,
                    52,
                    2,
                    85,
                    0
                ],
                "title": "Treatment Effects Inference with High-Dimensional Instruments and\n  Control Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment Effects Inference with High-Dimensional Instruments and\n  Control Variables"
                },
                "summary": "Obtaining valid treatment effect inference remains a challenging problem when\ndealing with numerous instruments and non-sparse control variables. In this\npaper, we propose a novel ridge regularization-based instrumental variables\nmethod for estimation and inference in the presence of both high-dimensional\ninstrumental variables and high-dimensional control variables. These methods\nare applicable both with and without sparsity assumptions. To remove the\nestimation bias, we introduce a two-step procedure employing a ridge regression\ncoupled with data-splitting in the first step, and a ridge style projection\nmatrix with a simple least squares regression in the second. We establish\nstatistical properties of the estimator, including consistency and asymptotic\nnormality. Furthermore, we develop practical statistical inference procedures\nby providing a consistent estimator for the asymptotic variance of the\nestimator. The finite sample performance of the proposed methods is evaluated\nthrough numerical simulations. Results indicate that the new estimator\nconsistently outperforms existing sparsity-based approaches across various\nsettings, offering valuable insights for complex scenarios. Finally, we provide\nan empirical application estimating the causal effect of schooling on earnings\naddressing potential endogeneity through the use of high-dimensional\ninstrumental variables and high-dimensional covariates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obtaining valid treatment effect inference remains a challenging problem when\ndealing with numerous instruments and non-sparse control variables. In this\npaper, we propose a novel ridge regularization-based instrumental variables\nmethod for estimation and inference in the presence of both high-dimensional\ninstrumental variables and high-dimensional control variables. These methods\nare applicable both with and without sparsity assumptions. To remove the\nestimation bias, we introduce a two-step procedure employing a ridge regression\ncoupled with data-splitting in the first step, and a ridge style projection\nmatrix with a simple least squares regression in the second. We establish\nstatistical properties of the estimator, including consistency and asymptotic\nnormality. Furthermore, we develop practical statistical inference procedures\nby providing a consistent estimator for the asymptotic variance of the\nestimator. The finite sample performance of the proposed methods is evaluated\nthrough numerical simulations. Results indicate that the new estimator\nconsistently outperforms existing sparsity-based approaches across various\nsettings, offering valuable insights for complex scenarios. Finally, we provide\nan empirical application estimating the causal effect of schooling on earnings\naddressing potential endogeneity through the use of high-dimensional\ninstrumental variables and high-dimensional covariates."
                },
                "authors": [
                    {
                        "name": "Xiduo Chen"
                    },
                    {
                        "name": "Xingdong Feng"
                    },
                    {
                        "name": "Antonio F. Galvao"
                    },
                    {
                        "name": "Yeheng Ge"
                    }
                ],
                "author_detail": {
                    "name": "Yeheng Ge"
                },
                "author": "Yeheng Ge",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20149v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20149v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08878v2",
                "updated": "2025-10-16T14:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    34,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2024-08-02T17:42:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    17,
                    42,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models"
                },
                "summary": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly."
                },
                "authors": [
                    {
                        "name": "Elisavet Koutsiana"
                    },
                    {
                        "name": "Johanna Walker"
                    },
                    {
                        "name": "Michelle Nwachukwu"
                    },
                    {
                        "name": "Albert MeroÃ±o-PeÃ±uela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05043v2",
                "updated": "2025-10-16T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    21,
                    45,
                    3,
                    289,
                    0
                ],
                "published": "2025-07-07T14:27:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    27,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "MoLink: Distributed and Efficient Serving Framework for Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLink: Distributed and Efficient Serving Framework for Large Models"
                },
                "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models. The source code\nis publicly available https://github.com/oldcpple/MoLink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models. The source code\nis publicly available https://github.com/oldcpple/MoLink."
                },
                "authors": [
                    {
                        "name": "Lewei Jin"
                    },
                    {
                        "name": "Yongqi Chen"
                    },
                    {
                        "name": "Kui Zhang"
                    },
                    {
                        "name": "Yifan Zhuo"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Bowei Yang"
                    },
                    {
                        "name": "Zhengong Cai"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14719v1",
                "updated": "2025-10-16T14:20:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    20,
                    0,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:20:00Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    20,
                    0,
                    3,
                    289,
                    0
                ],
                "title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous\n  References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous\n  References"
                },
                "summary": "Modern GPUs feature specialized hardware units that enable high-performance,\nasynchronous dataflow execution. However, the conventional SIMT programming\nmodel is fundamentally misaligned with this task-parallel hardware, creating a\nsignificant programmability gap. While hardware-level warp specialization is\nthe key to unlocking peak performance, it forces developers to manually\norchestrate complex, low-level communication and software pipelines--a process\nthat is labor-intensive, error-prone, and unsustainable. To address this\nchallenge, we present Tawa, an automated compiler that systematically generates\nhigh-performance, warp-specialized code from a high-level, tile-based program.\nCentral to our approach is a novel IR abstraction, asynchronous references\n(aref), which expresses warp-level communication without exposing low-level\nhardware details. Using this abstraction, Tawa automatically partitions\nprograms into producer-consumer roles and manages the intricate dataflow\npipeline, relieving developers of invasive kernel rewriting. Evaluation on\nNVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers\nhigh hardware utilization, achieving up to 1.1$\\times$ speedup over highly\noptimized cuBLAS GEMM kernels. For attention workloads, Tawa attains\n1.2$\\times$ speedup over Triton and matches the performance of the\nhand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming\neffort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs feature specialized hardware units that enable high-performance,\nasynchronous dataflow execution. However, the conventional SIMT programming\nmodel is fundamentally misaligned with this task-parallel hardware, creating a\nsignificant programmability gap. While hardware-level warp specialization is\nthe key to unlocking peak performance, it forces developers to manually\norchestrate complex, low-level communication and software pipelines--a process\nthat is labor-intensive, error-prone, and unsustainable. To address this\nchallenge, we present Tawa, an automated compiler that systematically generates\nhigh-performance, warp-specialized code from a high-level, tile-based program.\nCentral to our approach is a novel IR abstraction, asynchronous references\n(aref), which expresses warp-level communication without exposing low-level\nhardware details. Using this abstraction, Tawa automatically partitions\nprograms into producer-consumer roles and manages the intricate dataflow\npipeline, relieving developers of invasive kernel rewriting. Evaluation on\nNVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers\nhigh hardware utilization, achieving up to 1.1$\\times$ speedup over highly\noptimized cuBLAS GEMM kernels. For attention workloads, Tawa attains\n1.2$\\times$ speedup over Triton and matches the performance of the\nhand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming\neffort."
                },
                "authors": [
                    {
                        "name": "Hongzheng Chen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Alexander Collins"
                    },
                    {
                        "name": "Bastian Hagedorn"
                    },
                    {
                        "name": "Evghenii Gaburov"
                    },
                    {
                        "name": "Masahiro Masuda"
                    },
                    {
                        "name": "Matthew Brookhart"
                    },
                    {
                        "name": "Chris Sullivan"
                    },
                    {
                        "name": "Jason Knight"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Vinod Grover"
                    }
                ],
                "author_detail": {
                    "name": "Vinod Grover"
                },
                "author": "Vinod Grover",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03550v3",
                "updated": "2025-10-16T14:16:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    16,
                    18,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-05T15:18:36Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    15,
                    18,
                    36,
                    1,
                    217,
                    0
                ],
                "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via\n  Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via\n  Internal Representations"
                },
                "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using LLMs, a paradigm known as \"LLM-as-a-judge\". However,\nimproving its alignment with human preferences without complex prompts or\nfine-tuning remains challenging. Previous studies mainly optimize based on\nshallow outputs, overlooking rich cross-layer representations. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and task-relevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a post-hoc,\nplug-and-play framework for improving the alignment of LLM-as-a-Judge\npoint-wise evaluations with human scores by leveraging internal\nrepresentations. LAGER produces fine-grained judgment scores by aggregating\ncross-layer score-token logits and computing the expected score from a\nsoftmax-based distribution, while keeping the LLM backbone frozen and ensuring\nno impact on the inference process. LAGER fully leverages the complementary\ninformation across different layers, overcoming the limitations of relying\nsolely on the final layer. We evaluate our method on the standard alignment\nbenchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find\nthat LAGER achieves improvements of up to 7.5% over the best baseline across\nthese benchmarks. Without reasoning steps, LAGER matches or outperforms\nreasoning-based methods. Experiments on downstream applications, such as data\nselection and emotional understanding, further show the generalization of\nLAGER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using LLMs, a paradigm known as \"LLM-as-a-judge\". However,\nimproving its alignment with human preferences without complex prompts or\nfine-tuning remains challenging. Previous studies mainly optimize based on\nshallow outputs, overlooking rich cross-layer representations. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and task-relevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a post-hoc,\nplug-and-play framework for improving the alignment of LLM-as-a-Judge\npoint-wise evaluations with human scores by leveraging internal\nrepresentations. LAGER produces fine-grained judgment scores by aggregating\ncross-layer score-token logits and computing the expected score from a\nsoftmax-based distribution, while keeping the LLM backbone frozen and ensuring\nno impact on the inference process. LAGER fully leverages the complementary\ninformation across different layers, overcoming the limitations of relying\nsolely on the final layer. We evaluate our method on the standard alignment\nbenchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find\nthat LAGER achieves improvements of up to 7.5% over the best baseline across\nthese benchmarks. Without reasoning steps, LAGER matches or outperforms\nreasoning-based methods. Experiments on downstream applications, such as data\nselection and emotional understanding, further show the generalization of\nLAGER."
                },
                "authors": [
                    {
                        "name": "Peng Lai"
                    },
                    {
                        "name": "Jianjie Zheng"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14715v1",
                "updated": "2025-10-16T14:15:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    15,
                    1,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:15:01Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    15,
                    1,
                    3,
                    289,
                    0
                ],
                "title": "Numerical Studies on the Radio Afterglows in TDE (I): Forward Shock",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical Studies on the Radio Afterglows in TDE (I): Forward Shock"
                },
                "summary": "Recent long-term radio monitoring of tidal disruption events (TDEs) suggests\nthat radio afterglows are common. Most studies argue that these afterglows may\narise from forward shocks (FS) produced by the interaction between the TDE\noutflow and the hot, diffuse circumnuclear medium (CNM). Current theoretical\nmodels do not model the evolution of relativistic electrons in space, which\nintroduces uncertainties. Here we conducted hydrodynamic simulations to study\nthe hydrodynamic evolution of relativistic electrons, and calculated the\nsynchrotron spectra via radiative transfer. We focus on the FS scenario with\nnon-relativistic outflows, and various parameters of the outflow and CNM are\nexplored. A moderate outflow with kinetic energy of several $10^{50}$ erg in a\nGalactic center - like CNM can produce mJy-level radio afterglows at a distance\nof 100 Mpc. The self-absorption frequency exhibits a slow decline at early\ntimes and a rapid decrease at late times. We derived the temporal evolution of\nthe high-frequency radio flux, revealing its characteristic rise and decline\npattern. We also find that: (1) the radio spectra for narrow outflows are\nclearly anisotropic along different sight lines; (2) the FS parameters inferred\nfrom radio spectra using conventional analytical formulas deviate significantly\nfrom those in simulations, in which the inferred shock radii are half of those\nfrom simulations, and the inferred energies are an order of magnitude lower.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-term radio monitoring of tidal disruption events (TDEs) suggests\nthat radio afterglows are common. Most studies argue that these afterglows may\narise from forward shocks (FS) produced by the interaction between the TDE\noutflow and the hot, diffuse circumnuclear medium (CNM). Current theoretical\nmodels do not model the evolution of relativistic electrons in space, which\nintroduces uncertainties. Here we conducted hydrodynamic simulations to study\nthe hydrodynamic evolution of relativistic electrons, and calculated the\nsynchrotron spectra via radiative transfer. We focus on the FS scenario with\nnon-relativistic outflows, and various parameters of the outflow and CNM are\nexplored. A moderate outflow with kinetic energy of several $10^{50}$ erg in a\nGalactic center - like CNM can produce mJy-level radio afterglows at a distance\nof 100 Mpc. The self-absorption frequency exhibits a slow decline at early\ntimes and a rapid decrease at late times. We derived the temporal evolution of\nthe high-frequency radio flux, revealing its characteristic rise and decline\npattern. We also find that: (1) the radio spectra for narrow outflows are\nclearly anisotropic along different sight lines; (2) the FS parameters inferred\nfrom radio spectra using conventional analytical formulas deviate significantly\nfrom those in simulations, in which the inferred shock radii are half of those\nfrom simulations, and the inferred energies are an order of magnitude lower."
                },
                "authors": [
                    {
                        "name": "Guobin Mou"
                    }
                ],
                "author_detail": {
                    "name": "Guobin Mou"
                },
                "author": "Guobin Mou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12787v2",
                "updated": "2025-10-16T14:10:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    10,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-14T17:57:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics"
                },
                "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperforms them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperforms them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem."
                },
                "authors": [
                    {
                        "name": "Marco Del Tredici"
                    },
                    {
                        "name": "Jacob McCarran"
                    },
                    {
                        "name": "Benjamin Breen"
                    },
                    {
                        "name": "Javier Aspuru Mijares"
                    },
                    {
                        "name": "Weichen Winston Yin"
                    },
                    {
                        "name": "Jacob M. Taylor"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14703v1",
                "updated": "2025-10-16T14:06:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    6,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:06:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    6,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for\n  Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for\n  Function Calling"
                },
                "summary": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation."
                },
                "authors": [
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Yuanyuan Shi"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Renjie Ding"
                    },
                    {
                        "name": "Hairui Wang"
                    },
                    {
                        "name": "Yuxuan Peng"
                    },
                    {
                        "name": "Bizhe Bai"
                    },
                    {
                        "name": "Weixi Song"
                    },
                    {
                        "name": "Fengshuo Bai"
                    },
                    {
                        "name": "Huacan Chai"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16191v2",
                "updated": "2025-10-16T14:05:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    5,
                    53,
                    3,
                    289,
                    0
                ],
                "published": "2025-01-27T16:45:34Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    45,
                    34,
                    0,
                    27,
                    0
                ],
                "title": "The Last Dependency Crusade: Solving Python Dependency Conflicts with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Dependency Crusade: Solving Python Dependency Conflicts with\n  LLMs"
                },
                "summary": "Resolving Python dependency issues remains a tedious and error-prone process,\nforcing developers to manually trial compatible module versions and interpreter\nconfigurations. Existing automated solutions, such as knowledge-graph-based and\ndatabase-driven methods, face limitations due to the variety of dependency\nerror types, large sets of possible module versions, and conflicts among\ntransitive dependencies. This paper investigates the use of Large Language\nModels (LLMs) to automatically repair dependency issues in Python programs. We\npropose PLLM (pronounced \"plum\"), a novel retrieval-augmented generation (RAG)\napproach that iteratively infers missing or incorrect dependencies. PLLM builds\na test environment where the LLM proposes module combinations, observes\nexecution feedback, and refines its predictions using natural language\nprocessing (NLP) to parse error messages. We evaluate PLLM on the Gistable\nHG2.9K dataset, a curated collection of real-world Python programs. Using this\nbenchmark, we explore multiple PLLM configurations, including six open-source\nLLMs evaluated both with and without RAG. Our findings show that RAG\nconsistently improves fix rates, with the best performance achieved by Gemma-2\n9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo\nand ReadPyE, PLLM achieves significantly higher fix rates; +15.97\\% more than\nReadPyE and +21.58\\% more than PyEGo. Further analysis shows that PLLM is\nespecially effective for projects with numerous dependencies and those using\nspecialized numerical or machine-learning libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Python dependency issues remains a tedious and error-prone process,\nforcing developers to manually trial compatible module versions and interpreter\nconfigurations. Existing automated solutions, such as knowledge-graph-based and\ndatabase-driven methods, face limitations due to the variety of dependency\nerror types, large sets of possible module versions, and conflicts among\ntransitive dependencies. This paper investigates the use of Large Language\nModels (LLMs) to automatically repair dependency issues in Python programs. We\npropose PLLM (pronounced \"plum\"), a novel retrieval-augmented generation (RAG)\napproach that iteratively infers missing or incorrect dependencies. PLLM builds\na test environment where the LLM proposes module combinations, observes\nexecution feedback, and refines its predictions using natural language\nprocessing (NLP) to parse error messages. We evaluate PLLM on the Gistable\nHG2.9K dataset, a curated collection of real-world Python programs. Using this\nbenchmark, we explore multiple PLLM configurations, including six open-source\nLLMs evaluated both with and without RAG. Our findings show that RAG\nconsistently improves fix rates, with the best performance achieved by Gemma-2\n9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo\nand ReadPyE, PLLM achieves significantly higher fix rates; +15.97\\% more than\nReadPyE and +21.58\\% more than PyEGo. Further analysis shows that PLLM is\nespecially effective for projects with numerous dependencies and those using\nspecialized numerical or machine-learning libraries."
                },
                "authors": [
                    {
                        "name": "Antony Bartlett"
                    },
                    {
                        "name": "Cynthia Liem"
                    },
                    {
                        "name": "Annibale Panichella"
                    }
                ],
                "author_detail": {
                    "name": "Annibale Panichella"
                },
                "author": "Annibale Panichella",
                "arxiv_comment": "Pre-print - Accepted at the first annual workshop on Agentic Software\n  Engineering (AgenticSE) co-located with ASE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14702v1",
                "updated": "2025-10-16T14:05:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    5,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:05:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    5,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next\n  Point-of-Interest Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next\n  Point-of-Interest Prediction"
                },
                "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST."
                },
                "authors": [
                    {
                        "name": "Penglong Zhai"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Fanyi Di"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Yifang Yuan"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Sicong Wang"
                    },
                    {
                        "name": "Mingyang Yin"
                    },
                    {
                        "name": "Tingting Hu"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14700v1",
                "updated": "2025-10-16T14:04:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    4,
                    46,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:04:46Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    4,
                    46,
                    3,
                    289,
                    0
                ],
                "title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There\n  Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There\n  Yet?"
                },
                "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Guoai Xu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22358v2",
                "updated": "2025-10-16T14:03:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    3,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-28T13:38:21Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    21,
                    2,
                    148,
                    0
                ],
                "title": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in\n  LLMs Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in\n  LLMs Continual Learning"
                },
                "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in an end-to-end\ntraining stage. Specifically, OA-Adapter introduces a dynamic bottleneck\ndimension adaptation mechanism that simultaneously allocates an efficient\nparameter budget and optimizes task objectives without misalignment.To\neffectively preserve previously acquired knowledge while coordinating with the\ndynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter\nachieves higher average accuracy while using 58.5% fewer parameters on the\nstandard CL benchmark, and maintains its advantages on two larger benchmarks\ncomprising 15 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in an end-to-end\ntraining stage. Specifically, OA-Adapter introduces a dynamic bottleneck\ndimension adaptation mechanism that simultaneously allocates an efficient\nparameter budget and optimizes task objectives without misalignment.To\neffectively preserve previously acquired knowledge while coordinating with the\ndynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter\nachieves higher average accuracy while using 58.5% fewer parameters on the\nstandard CL benchmark, and maintains its advantages on two larger benchmarks\ncomprising 15 tasks."
                },
                "authors": [
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Wanrou Du"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Miao Pan"
                    },
                    {
                        "name": "Xiaoqi Qin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Qin"
                },
                "author": "Xiaoqi Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22369v2",
                "updated": "2025-10-16T14:00:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    0,
                    39,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-26T14:02:20Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    14,
                    2,
                    20,
                    4,
                    269,
                    0
                ],
                "title": "Role-Aware Multi-modal federated learning system for detecting phishing\n  webpages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Aware Multi-modal federated learning system for detecting phishing\n  webpages"
                },
                "summary": "We present a federated, multi-modal phishing website detector that supports\nURL, HTML, and IMAGE inputs without binding clients to a fixed modality at\ninference: any client can invoke any modality head trained elsewhere.\nMethodologically, we propose role-aware bucket aggregation on top of FedProx,\ninspired by Mixture-of-Experts and FedMM. We drop learnable routing and use\nhard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling\nseparate aggregation of modality-specific parameters to isolate cross-embedding\nconflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc\n97.5% with FPR 2.4% across two data types; on the image subset (ablation) it\nattains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an\nearly three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc\n96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results\nindicate that bucket aggregation with hard-gated experts enables stable\nfederated training under strict privacy, while improving the usability and\nflexibility of multi-modal phishing detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a federated, multi-modal phishing website detector that supports\nURL, HTML, and IMAGE inputs without binding clients to a fixed modality at\ninference: any client can invoke any modality head trained elsewhere.\nMethodologically, we propose role-aware bucket aggregation on top of FedProx,\ninspired by Mixture-of-Experts and FedMM. We drop learnable routing and use\nhard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling\nseparate aggregation of modality-specific parameters to isolate cross-embedding\nconflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc\n97.5% with FPR 2.4% across two data types; on the image subset (ablation) it\nattains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an\nearly three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc\n96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results\nindicate that bucket aggregation with hard-gated experts enables stable\nfederated training under strict privacy, while improving the usability and\nflexibility of multi-modal phishing detection."
                },
                "authors": [
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Imran Khan"
                    },
                    {
                        "name": "Martin White"
                    },
                    {
                        "name": "Natalia Beloff"
                    }
                ],
                "author_detail": {
                    "name": "Natalia Beloff"
                },
                "author": "Natalia Beloff",
                "arxiv_comment": "22 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13750v2",
                "updated": "2025-10-16T13:58:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    58,
                    27,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T16:55:56Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    55,
                    56,
                    2,
                    288,
                    0
                ],
                "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation"
                },
                "summary": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment."
                },
                "authors": [
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Vivek Datla"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Ritesh Soni"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Soni"
                },
                "author": "Ritesh Soni",
                "arxiv_comment": "UncertaiNLP at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14681v1",
                "updated": "2025-10-16T13:46:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    46,
                    45,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:46:45Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    46,
                    45,
                    3,
                    289,
                    0
                ],
                "title": "Hierarchical shot-noise Cox process mixtures for clustering across\n  groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical shot-noise Cox process mixtures for clustering across\n  groups"
                },
                "summary": "Clustering observations across partially exchangeable groups of data is a\nroutine task in Bayesian nonparametrics. Previously proposed models allow for\nclustering across groups by sharing atoms in the group-specific mixing\nmeasures. However, exact atom sharing can be overly rigid when groups differ\nsubtly, introducing a trade-off between clustering and density estimates and\nfragmenting across-group clusters, particularly at larger sample sizes. We\nintroduce the hierarchical shot-noise Cox process (HSNCP) mixture model, where\ngroup-specific atoms concentrate around shared centers through a kernel. This\nenables accurate density estimation within groups and flexible borrowing across\ngroups, overcoming the density-clustering trade-off of previous approaches. Our\nconstruction, built on the shot-noise Cox process, remains analytically\ntractable: we derive closed-form prior moments and an inter-group correlation,\nobtain the marginal law and predictive distribution for latent parameters, as\nwell as the posterior of the mixing measures given the latent parameters. We\ndevelop an efficient conditional MCMC algorithm for posterior inference. We\nassess the performance of the HSNCP model through simulations and an\napplication to a large galaxy dataset, demonstrating balanced across-group\nclusters and improved density estimates compared with the hierarchical\nDirichlet process, including under model misspecification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering observations across partially exchangeable groups of data is a\nroutine task in Bayesian nonparametrics. Previously proposed models allow for\nclustering across groups by sharing atoms in the group-specific mixing\nmeasures. However, exact atom sharing can be overly rigid when groups differ\nsubtly, introducing a trade-off between clustering and density estimates and\nfragmenting across-group clusters, particularly at larger sample sizes. We\nintroduce the hierarchical shot-noise Cox process (HSNCP) mixture model, where\ngroup-specific atoms concentrate around shared centers through a kernel. This\nenables accurate density estimation within groups and flexible borrowing across\ngroups, overcoming the density-clustering trade-off of previous approaches. Our\nconstruction, built on the shot-noise Cox process, remains analytically\ntractable: we derive closed-form prior moments and an inter-group correlation,\nobtain the marginal law and predictive distribution for latent parameters, as\nwell as the posterior of the mixing measures given the latent parameters. We\ndevelop an efficient conditional MCMC algorithm for posterior inference. We\nassess the performance of the HSNCP model through simulations and an\napplication to a large galaxy dataset, demonstrating balanced across-group\nclusters and improved density estimates compared with the hierarchical\nDirichlet process, including under model misspecification."
                },
                "authors": [
                    {
                        "name": "Alessandro Carminati"
                    },
                    {
                        "name": "Mario Beraha"
                    },
                    {
                        "name": "Federico Camerlenghi"
                    },
                    {
                        "name": "Alessandra Guglielmi"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Guglielmi"
                },
                "author": "Alessandra Guglielmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08236v2",
                "updated": "2025-10-16T13:44:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    44,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-09T14:00:40Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    0,
                    40,
                    3,
                    282,
                    0
                ],
                "title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs."
                },
                "authors": [
                    {
                        "name": "Konrad LÃ¶hr"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael FÃ¤rber"
                    }
                ],
                "author_detail": {
                    "name": "Michael FÃ¤rber"
                },
                "author": "Michael FÃ¤rber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05629v2",
                "updated": "2025-10-16T13:40:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    40,
                    55,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-07T17:59:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification"
                },
                "summary": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Zhou Ziheng"
                    },
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xinyu Ye"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08670v2",
                "updated": "2025-10-16T13:40:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    40,
                    44,
                    3,
                    289,
                    0
                ],
                "published": "2025-01-15T09:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    30,
                    2,
                    15,
                    0
                ],
                "title": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery"
                },
                "summary": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes."
                },
                "authors": [
                    {
                        "name": "Zeqin Liao"
                    },
                    {
                        "name": "Yuhong Nan"
                    },
                    {
                        "name": "Zixu Gao"
                    },
                    {
                        "name": "Henglong Liang"
                    },
                    {
                        "name": "Sicheng Hao"
                    },
                    {
                        "name": "Peifan Reng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "This is the author version of the article accepted for publication in\n  IEEE Transactions on Software Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04426v3",
                "updated": "2025-10-16T13:38:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    38,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-02-06T18:52:10Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    52,
                    10,
                    3,
                    37,
                    0
                ],
                "title": "The simulation of judgment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The simulation of judgment in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly embedded in evaluative\nprocesses, from information filtering to assessing and addressing knowledge\ngaps through explanation and credibility judgments. This raises the need to\nexamine how such evaluations are built, what assumptions they rely on, and how\ntheir strategies diverge from those of humans. We benchmark six LLMs against\nexpert ratings--NewsGuard and Media Bias/Fact Check--and against human\njudgments collected through a controlled experiment. We use news domains purely\nas a controlled benchmark for evaluative tasks, focusing on the underlying\nmechanisms rather than on news classification per se. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand nonexpert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, our findings show consistent differences in the observable criteria\nguiding model evaluations, suggesting that lexical associations and statistical\npriors could influence evaluations in ways that differ from contextual\nreasoning. This reliance is associated with systematic effects: political\nasymmetries and a tendency to confuse linguistic form with epistemic\nreliability--a dynamic we term epistemia, the illusion of knowledge that\nemerges when surface plausibility replaces verification. Indeed, delegating\njudgment to such systems may affect the heuristics underlying evaluative\nprocesses, suggesting a shift from normative reasoning toward pattern-based\napproximation and raising open questions about the role of LLMs in evaluative\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly embedded in evaluative\nprocesses, from information filtering to assessing and addressing knowledge\ngaps through explanation and credibility judgments. This raises the need to\nexamine how such evaluations are built, what assumptions they rely on, and how\ntheir strategies diverge from those of humans. We benchmark six LLMs against\nexpert ratings--NewsGuard and Media Bias/Fact Check--and against human\njudgments collected through a controlled experiment. We use news domains purely\nas a controlled benchmark for evaluative tasks, focusing on the underlying\nmechanisms rather than on news classification per se. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand nonexpert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, our findings show consistent differences in the observable criteria\nguiding model evaluations, suggesting that lexical associations and statistical\npriors could influence evaluations in ways that differ from contextual\nreasoning. This reliance is associated with systematic effects: political\nasymmetries and a tendency to confuse linguistic form with epistemic\nreliability--a dynamic we term epistemia, the illusion of knowledge that\nemerges when surface plausibility replaces verification. Indeed, delegating\njudgment to such systems may affect the heuristics underlying evaluative\nprocesses, suggesting a shift from normative reasoning toward pattern-based\napproximation and raising open questions about the role of LLMs in evaluative\nprocesses."
                },
                "authors": [
                    {
                        "name": "Edoardo Loru"
                    },
                    {
                        "name": "Jacopo Nudo"
                    },
                    {
                        "name": "NiccolÃ² Di Marco"
                    },
                    {
                        "name": "Alessandro Santirocchi"
                    },
                    {
                        "name": "Roberto Atzeni"
                    },
                    {
                        "name": "Matteo Cinelli"
                    },
                    {
                        "name": "Vincenzo Cestari"
                    },
                    {
                        "name": "Clelia Rossi-Arnaud"
                    },
                    {
                        "name": "Walter Quattrociocchi"
                    }
                ],
                "author_detail": {
                    "name": "Walter Quattrociocchi"
                },
                "author": "Walter Quattrociocchi",
                "arxiv_doi": "10.1073/pnas.2518443122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2518443122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.04426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Please refer to published version:\n  https://doi.org/10.1073/pnas.2518443122",
                "arxiv_journal_ref": "Proc. Natl. Acad. Sci. U.S.A. 122 (42) e2518443122, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14676v1",
                "updated": "2025-10-16T13:33:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    33,
                    10,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:33:10Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    33,
                    10,
                    3,
                    289,
                    0
                ],
                "title": "NAEL: Non-Anthropocentric Ethical Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAEL: Non-Anthropocentric Ethical Logic"
                },
                "summary": "We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical\nframework for artificial agents grounded in active inference and symbolic\nreasoning. Departing from conventional, human-centred approaches to AI ethics,\nNAEL formalizes ethical behaviour as an emergent property of intelligent\nsystems minimizing global expected free energy in dynamic, multi-agent\nenvironments. We propose a neuro-symbolic architecture to allow agents to\nevaluate the ethical consequences of their actions in uncertain settings. The\nproposed system addresses the limitations of existing ethical models by\nallowing agents to develop context-sensitive, adaptive, and relational ethical\nbehaviour without presupposing anthropomorphic moral intuitions. A case study\ninvolving ethical resource distribution illustrates NAEL's dynamic balancing of\nself-preservation, epistemic learning, and collective welfare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical\nframework for artificial agents grounded in active inference and symbolic\nreasoning. Departing from conventional, human-centred approaches to AI ethics,\nNAEL formalizes ethical behaviour as an emergent property of intelligent\nsystems minimizing global expected free energy in dynamic, multi-agent\nenvironments. We propose a neuro-symbolic architecture to allow agents to\nevaluate the ethical consequences of their actions in uncertain settings. The\nproposed system addresses the limitations of existing ethical models by\nallowing agents to develop context-sensitive, adaptive, and relational ethical\nbehaviour without presupposing anthropomorphic moral intuitions. A case study\ninvolving ethical resource distribution illustrates NAEL's dynamic balancing of\nself-preservation, epistemic learning, and collective welfare."
                },
                "authors": [
                    {
                        "name": "Bianca Maria Lerma"
                    },
                    {
                        "name": "Rafael PeÃ±aloza"
                    }
                ],
                "author_detail": {
                    "name": "Rafael PeÃ±aloza"
                },
                "author": "Rafael PeÃ±aloza",
                "arxiv_comment": "Accepted to the FEAR workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23315v2",
                "updated": "2025-10-16T13:29:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    29,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-07-31T07:47:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    47,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "Analysis of Hyperparameter Optimization Effects on Lightweight Deep\n  Models for Real-Time Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Hyperparameter Optimization Effects on Lightweight Deep\n  Models for Real-Time Image Classification"
                },
                "summary": "Lightweight convolutional and transformer-based networks are increasingly\npreferred for real-time image classification, especially on\nresource-constrained devices. This study evaluates the impact of hyperparameter\noptimization on the accuracy and deployment feasibility of seven modern\nlightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L,\nMobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced\nsubset of 90,000 images from ImageNet-1K. Under standardized training settings,\nthis paper investigates the influence of learning rate schedules, augmentation,\noptimizers, and initialization on model performance. Inference benchmarks are\nperformed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512,\ncapturing latency and throughput in real-time conditions. This work\ndemonstrates that controlled hyperparameter variation significantly alters\nconvergence dynamics in lightweight CNN and transformer backbones, providing\ninsight into stability regions and deployment feasibility in edge artificial\nintelligence. Our results reveal that tuning alone leads to a top-1 accuracy\nimprovement of 1.5 to 3.5 percent over baselines, and select models (e.g.,\nRepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800\nframes per second, making them ideal for edge deployment. This work provides\nreproducible, subset-based insights into lightweight hyperparameter tuning and\nits role in balancing speed and accuracy. The code and logs may be seen at:\nhttps://vineetkumarrakesh.github.io/lcnn-opt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight convolutional and transformer-based networks are increasingly\npreferred for real-time image classification, especially on\nresource-constrained devices. This study evaluates the impact of hyperparameter\noptimization on the accuracy and deployment feasibility of seven modern\nlightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L,\nMobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced\nsubset of 90,000 images from ImageNet-1K. Under standardized training settings,\nthis paper investigates the influence of learning rate schedules, augmentation,\noptimizers, and initialization on model performance. Inference benchmarks are\nperformed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512,\ncapturing latency and throughput in real-time conditions. This work\ndemonstrates that controlled hyperparameter variation significantly alters\nconvergence dynamics in lightweight CNN and transformer backbones, providing\ninsight into stability regions and deployment feasibility in edge artificial\nintelligence. Our results reveal that tuning alone leads to a top-1 accuracy\nimprovement of 1.5 to 3.5 percent over baselines, and select models (e.g.,\nRepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800\nframes per second, making them ideal for edge deployment. This work provides\nreproducible, subset-based insights into lightweight hyperparameter tuning and\nits role in balancing speed and accuracy. The code and logs may be seen at:\nhttps://vineetkumarrakesh.github.io/lcnn-opt"
                },
                "authors": [
                    {
                        "name": "Vineet Kumar Rakesh"
                    },
                    {
                        "name": "Soumya Mazumdar"
                    },
                    {
                        "name": "Tapas Samanta"
                    },
                    {
                        "name": "Hemendra Kumar Pandey"
                    },
                    {
                        "name": "Amitabha Das"
                    }
                ],
                "author_detail": {
                    "name": "Amitabha Das"
                },
                "author": "Amitabha Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14672v1",
                "updated": "2025-10-16T13:29:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    29,
                    2,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:29:02Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    29,
                    2,
                    3,
                    289,
                    0
                ],
                "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning"
                },
                "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io"
                },
                "authors": [
                    {
                        "name": "Jinglei Zhang"
                    },
                    {
                        "name": "Yuanfan Guo"
                    },
                    {
                        "name": "Rolandos Alexandros Potamias"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14665v1",
                "updated": "2025-10-16T13:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    44,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    44,
                    3,
                    289,
                    0
                ],
                "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Hallucinations: The Illusion of Understanding in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding."
                },
                "authors": [
                    {
                        "name": "Rikard Rosenbacke"
                    },
                    {
                        "name": "Carl Rosenbacke"
                    },
                    {
                        "name": "Victor Rosenbacke"
                    },
                    {
                        "name": "Martin McKee"
                    }
                ],
                "author_detail": {
                    "name": "Martin McKee"
                },
                "author": "Martin McKee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14664v1",
                "updated": "2025-10-16T13:19:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:19:07Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    7,
                    3,
                    289,
                    0
                ],
                "title": "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality\n  Evaluation"
                },
                "summary": "Generative speech technologies are progressing rapidly, but evaluating the\nperceptual quality of synthetic speech remains a core challenge. Existing\nmethods typically rely on scalar scores or binary decisions, which lack\ninterpretability and generalization across tasks and languages. We present\nSpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs)\nto conduct structured and explanation-based speech quality evaluation. To\nsupport this direction, we introduce SpeechEval, a large-scale dataset\ncontaining 32,207 multilingual speech clips and 128,754 annotations spanning\nfour tasks: quality assessment, pairwise comparison, improvement suggestion,\nand deepfake detection. Based on this resource, we develop SQ-LLM, a\nspeech-quality-aware LLM trained with chain-of-thought reasoning and reward\noptimization to improve capability. Experimental results show that SQ-LLM\ndelivers strong performance across tasks and languages, revealing the potential\nof this paradigm for advancing speech quality evaluation. Relevant resources\nwill be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative speech technologies are progressing rapidly, but evaluating the\nperceptual quality of synthetic speech remains a core challenge. Existing\nmethods typically rely on scalar scores or binary decisions, which lack\ninterpretability and generalization across tasks and languages. We present\nSpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs)\nto conduct structured and explanation-based speech quality evaluation. To\nsupport this direction, we introduce SpeechEval, a large-scale dataset\ncontaining 32,207 multilingual speech clips and 128,754 annotations spanning\nfour tasks: quality assessment, pairwise comparison, improvement suggestion,\nand deepfake detection. Based on this resource, we develop SQ-LLM, a\nspeech-quality-aware LLM trained with chain-of-thought reasoning and reward\noptimization to improve capability. Experimental results show that SQ-LLM\ndelivers strong performance across tasks and languages, revealing the potential\nof this paradigm for advancing speech quality evaluation. Relevant resources\nwill be open-sourced."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jinghua Zhao"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Junyang Chen"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Haoqin Sun"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Yong Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yong Qin"
                },
                "author": "Yong Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18122v2",
                "updated": "2025-10-16T13:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    17,
                    5,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-25T15:30:12Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    30,
                    12,
                    0,
                    237,
                    0
                ],
                "title": "Provable Mixed-Noise Learning with Flow-Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provable Mixed-Noise Learning with Flow-Matching"
                },
                "summary": "We study Bayesian inverse problems with mixed noise, modeled as a combination\nof additive and multiplicative Gaussian components. While traditional inference\nmethods often assume fixed or known noise characteristics, real-world\napplications, particularly in physics and chemistry, frequently involve noise\nwith unknown and heterogeneous structure. Motivated by recent advances in\nflow-based generative modeling, we propose a novel inference framework based on\nconditional flow matching embedded within an Expectation-Maximization (EM)\nalgorithm to jointly estimate posterior samplers and noise parameters. To\nenable high-dimensional inference and improve scalability, we use\nsimulation-free ODE-based flow matching as the generative model in the E-step\nof the EM algorithm. We prove that, under suitable assumptions, the EM updates\nconverge to the true noise parameters in the population limit of infinite\nobservations. Our numerical results illustrate the effectiveness of combining\nEM inference with flow matching for mixed-noise Bayesian inverse problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study Bayesian inverse problems with mixed noise, modeled as a combination\nof additive and multiplicative Gaussian components. While traditional inference\nmethods often assume fixed or known noise characteristics, real-world\napplications, particularly in physics and chemistry, frequently involve noise\nwith unknown and heterogeneous structure. Motivated by recent advances in\nflow-based generative modeling, we propose a novel inference framework based on\nconditional flow matching embedded within an Expectation-Maximization (EM)\nalgorithm to jointly estimate posterior samplers and noise parameters. To\nenable high-dimensional inference and improve scalability, we use\nsimulation-free ODE-based flow matching as the generative model in the E-step\nof the EM algorithm. We prove that, under suitable assumptions, the EM updates\nconverge to the true noise parameters in the population limit of infinite\nobservations. Our numerical results illustrate the effectiveness of combining\nEM inference with flow matching for mixed-noise Bayesian inverse problems."
                },
                "authors": [
                    {
                        "name": "Paul Hagemann"
                    },
                    {
                        "name": "Robert Gruhlke"
                    },
                    {
                        "name": "Bernhard Stankewitz"
                    },
                    {
                        "name": "Claudia Schillings"
                    },
                    {
                        "name": "Gabriele Steidl"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Steidl"
                },
                "author": "Gabriele Steidl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14660v1",
                "updated": "2025-10-16T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    15,
                    40,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:15:40Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    15,
                    40,
                    3,
                    289,
                    0
                ],
                "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs"
                },
                "summary": "Search augmentation empowers Large Language Models with retrieval\ncapabilities to overcome the limitations imposed by static parameters.\nRecently, Reinforcement Learning leverages tailored reward signals as a viable\ntechnique to enhance LLMs performing tasks involving search. However, existing\nreward modeling for search-augmented LLMs faces several limitations. Rule-based\nrewards, such as Exact Match, are verifiable but fragile to variations in\nexpression and cannot be applied to long-form workloads. In contrast,\ngenerative rewards improve robustness, but designing verifiable and stable\nrewards for long-form workloads in dynamic corpora remains challenging and also\nincurs high computational costs. In this paper, we propose a unified and\nverifiable paradigm, \"nugget-as-rubric\", which treats atomic information points\nas structured evaluation criteria for different search-augmentation workloads.\nShort-form tasks correspond to a single rubric, whereas long-form tasks expand\nto multiple rubrics aligned with the question's information needs. To support\nlong-form settings, we design an automatic rubric construction pipeline based\non query rewriting, which can automatically retrieve passages relevant to each\nquestion and extract rubrics from them, both from static corpora and from\ndynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a\n4B-parameter efficient generative verifier under our proposed verifiable\nparadigm, which is trained via the idea of distillation and a two-stage\nstrategy. Experimental results show that Search-Gen-V achieves strong\nverification accuracy across different workloads, making it a scalable, robust,\nand efficient verifiable reward constructor for search-augmented LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search augmentation empowers Large Language Models with retrieval\ncapabilities to overcome the limitations imposed by static parameters.\nRecently, Reinforcement Learning leverages tailored reward signals as a viable\ntechnique to enhance LLMs performing tasks involving search. However, existing\nreward modeling for search-augmented LLMs faces several limitations. Rule-based\nrewards, such as Exact Match, are verifiable but fragile to variations in\nexpression and cannot be applied to long-form workloads. In contrast,\ngenerative rewards improve robustness, but designing verifiable and stable\nrewards for long-form workloads in dynamic corpora remains challenging and also\nincurs high computational costs. In this paper, we propose a unified and\nverifiable paradigm, \"nugget-as-rubric\", which treats atomic information points\nas structured evaluation criteria for different search-augmentation workloads.\nShort-form tasks correspond to a single rubric, whereas long-form tasks expand\nto multiple rubrics aligned with the question's information needs. To support\nlong-form settings, we design an automatic rubric construction pipeline based\non query rewriting, which can automatically retrieve passages relevant to each\nquestion and extract rubrics from them, both from static corpora and from\ndynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a\n4B-parameter efficient generative verifier under our proposed verifiable\nparadigm, which is trained via the idea of distillation and a two-stage\nstrategy. Experimental results show that Search-Gen-V achieves strong\nverification accuracy across different workloads, making it a scalable, robust,\nand efficient verifiable reward constructor for search-augmented LLMs."
                },
                "authors": [
                    {
                        "name": "Linyue Ma"
                    },
                    {
                        "name": "Yilong Xu"
                    },
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Zhi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Zheng"
                },
                "author": "Zhi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14656v1",
                "updated": "2025-10-16T13:12:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    12,
                    26,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:12:26Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    12,
                    26,
                    3,
                    289,
                    0
                ],
                "title": "Parameter Identification for Partial Differential Equation with Jump\n  Discontinuities in Coefficients by Markov Switching Model and\n  Physics-Informed Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Identification for Partial Differential Equation with Jump\n  Discontinuities in Coefficients by Markov Switching Model and\n  Physics-Informed Machine Learning"
                },
                "summary": "Inverse problems involving partial differential equations (PDEs) with\ndiscontinuous coefficients are fundamental challenges in modeling complex\nspatiotemporal systems with heterogeneous structures and uncertain dynamics.\nTraditional numerical and machine learning approaches often face limitations in\naddressing these problems due to high dimensionality, inherent nonlinearity,\nand discontinuous parameter spaces. In this work, we propose a novel\ncomputational framework that synergistically integrates physics-informed deep\nlearning with Bayesian inference for accurate parameter identification in PDEs\nwith jump discontinuities in coefficients. The core innovation of our framework\nlies in a dual-network architecture employing a gradient-adaptive weighting\nstrategy: a main network approximates PDE solutions while a sub network samples\nits coefficients. To effectively identify mixture structures in parameter\nspaces, we employ Markovian dynamics methods to capture hidden state\ntransitions of complex spatiotemporal systems. The framework has applications\nin reconstruction of solutions and identification of parameter-varying regions.\nComprehensive numerical experiments on various PDEs with jump-varying\ncoefficients demonstrate the framework's exceptional adaptability, accuracy,\nand robustness compared to existing methods. This study provides a\ngeneralizable computational approach of parameter identification for PDEs with\ndiscontinuous parameter structures, particularly in non-stationary or\nheterogeneous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse problems involving partial differential equations (PDEs) with\ndiscontinuous coefficients are fundamental challenges in modeling complex\nspatiotemporal systems with heterogeneous structures and uncertain dynamics.\nTraditional numerical and machine learning approaches often face limitations in\naddressing these problems due to high dimensionality, inherent nonlinearity,\nand discontinuous parameter spaces. In this work, we propose a novel\ncomputational framework that synergistically integrates physics-informed deep\nlearning with Bayesian inference for accurate parameter identification in PDEs\nwith jump discontinuities in coefficients. The core innovation of our framework\nlies in a dual-network architecture employing a gradient-adaptive weighting\nstrategy: a main network approximates PDE solutions while a sub network samples\nits coefficients. To effectively identify mixture structures in parameter\nspaces, we employ Markovian dynamics methods to capture hidden state\ntransitions of complex spatiotemporal systems. The framework has applications\nin reconstruction of solutions and identification of parameter-varying regions.\nComprehensive numerical experiments on various PDEs with jump-varying\ncoefficients demonstrate the framework's exceptional adaptability, accuracy,\nand robustness compared to existing methods. This study provides a\ngeneralizable computational approach of parameter identification for PDEs with\ndiscontinuous parameter structures, particularly in non-stationary or\nheterogeneous systems."
                },
                "authors": [
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Guanyu Pan"
                    },
                    {
                        "name": "Xiangjun Wang"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Guangtao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhang"
                },
                "author": "Guangtao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12350v2",
                "updated": "2025-10-16T13:07:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    7,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-14T10:07:53Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    7,
                    53,
                    1,
                    287,
                    0
                ],
                "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis"
                },
                "summary": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians."
                },
                "authors": [
                    {
                        "name": "Ayush Khaitan"
                    },
                    {
                        "name": "Vijay Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Ganesh"
                },
                "author": "Vijay Ganesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B35, 68W30, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14647v1",
                "updated": "2025-10-16T12:59:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    59,
                    34,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:59:34Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    59,
                    34,
                    3,
                    289,
                    0
                ],
                "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation"
                },
                "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage."
                },
                "authors": [
                    {
                        "name": "Jialei Huang"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Yuanqing Gong"
                    },
                    {
                        "name": "Xuezhou Zhu"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Kaifeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifeng Zhang"
                },
                "author": "Kaifeng Zhang",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14642v1",
                "updated": "2025-10-16T12:54:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    54,
                    53,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:54:53Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    54,
                    53,
                    3,
                    289,
                    0
                ],
                "title": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon\n  Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon\n  Blockchain"
                },
                "summary": "In blockchain networks, the strategic ordering of transactions within blocks\nhas emerged as a significant source of profit extraction, known as Maximal\nExtractable Value (MEV). The transition from spam-based Priority Gas Auctions\nto structured auction mechanisms like Polygon Atlas has transformed MEV\nextraction from public bidding wars into sealed-bid competitions under extreme\ntime constraints. While this shift reduces network congestion, it introduces\ncomplex strategic challenges where searchers must make optimal bidding\ndecisions within a sub-second window without knowledge of competitor behavior\nor presence. Traditional game-theoretic approaches struggle in this\nhigh-frequency, partially observable environment due to their reliance on\ncomplete information and static equilibrium assumptions. We present a\nreinforcement learning framework for MEV extraction on Polygon Atlas and make\nthree contributions: (1) A novel simulation environment that accurately models\nthe stochastic arrival of arbitrage opportunities and probabilistic competition\nin Atlas auctions; (2) A PPO-based bidding agent optimized for real-time\nconstraints, capable of adaptive strategy formulation in continuous action\nspaces while maintaining production-ready inference speeds; (3) Empirical\nvalidation demonstrating our history-conditioned agent captures 49\\% of\navailable profits when deployed alongside existing searchers and 81\\% when\nreplacing the market leader, significantly outperforming static bidding\nstrategies. Our work establishes that reinforcement learning provides a\ncritical advantage in high-frequency MEV environments where traditional\noptimization methods fail, offering immediate value for industrial participants\nand protocol designers alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In blockchain networks, the strategic ordering of transactions within blocks\nhas emerged as a significant source of profit extraction, known as Maximal\nExtractable Value (MEV). The transition from spam-based Priority Gas Auctions\nto structured auction mechanisms like Polygon Atlas has transformed MEV\nextraction from public bidding wars into sealed-bid competitions under extreme\ntime constraints. While this shift reduces network congestion, it introduces\ncomplex strategic challenges where searchers must make optimal bidding\ndecisions within a sub-second window without knowledge of competitor behavior\nor presence. Traditional game-theoretic approaches struggle in this\nhigh-frequency, partially observable environment due to their reliance on\ncomplete information and static equilibrium assumptions. We present a\nreinforcement learning framework for MEV extraction on Polygon Atlas and make\nthree contributions: (1) A novel simulation environment that accurately models\nthe stochastic arrival of arbitrage opportunities and probabilistic competition\nin Atlas auctions; (2) A PPO-based bidding agent optimized for real-time\nconstraints, capable of adaptive strategy formulation in continuous action\nspaces while maintaining production-ready inference speeds; (3) Empirical\nvalidation demonstrating our history-conditioned agent captures 49\\% of\navailable profits when deployed alongside existing searchers and 81\\% when\nreplacing the market leader, significantly outperforming static bidding\nstrategies. Our work establishes that reinforcement learning provides a\ncritical advantage in high-frequency MEV environments where traditional\noptimization methods fail, offering immediate value for industrial participants\nand protocol designers alike."
                },
                "authors": [
                    {
                        "name": "Andrei Seoev"
                    },
                    {
                        "name": "Leonid Gremyachikh"
                    },
                    {
                        "name": "Anastasiia Smirnova"
                    },
                    {
                        "name": "Yash Madhwal"
                    },
                    {
                        "name": "Alisa Kalacheva"
                    },
                    {
                        "name": "Dmitry Belousov"
                    },
                    {
                        "name": "Ilia Zubov"
                    },
                    {
                        "name": "Aleksei Smirnov"
                    },
                    {
                        "name": "Denis Fedyanin"
                    },
                    {
                        "name": "Vladimir Gorgadze"
                    },
                    {
                        "name": "Yury Yanovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Yanovich"
                },
                "author": "Yury Yanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14640v2",
                "updated": "2025-10-17T11:18:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    11,
                    18,
                    40,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-16T12:54:40Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    54,
                    40,
                    3,
                    289,
                    0
                ],
                "title": "Intent Clustering with Shared Pseudo-Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Clustering with Shared Pseudo-Labels"
                },
                "summary": "In this paper, we propose an intuitive, training-free and label-free method\nfor intent clustering that makes minimal assumptions using lightweight and\nopen-source LLMs. Many current approaches rely on commercial LLMs, which are\ncostly, and offer limited transparency. Additionally, their methods often\nexplicitly depend on knowing the number of clusters in advance, which is often\nnot the case in realistic settings. To address these challenges, instead of\nasking the LLM to match similar text directly, we first ask it to generate\npseudo-labels for each text, and then perform multi-label classification in\nthis pseudo-label set for each text. This approach is based on the hypothesis\nthat texts belonging to the same cluster will share more labels, and will\ntherefore be closer when encoded into embeddings. These pseudo-labels are more\nhuman-readable than direct similarity matches. Our evaluation on four benchmark\nsets shows that our approach achieves results comparable to and better than\nrecent baselines, while remaining simple and computationally efficient. Our\nfindings indicate that our method can be applied in low-resource scenarios and\nis stable across multiple models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an intuitive, training-free and label-free method\nfor intent clustering that makes minimal assumptions using lightweight and\nopen-source LLMs. Many current approaches rely on commercial LLMs, which are\ncostly, and offer limited transparency. Additionally, their methods often\nexplicitly depend on knowing the number of clusters in advance, which is often\nnot the case in realistic settings. To address these challenges, instead of\nasking the LLM to match similar text directly, we first ask it to generate\npseudo-labels for each text, and then perform multi-label classification in\nthis pseudo-label set for each text. This approach is based on the hypothesis\nthat texts belonging to the same cluster will share more labels, and will\ntherefore be closer when encoded into embeddings. These pseudo-labels are more\nhuman-readable than direct similarity matches. Our evaluation on four benchmark\nsets shows that our approach achieves results comparable to and better than\nrecent baselines, while remaining simple and computationally efficient. Our\nfindings indicate that our method can be applied in low-resource scenarios and\nis stable across multiple models and datasets."
                },
                "authors": [
                    {
                        "name": "I-Fan Lin"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.14980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14980v1",
                "updated": "2025-10-16T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "Agentic Design of Compositional Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Design of Compositional Machines"
                },
                "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning."
                },
                "authors": [
                    {
                        "name": "Wenqian Zhang"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Zhen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Liu"
                },
                "author": "Zhen Liu",
                "arxiv_comment": "75 pages, 31 figures, Project Page: https://besiegefield.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14973v1",
                "updated": "2025-10-16T17:59:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need for KV Cache in Diffusion LLMs"
                },
                "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Quan Nguyen-Tri"
                    },
                    {
                        "name": "Mukul Ranjan"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "https://vila-lab.github.io/elastic-cache-webpage/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14972v1",
                "updated": "2025-10-16T17:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    45,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    45,
                    3,
                    289,
                    0
                ],
                "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar"
                },
                "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs."
                },
                "authors": [
                    {
                        "name": "Yinxi Li"
                    },
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Pengyu Nie"
                    }
                ],
                "author_detail": {
                    "name": "Pengyu Nie"
                },
                "author": "Pengyu Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14969v1",
                "updated": "2025-10-16T17:59:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:38Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training"
                },
                "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents."
                },
                "authors": [
                    {
                        "name": "Yiming Wang"
                    },
                    {
                        "name": "Da Yin"
                    },
                    {
                        "name": "Yuedong Cui"
                    },
                    {
                        "name": "Ruichen Zheng"
                    },
                    {
                        "name": "Zhiqian Li"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Xueqing Wu"
                    },
                    {
                        "name": "Chenchen Ye"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "arxiv_comment": "Preprint. Project page:\n  https://ui-simulator.notion.site/llms-as-scalable-digital-world-simulator;\n  Code and data: https://github.com/WadeYin9712/UI-Simulator",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14970v1",
                "updated": "2025-10-16T17:59:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:38Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    38,
                    3,
                    289,
                    0
                ],
                "title": "Biology-informed neural networks learn nonlinear representations from\n  omics data to improve genomic prediction and interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biology-informed neural networks learn nonlinear representations from\n  omics data to improve genomic prediction and interpretability"
                },
                "summary": "We extend biologically-informed neural networks (BINNs) for genomic\nprediction (GP) and selection (GS) in crops by integrating thousands of\nsingle-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior\nbiological knowledge. Traditional genotype-to-phenotype (G2P) models depend\nheavily on direct mappings that achieve only modest accuracy, forcing breeders\nto conduct large, costly field trials to maintain or marginally improve genetic\ngain. Models that incorporate intermediate molecular phenotypes such as gene\nexpression can achieve higher predictive fit, but they remain impractical for\nGS since such data are unavailable at deployment or design time. BINNs overcome\nthis limitation by encoding pathway-level inductive biases and leveraging\nmulti-omics data only during training, while using genotype data alone during\ninference. Applied to maize gene-expression and multi-environment field-trial\ndata, BINN improves rank-correlation accuracy by up to 56% within and across\nsubpopulations under sparse-data conditions and nonlinearly identifies genes\nthat GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic\nmetabolomics benchmark, BINN reduces prediction error by 75% relative to\nconventional neural nets and correctly identifies the most important nonlinear\npathway. Importantly, both cases show highly sensitive BINN latent variables\ncorrelate with the experimental quantities they represent, despite not being\ntrained on them. This suggests BINNs learn biologically-relevant\nrepresentations, nonlinear or linear, from genotype to phenotype. Together,\nBINNs establish a framework that leverages intermediate domain information to\nimprove genomic prediction accuracy and reveal nonlinear biological\nrelationships that can guide genomic selection, candidate gene selection,\npathway enrichment, and gene-editing prioritization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We extend biologically-informed neural networks (BINNs) for genomic\nprediction (GP) and selection (GS) in crops by integrating thousands of\nsingle-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior\nbiological knowledge. Traditional genotype-to-phenotype (G2P) models depend\nheavily on direct mappings that achieve only modest accuracy, forcing breeders\nto conduct large, costly field trials to maintain or marginally improve genetic\ngain. Models that incorporate intermediate molecular phenotypes such as gene\nexpression can achieve higher predictive fit, but they remain impractical for\nGS since such data are unavailable at deployment or design time. BINNs overcome\nthis limitation by encoding pathway-level inductive biases and leveraging\nmulti-omics data only during training, while using genotype data alone during\ninference. Applied to maize gene-expression and multi-environment field-trial\ndata, BINN improves rank-correlation accuracy by up to 56% within and across\nsubpopulations under sparse-data conditions and nonlinearly identifies genes\nthat GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic\nmetabolomics benchmark, BINN reduces prediction error by 75% relative to\nconventional neural nets and correctly identifies the most important nonlinear\npathway. Importantly, both cases show highly sensitive BINN latent variables\ncorrelate with the experimental quantities they represent, despite not being\ntrained on them. This suggests BINNs learn biologically-relevant\nrepresentations, nonlinear or linear, from genotype to phenotype. Together,\nBINNs establish a framework that leverages intermediate domain information to\nimprove genomic prediction accuracy and reveal nonlinear biological\nrelationships that can guide genomic selection, candidate gene selection,\npathway enrichment, and gene-editing prioritization."
                },
                "authors": [
                    {
                        "name": "Katiana Kontolati"
                    },
                    {
                        "name": "Rini Jasmine Gladstone"
                    },
                    {
                        "name": "Ian Davis"
                    },
                    {
                        "name": "Ethan Pickering"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Pickering"
                },
                "author": "Ethan Pickering",
                "arxiv_comment": "35 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14967v1",
                "updated": "2025-10-16T17:59:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    32,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:32Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    32,
                    3,
                    289,
                    0
                ],
                "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents"
                },
                "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency."
                },
                "authors": [
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Guangze Ye"
                    },
                    {
                        "name": "Zeyu Gan"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhe Ying"
                },
                "author": "Zhenzhe Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14966v1",
                "updated": "2025-10-16T17:59:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    25,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:25Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    25,
                    3,
                    289,
                    0
                ],
                "title": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity\n  in TVD-MI Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity\n  in TVD-MI Scores"
                },
                "summary": "Pairwise comparisons of large language models using total variation distance\nmutual information (TVD-MI) produce binary critic decisions per pair. We show\nthat averaging TVD-MI's binary trials yields centered-probability scores with\nadditive structure suitable for item-response theory (IRT) without nonlinear\nlink functions. Maximum-likelihood approaches to IRT use logistic links, but we\nfind empirically that these transformations introduce curvature that breaks\nadditivity: across three domains, the identity link yields median curl on raw\ndata of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce\nsubstantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We\nderive this clipped-linear model from Gini entropy maximization, yielding a\nbox-constrained least-squares formulation that handles boundary saturation. At\n33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent\nrankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations\nthan full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows\nstrong agreement in agent rankings ($\\rho = 0.872$) and consistent\nidentity-link advantage. TVD-MI's geometry is best preserved by identity\nmapping for efficient LLM evaluation, applicable to other bounded-response\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise comparisons of large language models using total variation distance\nmutual information (TVD-MI) produce binary critic decisions per pair. We show\nthat averaging TVD-MI's binary trials yields centered-probability scores with\nadditive structure suitable for item-response theory (IRT) without nonlinear\nlink functions. Maximum-likelihood approaches to IRT use logistic links, but we\nfind empirically that these transformations introduce curvature that breaks\nadditivity: across three domains, the identity link yields median curl on raw\ndata of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce\nsubstantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We\nderive this clipped-linear model from Gini entropy maximization, yielding a\nbox-constrained least-squares formulation that handles boundary saturation. At\n33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent\nrankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations\nthan full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows\nstrong agreement in agent rankings ($\\rho = 0.872$) and consistent\nidentity-link advantage. TVD-MI's geometry is best preserved by identity\nmapping for efficient LLM evaluation, applicable to other bounded-response\ndomains."
                },
                "authors": [
                    {
                        "name": "Zachary Robertson"
                    }
                ],
                "author_detail": {
                    "name": "Zachary Robertson"
                },
                "author": "Zachary Robertson",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14965v1",
                "updated": "2025-10-16T17:59:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:59:16Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    59,
                    16,
                    3,
                    289,
                    0
                ],
                "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChangingGrounding: 3D Visual Grounding in Changing Scenes"
                },
                "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ ."
                },
                "authors": [
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Zhiwei Huang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Nanning Zheng"
                    },
                    {
                        "name": "Runsen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Runsen Xu"
                },
                "author": "Runsen Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14958v1",
                "updated": "2025-10-16T17:58:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    58,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:58:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    58,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal\n  Mathematical Reasoning"
                },
                "summary": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have excelled in textual reasoning, they\nstruggle with mathematical domains like geometry that intrinsically rely on\nvisual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often\nlimited by rigid external tools or fail to generate the high-fidelity,\nstrategically-timed diagrams necessary for complex problem-solving. To bridge\nthis gap, we introduce MathCanvas, a comprehensive framework designed to endow\nunified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for\nmathematics. Our approach consists of two phases. First, a Visual Manipulation\nstage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M\ncaption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing\ntrajectories (MathCanvas-Edit), to master diagram generation and editing.\nSecond, a Strategic Visual-Aided Reasoning stage fine-tunes the model on\nMathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual\nreasoning paths, teaching it when and how to leverage visual aids. To\nfacilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging\nbenchmark with 3K problems that require models to produce interleaved\nvisual-textual solutions. Our model, BAGEL-Canvas, trained under this\nframework, achieves an 86% relative improvement over strong LMM baselines on\nMathCanvas-Bench, demonstrating excellent generalization to other public math\nbenchmarks. Our work provides a complete toolkit-framework, datasets, and\nbenchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project\nPage: https://mathcanvas.github.io/"
                },
                "authors": [
                    {
                        "name": "Weikang Shi"
                    },
                    {
                        "name": "Aldrich Yu"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Houxing Ren"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Changyao Tian"
                    },
                    {
                        "name": "Xinyu Fu"
                    },
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Zimu Lu"
                    },
                    {
                        "name": "Linjiang Huang"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Hongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongsheng Li"
                },
                "author": "Hongsheng Li",
                "arxiv_comment": "Project Page: https://mathcanvas.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14959v1",
                "updated": "2025-10-16T17:58:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    58,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:58:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    58,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control\n  Barrier Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control\n  Barrier Functions"
                },
                "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter."
                },
                "authors": [
                    {
                        "name": "Lizhi Yang"
                    },
                    {
                        "name": "Blake Werner"
                    },
                    {
                        "name": "Massimiliano de Sa Aaron D. Ames"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano de Sa Aaron D. Ames"
                },
                "author": "Massimiliano de Sa Aaron D. Ames",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14952v1",
                "updated": "2025-10-16T17:57:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    57,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:57:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    57,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Language to Locomotion: Retargeting-free Humanoid Control via\n  Motion Latent Guidance"
                },
                "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems."
                },
                "authors": [
                    {
                        "name": "Zhe Li"
                    },
                    {
                        "name": "Cheng Chi"
                    },
                    {
                        "name": "Yangyang Wei"
                    },
                    {
                        "name": "Boan Zhu"
                    },
                    {
                        "name": "Yibo Peng"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Pengwei Wang"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14946v1",
                "updated": "2025-10-16T17:55:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    56,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:55:56Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    56,
                    3,
                    289,
                    0
                ],
                "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge\n  Devices"
                },
                "summary": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of efficient and accurate Deep Learning models has long been a\nchallenge in autonomous navigation, particularly for real-time applications on\nresource-constrained edge devices. Edge devices are limited in computing power\nand memory, making model efficiency and compression essential. In this work, we\npropose EdgeNavMamba, a reinforcement learning-based framework for\ngoal-directed navigation using an efficient Mamba object detection model. To\ntrain and evaluate the detector, we introduce a custom shape detection dataset\ncollected in diverse indoor settings, reflecting visual cues common in\nreal-world navigation. The object detector serves as a pre-processing module,\nextracting bounding boxes (BBOX) from visual input, which are then passed to an\nRL policy to control goal-oriented navigation. Experimental results show that\nthe student model achieved a reduction of 67% in size, and up to 73% in energy\nper inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5,\nwhile keeping the same performance as the teacher model. EdgeNavMamba also\nmaintains high detection accuracy in MiniWorld and IsaacLab simulators while\nreducing parameters by 31% compared to the baseline. In the MiniWorld\nsimulator, the navigation policy achieves over 90% success across environments\nof varying complexity."
                },
                "authors": [
                    {
                        "name": "Romina Aalishah"
                    },
                    {
                        "name": "Mozhgan Navardi"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "The 11th IEEE International Conference on Edge Computing and Scalable\n  Cloud (IEEE EdgeCom 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14944v1",
                "updated": "2025-10-16T17:55:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    14,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:55:14Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    14,
                    3,
                    289,
                    0
                ],
                "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities on\ngeneral text; however, their proficiency in specialized scientific domains that\nrequire deep, interconnected knowledge remains largely uncharacterized.\nMetabolomics presents unique challenges with its complex biochemical pathways,\nheterogeneous identifier systems, and fragmented databases. To systematically\nevaluate LLM capabilities in this domain, we introduce MetaBench, the first\nbenchmark for metabolomics assessment. Curated from authoritative public\nresources, MetaBench evaluates five capabilities essential for metabolomics\nresearch: knowledge, understanding, grounding, reasoning, and research. Our\nevaluation of 25 open- and closed-source LLMs reveals distinct performance\npatterns across metabolomics tasks: while models perform well on text\ngeneration tasks, cross-database identifier grounding remains challenging even\nwith retrieval augmentation. Model performance also decreases on long-tail\nmetabolites with sparse annotations. With MetaBench, we provide essential\ninfrastructure for developing and evaluating metabolomics AI systems, enabling\nsystematic progress toward reliable computational tools for metabolomics\nresearch."
                },
                "authors": [
                    {
                        "name": "Yuxing Lu"
                    },
                    {
                        "name": "Xukai Zhao"
                    },
                    {
                        "name": "J. Ben Tamo"
                    },
                    {
                        "name": "Micky C. Nnamdi"
                    },
                    {
                        "name": "Rui Peng"
                    },
                    {
                        "name": "Shuang Zeng"
                    },
                    {
                        "name": "Xingyu Hu"
                    },
                    {
                        "name": "Jinzhuo Wang"
                    },
                    {
                        "name": "May D. Wang"
                    }
                ],
                "author_detail": {
                    "name": "May D. Wang"
                },
                "author": "May D. Wang",
                "arxiv_comment": "22 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14943v1",
                "updated": "2025-10-16T17:55:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    11,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:55:11Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    55,
                    11,
                    3,
                    289,
                    0
                ],
                "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na core paradigm for enhancing the reasoning capabilities of Large Language\nModels (LLMs). To address the lack of verification signals at test time, prior\nstudies incorporate the training of model's self-verification capability into\nthe standard RLVR process, thereby unifying reasoning and verification\ncapabilities within a single LLM. However, previous practice requires the LLM\nto sequentially generate solutions and self-verifications using two separate\nprompt templates, which significantly reduces efficiency. In this work, we\ntheoretically reveal that the closed-form solution to the RL objective of\nself-verification can be reduced to a remarkably simple form: the true\nreasoning reward of a solution is equal to its last-token self-rewarding score,\nwhich is computed as the difference between the policy model's next-token\nlog-probability assigned to any pre-specified token at the solution's last\ntoken and a pre-calculated constant, scaled by the KL coefficient. Based on\nthis insight, we propose LaSeR (Reinforcement Learning with Last-Token\nSelf-Rewarding), an algorithm that simply augments the original RLVR loss with\na MSE loss that aligns the last-token self-rewarding scores with verifier-based\nreasoning rewards, jointly optimizing the reasoning and self-rewarding\ncapabilities of LLMs. The optimized self-rewarding scores can be utilized in\nboth training and testing to enhance model performance. Notably, our algorithm\nderives these scores from the predicted next-token probability distribution of\nthe last token immediately after generation, incurring only the minimal extra\ncost of one additional token inference. Experiments show that our method not\nonly improves the model's reasoning performance but also equips it with\nremarkable self-rewarding capability, thereby boosting its inference-time\nscaling performance."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Yiju Guo"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Yankai Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yankai Lin"
                },
                "author": "Yankai Lin",
                "arxiv_comment": "Work in progress. Github repo: https://github.com/RUCBM/LaSeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14942v1",
                "updated": "2025-10-16T17:54:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    54,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:54:07Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    54,
                    7,
                    3,
                    289,
                    0
                ],
                "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for\n  Step-Level Reasoning"
                },
                "summary": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large\nLanguage Models (LLMs) by supervising intermediate steps and identifying\nerrors. However, building effective PRMs remains challenging due to the lack of\nscalable, high-quality annotations. Existing approaches rely on costly human\nlabeling, LLM-based self-evaluation that is prone to hallucination, or Monte\nCarlo (MC) estimation, which infers step quality solely from rollout outcomes\nand often introduces noisy, misaligned supervision due to credit\nmisattribution. These issues result in three core limitations: noisy rewards,\nlow factual fidelity, and misalignment with step-level reasoning objectives. To\naddress these challenges, we introduce GroundedPRM, a tree-guided and\nfidelity-aware framework for automatic process supervision. To reduce reward\nnoise and enable fine-grained credit assignment, we construct structured\nreasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated\nsupervision, we validate each intermediate step using an external tool,\nproviding execution-grounded correctness signals. To combine both step-level\nvalidation and global outcome assessment, we design a hybrid reward aggregation\nmechanism that fuses tool-based verification with MCTS-derived feedback.\nFinally, we format the reward signal into a rationale-enhanced, generative\nstructure to promote interpretability and compatibility with instruction-tuned\nLLMs. GroundedPRM is trained on only 40K automatically labeled samples,\namounting to just 10% of the data used by the best-performing PRM trained with\nauto-labeled supervision. Nevertheless, it achieves up to a 26% relative\nimprovement in average performance on ProcessBench. When used for reward-guided\ngreedy search, GroundedPRM outperforms even PRMs trained with human-labeled\nsupervision, offering a scalable and verifiable path toward high-quality\nprocess-level reasoning."
                },
                "authors": [
                    {
                        "name": "Yao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Weiguo Li"
                    },
                    {
                        "name": "Haokun Chen"
                    },
                    {
                        "name": "Jingpei Wu"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Zhen Han"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14937v1",
                "updated": "2025-10-16T17:50:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    50,
                    4,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:50:04Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    50,
                    4,
                    3,
                    289,
                    0
                ],
                "title": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World\n  Clinical Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Powered Early Diagnosis of Mental Health Disorders from Real-World\n  Clinical Conversations"
                },
                "summary": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders remain among the leading cause of disability\nworldwide, yet conditions such as depression, anxiety, and Post-Traumatic\nStress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to\nsubjective assessments, limited clinical resources, and stigma and low\nawareness. In primary care settings, studies show that providers misidentify\ndepression or anxiety in over 60% of cases, highlighting the urgent need for\nscalable, accessible, and context-aware diagnostic tools that can support early\ndetection and intervention. In this study, we evaluate the effectiveness of\nmachine learning models for mental health screening using a unique dataset of\n553 real-world, semistructured interviews, each paried with ground-truth\ndiagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We\nbenchmark multiple model classes, including zero-shot prompting with GPT-4.1\nMini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank\nAdaptation (LoRA). Our models achieve over 80% accuracy across diagnostic\ncategories, with especially strongperformance on PTSD (up to 89% accuracy and\n98% recall). We also find that using shorter context, focused context segments\nimproves recall, suggesting that focused narrative cues enhance detection\nsensitivity. LoRA fine-tuning proves both efficient and effective, with\nlower-rank configurations (e.g., rank 8 and 16) maintaining competitive\nperformance across evaluation metrics. Our results demonstrate that LLM-based\nmodels can offer substantial improvements over traditional self-report\nscreening tools, providing a path toward low-barrier, AI-powerd early\ndiagnosis. This work lays the groundwork for integrating machine learning into\nreal-world clinical workflows, particularly in low-resource or high-stigma\nenvironments where access to timely mental health care is most limited."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Julina Maharjan"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Karin G. Coifman"
                    },
                    {
                        "name": "Ruoming Jin"
                    }
                ],
                "author_detail": {
                    "name": "Ruoming Jin"
                },
                "author": "Ruoming Jin",
                "arxiv_comment": "7 pages 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14936v1",
                "updated": "2025-10-16T17:49:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    49,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:49:41Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    49,
                    41,
                    3,
                    289,
                    0
                ],
                "title": "Circuit Insights: Towards Interpretability Beyond Activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit Insights: Towards Interpretability Beyond Activations"
                },
                "summary": "The fields of explainable AI and mechanistic interpretability aim to uncover\nthe internal structure of neural networks, with circuit discovery as a central\ntool for understanding model computations. Existing approaches, however, rely\non manual inspection and remain limited to toy tasks. Automated\ninterpretability offers scalability by analyzing isolated features and their\nactivations, but it often misses interactions between features and depends\nstrongly on external LLMs and dataset quality. Transcoders have recently made\nit possible to separate feature attributions into input-dependent and\ninput-invariant components, providing a foundation for more systematic circuit\nanalysis. Building on this, we propose WeightLens and CircuitLens, two\ncomplementary methods that go beyond activation-based analysis. WeightLens\ninterprets features directly from their learned weights, removing the need for\nexplainer models or datasets while matching or exceeding the performance of\nexisting methods on context-independent features. CircuitLens captures how\nfeature activations arise from interactions between components, revealing\ncircuit-level dynamics that activation-only approaches cannot identify.\nTogether, these methods increase interpretability robustness and enhance\nscalable mechanistic analysis of circuits while maintaining efficiency and\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fields of explainable AI and mechanistic interpretability aim to uncover\nthe internal structure of neural networks, with circuit discovery as a central\ntool for understanding model computations. Existing approaches, however, rely\non manual inspection and remain limited to toy tasks. Automated\ninterpretability offers scalability by analyzing isolated features and their\nactivations, but it often misses interactions between features and depends\nstrongly on external LLMs and dataset quality. Transcoders have recently made\nit possible to separate feature attributions into input-dependent and\ninput-invariant components, providing a foundation for more systematic circuit\nanalysis. Building on this, we propose WeightLens and CircuitLens, two\ncomplementary methods that go beyond activation-based analysis. WeightLens\ninterprets features directly from their learned weights, removing the need for\nexplainer models or datasets while matching or exceeding the performance of\nexisting methods on context-independent features. CircuitLens captures how\nfeature activations arise from interactions between components, revealing\ncircuit-level dynamics that activation-only approaches cannot identify.\nTogether, these methods increase interpretability robustness and enhance\nscalable mechanistic analysis of circuits while maintaining efficiency and\nquality."
                },
                "authors": [
                    {
                        "name": "Elena Golimblevskaia"
                    },
                    {
                        "name": "Aakriti Jain"
                    },
                    {
                        "name": "Bruno Puri"
                    },
                    {
                        "name": "Ammar Ibrahim"
                    },
                    {
                        "name": "Wojciech Samek"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Lapuschkin"
                },
                "author": "Sebastian Lapuschkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23339v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23339v2",
                "updated": "2025-10-16T17:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    43,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-06-29T17:17:04Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    17,
                    17,
                    4,
                    6,
                    180,
                    0
                ],
                "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular\n  Design"
                },
                "summary": "Large Language Models demonstrate substantial promise for advancing\nscientific discovery, yet their deployment in disciplines demanding factual\nprecision and specialized domain constraints presents significant challenges.\nWithin molecular design for pharmaceutical development, these models can\npropose innovative molecular modifications but frequently generate chemically\ninfeasible structures. We introduce VALID-Mol, a comprehensive framework that\nintegrates chemical validation with LLM-driven molecular design, achieving an\nimprovement in valid chemical structure generation from 3% to 83%. Our\nmethodology synthesizes systematic prompt optimization, automated chemical\nverification, and domain-adapted fine-tuning to ensure dependable generation of\nsynthesizable molecules with enhanced properties. Our contribution extends\nbeyond implementation details to provide a transferable methodology for\nscientifically-constrained LLM applications with measurable reliability\nenhancements. Computational analyses indicate our framework generates promising\nsynthesis candidates with up to 17-fold predicted improvements in target\nbinding affinity while preserving synthetic feasibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models demonstrate substantial promise for advancing\nscientific discovery, yet their deployment in disciplines demanding factual\nprecision and specialized domain constraints presents significant challenges.\nWithin molecular design for pharmaceutical development, these models can\npropose innovative molecular modifications but frequently generate chemically\ninfeasible structures. We introduce VALID-Mol, a comprehensive framework that\nintegrates chemical validation with LLM-driven molecular design, achieving an\nimprovement in valid chemical structure generation from 3% to 83%. Our\nmethodology synthesizes systematic prompt optimization, automated chemical\nverification, and domain-adapted fine-tuning to ensure dependable generation of\nsynthesizable molecules with enhanced properties. Our contribution extends\nbeyond implementation details to provide a transferable methodology for\nscientifically-constrained LLM applications with measurable reliability\nenhancements. Computational analyses indicate our framework generates promising\nsynthesis candidates with up to 17-fold predicted improvements in target\nbinding affinity while preserving synthetic feasibility."
                },
                "authors": [
                    {
                        "name": "Malikussaid"
                    },
                    {
                        "name": "Hilal Hudan Nuha"
                    },
                    {
                        "name": "Isman Kurniawan"
                    }
                ],
                "author_detail": {
                    "name": "Isman Kurniawan"
                },
                "author": "Isman Kurniawan",
                "arxiv_comment": "6 pages, 1 figure, 1 algorithm, 5 tables, to be published in ISPACS\n  2025, unabridged version exists as arXiv:2506.23339v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23339v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23339v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50 (Primary) 92E10, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; I.2.1; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14925v1",
                "updated": "2025-10-16T17:40:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:40:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    40,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters\n  to Large Language Models"
                },
                "summary": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback\nstability, viewing reason as a regulator that keeps inference within the bounds\nof possible experience. We formalize this intuition via a composite instability\nindex (H-Risk) combining spectral margin, conditioning, temporal sensitivity,\nand innovation amplification. In linear-Gaussian simulations, higher H-Risk\npredicts overconfident errors even under formal stability, revealing a gap\nbetween nominal and epistemic stability. Extending to large language models\n(LLMs), we find that fragile internal dynamics correlate with miscalibration\nand hallucination, while critique-style prompts show mixed effects on\ncalibration and hallucination. These results suggest a structural bridge\nbetween Kantian self-limitation and feedback control, offering a principled\nlens for diagnosing -- and selectively reducing -- overconfidence in reasoning\nsystems. This is a preliminary version; supplementary experiments and broader\nreplication will be reported in a future revision."
                },
                "authors": [
                    {
                        "name": "Akira Okutomi"
                    }
                ],
                "author_detail": {
                    "name": "Akira Okutomi"
                },
                "author": "Akira Okutomi",
                "arxiv_comment": "19 pages, 2 figures, preliminary version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13091v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13091v2",
                "updated": "2025-10-16T17:39:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    39,
                    20,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T02:17:19Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    2,
                    17,
                    19,
                    2,
                    288,
                    0
                ],
                "title": "Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments\n  on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments\n  on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents"
                },
                "summary": "Online freelance marketplaces, a rapidly growing part of the global labor\nmarket, are creating a fair environment where professional skills are the main\nfactor for hiring. While these platforms can reduce bias from traditional\nhiring, the personal information in user profiles raises concerns about ongoing\ndiscrimination. Past studies on this topic have mostly used existing data,\nwhich makes it hard to control for other factors and clearly see the effect of\nthings like gender or race. To solve these problems, this paper presents a new\nmethod that uses Retrieval-Augmented Generation (RAG) with a Large Language\nModel (LLM) to create realistic, artificial freelancer profiles for controlled\nexperiments. This approach effectively separates individual factors, enabling a\nclearer statistical analysis of how different variables influence the\nfreelancer project process. In addition to analyzing extracted data with\ntraditional statistical methods for post-project stage analysis, our research\nutilizes a dataset with highly controlled variables, generated by an RAG-LLM,\nto conduct a simulated hiring experiment for pre-project stage analysis. The\nresults of our experiments show that, regarding gender, while no significant\npreference emerged in initial hiring decisions, female freelancers are\nsubstantially more likely to receive imperfect ratings post-project stage.\nRegarding regional bias, a strong and consistent preference favoring US-based\nfreelancers shows that people are more likely to be selected in the simulated\nexperiments, perceived as more leader-like, and receive higher ratings on the\nlive platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online freelance marketplaces, a rapidly growing part of the global labor\nmarket, are creating a fair environment where professional skills are the main\nfactor for hiring. While these platforms can reduce bias from traditional\nhiring, the personal information in user profiles raises concerns about ongoing\ndiscrimination. Past studies on this topic have mostly used existing data,\nwhich makes it hard to control for other factors and clearly see the effect of\nthings like gender or race. To solve these problems, this paper presents a new\nmethod that uses Retrieval-Augmented Generation (RAG) with a Large Language\nModel (LLM) to create realistic, artificial freelancer profiles for controlled\nexperiments. This approach effectively separates individual factors, enabling a\nclearer statistical analysis of how different variables influence the\nfreelancer project process. In addition to analyzing extracted data with\ntraditional statistical methods for post-project stage analysis, our research\nutilizes a dataset with highly controlled variables, generated by an RAG-LLM,\nto conduct a simulated hiring experiment for pre-project stage analysis. The\nresults of our experiments show that, regarding gender, while no significant\npreference emerged in initial hiring decisions, female freelancers are\nsubstantially more likely to receive imperfect ratings post-project stage.\nRegarding regional bias, a strong and consistent preference favoring US-based\nfreelancers shows that people are more likely to be selected in the simulated\nexperiments, perceived as more leader-like, and receive higher ratings on the\nlive platform."
                },
                "authors": [
                    {
                        "name": "Wugeng Zheng"
                    },
                    {
                        "name": "Guohou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Guohou Shan"
                },
                "author": "Guohou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13091v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13091v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14919v1",
                "updated": "2025-10-16T17:35:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    35,
                    18,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:35:18Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    35,
                    18,
                    3,
                    289,
                    0
                ],
                "title": "Predicting Task Performance with Context-aware Scaling Laws",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting Task Performance with Context-aware Scaling Laws"
                },
                "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling."
                },
                "authors": [
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "David Park"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Michael Bendersky"
                    },
                    {
                        "name": "Beliz Gunel"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Chenguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Wang"
                },
                "author": "Chenguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14915v1",
                "updated": "2025-10-16T17:30:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    30,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:30:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    30,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models\n(LLMs) to generate accurate and reliable responses that are grounded in\nretrieved context. However, LLMs often generate inconsistent outputs for\nsemantically equivalent inputs, a problem compounded by the scarcity of\nconsistency-focused training data and the limitations of current fine-tuning\ntechniques in enhancing output consistency. We propose a new approach combining\nsystematic synthetic data generation, triplet loss for better embeddings, and a\nnovel layer-wise model merging approach. Using consistency-aware weights\nderived from intermediate layer activations, our method effectively integrates\nknowledge from specialized models. Experimental results how that our merged\nmodel significantly enhances output consistency, achieving a ~47.5\\%\nimprovement in response similarity over the baseline, thus offering a practical\nsolution for increasing the reliability of an industrial RAG system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models\n(LLMs) to generate accurate and reliable responses that are grounded in\nretrieved context. However, LLMs often generate inconsistent outputs for\nsemantically equivalent inputs, a problem compounded by the scarcity of\nconsistency-focused training data and the limitations of current fine-tuning\ntechniques in enhancing output consistency. We propose a new approach combining\nsystematic synthetic data generation, triplet loss for better embeddings, and a\nnovel layer-wise model merging approach. Using consistency-aware weights\nderived from intermediate layer activations, our method effectively integrates\nknowledge from specialized models. Experimental results how that our merged\nmodel significantly enhances output consistency, achieving a ~47.5\\%\nimprovement in response similarity over the baseline, thus offering a practical\nsolution for increasing the reliability of an industrial RAG system."
                },
                "authors": [
                    {
                        "name": "Xujun Peng"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Jingyu Wu"
                    },
                    {
                        "name": "Parker Glenn"
                    },
                    {
                        "name": "Daben Liu"
                    }
                ],
                "author_detail": {
                    "name": "Daben Liu"
                },
                "author": "Daben Liu",
                "arxiv_comment": "EMNLP 2025 Industry track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06948v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06948v2",
                "updated": "2025-10-16T17:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    30,
                    21,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-08T17:58:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning"
                },
                "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach suffers from catastrophic forgetting: second-stage\nRL gradually loses SFT-acquired behaviors and inefficiently explores new\npatterns. This study introduces a novel method for learning reasoning models\nthat employs bilevel optimization to facilitate better cooperation between\nthese training paradigms. By conditioning the SFT objective on the optimal RL\npolicy, our approach enables SFT to meta-learn how to guide RL's optimization\nprocess. During training, the lower level performs RL updates while\nsimultaneously receiving SFT supervision, and the upper level explicitly\nmaximizes the cooperative gain-the performance advantage of joint SFT-RL\ntraining over RL alone. Empirical evaluations on five reasoning benchmarks\ndemonstrate that our method consistently outperforms baselines and achieves a\nbetter balance between effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach suffers from catastrophic forgetting: second-stage\nRL gradually loses SFT-acquired behaviors and inefficiently explores new\npatterns. This study introduces a novel method for learning reasoning models\nthat employs bilevel optimization to facilitate better cooperation between\nthese training paradigms. By conditioning the SFT objective on the optimal RL\npolicy, our approach enables SFT to meta-learn how to guide RL's optimization\nprocess. During training, the lower level performs RL updates while\nsimultaneously receiving SFT supervision, and the upper level explicitly\nmaximizes the cooperative gain-the performance advantage of joint SFT-RL\ntraining over RL alone. Empirical evaluations on five reasoning benchmarks\ndemonstrate that our method consistently outperforms baselines and achieves a\nbetter balance between effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Xueting Han"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06948v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06948v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10315v3",
                "updated": "2025-10-16T17:26:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    26,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-11T18:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    18,
                    47,
                    14,
                    5,
                    284,
                    0
                ],
                "title": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Misinformation More Open? A Study of robots.txt Gatekeeping on the\n  Web"
                },
                "summary": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly relying on web crawling to stay\nup to date and accurately answer user queries. These crawlers are expected to\nhonor robots.txt files, which govern automated access. In this study, for the\nfirst time, we investigate whether reputable news websites and misinformation\nsites differ in how they configure these files, particularly in relation to AI\ncrawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of\nreputable sites disallow at least one AI crawler, compared to just 9.1% of\nmisinformation sites in their robots.txt files. Reputable sites forbid an\naverage of 15.5 AI user agents, while misinformation sites prohibit fewer than\none. We then measure active blocking behavior, where websites refuse to return\ncontent when HTTP requests include AI crawler user agents, and reveal that both\ncategories of websites utilize it. Notably, the behavior of reputable news\nwebsites in this regard aligns more closely with their declared robots.txt\ndirective than that of misinformation websites. Finally, our longitudinal\nanalysis reveals that this gap has widened over time, with AI-blocking by\nreputable sites rising from 23% in September 2023 to nearly 60% by May 2025.\nOur findings highlight a growing asymmetry in content accessibility that may\nshape the training data available to LLMs, raising essential questions for web\ntransparency, data ethics, and the future of AI training practices."
                },
                "authors": [
                    {
                        "name": "Nicolas Steinacker-Olsztyn"
                    },
                    {
                        "name": "Devashish Gosain"
                    },
                    {
                        "name": "Ha Dao"
                    }
                ],
                "author_detail": {
                    "name": "Ha Dao"
                },
                "author": "Ha Dao",
                "arxiv_comment": "10 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14901v1",
                "updated": "2025-10-16T17:18:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    18,
                    11,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:18:11Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    18,
                    11,
                    3,
                    289,
                    0
                ],
                "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning with Sampling: Your Base Model is Smarter Than You Think"
                },
                "summary": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier reasoning models have exhibited incredible capabilities across a\nwide array of disciplines, driven by posttraining large language models (LLMs)\nwith reinforcement learning (RL). However, despite the widespread success of\nthis paradigm, much of the literature has been devoted to disentangling truly\nnovel behaviors that emerge during RL but are not present in the base models.\nIn our work, we approach this question from a different angle, instead asking\nwhether comparable reasoning capabilites can be elicited from base models at\ninference time by pure sampling, without any additional training. Inspired by\nMarkov chain Monte Carlo (MCMC) techniques for sampling from sharpened\ndistributions, we propose a simple iterative sampling algorithm leveraging the\nbase models' own likelihoods. Over different base models, we show that our\nalgorithm offers substantial boosts in reasoning that nearly match and even\noutperform those from RL on a wide variety of single-shot tasks, including\nMATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in\ndiversity over multiple samples that is characteristic of RL-posttraining.\nCrucially, our method does not require training, curated datasets, or a\nverifier, suggesting broad applicability beyond easily verifiable domains."
                },
                "authors": [
                    {
                        "name": "Aayush Karan"
                    },
                    {
                        "name": "Yilun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yilun Du"
                },
                "author": "Yilun Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14900v1",
                "updated": "2025-10-16T17:17:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    17,
                    0,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:17:00Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    17,
                    0,
                    3,
                    289,
                    0
                ],
                "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent\n  That Improves Without Labels or Model Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent\n  That Improves Without Labels or Model Updates"
                },
                "summary": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions."
                },
                "authors": [
                    {
                        "name": "Wen-Kwang Tsao"
                    },
                    {
                        "name": "Yao-Ching Yu"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14896v1",
                "updated": "2025-10-16T17:13:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    13,
                    33,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:13:33Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    13,
                    33,
                    3,
                    289,
                    0
                ],
                "title": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Multimodal LLM Descriptions of Activity for Explainable\n  Semi-Supervised Video Anomaly Detection"
                },
                "summary": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing semi-supervised video anomaly detection (VAD) methods often struggle\nwith detecting complex anomalies involving object interactions and generally\nlack explainability. To overcome these limitations, we propose a novel VAD\nframework leveraging Multimodal Large Language Models (MLLMs). Unlike previous\nMLLM-based approaches that make direct anomaly judgments at the frame level,\nour method focuses on extracting and interpreting object activity and\ninteractions over time. By querying an MLLM with visual inputs of object pairs\nat different moments, we generate textual descriptions of the activity and\ninteractions from nominal videos. These textual descriptions serve as a\nhigh-level representation of the activity and interactions of objects in a\nvideo. They are used to detect anomalies during test time by comparing them to\ntextual descriptions found in nominal training videos. Our approach inherently\nprovides explainability and can be combined with many traditional VAD methods\nto further enhance their interpretability. Extensive experiments on benchmark\ndatasets demonstrate that our method not only detects complex interaction-based\nanomalies effectively but also achieves state-of-the-art performance on\ndatasets without interaction anomalies."
                },
                "authors": [
                    {
                        "name": "Furkan Mumcu"
                    },
                    {
                        "name": "Michael J. Jones"
                    },
                    {
                        "name": "Anoop Cherian"
                    },
                    {
                        "name": "Yasin Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Yasin Yilmaz"
                },
                "author": "Yasin Yilmaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14885v1",
                "updated": "2025-10-16T17:04:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    4,
                    25,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:04:25Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    4,
                    25,
                    3,
                    289,
                    0
                ],
                "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition\n  Capabilities of Multimodal Large Language Models with Answer Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You May Speak Freely: Improving the Fine-Grained Visual Recognition\n  Capabilities of Multimodal Large Language Models with Answer Extraction"
                },
                "summary": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the renewed interest in zero-shot visual classification due to the\nrise of Multimodal Large Language Models (MLLMs), the problem of evaluating\nfree-form responses of auto-regressive models remains a persistent challenge.\nMost existing works focus on language-only tasks or don't consider Multiple\nChoice Questions (MCQs) beyond 5-way options, both of which are critical\ncapabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where\nchoice counts are in the hundreds to thousands and the choices are highly\nrelated. Furthermore, in this highly multi-way MCQ setting it is not clear how\nto extend LLM choice extraction to retrieval-based problems, where computing\nprobabilities over the choice set is computationally costly. In this work we\ninvestigate nlg2choice, a simple two-stage method which first asks the MLLM an\nopen-ended question for the task with minimal constraints, then uses text-only\nconstrained decoding to predict the most likely choice. In retrieval settings,\nwe compute the probability of the constrained response taking that choice with\nan early stopping method to significantly improve throughput. Our results show\nimprovement over a suite of seven fine-grained visual datasets when evaluating\nin terms of classification and retrieval, and show that this performance holds\nover the various ways that users of LLMs can implement tasks in natural\nlanguage."
                },
                "authors": [
                    {
                        "name": "Logan Lawrence"
                    },
                    {
                        "name": "Oindrila Saha"
                    },
                    {
                        "name": "Megan Wei"
                    },
                    {
                        "name": "Chen Sun"
                    },
                    {
                        "name": "Subhransu Maji"
                    },
                    {
                        "name": "Grant Van Horn"
                    }
                ],
                "author_detail": {
                    "name": "Grant Van Horn"
                },
                "author": "Grant Van Horn",
                "arxiv_comment": "Accepted to WACV26. 12 pages, 8 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14881v1",
                "updated": "2025-10-16T17:00:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T17:00:42Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    17,
                    0,
                    42,
                    3,
                    289,
                    0
                ],
                "title": "The Gatekeeper Knows Enough",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Gatekeeper Knows Enough"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain."
                },
                "authors": [
                    {
                        "name": "Fikresilase Wondmeneh Abebayew"
                    }
                ],
                "author_detail": {
                    "name": "Fikresilase Wondmeneh Abebayew"
                },
                "author": "Fikresilase Wondmeneh Abebayew",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14876v1",
                "updated": "2025-10-16T16:55:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    55,
                    30,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:55:30Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    55,
                    30,
                    3,
                    289,
                    0
                ],
                "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data"
                },
                "summary": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research."
                },
                "authors": [
                    {
                        "name": "Roni Goldshmidt"
                    },
                    {
                        "name": "Hamish Scott"
                    },
                    {
                        "name": "Lorenzo Niccolini"
                    },
                    {
                        "name": "Shizhan Zhu"
                    },
                    {
                        "name": "Daniel Moura"
                    },
                    {
                        "name": "Orly Zvitia"
                    }
                ],
                "author_detail": {
                    "name": "Orly Zvitia"
                },
                "author": "Orly Zvitia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12748v2",
                "updated": "2025-10-16T16:53:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    53,
                    39,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-16T07:08:11Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    8,
                    11,
                    1,
                    259,
                    0
                ],
                "title": "NEFT: A Unified Transformer Framework for Efficient Near-Field CSI\n  Feedback in XL-MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEFT: A Unified Transformer Framework for Efficient Near-Field CSI\n  Feedback in XL-MIMO Systems"
                },
                "summary": "Extremely large-scale multiple-input multiple-output (XL-MIMO) systems,\noperating in the near-field region due to their massive antenna arrays, are key\nenablers of next-generation wireless communications but face significant\nchallenges in channel state information (CSI) feedback. Deep learning has\nemerged as a powerful tool by learning compact CSI representations for\nfeedback. However, existing methods struggle to capture the intricate structure\nof near-field CSI and incur prohibitive computational overhead on practical\nmobile devices.\n  To overcome these limitations, we propose the Near-Field Efficient Feedback\nTransformer (NEFT) family for accurate and efficient near-field CSI feedback\nacross diverse hardware platforms. Built on a hierarchical Vision Transformer\nbackbone, NEFT is extended with lightweight variants to meet various deployment\nconstraints: NEFT-Compact applies multi-level knowledge distillation (KD) to\nreduce complexity while maintaining accuracy, whereas NEFT-Hybrid and NEFT-Edge\naddress encoder- and edge-constrained scenarios via attention-free encoding and\nKD.\n  Extensive simulations show that NEFT achieves a 15--21 dB improvement in\nnormalized mean-squared error (NMSE) over state-of-the-art methods, while\nNEFT-Compact and NEFT-Edge reduce total FLOPs by 25--36% with negligible\naccuracy loss. Moreover, NEFT-Hybrid reduces encoder-side complexity by up to\n64%, enabling deployment in highly asymmetric device scenarios. These results\nestablish NEFT as a practical and scalable solution for near-field CSI feedback\nin XL-MIMO systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely large-scale multiple-input multiple-output (XL-MIMO) systems,\noperating in the near-field region due to their massive antenna arrays, are key\nenablers of next-generation wireless communications but face significant\nchallenges in channel state information (CSI) feedback. Deep learning has\nemerged as a powerful tool by learning compact CSI representations for\nfeedback. However, existing methods struggle to capture the intricate structure\nof near-field CSI and incur prohibitive computational overhead on practical\nmobile devices.\n  To overcome these limitations, we propose the Near-Field Efficient Feedback\nTransformer (NEFT) family for accurate and efficient near-field CSI feedback\nacross diverse hardware platforms. Built on a hierarchical Vision Transformer\nbackbone, NEFT is extended with lightweight variants to meet various deployment\nconstraints: NEFT-Compact applies multi-level knowledge distillation (KD) to\nreduce complexity while maintaining accuracy, whereas NEFT-Hybrid and NEFT-Edge\naddress encoder- and edge-constrained scenarios via attention-free encoding and\nKD.\n  Extensive simulations show that NEFT achieves a 15--21 dB improvement in\nnormalized mean-squared error (NMSE) over state-of-the-art methods, while\nNEFT-Compact and NEFT-Edge reduce total FLOPs by 25--36% with negligible\naccuracy loss. Moreover, NEFT-Hybrid reduces encoder-side complexity by up to\n64%, enabling deployment in highly asymmetric device scenarios. These results\nestablish NEFT as a practical and scalable solution for near-field CSI feedback\nin XL-MIMO systems."
                },
                "authors": [
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Tianqi Mao"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Ruiqi Liu"
                    },
                    {
                        "name": "Shunyu Li"
                    },
                    {
                        "name": "Zhaocheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaocheng Wang"
                },
                "author": "Zhaocheng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18355v2",
                "updated": "2025-10-16T16:44:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    44,
                    51,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-22T19:31:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    31,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Chiplet-Based RISC-V SoC with Modular AI Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chiplet-Based RISC-V SoC with Modular AI Acceleration"
                },
                "summary": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving high performance, energy efficiency, and cost-effectiveness while\nmaintaining architectural flexibility is a critical challenge in the\ndevelopment and deployment of edge AI devices. Monolithic SoC designs struggle\nwith this complex balance mainly due to low manufacturing yields (below 16%) at\nadvanced 360 mm^2 process nodes. This paper presents a novel chiplet-based\nRISC-V SoC architecture that addresses these limitations through modular AI\nacceleration and intelligent system level optimization. Our proposed design\nintegrates 4 different key innovations in a 30mm x 30mm silicon interposer:\nadaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware\nUniversal Chiplet Interconnect Express (UCIe) protocol extensions featuring\nstreaming flow control units and compression-aware transfers; distributed\ncryptographic security across heterogeneous chiplets; and intelligent\nsensor-driven load migration. The proposed architecture integrates a 7nm RISC-V\nCPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory\nstacks, and dedicated power management controllers. Experimental results across\nindustry standard benchmarks like MobileNetV2, ResNet-50 and real-time video\nprocessing demonstrate significant performance improvements. The AI-optimized\nconfiguration achieves ~14.7% latency reduction, 17.3% throughput improvement,\nand 16.2% power reduction compared to previous basic chiplet implementations.\nThese improvements collectively translate to a 40.1% efficiency gain\ncorresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while\nmaintaining sub-5ms real-time capability across all experimented workloads.\nThese performance upgrades demonstrate that modular chiplet designs can achieve\nnear-monolithic computational density while enabling cost efficiency,\nscalability and upgradeability, crucial for next-generation edge AI device\napplications."
                },
                "authors": [
                    {
                        "name": "P. Ramkumar"
                    },
                    {
                        "name": "S. S. Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "S. S. Bharadwaj"
                },
                "author": "S. S. Bharadwaj",
                "arxiv_comment": "3 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13450v2",
                "updated": "2025-10-16T16:44:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    44,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-16T18:36:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    18,
                    36,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "SteeringSafety: A Systematic Safety Evaluation Framework of\n  Representation Steering in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SteeringSafety: A Systematic Safety Evaluation Framework of\n  Representation Steering in LLMs"
                },
                "summary": "We introduce SteeringSafety, a systematic framework for evaluating\nrepresentation steering methods across seven safety perspectives spanning 17\ndatasets. While prior work highlights general capabilities of representation\nsteering, we systematically explore safety perspectives including bias,\nharmfulness, hallucination, social behaviors, reasoning, epistemic integrity,\nand normative judgment. Our framework provides modularized building blocks for\nstate-of-the-art steering methods, enabling unified implementation of DIM, ACE,\nCAA, PCA, and LAT with recent enhancements like conditional steering. Results\non Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering\nperformance depends critically on pairing of method, model, and specific\nperspective. DIM shows consistent effectiveness, but all methods exhibit\nsubstantial entanglement: social behaviors show highest vulnerability (reaching\ndegradation as high as 76%), jailbreaking often compromises normative judgment,\nand hallucination steering unpredictably shifts political views. Our findings\nunderscore the critical need for holistic safety evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SteeringSafety, a systematic framework for evaluating\nrepresentation steering methods across seven safety perspectives spanning 17\ndatasets. While prior work highlights general capabilities of representation\nsteering, we systematically explore safety perspectives including bias,\nharmfulness, hallucination, social behaviors, reasoning, epistemic integrity,\nand normative judgment. Our framework provides modularized building blocks for\nstate-of-the-art steering methods, enabling unified implementation of DIM, ACE,\nCAA, PCA, and LAT with recent enhancements like conditional steering. Results\non Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering\nperformance depends critically on pairing of method, model, and specific\nperspective. DIM shows consistent effectiveness, but all methods exhibit\nsubstantial entanglement: social behaviors show highest vulnerability (reaching\ndegradation as high as 76%), jailbreaking often compromises normative judgment,\nand hallucination steering unpredictably shifts political views. Our findings\nunderscore the critical need for holistic safety evaluations."
                },
                "authors": [
                    {
                        "name": "Vincent Siu"
                    },
                    {
                        "name": "Nicholas Crispino"
                    },
                    {
                        "name": "David Park"
                    },
                    {
                        "name": "Nathan W. Henry"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Chenguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Wang"
                },
                "author": "Chenguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03567v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03567v3",
                "updated": "2025-10-16T16:42:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    42,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-03T23:32:21Z",
                "published_parsed": [
                    2025,
                    10,
                    3,
                    23,
                    32,
                    21,
                    4,
                    276,
                    0
                ],
                "title": "Machine Unlearning Meets Adversarial Robustness via Constrained\n  Interventions on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Unlearning Meets Adversarial Robustness via Constrained\n  Interventions on LLMs"
                },
                "summary": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of Large Language Models (LLMs), more\ncustomization is needed to ensure privacy-preserving and safe generation. We\naddress this objective from two critical aspects: unlearning of sensitive\ninformation and robustness to jail-breaking attacks. We investigate various\nconstrained optimization formulations that address both aspects in a\n\\emph{unified manner}, by finding the smallest possible interventions on LLM\nweights that either make a given vocabulary set unreachable or embed the LLM\nwith robustness to tailored attacks by shifting part of the weights to a\n\\emph{safer} region. Beyond unifying two key properties, this approach\ncontrasts with previous work in that it doesn't require an oracle classifier\nthat is typically not available or represents a computational overhead.\nSurprisingly, we find that the simplest point-wise constraint-based\nintervention we propose leads to better performance than max-min interventions,\nwhile having a lower computational cost. Comparison against state-of-the-art\ndefense methods demonstrates superior performance of the proposed approach."
                },
                "authors": [
                    {
                        "name": "Fatmazohra Rezkellah"
                    },
                    {
                        "name": "Ramzi Dakhmouche"
                    }
                ],
                "author_detail": {
                    "name": "Ramzi Dakhmouche"
                },
                "author": "Ramzi Dakhmouche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03567v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03567v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12052v2",
                "updated": "2025-10-16T16:37:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    37,
                    59,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-15T15:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    15,
                    34,
                    2,
                    0,
                    258,
                    0
                ],
                "title": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided\n  Autoregressive Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided\n  Autoregressive Perspective"
                },
                "summary": "Talking-head animation focuses on generating realistic facial videos from\naudio input. Following Generative Adversarial Networks (GANs), diffusion models\nhave become the mainstream, owing to their robust generative capacities.\nHowever, inherent limitations of the diffusion process often lead to\ninter-frame flicker and slow inference, restricting their practical deployment.\nTo address this, we introduce AvatarSync, an autoregressive framework on\nphoneme representations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly by text or audio\ninput. To mitigate flicker and ensure continuity, AvatarSync leverages an\nautoregressive pipeline that enhances temporal modeling. In addition, to ensure\ncontrollability, we introduce phonemes, which are the basic units of speech\nsounds, and construct a many-to-one mapping from text/audio to phonemes,\nenabling precise phoneme-to-visual alignment. Additionally, to further\naccelerate inference, we adopt a two-stage generation strategy that decouples\nsemantic modeling from visual dynamics, and incorporate a customized\nPhoneme-Frame Causal Attention Mask to support multi-step parallel\nacceleration. Extensive experiments conducted on both Chinese (CMLR) and\nEnglish (HDTF) datasets demonstrate that AvatarSync outperforms existing\ntalking-head animation methods in visual fidelity, temporal consistency, and\ncomputational efficiency, providing a scalable and controllable solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talking-head animation focuses on generating realistic facial videos from\naudio input. Following Generative Adversarial Networks (GANs), diffusion models\nhave become the mainstream, owing to their robust generative capacities.\nHowever, inherent limitations of the diffusion process often lead to\ninter-frame flicker and slow inference, restricting their practical deployment.\nTo address this, we introduce AvatarSync, an autoregressive framework on\nphoneme representations that generates realistic and controllable talking-head\nanimations from a single reference image, driven directly by text or audio\ninput. To mitigate flicker and ensure continuity, AvatarSync leverages an\nautoregressive pipeline that enhances temporal modeling. In addition, to ensure\ncontrollability, we introduce phonemes, which are the basic units of speech\nsounds, and construct a many-to-one mapping from text/audio to phonemes,\nenabling precise phoneme-to-visual alignment. Additionally, to further\naccelerate inference, we adopt a two-stage generation strategy that decouples\nsemantic modeling from visual dynamics, and incorporate a customized\nPhoneme-Frame Causal Attention Mask to support multi-step parallel\nacceleration. Extensive experiments conducted on both Chinese (CMLR) and\nEnglish (HDTF) datasets demonstrate that AvatarSync outperforms existing\ntalking-head animation methods in visual fidelity, temporal consistency, and\ncomputational efficiency, providing a scalable and controllable solution."
                },
                "authors": [
                    {
                        "name": "Yuchen Deng"
                    },
                    {
                        "name": "Xiuyang Wu"
                    },
                    {
                        "name": "Hai-Tao Zheng"
                    },
                    {
                        "name": "Suiyang Zhang"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14853v1",
                "updated": "2025-10-16T16:24:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    24,
                    36,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:24:36Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    24,
                    36,
                    3,
                    289,
                    0
                ],
                "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online\n  Adaptation in Mixture-of-Expert models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online\n  Adaptation in Mixture-of-Expert models"
                },
                "summary": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose \\textit{a data-free, online test-time framework} that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse\nexpert activation, but often suffer from suboptimal routing decisions due to\ndistribution shifts in deployment. While existing test-time adaptation methods\ncould potentially address these issues, they primarily focus on dense models\nand require access to external data, limiting their practical applicability to\nMoE architectures. However, we find that, instead of relying on reference data,\nwe can optimize MoE expert selection on-the-fly based only on input context. As\nsuch, we propose \\textit{a data-free, online test-time framework} that\ncontinuously adapts MoE routing decisions during text generation without\nexternal supervision or data. Our method cycles between two phases: During the\nprefill stage, and later in regular intervals, we optimize the routing\ndecisions of the model using self-supervision based on the already generated\nsequence. Then, we generate text as normal, maintaining the modified router\nuntil the next adaption. We implement this through lightweight additive vectors\nthat only update router logits in selected layers, maintaining computational\nefficiency while preventing over-adaptation. The experimental results show\nconsistent performance gains on challenging reasoning tasks while maintaining\nrobustness to context shifts. For example, our method achieves a 5.5\\%\nimprovement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play\nproperty, our method naturally complements existing test-time scaling\ntechniques, e.g., achieving 6\\% average gains when incorporated with\nself-consistency on DeepSeek-V2-Lite."
                },
                "authors": [
                    {
                        "name": "Guinan Su"
                    },
                    {
                        "name": "Yanwu Yang"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Jonas Geiping"
                    }
                ],
                "author_detail": {
                    "name": "Jonas Geiping"
                },
                "author": "Jonas Geiping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21097v2",
                "updated": "2025-10-16T16:20:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    20,
                    17,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-27T12:22:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    12,
                    22,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "Thinker: Learning to Think Fast and Slow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinker: Learning to Think Fast and Slow"
                },
                "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training. Additionally, we have open-sourced both the trained models\nand the source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training. Additionally, we have open-sourced both the trained models\nand the source code."
                },
                "authors": [
                    {
                        "name": "Stephen Chung"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14846v1",
                "updated": "2025-10-16T16:18:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    18,
                    37,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:18:37Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    18,
                    37,
                    3,
                    289,
                    0
                ],
                "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents"
                },
                "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs."
                },
                "authors": [
                    {
                        "name": "Zhuo-Yang Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhuo-Yang Song"
                },
                "author": "Zhuo-Yang Song",
                "arxiv_comment": "10 pages, 2 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14842v1",
                "updated": "2025-10-16T16:15:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    15,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:15:58Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    15,
                    58,
                    3,
                    289,
                    0
                ],
                "title": "Boosting Instruction Following at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Instruction Following at Scale"
                },
                "summary": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A typical approach developers follow to influence an LLM's behavior in an\napplication is through careful manipulation of the prompt, such as by adding or\nmodifying instructions. However, merely adding more instructions provides\nlittle assurance that they will actually be followed. We introduce Instruction\nBoosting as a post-generation method to increase the reliability of LLM prompt\ninstructions. We show that Instruction Boosting improves the instruction\nfollowing rate by up to 7 points for two instructions and up to 4 points for\nten instructions. To demonstrate these results we introduce SCALEDIF, a\nbenchmark with a scaled instruction volume of up to ten instructions per data\nsample. We also present an analysis of the commonly observed trend that\nperformance degrades as more instructions are added. We show that an important\nfactor contributing to this trend is the degree of tension and conflict that\narises as the number of instructions is increased. We contribute a quantitative\nconflict scoring tool that explains the observed performance trends and\nprovides feedback to developers on the impact that additional prompt\ninstructions have on a model's performance."
                },
                "authors": [
                    {
                        "name": "Ben Elder"
                    },
                    {
                        "name": "Evelyn Duesterwald"
                    },
                    {
                        "name": "Vinod Muthusamy"
                    }
                ],
                "author_detail": {
                    "name": "Vinod Muthusamy"
                },
                "author": "Vinod Muthusamy",
                "arxiv_comment": "6+4 pages, 7 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14832v1",
                "updated": "2025-10-16T16:08:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    8,
                    14,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:08:14Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    8,
                    14,
                    3,
                    289,
                    0
                ],
                "title": "Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction\n  in 6G Multi-RAT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction\n  in 6G Multi-RAT Networks"
                },
                "summary": "The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)\nnetworks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,\nrequires mobility decisions that remain reliable under fast channel dynamics,\ninterference, and heterogeneous coverage. Handover in multi-RAT deployments is\nstill highly reactive and event-triggered, relying on instantaneous\nmeasurements and threshold events. This work proposes a Machine Learning\n(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a\nmodel-driven and short-horizon signal quality forecasts. We present a\ngeneralized P-CHO sequence workflow orchestrated by a RAT Steering Controller,\nwhich standardizes data collection, parallel per-RAT predictions, decision\nlogic with hysteresis-based conditions, and CHO execution. Considering a\nrealistic multi-RAT environment, we train RAT-aware Long Short Term Memory\n(LSTM) networks to forecast the signal quality indicators of mobile users along\nrandomized trajectories. The proposed P-CHO models are trained and evaluated\nunder different channel models for cellular and IEEE 802.11 WiFi integrated\ncoverage. We study the impact of hyperparameter tuning of LSTM models under\ndifferent system settings, and compare direct multi-step versus recursive P-CHO\nvariants. Comparisons against baseline predictors are also carried out.\nFinally, the proposed P-CHO is tested under soft and hard handover settings,\nshowing that hysteresis-enabled P-CHO scheme is able to reduce handover\nfailures and ping-pong events. Overall, the proposed P-CHO framework can enable\naccurate, low-latency, and proactive handovers suitable for ML-assisted\nhandover steering in 6G multi-RAT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)\nnetworks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,\nrequires mobility decisions that remain reliable under fast channel dynamics,\ninterference, and heterogeneous coverage. Handover in multi-RAT deployments is\nstill highly reactive and event-triggered, relying on instantaneous\nmeasurements and threshold events. This work proposes a Machine Learning\n(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a\nmodel-driven and short-horizon signal quality forecasts. We present a\ngeneralized P-CHO sequence workflow orchestrated by a RAT Steering Controller,\nwhich standardizes data collection, parallel per-RAT predictions, decision\nlogic with hysteresis-based conditions, and CHO execution. Considering a\nrealistic multi-RAT environment, we train RAT-aware Long Short Term Memory\n(LSTM) networks to forecast the signal quality indicators of mobile users along\nrandomized trajectories. The proposed P-CHO models are trained and evaluated\nunder different channel models for cellular and IEEE 802.11 WiFi integrated\ncoverage. We study the impact of hyperparameter tuning of LSTM models under\ndifferent system settings, and compare direct multi-step versus recursive P-CHO\nvariants. Comparisons against baseline predictors are also carried out.\nFinally, the proposed P-CHO is tested under soft and hard handover settings,\nshowing that hysteresis-enabled P-CHO scheme is able to reduce handover\nfailures and ping-pong events. Overall, the proposed P-CHO framework can enable\naccurate, low-latency, and proactive handovers suitable for ML-assisted\nhandover steering in 6G multi-RAT deployments."
                },
                "authors": [
                    {
                        "name": "Maria Lamprini A. Bartsioka"
                    },
                    {
                        "name": "Anastasios Giannopoulos"
                    },
                    {
                        "name": "Sotirios Spantideas"
                    }
                ],
                "author_detail": {
                    "name": "Sotirios Spantideas"
                },
                "author": "Sotirios Spantideas",
                "arxiv_comment": "9 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14825v1",
                "updated": "2025-10-16T16:02:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:02:42Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    42,
                    3,
                    289,
                    0
                ],
                "title": "Programmatic Representation Learning with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programmatic Representation Learning with Language Models"
                },
                "summary": "Classical models for supervised machine learning, such as decision trees, are\nefficient and interpretable predictors, but their quality is highly dependent\non the particular choice of input features. Although neural networks can learn\nuseful representations directly from raw data (e.g., images or text), this\ncomes at the expense of interpretability and the need for specialized hardware\nto run them efficiently. In this paper, we explore a hypothesis class we call\nLearned Programmatic Representations (LeaPR) models, which stack arbitrary\nfeatures represented as code (functions from data points to scalars) and\ndecision tree predictors. We synthesize feature functions using Large Language\nModels (LLMs), which have rich prior knowledge in a wide range of domains and a\nremarkable ability to write code using existing domain-specific libraries. We\npropose two algorithms to learn LeaPR models from supervised data. First, we\ndesign an adaptation of FunSearch to learn features rather than directly\ngenerate predictors. Then, we develop a novel variant of the classical ID3\nalgorithm for decision tree learning, where new features are generated on\ndemand when splitting leaf nodes. In experiments from chess position evaluation\nto image and text classification, our methods learn high-quality, neural\nnetwork-free predictors often competitive with neural networks. Our work\nsuggests a flexible paradigm for learning interpretable representations\nend-to-end where features and predictions can be readily inspected and\nunderstood.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical models for supervised machine learning, such as decision trees, are\nefficient and interpretable predictors, but their quality is highly dependent\non the particular choice of input features. Although neural networks can learn\nuseful representations directly from raw data (e.g., images or text), this\ncomes at the expense of interpretability and the need for specialized hardware\nto run them efficiently. In this paper, we explore a hypothesis class we call\nLearned Programmatic Representations (LeaPR) models, which stack arbitrary\nfeatures represented as code (functions from data points to scalars) and\ndecision tree predictors. We synthesize feature functions using Large Language\nModels (LLMs), which have rich prior knowledge in a wide range of domains and a\nremarkable ability to write code using existing domain-specific libraries. We\npropose two algorithms to learn LeaPR models from supervised data. First, we\ndesign an adaptation of FunSearch to learn features rather than directly\ngenerate predictors. Then, we develop a novel variant of the classical ID3\nalgorithm for decision tree learning, where new features are generated on\ndemand when splitting leaf nodes. In experiments from chess position evaluation\nto image and text classification, our methods learn high-quality, neural\nnetwork-free predictors often competitive with neural networks. Our work\nsuggests a flexible paradigm for learning interpretable representations\nend-to-end where features and predictions can be readily inspected and\nunderstood."
                },
                "authors": [
                    {
                        "name": "Gabriel Poesia"
                    },
                    {
                        "name": "Georgia Gabriela Sampaio"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Gabriela Sampaio"
                },
                "author": "Georgia Gabriela Sampaio",
                "arxiv_comment": "Code available at https://github.com/gpoesia/leapr/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14824v1",
                "updated": "2025-10-16T16:02:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    27,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T16:02:27Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    16,
                    2,
                    27,
                    3,
                    289,
                    0
                ],
                "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning or Contrastive Learning? Towards Better\n  Multimodal LLM Reranking"
                },
                "summary": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In information retrieval, training reranking models mainly focuses on two\ntypes of objectives: metric learning (e.g. contrastive loss to increase the\npredicted scores on relevant query-document pairs) and classification (binary\nlabel prediction of relevance vs. irrelevance). For BERT-style encoders,\nvarious studies have shown that contrastive learning (CL) can be more effective\nthan discriminative (classification) learning. However, for large language\nmodels (LLMs), classification via supervised fine-tuning (SFT), which predicts\n''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears\nmore promising as it aligns well with the generative nature of LLMs. This\ndivergence raises a central question: which objective is intrinsically better\nsuited to LLM-based reranking, and what mechanism underlies the difference? In\nthis work, we conduct a comprehensive comparison and analysis between CL and\nSFT for reranking, taking the universal multimodal retrieval (UMR) as the\nexperimental playground. We first decompose the objectives into two components:\nweight, which controls the magnitude of those updates, and direction, which\nguides the model updates, then present a unified framework for understanding\ntheir interactions. Through probing experiments, we find that SFT provides a\nsubstantially stronger weighting scheme than CL, whereas the preferred scoring\ndirection shows no clear winner. Taken together, these results point to a\nconsistent advantage of SFT over CL for LLM reranking. To further validate our\nfindings, we conduct large-scale training with SFT and present new\nstate-of-the-art rerankers on the MRB benchmark. We also provide ablations on\nSFT settings and expect our findings to benefit future research and\napplications in this area."
                },
                "authors": [
                    {
                        "name": "Ziqi Dai"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Mingxin Li"
                    },
                    {
                        "name": "Yanzhao Zhang"
                    },
                    {
                        "name": "Dingkun Long"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Meishan Zhang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14808v1",
                "updated": "2025-10-16T15:42:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    42,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:42:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    42,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Agentic NL2SQL to Reduce Computational Costs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic NL2SQL to Reduce Computational Costs"
                },
                "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Dominik Jehle"
                    },
                    {
                        "name": "Lennart Purucker"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "Accepted at the NeurIPS 2025 Workshop on Efficient Reasoning. 10\n  pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14807v1",
                "updated": "2025-10-16T15:40:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    40,
                    49,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:40:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    40,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "SimKO: Simple Pass@K Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimKO: Simple Pass@K Policy Optimization"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has advanced the\nreasoning capabilities of large language models (LLMs). However, prevailing\nRLVR methods exhibit a systematic bias toward exploitation over exploration, as\nevidenced by improved pass@1 but reduced pass@K (K>1) performance. To\nunderstand this issue, we analyze training dynamics of RLVR methods by tracking\nthe token-level probability distributions over vocabulary candidates. Our\nanalysis reveals a consistent probability concentration effect where the top-1\ncandidate increasingly accumulates probability mass and suppresses that of\nother candidates. More importantly, stronger over-concentration correlates with\nworse pass@K performance. Inspired by this finding, we propose Simple Pass@K\nOptimization (SimKO), a method designed to mitigate the over-concentration\nissue, thereby encouraging exploration. SimKO operates in an asymmetrical\nmanner. For verified-correct responses, it boosts the probabilities of the\ntop-K candidates. For verified-incorrect responses, it applies stronger\npenalties to the top-1 candidate. We observe that this asymmetric design is\nparticularly effective at mitigating over-concentration when applied at tokens\nwith high entropy. Across various math and logical-reasoning benchmarks, SimKO\nconsistently yields higher pass@K for a wide range of K, providing a simple way\nto improve RLVR's exploration."
                },
                "authors": [
                    {
                        "name": "Ruotian Peng"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Yandong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yandong Wen"
                },
                "author": "Yandong Wen",
                "arxiv_comment": "Technical report (20 pages, 10 figures, project page:\n  https://spherelab.ai/simko/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12749v2",
                "updated": "2025-10-16T15:34:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    34,
                    29,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-18T09:20:02Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    20,
                    2,
                    0,
                    230,
                    0
                ],
                "title": "Deep Anomaly Detection for Active Attacks on the Receiver in Quantum Key\n  Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Anomaly Detection for Active Attacks on the Receiver in Quantum Key\n  Distribution"
                },
                "summary": "Traditional countermeasures against attacks targeting the receiver in quantum\nkey distribution (QKD) systems often suffer from poor compatibility with\ndeployed infrastructure, the risk of introducing new vulnerabilities, and\nlimited applicability to specific types of active attacks. In this work, we\npropose an anomaly detection (AD) model based on one-class machine learning to\naddress active attacks targeting the receiver. By constructing a dataset from\nthe QKD system's operational states, the AD model learns the characteristics of\nnormal behavior under secure conditions. When an active attack occurs, the\nsystem's state deviates from the learned normal patterns and is identified as\nanomalous by the model. Experimental results show that the AD model achieves an\narea under the curve (AUC) exceeding 99%, effectively safeguarding the receiver\nof the QKD system. Compared to traditional approaches, our model can be\ndeployed with minimal cost in existing QKD networks without requiring\nadditional optical or electrical components, thus avoiding the introduction of\nnew side channels. Furthermore, unlike multi-class machine learning algorithms,\nour approach does not rely on prior knowledge of specific attack types and is\npotentially able to detect unknown active attacks. These advantages-generality,\nease of deployment, low cost, and high accuracy-make our model a practical and\neffective tool for protecting the receiver of QKD systems against active\nattacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional countermeasures against attacks targeting the receiver in quantum\nkey distribution (QKD) systems often suffer from poor compatibility with\ndeployed infrastructure, the risk of introducing new vulnerabilities, and\nlimited applicability to specific types of active attacks. In this work, we\npropose an anomaly detection (AD) model based on one-class machine learning to\naddress active attacks targeting the receiver. By constructing a dataset from\nthe QKD system's operational states, the AD model learns the characteristics of\nnormal behavior under secure conditions. When an active attack occurs, the\nsystem's state deviates from the learned normal patterns and is identified as\nanomalous by the model. Experimental results show that the AD model achieves an\narea under the curve (AUC) exceeding 99%, effectively safeguarding the receiver\nof the QKD system. Compared to traditional approaches, our model can be\ndeployed with minimal cost in existing QKD networks without requiring\nadditional optical or electrical components, thus avoiding the introduction of\nnew side channels. Furthermore, unlike multi-class machine learning algorithms,\nour approach does not rely on prior knowledge of specific attack types and is\npotentially able to detect unknown active attacks. These advantages-generality,\nease of deployment, low cost, and high accuracy-make our model a practical and\neffective tool for protecting the receiver of QKD systems against active\nattacks."
                },
                "authors": [
                    {
                        "name": "Junxuan Liu"
                    },
                    {
                        "name": "Bingcheng Huang"
                    },
                    {
                        "name": "Jialei Su"
                    },
                    {
                        "name": "Qingquan Peng"
                    },
                    {
                        "name": "Anqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Huang"
                },
                "author": "Anqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20921v2",
                "updated": "2025-10-16T15:31:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    31,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-06-26T01:03:44Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    3,
                    44,
                    3,
                    177,
                    0
                ],
                "title": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-guided Chemical Process Optimization with a Multi-Agent Approach"
                },
                "summary": "Chemical process optimization maximizes production efficiency and economic\nperformance, but optimization algorithms, including gradient-based solvers,\nnumerical methods, and parameter grid searches, become impractical when\noperating constraints are ill-defined or unavailable. We present a multi-agent\nLLM framework that autonomously infers operating constraints from minimal\nprocess descriptions, then collaboratively guides optimization. Our\nAutoGen-based framework employs OpenAI's o3 model with specialized agents for\nconstraint generation, parameter validation, simulation, and optimization\nguidance. Through autonomous constraint generation and iterative multi-agent\noptimization, the framework eliminates the need for predefined operational\nbounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost\nratio metrics, the framework achieved competitive performance with conventional\nmethods while reducing wall-time 31-fold relative to grid search, converging in\nunder 20 minutes. The reasoning-guided search demonstrates sophisticated\nprocess understanding, correctly identifying utility trade-offs and applying\ndomain-informed heuristics. Unlike conventional methods requiring predefined\nconstraints, our approach uniquely combines autonomous constraint generation\nwith interpretable parameter exploration. Model comparison reveals\nreasoning-capable architectures (o3, o1) are essential for successful\noptimization, while standard models fail to converge. This approach is\nparticularly valuable for emerging processes and retrofit applications where\noperational constraints are poorly characterized or unavailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemical process optimization maximizes production efficiency and economic\nperformance, but optimization algorithms, including gradient-based solvers,\nnumerical methods, and parameter grid searches, become impractical when\noperating constraints are ill-defined or unavailable. We present a multi-agent\nLLM framework that autonomously infers operating constraints from minimal\nprocess descriptions, then collaboratively guides optimization. Our\nAutoGen-based framework employs OpenAI's o3 model with specialized agents for\nconstraint generation, parameter validation, simulation, and optimization\nguidance. Through autonomous constraint generation and iterative multi-agent\noptimization, the framework eliminates the need for predefined operational\nbounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost\nratio metrics, the framework achieved competitive performance with conventional\nmethods while reducing wall-time 31-fold relative to grid search, converging in\nunder 20 minutes. The reasoning-guided search demonstrates sophisticated\nprocess understanding, correctly identifying utility trade-offs and applying\ndomain-informed heuristics. Unlike conventional methods requiring predefined\nconstraints, our approach uniquely combines autonomous constraint generation\nwith interpretable parameter exploration. Model comparison reveals\nreasoning-capable architectures (o3, o1) are essential for successful\noptimization, while standard models fail to converge. This approach is\nparticularly valuable for emerging processes and retrofit applications where\noperational constraints are poorly characterized or unavailable."
                },
                "authors": [
                    {
                        "name": "Tong Zeng"
                    },
                    {
                        "name": "Srivathsan Badrinarayanan"
                    },
                    {
                        "name": "Janghoon Ock"
                    },
                    {
                        "name": "Cheng-Kai Lai"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "16 pages (main manuscript without references), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14788v1",
                "updated": "2025-10-16T15:20:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    20,
                    49,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:20:49Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    20,
                    49,
                    3,
                    289,
                    0
                ],
                "title": "Cross-Scenario Unified Modeling of User Interests at Billion Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Scenario Unified Modeling of User Interests at Billion Scale"
                },
                "summary": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User interests on content platforms are inherently diverse, manifesting\nthrough complex behavioral patterns across heterogeneous scenarios such as\nsearch, feed browsing, and content discovery. Traditional recommendation\nsystems typically prioritize business metric optimization within isolated\nspecific scenarios, neglecting cross-scenario behavioral signals and struggling\nto integrate advanced techniques like LLMs at billion-scale deployments, which\nfinally limits their ability to capture holistic user interests across platform\ntouchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender\nEngine for Diversified scenarios, tailored for industry-level content\nrecommendation systems. RED-Rec unifies user interest representations across\nmultiple behavioral contexts by aggregating and synthesizing actions from\nvaried scenarios, resulting in comprehensive item and user modeling. At its\ncore, a two-tower LLM-powered framework enables nuanced, multifaceted\nrepresentations with deployment efficiency, and a scenario-aware dense mixing\nand querying policy effectively fuses diverse behavioral signals to capture\ncross-scenario user intent patterns and express fine-grained, context-specific\nintents during serving. We validate RED-Rec through online A/B testing on\nhundreds of millions of users in RedNote through online A/B testing, showing\nsubstantial performance gains in both content recommendation and advertisement\ntargeting tasks. We further introduce a million-scale sequential recommendation\ndataset, RED-MMU, for comprehensive offline training and evaluation. Our work\nadvances unified user modeling, unlocking deeper personalization and fostering\nmore meaningful user engagement in large-scale UGC platforms."
                },
                "authors": [
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Zejian Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Yixin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Zhu"
                },
                "author": "Yixin Zhu",
                "arxiv_comment": "The dataset, code, and models will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14783v1",
                "updated": "2025-10-16T15:16:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    16,
                    26,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:16:26Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    16,
                    26,
                    3,
                    289,
                    0
                ],
                "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with\n  Model-Based Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with\n  Model-Based Reinforcement Learning"
                },
                "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight."
                },
                "authors": [
                    {
                        "name": "Aderik Verraest"
                    },
                    {
                        "name": "Stavrow Bahnam"
                    },
                    {
                        "name": "Robin Ferede"
                    },
                    {
                        "name": "Guido de Croon"
                    },
                    {
                        "name": "Christophe De Wagter"
                    }
                ],
                "author_detail": {
                    "name": "Christophe De Wagter"
                },
                "author": "Christophe De Wagter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.02103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.02103v2",
                "updated": "2025-10-16T15:15:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    15,
                    17,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-02T15:09:10Z",
                "published_parsed": [
                    2025,
                    10,
                    2,
                    15,
                    9,
                    10,
                    3,
                    275,
                    0
                ],
                "title": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing\n  Unauthorized Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing\n  Unauthorized Sensing"
                },
                "summary": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of integrated sensing and communication (ISAC) brings along\nunprecedented vulnerabilities to authorized sensing, necessitating the\ndevelopment of secure solutions. Sensing parameters are embedded within the\ntarget-reflected signal leaked to unauthorized passive radar sensing\neavesdroppers (Eve), implying that they can silently extract sensory\ninformation without prior knowledge of the information data. To overcome this\nlimitation, we propose a sensing-secure ISAC framework that ensures secure\ntarget detection and estimation for the legitimate system, while obfuscating\nunauthorized sensing without requiring any prior knowledge of Eve. By\nintroducing artificial imperfections into the ambiguity function (AF) of ISAC\nsignals, we introduce artificial targets into Eve's range profile which\nincrease its range estimation ambiguity. In contrast, the legitimate sensing\nreceiver (Alice) can suppress these AF artifacts using mismatched filtering,\nalbeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM\nsignal, a structured subcarrier power allocation scheme is designed to shape\nthe secure autocorrelation function (ACF), inserting periodic peaks to mislead\nEve's range estimation and degrade target detection performance. To quantify\nthe sensing security, we introduce peak sidelobe level (PSL) and integrated\nsidelobe level (ISL) as key performance metrics. Then, we analyze the three-way\ntrade-offs between communication, legitimate sensing, and sensing security,\nhighlighting the impact of the proposed sensing-secure ISAC signaling on system\nperformance. We formulate a convex optimization problem to maximize ISAC\nperformance while guaranteeing a certain sensing security level. Numerical\nresults validate the effectiveness of the proposed sensing-secure ISAC\nsignaling, demonstrating its ability to degrade Eve's target estimation while\npreserving Alice's performance."
                },
                "authors": [
                    {
                        "name": "Kawon Han"
                    },
                    {
                        "name": "Kaitao Meng"
                    },
                    {
                        "name": "Christos Masouros"
                    }
                ],
                "author_detail": {
                    "name": "Christos Masouros"
                },
                "author": "Christos Masouros",
                "arxiv_doi": "10.1109/TWC.2025.3618121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TWC.2025.3618121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.02103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.02103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14773v1",
                "updated": "2025-10-16T15:09:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    9,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T15:09:22Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    9,
                    22,
                    3,
                    289,
                    0
                ],
                "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large\n  Language Models with Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Answers in Thought Matters: Revisiting Evaluation on Large\n  Language Models with Reasoning"
                },
                "summary": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating generative models, such as large language models (LLMs), commonly\ninvolves question-answering tasks where the final answer is selected based on\nprobability of answer choices. On the other hand, for models requiring\nreasoning, the method of answer extraction plays a critical role. Our research\nreveals that the performance of reasoning models and their final answer\ndistributions are highly sensitive to the answer extraction algorithm employed.\nIn order to mitigate this, we propose a basic framework: Answer Regeneration.\nThe method uses an additional model inference, providing the prior input and\noutput prefaced by the prompt \"Answer:\". The final answer is then selected or\nextracted from the regenerated output. We show that this\nextraction-rule-agnostic approach exhibits improved performance and enhanced\nrobustness. Furthermore, we have applied this framework to general math\nproblems and open-ended question answering tasks. Our analysis and this\nframework could offer a more reliable results for model evaluation."
                },
                "authors": [
                    {
                        "name": "Hwiyeol Jo"
                    },
                    {
                        "name": "Joosung Lee"
                    },
                    {
                        "name": "Jaehone Lee"
                    },
                    {
                        "name": "Sang-Woo Lee"
                    },
                    {
                        "name": "Joonsuk Park"
                    },
                    {
                        "name": "Kang Min Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Kang Min Yoo"
                },
                "author": "Kang Min Yoo",
                "arxiv_comment": "ARR Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20934v2",
                "updated": "2025-10-16T15:08:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    8,
                    16,
                    3,
                    289,
                    0
                ],
                "published": "2025-03-26T19:05:20Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    19,
                    5,
                    20,
                    2,
                    85,
                    0
                ],
                "title": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method\n  Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method\n  Refactoring"
                },
                "summary": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools\nthat recommend which methods to move and where, these recommendations do not\nalign with how expert developers perform MOVEMETHOD. Given the extensive\ntraining of Large Language Models and their reliance upon naturalness of code,\nthey should expertly recommend which methods are misplaced in a given class and\nwhich classes are better hosts. Our formative study of 2016 LLM recommendations\nrevealed that LLMs give expert suggestions, yet they are unreliable: up to 80%\nof the suggestions are hallucinations. We introduce the first LLM fully powered\nassistant for MOVEMETHOD refactoring that automates its whole end-to-end\nlifecycle, from recommendation to execution. We designed novel solutions that\nautomatically filter LLM hallucinations using static analysis from IDEs and a\nnovel workflow that requires LLMs to be self-consistent, critique, and rank\nrefactoring suggestions. As MOVEMETHOD refactoring requires global,\nprojectlevel reasoning, we solved the limited context size of LLMs by employing\nrefactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,\nsynergistically combines the strengths of the LLM, IDE, static analysis, and\nsemantic relevance. In our thorough, multi-methodology empirical evaluation, we\ncompare MM-assist with the previous state-of-the-art approaches. MM-assist\nsignificantly outperforms them: (i) on a benchmark widely used by other\nresearchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a\ncorpus of 210 recent refactorings from Open-source software, our Recall rates\nimprove by at least 2.4x. Lastly, we conducted a user study with 30 experienced\nparticipants who used MM-assist to refactor their own code for one week. They\nrated 82.8% of MM-assist recommendations positively. This shows that MM-assist\nis both effective and useful."
                },
                "authors": [
                    {
                        "name": "Abhiram Bellur"
                    },
                    {
                        "name": "Fraol Batole"
                    },
                    {
                        "name": "Mohammed Raihan Ullah"
                    },
                    {
                        "name": "Malinda Dilhara"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    },
                    {
                        "name": "Timofey Bryksin"
                    },
                    {
                        "name": "Kai Ishikawa"
                    },
                    {
                        "name": "Haifeng Chen"
                    },
                    {
                        "name": "Masaharu Morimoto"
                    },
                    {
                        "name": "Shota Motoura"
                    },
                    {
                        "name": "Takeo Hosomi"
                    },
                    {
                        "name": "Tien N. Nguyen"
                    },
                    {
                        "name": "Hridesh Rajan"
                    },
                    {
                        "name": "Nikolaos Tsantalis"
                    },
                    {
                        "name": "Danny Dig"
                    }
                ],
                "author_detail": {
                    "name": "Danny Dig"
                },
                "author": "Danny Dig",
                "arxiv_comment": "Published at the International Conference on Software Maintenance and\n  Evolution (ICSME'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17843v2",
                "updated": "2025-10-16T15:05:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    5,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2024-09-26T13:50:40Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    13,
                    50,
                    40,
                    3,
                    270,
                    0
                ],
                "title": "Auction-based Adaptive Resource Allocation Optimization in Dense and\n  Heterogeneous IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auction-based Adaptive Resource Allocation Optimization in Dense and\n  Heterogeneous IoT Networks"
                },
                "summary": "Efficient and reliable resource allocation within densely-deployed massive\nIoT networks remains a key challenge due to resource constraints among\nlow-size, weight, and power (SWaP) IoT devices and within the network and\nlimitations of conventional centralized methods under incomplete information.\nWe propose a novel auction-based framework for adaptive resource allocation,\ncombining space-time-frequency spreading (STFS) techniques with Bayesian Game\napproaches. We introduce novel modified Simultaneous Ascending Auction (mSAA)\nmechanism tailored to densely-deployed and low-complexity IoT networks,\nenabling distributed computation and reduced power consumption. By\nincorporating Bayesian game-based bidding strategies and optimizing dispersion\nmatrices for signal transmission, the proposed approach ensures enhanced\nchannel throughput and energy efficiency. Comparative analysis against\ntraditional auction types, including First-Price and Second-Price Sealed-Bid\nAuctions, as well as the Vickery-Clarke-Groves (VCG) mechanism, demonstrates\nthe superiority of mSAA in terms of surplus maximization, revenue efficiency,\nand robustness in risk-prone bidding environments. Simulation results validate\nthe model's adaptability to heterogeneous IoT nodes and its potential for dense\ndeployment across different environments and verticals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and reliable resource allocation within densely-deployed massive\nIoT networks remains a key challenge due to resource constraints among\nlow-size, weight, and power (SWaP) IoT devices and within the network and\nlimitations of conventional centralized methods under incomplete information.\nWe propose a novel auction-based framework for adaptive resource allocation,\ncombining space-time-frequency spreading (STFS) techniques with Bayesian Game\napproaches. We introduce novel modified Simultaneous Ascending Auction (mSAA)\nmechanism tailored to densely-deployed and low-complexity IoT networks,\nenabling distributed computation and reduced power consumption. By\nincorporating Bayesian game-based bidding strategies and optimizing dispersion\nmatrices for signal transmission, the proposed approach ensures enhanced\nchannel throughput and energy efficiency. Comparative analysis against\ntraditional auction types, including First-Price and Second-Price Sealed-Bid\nAuctions, as well as the Vickery-Clarke-Groves (VCG) mechanism, demonstrates\nthe superiority of mSAA in terms of surplus maximization, revenue efficiency,\nand robustness in risk-prone bidding environments. Simulation results validate\nthe model's adaptability to heterogeneous IoT nodes and its potential for dense\ndeployment across different environments and verticals."
                },
                "authors": [
                    {
                        "name": "Nirmal D. Wickramasinghe"
                    },
                    {
                        "name": "John Dooley"
                    },
                    {
                        "name": "Dirk Pesch"
                    },
                    {
                        "name": "Indrakshi Dey"
                    }
                ],
                "author_detail": {
                    "name": "Indrakshi Dey"
                },
                "author": "Indrakshi Dey",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07887v2",
                "updated": "2025-10-16T14:59:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    59,
                    50,
                    3,
                    289,
                    0
                ],
                "published": "2025-04-10T16:00:59Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    16,
                    0,
                    59,
                    3,
                    100,
                    0
                ],
                "title": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Adversarial Robustness to Bias Elicitation in Large\n  Language Models: Scalable Automated Assessment with LLM-as-a-Judge"
                },
                "summary": "The growing integration of Large Language Models (LLMs) into critical\nsocietal domains has raised concerns about embedded biases that can perpetuate\nstereotypes and undermine fairness. Such biases may stem from historical\ninequalities in training data, linguistic imbalances, or adversarial\nmanipulation. Despite mitigation efforts, recent studies show that LLMs remain\nvulnerable to adversarial attacks that elicit biased outputs. This work\nproposes a scalable benchmarking framework to assess LLM robustness to\nadversarial bias elicitation. Our methodology involves: (i) systematically\nprobing models across multiple tasks targeting diverse sociocultural biases,\n(ii) quantifying robustness through safety scores using an LLM-as-a-Judge\napproach, and (iii) employing jailbreak techniques to reveal safety\nvulnerabilities. To facilitate systematic benchmarking, we release a curated\ndataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying\nDeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is\nuneven, with age, disability, and intersectional biases among the most\nprominent. Some small models outperform larger ones in safety, suggesting that\ntraining and architecture may matter more than scale. However, no model is\nfully robust to adversarial elicitation, with jailbreak attacks using\nlow-resource languages or refusal suppression proving effective across model\nfamilies. We also find that successive LLM generations exhibit slight safety\ngains, while models fine-tuned for the medical domain tend to be less safe than\ntheir general-purpose counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing integration of Large Language Models (LLMs) into critical\nsocietal domains has raised concerns about embedded biases that can perpetuate\nstereotypes and undermine fairness. Such biases may stem from historical\ninequalities in training data, linguistic imbalances, or adversarial\nmanipulation. Despite mitigation efforts, recent studies show that LLMs remain\nvulnerable to adversarial attacks that elicit biased outputs. This work\nproposes a scalable benchmarking framework to assess LLM robustness to\nadversarial bias elicitation. Our methodology involves: (i) systematically\nprobing models across multiple tasks targeting diverse sociocultural biases,\n(ii) quantifying robustness through safety scores using an LLM-as-a-Judge\napproach, and (iii) employing jailbreak techniques to reveal safety\nvulnerabilities. To facilitate systematic benchmarking, we release a curated\ndataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying\nDeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is\nuneven, with age, disability, and intersectional biases among the most\nprominent. Some small models outperform larger ones in safety, suggesting that\ntraining and architecture may matter more than scale. However, no model is\nfully robust to adversarial elicitation, with jailbreak attacks using\nlow-resource languages or refusal suppression proving effective across model\nfamilies. We also find that successive LLM generations exhibit slight safety\ngains, while models fine-tuned for the medical domain tend to be less safe than\ntheir general-purpose counterparts."
                },
                "authors": [
                    {
                        "name": "Riccardo Cantini"
                    },
                    {
                        "name": "Alessio Orsino"
                    },
                    {
                        "name": "Massimo Ruggiero"
                    },
                    {
                        "name": "Domenico Talia"
                    }
                ],
                "author_detail": {
                    "name": "Domenico Talia"
                },
                "author": "Domenico Talia",
                "arxiv_doi": "10.1007/s10994-025-06862-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10994-025-06862-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Cantini, R., Orsino, A., Ruggiero, M., Talia, D. Benchmarking\n  adversarial robustness to bias elicitation in large language models: scalable\n  automated assessment with LLM-as-a-judge. Mach Learn 114, 249 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14756v1",
                "updated": "2025-10-16T14:57:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    57,
                    1,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:57:01Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    57,
                    1,
                    3,
                    289,
                    0
                ],
                "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware\n  Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware\n  Code"
                },
                "summary": "Large Language Models (LLMs) are increasingly used to automate hardware\ndesign tasks, including the generation of Verilog code. While early benchmarks\nfocus primarily on functional correctness, efficient hardware design demands\nadditional optimization for synthesis metrics such as area, delay, and power.\nExisting benchmarks fall short in evaluating these aspects comprehensively:\nthey often lack optimized baselines or testbenches for verification. To address\nthese gaps, we present Pluto, a benchmark and evaluation framework designed to\nassess the efficiency of LLM-generated Verilog designs. Pluto presents a\ncomprehensive evaluation set of 114 problems with self-checking testbenches and\nmultiple Pareto-optimal reference implementations. Experimental results show\nthat state-of-the-art LLMs can achieve high functional correctness, reaching\n78.3\\% at pass@1, but their synthesis efficiency still lags behind\nexpert-crafted implementations, with area efficiency of 63.8\\%, delay\nefficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights\nthe need for efficiency-aware evaluation frameworks such as Pluto to drive\nprogress in hardware-focused LLM research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used to automate hardware\ndesign tasks, including the generation of Verilog code. While early benchmarks\nfocus primarily on functional correctness, efficient hardware design demands\nadditional optimization for synthesis metrics such as area, delay, and power.\nExisting benchmarks fall short in evaluating these aspects comprehensively:\nthey often lack optimized baselines or testbenches for verification. To address\nthese gaps, we present Pluto, a benchmark and evaluation framework designed to\nassess the efficiency of LLM-generated Verilog designs. Pluto presents a\ncomprehensive evaluation set of 114 problems with self-checking testbenches and\nmultiple Pareto-optimal reference implementations. Experimental results show\nthat state-of-the-art LLMs can achieve high functional correctness, reaching\n78.3\\% at pass@1, but their synthesis efficiency still lags behind\nexpert-crafted implementations, with area efficiency of 63.8\\%, delay\nefficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights\nthe need for efficiency-aware evaluation frameworks such as Pluto to drive\nprogress in hardware-focused LLM research."
                },
                "authors": [
                    {
                        "name": "Manar Abdelatty"
                    },
                    {
                        "name": "Maryam Nouh"
                    },
                    {
                        "name": "Jacob K. Rosenstein"
                    },
                    {
                        "name": "Sherief Reda"
                    }
                ],
                "author_detail": {
                    "name": "Sherief Reda"
                },
                "author": "Sherief Reda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14751v1",
                "updated": "2025-10-16T14:52:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    52,
                    52,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:52:52Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    52,
                    52,
                    3,
                    289,
                    0
                ],
                "title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries"
                },
                "summary": "Next-token prediction (NTP) has driven the success of large language models\n(LLMs), but it struggles with long-horizon reasoning, planning, and creative\nwriting, with these limitations largely attributed to teacher-forced training.\nMulti-token prediction (MTP) partially mitigates these issues by predicting\nseveral future tokens at once, but it mostly captures short-range dependencies\nand offers limited improvement. We propose future summary prediction (FSP),\nwhich trains an auxiliary head to predict a compact representation of the\nlong-term future, preserving information relevant for long-form generations. We\nexplore two variants of FSP: handcrafted summaries, for example, a bag of words\nsummary of the future of the sequence, and learned summaries, which use\nembeddings produced by a reverse language model trained from right to left.\nLarge-scale pretraining experiments (3B and 8B-parameter models) demonstrate\nthat FSP provides improvements over both NTP and MTP across math, reasoning,\nand coding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-token prediction (NTP) has driven the success of large language models\n(LLMs), but it struggles with long-horizon reasoning, planning, and creative\nwriting, with these limitations largely attributed to teacher-forced training.\nMulti-token prediction (MTP) partially mitigates these issues by predicting\nseveral future tokens at once, but it mostly captures short-range dependencies\nand offers limited improvement. We propose future summary prediction (FSP),\nwhich trains an auxiliary head to predict a compact representation of the\nlong-term future, preserving information relevant for long-form generations. We\nexplore two variants of FSP: handcrafted summaries, for example, a bag of words\nsummary of the future of the sequence, and learned summaries, which use\nembeddings produced by a reverse language model trained from right to left.\nLarge-scale pretraining experiments (3B and 8B-parameter models) demonstrate\nthat FSP provides improvements over both NTP and MTP across math, reasoning,\nand coding benchmarks."
                },
                "authors": [
                    {
                        "name": "Divyat Mahajan"
                    },
                    {
                        "name": "Sachin Goyal"
                    },
                    {
                        "name": "Badr Youbi Idrissi"
                    },
                    {
                        "name": "Mohammad Pezeshki"
                    },
                    {
                        "name": "Ioannis Mitliagkas"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Kartik Ahuja"
                    }
                ],
                "author_detail": {
                    "name": "Kartik Ahuja"
                },
                "author": "Kartik Ahuja",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26238v2",
                "updated": "2025-10-16T14:51:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    51,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-30T13:32:59Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    13,
                    32,
                    59,
                    1,
                    273,
                    0
                ],
                "title": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Linear Probes: Dynamic Safety Monitoring for Language Models"
                },
                "summary": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring large language models' (LLMs) activations is an effective way to\ndetect harmful requests before they lead to unsafe outputs. However,\ntraditional safety monitors often require the same amount of compute for every\nquery. This creates a trade-off: expensive monitors waste resources on easy\ninputs, while cheap ones risk missing subtle cases. We argue that safety\nmonitors should be flexible--costs should rise only when inputs are difficult\nto assess, or when more compute is available. To achieve this, we introduce\nTruncated Polynomial Classifiers (TPCs), a natural extension of linear probes\nfor dynamic activation monitoring. Our key insight is that polynomials can be\ntrained and evaluated progressively, term-by-term. At test-time, one can\nearly-stop for lightweight monitoring, or use more terms for stronger\nguardrails when needed. TPCs provide two modes of use. First, as a safety dial:\nby evaluating more terms, developers and regulators can \"buy\" stronger\nguardrails from the same model. Second, as an adaptive cascade: clear cases\nexit early after low-order checks, and higher-order guardrails are evaluated\nonly for ambiguous inputs, reducing overall monitoring costs. On two\nlarge-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with\nup to 30B parameters, we show that TPCs compete with or outperform MLP-based\nprobe baselines of the same size, all the while being more interpretable than\ntheir black-box counterparts. Our code is available at\nhttp://github.com/james-oldfield/tpc."
                },
                "authors": [
                    {
                        "name": "James Oldfield"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Ioannis Patras"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Fazl Barez"
                    }
                ],
                "author_detail": {
                    "name": "Fazl Barez"
                },
                "author": "Fazl Barez",
                "arxiv_comment": "Project page: http://james-oldfield.github.io/tpc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08878v2",
                "updated": "2025-10-16T14:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    34,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2024-08-02T17:42:03Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    17,
                    42,
                    3,
                    4,
                    215,
                    0
                ],
                "title": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Prompting: How Knowledge Engineers Use Large Language Models"
                },
                "summary": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite many advances in knowledge engineering (KE), challenges remain in\nareas such as engineering knowledge graphs (KGs) at scale, keeping up with\nevolving domain knowledge, multilingualism, and multimodality. Recently, KE has\nused LLMs to support semi-automatic tasks, but the most effective use of LLMs\nto support knowledge engineers across the KE activites is still in its infancy.\nTo explore the vision of LLM copilots for KE and change existing KE practices,\nwe conducted a multimethod study during a KE hackathon. We investigated\nparticipants' views on the use of LLMs, the challenges they face, the skills\nthey may need to integrate LLMs into their practices, and how they use LLMs\nresponsibly. We found participants felt LLMs could contribute to improving\nefficiency when engineering KGs, but presented increased challenges around the\nalready complex issues of evaluating the KE tasks. We discovered prompting to\nbe a useful but undervalued skill for knowledge engineers working with LLMs,\nand note that natural language processing skills may become more relevant\nacross more roles in KG construction. Integrating LLMs into KE tasks needs to\nbe mindful of potential risks and harms related to responsible AI. Given the\nlimited ethical training, most knowledge engineers receive solutions such as\nour suggested `KG cards' based on data cards could be a useful guide for KG\nconstruction. Our findings can support designers of KE AI copilots, KE\nresearchers, and practitioners using advanced AI to develop trustworthy\napplications, propose new methodologies for KE and operate new technologies\nresponsibly."
                },
                "authors": [
                    {
                        "name": "Elisavet Koutsiana"
                    },
                    {
                        "name": "Johanna Walker"
                    },
                    {
                        "name": "Michelle Nwachukwu"
                    },
                    {
                        "name": "Albert MeroÃ±o-PeÃ±uela"
                    },
                    {
                        "name": "Elena Simperl"
                    }
                ],
                "author_detail": {
                    "name": "Elena Simperl"
                },
                "author": "Elena Simperl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05043v2",
                "updated": "2025-10-16T14:21:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    21,
                    45,
                    3,
                    289,
                    0
                ],
                "published": "2025-07-07T14:27:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    14,
                    27,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "MoLink: Distributed and Efficient Serving Framework for Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoLink: Distributed and Efficient Serving Framework for Large Models"
                },
                "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models. The source code\nis publicly available https://github.com/oldcpple/MoLink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models. The source code\nis publicly available https://github.com/oldcpple/MoLink."
                },
                "authors": [
                    {
                        "name": "Lewei Jin"
                    },
                    {
                        "name": "Yongqi Chen"
                    },
                    {
                        "name": "Kui Zhang"
                    },
                    {
                        "name": "Yifan Zhuo"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Bowei Yang"
                    },
                    {
                        "name": "Zhengong Cai"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14719v1",
                "updated": "2025-10-16T14:20:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    20,
                    0,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:20:00Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    20,
                    0,
                    3,
                    289,
                    0
                ],
                "title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous\n  References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous\n  References"
                },
                "summary": "Modern GPUs feature specialized hardware units that enable high-performance,\nasynchronous dataflow execution. However, the conventional SIMT programming\nmodel is fundamentally misaligned with this task-parallel hardware, creating a\nsignificant programmability gap. While hardware-level warp specialization is\nthe key to unlocking peak performance, it forces developers to manually\norchestrate complex, low-level communication and software pipelines--a process\nthat is labor-intensive, error-prone, and unsustainable. To address this\nchallenge, we present Tawa, an automated compiler that systematically generates\nhigh-performance, warp-specialized code from a high-level, tile-based program.\nCentral to our approach is a novel IR abstraction, asynchronous references\n(aref), which expresses warp-level communication without exposing low-level\nhardware details. Using this abstraction, Tawa automatically partitions\nprograms into producer-consumer roles and manages the intricate dataflow\npipeline, relieving developers of invasive kernel rewriting. Evaluation on\nNVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers\nhigh hardware utilization, achieving up to 1.1$\\times$ speedup over highly\noptimized cuBLAS GEMM kernels. For attention workloads, Tawa attains\n1.2$\\times$ speedup over Triton and matches the performance of the\nhand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming\neffort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs feature specialized hardware units that enable high-performance,\nasynchronous dataflow execution. However, the conventional SIMT programming\nmodel is fundamentally misaligned with this task-parallel hardware, creating a\nsignificant programmability gap. While hardware-level warp specialization is\nthe key to unlocking peak performance, it forces developers to manually\norchestrate complex, low-level communication and software pipelines--a process\nthat is labor-intensive, error-prone, and unsustainable. To address this\nchallenge, we present Tawa, an automated compiler that systematically generates\nhigh-performance, warp-specialized code from a high-level, tile-based program.\nCentral to our approach is a novel IR abstraction, asynchronous references\n(aref), which expresses warp-level communication without exposing low-level\nhardware details. Using this abstraction, Tawa automatically partitions\nprograms into producer-consumer roles and manages the intricate dataflow\npipeline, relieving developers of invasive kernel rewriting. Evaluation on\nNVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers\nhigh hardware utilization, achieving up to 1.1$\\times$ speedup over highly\noptimized cuBLAS GEMM kernels. For attention workloads, Tawa attains\n1.2$\\times$ speedup over Triton and matches the performance of the\nhand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming\neffort."
                },
                "authors": [
                    {
                        "name": "Hongzheng Chen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Alexander Collins"
                    },
                    {
                        "name": "Bastian Hagedorn"
                    },
                    {
                        "name": "Evghenii Gaburov"
                    },
                    {
                        "name": "Masahiro Masuda"
                    },
                    {
                        "name": "Matthew Brookhart"
                    },
                    {
                        "name": "Chris Sullivan"
                    },
                    {
                        "name": "Jason Knight"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Vinod Grover"
                    }
                ],
                "author_detail": {
                    "name": "Vinod Grover"
                },
                "author": "Vinod Grover",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14718v1",
                "updated": "2025-10-16T14:18:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    18,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:18:31Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    18,
                    31,
                    3,
                    289,
                    0
                ],
                "title": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface\n  Unintended Harms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Model Risk in Healthcare AI: Using Storytelling to Surface\n  Unintended Harms"
                },
                "summary": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling\nfast development of tools like stress monitors, wellness trackers, and mental\nhealth chatbots. However, rapid and low-barrier development can introduce risks\nof bias, privacy violations, and unequal access, especially when systems ignore\nreal-world contexts and diverse user needs. Many recent methods use AI to\ndetect risks automatically, but this can reduce human engagement in\nunderstanding how harms arise and who they affect. We present a human-centered\nframework that generates user stories and supports multi-agent discussions to\nhelp people think creatively about potential benefits and harms before\ndeployment. In a user study, participants who read stories recognized a broader\nrange of harms, distributing their responses more evenly across all 13 harm\ntypes. In contrast, those who did not read stories focused primarily on privacy\nand well-being (58.3%). Our findings show that storytelling helped participants\nspeculate about a broader range of harms and benefits and think more creatively\nabout AI's impact on users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is rapidly transforming healthcare, enabling\nfast development of tools like stress monitors, wellness trackers, and mental\nhealth chatbots. However, rapid and low-barrier development can introduce risks\nof bias, privacy violations, and unequal access, especially when systems ignore\nreal-world contexts and diverse user needs. Many recent methods use AI to\ndetect risks automatically, but this can reduce human engagement in\nunderstanding how harms arise and who they affect. We present a human-centered\nframework that generates user stories and supports multi-agent discussions to\nhelp people think creatively about potential benefits and harms before\ndeployment. In a user study, participants who read stories recognized a broader\nrange of harms, distributing their responses more evenly across all 13 harm\ntypes. In contrast, those who did not read stories focused primarily on privacy\nand well-being (58.3%). Our findings show that storytelling helped participants\nspeculate about a broader range of harms and benefits and think more creatively\nabout AI's impact on users."
                },
                "authors": [
                    {
                        "name": "Xingmeng Zhao"
                    },
                    {
                        "name": "Dan Schumacher"
                    },
                    {
                        "name": "Veronica Rammouz"
                    },
                    {
                        "name": "Anthony Rios"
                    }
                ],
                "author_detail": {
                    "name": "Anthony Rios"
                },
                "author": "Anthony Rios",
                "arxiv_comment": "8 pages main + Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03550v3",
                "updated": "2025-10-16T14:16:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    16,
                    18,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-05T15:18:36Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    15,
                    18,
                    36,
                    1,
                    217,
                    0
                ],
                "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via\n  Internal Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via\n  Internal Representations"
                },
                "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using LLMs, a paradigm known as \"LLM-as-a-judge\". However,\nimproving its alignment with human preferences without complex prompts or\nfine-tuning remains challenging. Previous studies mainly optimize based on\nshallow outputs, overlooking rich cross-layer representations. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and task-relevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a post-hoc,\nplug-and-play framework for improving the alignment of LLM-as-a-Judge\npoint-wise evaluations with human scores by leveraging internal\nrepresentations. LAGER produces fine-grained judgment scores by aggregating\ncross-layer score-token logits and computing the expected score from a\nsoftmax-based distribution, while keeping the LLM backbone frozen and ensuring\nno impact on the inference process. LAGER fully leverages the complementary\ninformation across different layers, overcoming the limitations of relying\nsolely on the final layer. We evaluate our method on the standard alignment\nbenchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find\nthat LAGER achieves improvements of up to 7.5% over the best baseline across\nthese benchmarks. Without reasoning steps, LAGER matches or outperforms\nreasoning-based methods. Experiments on downstream applications, such as data\nselection and emotional understanding, further show the generalization of\nLAGER.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using LLMs, a paradigm known as \"LLM-as-a-judge\". However,\nimproving its alignment with human preferences without complex prompts or\nfine-tuning remains challenging. Previous studies mainly optimize based on\nshallow outputs, overlooking rich cross-layer representations. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and task-relevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a post-hoc,\nplug-and-play framework for improving the alignment of LLM-as-a-Judge\npoint-wise evaluations with human scores by leveraging internal\nrepresentations. LAGER produces fine-grained judgment scores by aggregating\ncross-layer score-token logits and computing the expected score from a\nsoftmax-based distribution, while keeping the LLM backbone frozen and ensuring\nno impact on the inference process. LAGER fully leverages the complementary\ninformation across different layers, overcoming the limitations of relying\nsolely on the final layer. We evaluate our method on the standard alignment\nbenchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find\nthat LAGER achieves improvements of up to 7.5% over the best baseline across\nthese benchmarks. Without reasoning steps, LAGER matches or outperforms\nreasoning-based methods. Experiments on downstream applications, such as data\nselection and emotional understanding, further show the generalization of\nLAGER."
                },
                "authors": [
                    {
                        "name": "Peng Lai"
                    },
                    {
                        "name": "Jianjie Zheng"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12787v2",
                "updated": "2025-10-16T14:10:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    10,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-14T17:57:04Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    17,
                    57,
                    4,
                    1,
                    287,
                    0
                ],
                "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics"
                },
                "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperforms them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperforms them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem."
                },
                "authors": [
                    {
                        "name": "Marco Del Tredici"
                    },
                    {
                        "name": "Jacob McCarran"
                    },
                    {
                        "name": "Benjamin Breen"
                    },
                    {
                        "name": "Javier Aspuru Mijares"
                    },
                    {
                        "name": "Weichen Winston Yin"
                    },
                    {
                        "name": "Jacob M. Taylor"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Dirk Englund"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Englund"
                },
                "author": "Dirk Englund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14703v1",
                "updated": "2025-10-16T14:06:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    6,
                    3,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:06:03Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    6,
                    3,
                    3,
                    289,
                    0
                ],
                "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for\n  Function Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for\n  Function Calling"
                },
                "summary": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly demonstrating strong\ncapabilities as autonomous agents, with function calling serving as a core\nmechanism for interaction with the environment. Meanwhile, inference scaling\nhas become a cutting-edge technique to enhance LLM performance by allocating\nmore computational resources during the inference process. However, current\nresearch on inference scaling primarily focuses on unstructured output\ngeneration tasks, leaving its application in structured outputs, like function\ncalling, largely underexplored. To bridge this gap, we propose an inference\nscaling framework that combines fine-grained beam search with a process reward\nmodel, ToolPRM, which scores the internal steps of each single function call.\nTo train ToolPRM, we construct the first fine-grained intra-call process\nsupervision dataset, automatically annotated with function-masking techniques\nto provide step-level rewards for structured tool-use reasoning. Extensive\nexperiments demonstrate that ToolPRM beats the coarse-grained and outcome\nreward models in terms of predictive accuracy, indicating its stronger\ncapability in supervising the function calling inference process. Inference\nscaling technique equipped with ToolPRM also significantly improves the\nbackbone model performance across various function calling tasks and\nbenchmarks. More importantly, we reveal a key principle for applying inference\nscaling techniques to structured outputs: \"explore more but retain less\" due to\nthe unrecoverability characteristics of structured function calling generation."
                },
                "authors": [
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Yuanyuan Shi"
                    },
                    {
                        "name": "Xin Peng"
                    },
                    {
                        "name": "Renjie Ding"
                    },
                    {
                        "name": "Hairui Wang"
                    },
                    {
                        "name": "Yuxuan Peng"
                    },
                    {
                        "name": "Bizhe Bai"
                    },
                    {
                        "name": "Weixi Song"
                    },
                    {
                        "name": "Fengshuo Bai"
                    },
                    {
                        "name": "Huacan Chai"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16191v2",
                "updated": "2025-10-16T14:05:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    5,
                    53,
                    3,
                    289,
                    0
                ],
                "published": "2025-01-27T16:45:34Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    16,
                    45,
                    34,
                    0,
                    27,
                    0
                ],
                "title": "The Last Dependency Crusade: Solving Python Dependency Conflicts with\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Dependency Crusade: Solving Python Dependency Conflicts with\n  LLMs"
                },
                "summary": "Resolving Python dependency issues remains a tedious and error-prone process,\nforcing developers to manually trial compatible module versions and interpreter\nconfigurations. Existing automated solutions, such as knowledge-graph-based and\ndatabase-driven methods, face limitations due to the variety of dependency\nerror types, large sets of possible module versions, and conflicts among\ntransitive dependencies. This paper investigates the use of Large Language\nModels (LLMs) to automatically repair dependency issues in Python programs. We\npropose PLLM (pronounced \"plum\"), a novel retrieval-augmented generation (RAG)\napproach that iteratively infers missing or incorrect dependencies. PLLM builds\na test environment where the LLM proposes module combinations, observes\nexecution feedback, and refines its predictions using natural language\nprocessing (NLP) to parse error messages. We evaluate PLLM on the Gistable\nHG2.9K dataset, a curated collection of real-world Python programs. Using this\nbenchmark, we explore multiple PLLM configurations, including six open-source\nLLMs evaluated both with and without RAG. Our findings show that RAG\nconsistently improves fix rates, with the best performance achieved by Gemma-2\n9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo\nand ReadPyE, PLLM achieves significantly higher fix rates; +15.97\\% more than\nReadPyE and +21.58\\% more than PyEGo. Further analysis shows that PLLM is\nespecially effective for projects with numerous dependencies and those using\nspecialized numerical or machine-learning libraries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolving Python dependency issues remains a tedious and error-prone process,\nforcing developers to manually trial compatible module versions and interpreter\nconfigurations. Existing automated solutions, such as knowledge-graph-based and\ndatabase-driven methods, face limitations due to the variety of dependency\nerror types, large sets of possible module versions, and conflicts among\ntransitive dependencies. This paper investigates the use of Large Language\nModels (LLMs) to automatically repair dependency issues in Python programs. We\npropose PLLM (pronounced \"plum\"), a novel retrieval-augmented generation (RAG)\napproach that iteratively infers missing or incorrect dependencies. PLLM builds\na test environment where the LLM proposes module combinations, observes\nexecution feedback, and refines its predictions using natural language\nprocessing (NLP) to parse error messages. We evaluate PLLM on the Gistable\nHG2.9K dataset, a curated collection of real-world Python programs. Using this\nbenchmark, we explore multiple PLLM configurations, including six open-source\nLLMs evaluated both with and without RAG. Our findings show that RAG\nconsistently improves fix rates, with the best performance achieved by Gemma-2\n9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo\nand ReadPyE, PLLM achieves significantly higher fix rates; +15.97\\% more than\nReadPyE and +21.58\\% more than PyEGo. Further analysis shows that PLLM is\nespecially effective for projects with numerous dependencies and those using\nspecialized numerical or machine-learning libraries."
                },
                "authors": [
                    {
                        "name": "Antony Bartlett"
                    },
                    {
                        "name": "Cynthia Liem"
                    },
                    {
                        "name": "Annibale Panichella"
                    }
                ],
                "author_detail": {
                    "name": "Annibale Panichella"
                },
                "author": "Annibale Panichella",
                "arxiv_comment": "Pre-print - Accepted at the first annual workshop on Agentic Software\n  Engineering (AgenticSE) co-located with ASE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14702v1",
                "updated": "2025-10-16T14:05:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    5,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:05:28Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    5,
                    28,
                    3,
                    289,
                    0
                ],
                "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next\n  Point-of-Interest Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next\n  Point-of-Interest Prediction"
                },
                "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST."
                },
                "authors": [
                    {
                        "name": "Penglong Zhai"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Fanyi Di"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Yifang Yuan"
                    },
                    {
                        "name": "Jie Huang"
                    },
                    {
                        "name": "Peng Wu"
                    },
                    {
                        "name": "Sicong Wang"
                    },
                    {
                        "name": "Mingyang Yin"
                    },
                    {
                        "name": "Tingting Hu"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Xin Li"
                    }
                ],
                "author_detail": {
                    "name": "Xin Li"
                },
                "author": "Xin Li",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14700v1",
                "updated": "2025-10-16T14:04:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    4,
                    46,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T14:04:46Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    4,
                    46,
                    3,
                    289,
                    0
                ],
                "title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There\n  Yet?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There\n  Yet?"
                },
                "summary": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents have demonstrated remarkable capabilities\nin software engineering and cybersecurity tasks, including code generation,\nvulnerability discovery, and automated testing. One critical but underexplored\napplication is automated web vulnerability reproduction, which transforms\nvulnerability reports into working exploits. Although recent advances suggest\npromising potential, challenges remain in applying LLM agents to real-world web\nvulnerability reproduction scenarios. In this paper, we present the first\ncomprehensive evaluation of state-of-the-art LLM agents for automated web\nvulnerability reproduction. We systematically assess 20 agents from software\nengineering, cybersecurity, and general domains across 16 dimensions, including\ntechnical capabilities, environment adaptability, and user experience factors,\non 3 representative web vulnerabilities. Based on the results, we select three\ntop-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation\non our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types\nand 6 web technologies. Our results reveal that while LLM agents achieve\nreasonable success on simple library-based vulnerabilities, they consistently\nfail on complex service-based vulnerabilities requiring multi-component\nenvironments. Complex environment configurations and authentication barriers\ncreate a gap where agents can execute exploit code but fail to trigger actual\nvulnerabilities. We observe high sensitivity to input guidance, with\nperformance degrading by over 33% under incomplete authentication information.\nOur findings highlight the significant gap between current LLM agent\ncapabilities and the demands of reliable automated vulnerability reproduction,\nemphasizing the need for advances in environmental adaptation and autonomous\nproblem-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Guoai Xu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22358v2",
                "updated": "2025-10-16T14:03:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    14,
                    3,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-28T13:38:21Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    13,
                    38,
                    21,
                    2,
                    148,
                    0
                ],
                "title": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in\n  LLMs Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in\n  LLMs Continual Learning"
                },
                "summary": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in an end-to-end\ntraining stage. Specifically, OA-Adapter introduces a dynamic bottleneck\ndimension adaptation mechanism that simultaneously allocates an efficient\nparameter budget and optimizes task objectives without misalignment.To\neffectively preserve previously acquired knowledge while coordinating with the\ndynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter\nachieves higher average accuracy while using 58.5% fewer parameters on the\nstandard CL benchmark, and maintains its advantages on two larger benchmarks\ncomprising 15 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often suffer from catastrophic forgetting in\ncontinual learning (CL) scenarios, where performance on previously learned\ntasks degrades severely while training on sequentially arriving tasks. Although\npioneering CL approaches using orthogonal subspaces can mitigate task\ninterference, they typically employ fixed budget allocation, neglecting the\nvarying complexity across tasks and layers. Besides, recent budget-adaptive\ntuning methods for LLMs often adopt multi-stage paradigms that decouple\noptimization and budget allocation. Such decoupling results in potential\nmisalignment, which hinders those approaches' practical application in CL\nscenarios. To address these limitations, we propose OA-Adapter, a novel\nparameter-efficient approach for continual learning in LLMs that unifies\ndynamic budget adaptation with orthogonal subspace learning in an end-to-end\ntraining stage. Specifically, OA-Adapter introduces a dynamic bottleneck\ndimension adaptation mechanism that simultaneously allocates an efficient\nparameter budget and optimizes task objectives without misalignment.To\neffectively preserve previously acquired knowledge while coordinating with the\ndynamic budget allocation, orthogonal constraints are applied specifically\nbetween the parameter subspace of the current task and the dynamically\nallocated parameter subspaces of historical tasks. Experimental results on\ncontinual learning benchmarks demonstrate that OA-Adapter outperforms\nstate-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter\nachieves higher average accuracy while using 58.5% fewer parameters on the\nstandard CL benchmark, and maintains its advantages on two larger benchmarks\ncomprising 15 tasks."
                },
                "authors": [
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Wanrou Du"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Miao Pan"
                    },
                    {
                        "name": "Xiaoqi Qin"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Qin"
                },
                "author": "Xiaoqi Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13750v2",
                "updated": "2025-10-16T13:58:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    58,
                    27,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T16:55:56Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    55,
                    56,
                    2,
                    288,
                    0
                ],
                "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation"
                },
                "summary": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment."
                },
                "authors": [
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Vivek Datla"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Ritesh Soni"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Soni"
                },
                "author": "Ritesh Soni",
                "arxiv_comment": "UncertaiNLP at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14686v1",
                "updated": "2025-10-16T13:53:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:53:47Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    53,
                    47,
                    3,
                    289,
                    0
                ],
                "title": "xLLM Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xLLM Technical Report"
                },
                "summary": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM)\ninference framework designed for high-performance, large-scale enterprise-grade\nserving, with deep optimizations for diverse AI accelerators. To address these\nchallenges, xLLM builds a novel decoupled service-engine architecture. At the\nservice layer, xLLM-Service features an intelligent scheduling module that\nefficiently processes multimodal requests and co-locates online and offline\ntasks through unified elastic scheduling to maximize cluster utilization. This\nmodule also relies on a workload-adaptive dynamic Prefill-Decode (PD)\ndisaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation\npolicy designed for multimodal inputs. Furthermore, it incorporates a\ndistributed architecture to provide global KV Cache management and robust\nfault-tolerant capabilities for high availability. At the engine layer,\nxLLM-Engine co-optimizes system and algorithm designs to fully saturate\ncomputing resources. This is achieved through comprehensive multi-layer\nexecution pipeline optimizations, an adaptive graph mode and an xTensor memory\nmanagement. xLLM-Engine also further integrates algorithmic enhancements such\nas optimized speculative decoding and dynamic EPLB, collectively serving to\nsubstantially boost throughput and inference efficiency. Extensive evaluations\ndemonstrate that xLLM delivers significantly superior performance and resource\nefficiency. Under identical TPOT constraints, xLLM achieves throughput up to\n1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while\nmaintaining an average throughput of 1.7x that of MindIE with Deepseek-series\nmodels. xLLM framework is publicly available at\nhttps://github.com/jd-opensource/xllm and\nhttps://github.com/jd-opensource/xllm-service."
                },
                "authors": [
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Xiaoyang Zhao"
                    },
                    {
                        "name": "Xiusheng Lu"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Jun Xiong"
                    },
                    {
                        "name": "Donghe Jin"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Jinrong Guo"
                    },
                    {
                        "name": "Yingxu Deng"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xianzhe Dong"
                    },
                    {
                        "name": "Siqi Wang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Zihan Tang"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Jinguang Liu"
                    },
                    {
                        "name": "Meng Kang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Yunlong Wang"
                    },
                    {
                        "name": "Yiming Liu"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Yichen Zhang"
                    },
                    {
                        "name": "Jinrun Yin"
                    },
                    {
                        "name": "Keyang Zheng"
                    },
                    {
                        "name": "Jiawei Yin"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziyue Wang"
                    },
                    {
                        "name": "Xiaobo Lin"
                    },
                    {
                        "name": "Liangyu Liu"
                    },
                    {
                        "name": "Liwei Lan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Chunhua Peng"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Songcheng Ren"
                    },
                    {
                        "name": "Xuezhu Wang"
                    },
                    {
                        "name": "Yunheng Shen"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Hailong Yang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Guiguang Ding"
                    },
                    {
                        "name": "Ke Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Zhang"
                },
                "author": "Ke Zhang",
                "arxiv_comment": "39 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08236v2",
                "updated": "2025-10-16T13:44:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    44,
                    28,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-09T14:00:40Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    0,
                    40,
                    3,
                    282,
                    0
                ],
                "title": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes\n  in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integral to information\ndissemination and decision-making processes. Given their growing societal\ninfluence, understanding potential biases, particularly within the political\ndomain, is crucial to prevent undue influence on public opinion and democratic\nprocesses. This work investigates political bias and stereotype propagation\nacross eight prominent LLMs using the two-dimensional Political Compass Test\n(PCT). Initially, the PCT is employed to assess the inherent political leanings\nof these models. Subsequently, persona prompting with the PCT is used to\nexplore explicit stereotypes across various social dimensions. In a final step,\nimplicit stereotypes are uncovered by evaluating models with multilingual\nversions of the PCT. Key findings reveal a consistent left-leaning political\nalignment across all investigated models. Furthermore, while the nature and\nextent of stereotypes vary considerably between models, implicit stereotypes\nelicited through language variation are more pronounced than those identified\nvia explicit persona prompting. Interestingly, for most models, implicit and\nexplicit stereotypes show a notable alignment, suggesting a degree of\ntransparency or \"awareness\" regarding their inherent biases. This study\nunderscores the complex interplay of political bias and stereotypes in LLMs."
                },
                "authors": [
                    {
                        "name": "Konrad LÃ¶hr"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael FÃ¤rber"
                    }
                ],
                "author_detail": {
                    "name": "Michael FÃ¤rber"
                },
                "author": "Michael FÃ¤rber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05629v2",
                "updated": "2025-10-16T13:40:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    40,
                    55,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-07T17:59:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification"
                },
                "summary": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Zhou Ziheng"
                    },
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xinyu Ye"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08670v2",
                "updated": "2025-10-16T13:40:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    40,
                    44,
                    3,
                    289,
                    0
                ],
                "published": "2025-01-15T09:04:30Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    9,
                    4,
                    30,
                    2,
                    15,
                    0
                ],
                "title": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Smart Contract Decompiler Output through Fine-grained\n  Dependency Analysis and LLM-facilitated Semantic Recovery"
                },
                "summary": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decompiler is a specialized type of reverse engineering tool extensively\nemployed in program analysis tasks, particularly in program comprehension and\nvulnerability detection. However, current Solidity smart contract decompilers\nface significant limitations in reconstructing the original source code. In\nparticular, the bottleneck of SOTA decompilers lies in inaccurate method\nidentification, incorrect variable type recovery, and missing contract\nattributes. These deficiencies hinder downstream tasks and understanding of the\nprogram logic. To address these challenges, we propose SmartHalo, a new\nframework that enhances decompiler output by combining static analysis (SA) and\nlarge language models (LLM). SmartHalo leverages the complementary strengths of\nSA's accuracy in control and data flow analysis and LLM's capability in\nsemantic prediction. More specifically, \\system{} constructs a new data\nstructure - Dependency Graph (DG), to extract semantic dependencies via static\nanalysis. Then, it takes DG to create prompts for LLM optimization. Finally,\nthe correctness of LLM outputs is validated through symbolic execution and\nformal verification. Evaluation on a dataset consisting of 465 randomly\nselected smart contract methods shows that SmartHalo significantly improves the\nquality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).\nNotably, integrating GPT-4o with SmartHalo further enhances its performance,\nachieving precision rates of 87.39% for method boundaries, 90.39% for variable\ntypes, and 80.65% for contract attributes."
                },
                "authors": [
                    {
                        "name": "Zeqin Liao"
                    },
                    {
                        "name": "Yuhong Nan"
                    },
                    {
                        "name": "Zixu Gao"
                    },
                    {
                        "name": "Henglong Liang"
                    },
                    {
                        "name": "Sicheng Hao"
                    },
                    {
                        "name": "Peifan Reng"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "This is the author version of the article accepted for publication in\n  IEEE Transactions on Software Engineering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04426v3",
                "updated": "2025-10-16T13:38:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    38,
                    8,
                    3,
                    289,
                    0
                ],
                "published": "2025-02-06T18:52:10Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    18,
                    52,
                    10,
                    3,
                    37,
                    0
                ],
                "title": "The simulation of judgment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The simulation of judgment in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly embedded in evaluative\nprocesses, from information filtering to assessing and addressing knowledge\ngaps through explanation and credibility judgments. This raises the need to\nexamine how such evaluations are built, what assumptions they rely on, and how\ntheir strategies diverge from those of humans. We benchmark six LLMs against\nexpert ratings--NewsGuard and Media Bias/Fact Check--and against human\njudgments collected through a controlled experiment. We use news domains purely\nas a controlled benchmark for evaluative tasks, focusing on the underlying\nmechanisms rather than on news classification per se. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand nonexpert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, our findings show consistent differences in the observable criteria\nguiding model evaluations, suggesting that lexical associations and statistical\npriors could influence evaluations in ways that differ from contextual\nreasoning. This reliance is associated with systematic effects: political\nasymmetries and a tendency to confuse linguistic form with epistemic\nreliability--a dynamic we term epistemia, the illusion of knowledge that\nemerges when surface plausibility replaces verification. Indeed, delegating\njudgment to such systems may affect the heuristics underlying evaluative\nprocesses, suggesting a shift from normative reasoning toward pattern-based\napproximation and raising open questions about the role of LLMs in evaluative\nprocesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly embedded in evaluative\nprocesses, from information filtering to assessing and addressing knowledge\ngaps through explanation and credibility judgments. This raises the need to\nexamine how such evaluations are built, what assumptions they rely on, and how\ntheir strategies diverge from those of humans. We benchmark six LLMs against\nexpert ratings--NewsGuard and Media Bias/Fact Check--and against human\njudgments collected through a controlled experiment. We use news domains purely\nas a controlled benchmark for evaluative tasks, focusing on the underlying\nmechanisms rather than on news classification per se. To enable direct\ncomparison, we implement a structured agentic framework in which both models\nand nonexpert participants follow the same evaluation procedure: selecting\ncriteria, retrieving content, and producing justifications. Despite output\nalignment, our findings show consistent differences in the observable criteria\nguiding model evaluations, suggesting that lexical associations and statistical\npriors could influence evaluations in ways that differ from contextual\nreasoning. This reliance is associated with systematic effects: political\nasymmetries and a tendency to confuse linguistic form with epistemic\nreliability--a dynamic we term epistemia, the illusion of knowledge that\nemerges when surface plausibility replaces verification. Indeed, delegating\njudgment to such systems may affect the heuristics underlying evaluative\nprocesses, suggesting a shift from normative reasoning toward pattern-based\napproximation and raising open questions about the role of LLMs in evaluative\nprocesses."
                },
                "authors": [
                    {
                        "name": "Edoardo Loru"
                    },
                    {
                        "name": "Jacopo Nudo"
                    },
                    {
                        "name": "NiccolÃ² Di Marco"
                    },
                    {
                        "name": "Alessandro Santirocchi"
                    },
                    {
                        "name": "Roberto Atzeni"
                    },
                    {
                        "name": "Matteo Cinelli"
                    },
                    {
                        "name": "Vincenzo Cestari"
                    },
                    {
                        "name": "Clelia Rossi-Arnaud"
                    },
                    {
                        "name": "Walter Quattrociocchi"
                    }
                ],
                "author_detail": {
                    "name": "Walter Quattrociocchi"
                },
                "author": "Walter Quattrociocchi",
                "arxiv_doi": "10.1073/pnas.2518443122",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1073/pnas.2518443122",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.04426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Please refer to published version:\n  https://doi.org/10.1073/pnas.2518443122",
                "arxiv_journal_ref": "Proc. Natl. Acad. Sci. U.S.A. 122 (42) e2518443122, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23315v2",
                "updated": "2025-10-16T13:29:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    29,
                    58,
                    3,
                    289,
                    0
                ],
                "published": "2025-07-31T07:47:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    47,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "Analysis of Hyperparameter Optimization Effects on Lightweight Deep\n  Models for Real-Time Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Hyperparameter Optimization Effects on Lightweight Deep\n  Models for Real-Time Image Classification"
                },
                "summary": "Lightweight convolutional and transformer-based networks are increasingly\npreferred for real-time image classification, especially on\nresource-constrained devices. This study evaluates the impact of hyperparameter\noptimization on the accuracy and deployment feasibility of seven modern\nlightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L,\nMobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced\nsubset of 90,000 images from ImageNet-1K. Under standardized training settings,\nthis paper investigates the influence of learning rate schedules, augmentation,\noptimizers, and initialization on model performance. Inference benchmarks are\nperformed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512,\ncapturing latency and throughput in real-time conditions. This work\ndemonstrates that controlled hyperparameter variation significantly alters\nconvergence dynamics in lightweight CNN and transformer backbones, providing\ninsight into stability regions and deployment feasibility in edge artificial\nintelligence. Our results reveal that tuning alone leads to a top-1 accuracy\nimprovement of 1.5 to 3.5 percent over baselines, and select models (e.g.,\nRepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800\nframes per second, making them ideal for edge deployment. This work provides\nreproducible, subset-based insights into lightweight hyperparameter tuning and\nits role in balancing speed and accuracy. The code and logs may be seen at:\nhttps://vineetkumarrakesh.github.io/lcnn-opt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight convolutional and transformer-based networks are increasingly\npreferred for real-time image classification, especially on\nresource-constrained devices. This study evaluates the impact of hyperparameter\noptimization on the accuracy and deployment feasibility of seven modern\nlightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L,\nMobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced\nsubset of 90,000 images from ImageNet-1K. Under standardized training settings,\nthis paper investigates the influence of learning rate schedules, augmentation,\noptimizers, and initialization on model performance. Inference benchmarks are\nperformed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512,\ncapturing latency and throughput in real-time conditions. This work\ndemonstrates that controlled hyperparameter variation significantly alters\nconvergence dynamics in lightweight CNN and transformer backbones, providing\ninsight into stability regions and deployment feasibility in edge artificial\nintelligence. Our results reveal that tuning alone leads to a top-1 accuracy\nimprovement of 1.5 to 3.5 percent over baselines, and select models (e.g.,\nRepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800\nframes per second, making them ideal for edge deployment. This work provides\nreproducible, subset-based insights into lightweight hyperparameter tuning and\nits role in balancing speed and accuracy. The code and logs may be seen at:\nhttps://vineetkumarrakesh.github.io/lcnn-opt"
                },
                "authors": [
                    {
                        "name": "Vineet Kumar Rakesh"
                    },
                    {
                        "name": "Soumya Mazumdar"
                    },
                    {
                        "name": "Tapas Samanta"
                    },
                    {
                        "name": "Hemendra Kumar Pandey"
                    },
                    {
                        "name": "Amitabha Das"
                    }
                ],
                "author_detail": {
                    "name": "Amitabha Das"
                },
                "author": "Amitabha Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14672v1",
                "updated": "2025-10-16T13:29:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    29,
                    2,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:29:02Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    29,
                    2,
                    3,
                    289,
                    0
                ],
                "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning"
                },
                "summary": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, video question answering based on multimodal large language\nmodels (MLLM) has garnered considerable attention, due to the benefits from the\nsubstantial advancements in LLMs. However, these models have a notable\ndeficiency in the domains of video temporal grounding and reasoning, posing\nchallenges to the development of effective real-world video understanding\nsystems. Inspired by how humans use video players to interact with the progress\nbar for video comprehension, we introduce VTimeCoT, a simple yet effective\ntraining-free framework, designed for high-performance video grounding and\nreasoning. The proposed framework incorporates two novel visual tools of the\nprogress bar: a plug-and-play progress bar integration tool and a\nhigh-efficiency highlighting tool. In addition, to address the limitations of\nconventional text-based chain-of-thought (CoT) approaches, we introduce a\nvisuotemporal CoT process that integrates cross-modality reasoning across both\nvideo and text. Our approach demonstrates significant performance improvements\non both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and\nreasoning-based question answering. Finally, we showcase that the proposed\nframework achieves a compositional and interpretable reasoning process. Project\npage: https://vtimecot.github.io"
                },
                "authors": [
                    {
                        "name": "Jinglei Zhang"
                    },
                    {
                        "name": "Yuanfan Guo"
                    },
                    {
                        "name": "Rolandos Alexandros Potamias"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v5",
                "updated": "2025-10-16T13:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    25,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods. Our code is available at\nhttps://github.com/FFY0/AdaKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14665v1",
                "updated": "2025-10-16T13:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    44,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    44,
                    3,
                    289,
                    0
                ],
                "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Hallucinations: The Illusion of Understanding in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming deeply embedded in human\ncommunication and decision-making, yet they inherit the ambiguity, bias, and\nlack of direct access to truth inherent in language itself. While their outputs\nare fluent, emotionally resonant, and coherent, they are generated through\nstatistical prediction rather than grounded reasoning. This creates the risk of\nhallucination, responses that sound convincing but lack factual validity.\nBuilding on Geoffrey Hinton's observation that AI mirrors human intuition\nrather than reasoning, this paper argues that LLMs operationalize System 1\ncognition at scale: fast, associative, and persuasive, but without reflection\nor falsification. To address this, we introduce the Rose-Frame, a\nthree-dimensional framework for diagnosing cognitive and epistemic drift in\nhuman-AI interaction. The three axes are: (i) Map vs. Territory, which\ndistinguishes representations of reality (epistemology) from reality itself\n(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to\nseparate fast, emotional judgments from slow, reflective thinking; and (iii)\nConflict vs. Confirmation, which examines whether ideas are critically tested\nthrough disagreement or simply reinforced through mutual validation. Each\ndimension captures a distinct failure mode, and their combination amplifies\nmisalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.\nInstead, it offers a reflective tool that makes both the model's limitations\nand the user's assumptions visible, enabling more transparent and critically\naware AI deployment. It reframes alignment as cognitive governance: intuition,\nwhether human or artificial, must remain governed by human reason. Only by\nembedding reflective, falsifiable oversight can we align machine fluency with\nhuman understanding."
                },
                "authors": [
                    {
                        "name": "Rikard Rosenbacke"
                    },
                    {
                        "name": "Carl Rosenbacke"
                    },
                    {
                        "name": "Victor Rosenbacke"
                    },
                    {
                        "name": "Martin McKee"
                    }
                ],
                "author_detail": {
                    "name": "Martin McKee"
                },
                "author": "Martin McKee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14664v1",
                "updated": "2025-10-16T13:19:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    7,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:19:07Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    19,
                    7,
                    3,
                    289,
                    0
                ],
                "title": "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality\n  Evaluation"
                },
                "summary": "Generative speech technologies are progressing rapidly, but evaluating the\nperceptual quality of synthetic speech remains a core challenge. Existing\nmethods typically rely on scalar scores or binary decisions, which lack\ninterpretability and generalization across tasks and languages. We present\nSpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs)\nto conduct structured and explanation-based speech quality evaluation. To\nsupport this direction, we introduce SpeechEval, a large-scale dataset\ncontaining 32,207 multilingual speech clips and 128,754 annotations spanning\nfour tasks: quality assessment, pairwise comparison, improvement suggestion,\nand deepfake detection. Based on this resource, we develop SQ-LLM, a\nspeech-quality-aware LLM trained with chain-of-thought reasoning and reward\noptimization to improve capability. Experimental results show that SQ-LLM\ndelivers strong performance across tasks and languages, revealing the potential\nof this paradigm for advancing speech quality evaluation. Relevant resources\nwill be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative speech technologies are progressing rapidly, but evaluating the\nperceptual quality of synthetic speech remains a core challenge. Existing\nmethods typically rely on scalar scores or binary decisions, which lack\ninterpretability and generalization across tasks and languages. We present\nSpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs)\nto conduct structured and explanation-based speech quality evaluation. To\nsupport this direction, we introduce SpeechEval, a large-scale dataset\ncontaining 32,207 multilingual speech clips and 128,754 annotations spanning\nfour tasks: quality assessment, pairwise comparison, improvement suggestion,\nand deepfake detection. Based on this resource, we develop SQ-LLM, a\nspeech-quality-aware LLM trained with chain-of-thought reasoning and reward\noptimization to improve capability. Experimental results show that SQ-LLM\ndelivers strong performance across tasks and languages, revealing the potential\nof this paradigm for advancing speech quality evaluation. Relevant resources\nwill be open-sourced."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jinghua Zhao"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Junyang Chen"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Jinyu Li"
                    },
                    {
                        "name": "Jiaming Zhou"
                    },
                    {
                        "name": "Haoqin Sun"
                    },
                    {
                        "name": "Yan Lu"
                    },
                    {
                        "name": "Yong Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yong Qin"
                },
                "author": "Yong Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14660v1",
                "updated": "2025-10-16T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    15,
                    40,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T13:15:40Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    15,
                    40,
                    3,
                    289,
                    0
                ],
                "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs"
                },
                "summary": "Search augmentation empowers Large Language Models with retrieval\ncapabilities to overcome the limitations imposed by static parameters.\nRecently, Reinforcement Learning leverages tailored reward signals as a viable\ntechnique to enhance LLMs performing tasks involving search. However, existing\nreward modeling for search-augmented LLMs faces several limitations. Rule-based\nrewards, such as Exact Match, are verifiable but fragile to variations in\nexpression and cannot be applied to long-form workloads. In contrast,\ngenerative rewards improve robustness, but designing verifiable and stable\nrewards for long-form workloads in dynamic corpora remains challenging and also\nincurs high computational costs. In this paper, we propose a unified and\nverifiable paradigm, \"nugget-as-rubric\", which treats atomic information points\nas structured evaluation criteria for different search-augmentation workloads.\nShort-form tasks correspond to a single rubric, whereas long-form tasks expand\nto multiple rubrics aligned with the question's information needs. To support\nlong-form settings, we design an automatic rubric construction pipeline based\non query rewriting, which can automatically retrieve passages relevant to each\nquestion and extract rubrics from them, both from static corpora and from\ndynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a\n4B-parameter efficient generative verifier under our proposed verifiable\nparadigm, which is trained via the idea of distillation and a two-stage\nstrategy. Experimental results show that Search-Gen-V achieves strong\nverification accuracy across different workloads, making it a scalable, robust,\nand efficient verifiable reward constructor for search-augmented LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search augmentation empowers Large Language Models with retrieval\ncapabilities to overcome the limitations imposed by static parameters.\nRecently, Reinforcement Learning leverages tailored reward signals as a viable\ntechnique to enhance LLMs performing tasks involving search. However, existing\nreward modeling for search-augmented LLMs faces several limitations. Rule-based\nrewards, such as Exact Match, are verifiable but fragile to variations in\nexpression and cannot be applied to long-form workloads. In contrast,\ngenerative rewards improve robustness, but designing verifiable and stable\nrewards for long-form workloads in dynamic corpora remains challenging and also\nincurs high computational costs. In this paper, we propose a unified and\nverifiable paradigm, \"nugget-as-rubric\", which treats atomic information points\nas structured evaluation criteria for different search-augmentation workloads.\nShort-form tasks correspond to a single rubric, whereas long-form tasks expand\nto multiple rubrics aligned with the question's information needs. To support\nlong-form settings, we design an automatic rubric construction pipeline based\non query rewriting, which can automatically retrieve passages relevant to each\nquestion and extract rubrics from them, both from static corpora and from\ndynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a\n4B-parameter efficient generative verifier under our proposed verifiable\nparadigm, which is trained via the idea of distillation and a two-stage\nstrategy. Experimental results show that Search-Gen-V achieves strong\nverification accuracy across different workloads, making it a scalable, robust,\nand efficient verifiable reward constructor for search-augmented LLMs."
                },
                "authors": [
                    {
                        "name": "Linyue Ma"
                    },
                    {
                        "name": "Yilong Xu"
                    },
                    {
                        "name": "Xiang Long"
                    },
                    {
                        "name": "Zhi Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Zheng"
                },
                "author": "Zhi Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12350v2",
                "updated": "2025-10-16T13:07:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    13,
                    7,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-14T10:07:53Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    10,
                    7,
                    53,
                    1,
                    287,
                    0
                ],
                "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis"
                },
                "summary": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have recently demonstrated advanced capabilities in\nsolving IMO and Putnam problems; yet their role in research mathematics has\nremained fairly limited. The key difficulty is verification: suggested proofs\nmay look plausible, but cannot be trusted without rigorous checking. We present\na framework, called LLM+CAS, and an associated tool, O-Forge, that couples\nfrontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic\nFeedback loop to produce proofs that are both creative and symbolically\nverified. Our focus is on asymptotic inequalities, a topic that often involves\ndifficult proofs and appropriate decomposition of the domain into the \"right\"\nsubdomains. Many mathematicians, including Terry Tao, have suggested that using\nAI tools to find the right decompositions can be very useful for research-level\nasymptotic analysis. In this paper, we show that our framework LLM+CAS turns\nout to be remarkably effective at proposing such decompositions via a\ncombination of a frontier LLM and a CAS. More precisely, we use an LLM to\nsuggest domain decomposition, and a CAS (such as Mathematica) that provides a\nverification of each piece axiomatically. Using this loop, we answer a question\nposed by Terence Tao: whether LLMs coupled with a verifier can be used to help\nprove intricate asymptotic inequalities. More broadly, we show how AI can move\nbeyond contest math towards research-level tools for professional\nmathematicians."
                },
                "authors": [
                    {
                        "name": "Ayush Khaitan"
                    },
                    {
                        "name": "Vijay Ganesh"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Ganesh"
                },
                "author": "Vijay Ganesh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B35, 68W30, 68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14640v2",
                "updated": "2025-10-17T11:18:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    17,
                    11,
                    18,
                    40,
                    4,
                    290,
                    0
                ],
                "published": "2025-10-16T12:54:40Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    54,
                    40,
                    3,
                    289,
                    0
                ],
                "title": "Intent Clustering with Shared Pseudo-Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intent Clustering with Shared Pseudo-Labels"
                },
                "summary": "In this paper, we propose an intuitive, training-free and label-free method\nfor intent clustering that makes minimal assumptions using lightweight and\nopen-source LLMs. Many current approaches rely on commercial LLMs, which are\ncostly, and offer limited transparency. Additionally, their methods often\nexplicitly depend on knowing the number of clusters in advance, which is often\nnot the case in realistic settings. To address these challenges, instead of\nasking the LLM to match similar text directly, we first ask it to generate\npseudo-labels for each text, and then perform multi-label classification in\nthis pseudo-label set for each text. This approach is based on the hypothesis\nthat texts belonging to the same cluster will share more labels, and will\ntherefore be closer when encoded into embeddings. These pseudo-labels are more\nhuman-readable than direct similarity matches. Our evaluation on four benchmark\nsets shows that our approach achieves results comparable to and better than\nrecent baselines, while remaining simple and computationally efficient. Our\nfindings indicate that our method can be applied in low-resource scenarios and\nis stable across multiple models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an intuitive, training-free and label-free method\nfor intent clustering that makes minimal assumptions using lightweight and\nopen-source LLMs. Many current approaches rely on commercial LLMs, which are\ncostly, and offer limited transparency. Additionally, their methods often\nexplicitly depend on knowing the number of clusters in advance, which is often\nnot the case in realistic settings. To address these challenges, instead of\nasking the LLM to match similar text directly, we first ask it to generate\npseudo-labels for each text, and then perform multi-label classification in\nthis pseudo-label set for each text. This approach is based on the hypothesis\nthat texts belonging to the same cluster will share more labels, and will\ntherefore be closer when encoded into embeddings. These pseudo-labels are more\nhuman-readable than direct similarity matches. Our evaluation on four benchmark\nsets shows that our approach achieves results comparable to and better than\nrecent baselines, while remaining simple and computationally efficient. Our\nfindings indicate that our method can be applied in low-resource scenarios and\nis stable across multiple models and datasets."
                },
                "authors": [
                    {
                        "name": "I-Fan Lin"
                    },
                    {
                        "name": "Faegheh Hasibi"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14636v1",
                "updated": "2025-10-16T12:50:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    50,
                    40,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:50:40Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    50,
                    40,
                    3,
                    289,
                    0
                ],
                "title": "Performance of the Prototype Station of the IceCube Surface Array\n  Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of the Prototype Station of the IceCube Surface Array\n  Enhancement"
                },
                "summary": "The prototype station of the Surface Array Enhancement at the IceCube\nNeutrino Observatory has been taking data in its final design since 2023. This\nstation is part of the planned extension within the footprint of the existing\nsurface array, IceTop. One station consists of 8 scintillator detectors, 3\nradio antennas, and a central DAQ. The final upgrade of the scintillation\ndetectors and their firmware at the prototype station has extended the dynamic\nrange and increased the data-taking up-time, thereby expanding the observation\nwindow for air showers. This contribution will discuss the performance of the\nupgraded prototype station after commissioning and its angular resolution\ncapabilities when observing air showers with the scintillation detectors and in\ncoincidence with IceTop. Furthermore, the integration of additional stations\nduring the most recent deployment will be discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prototype station of the Surface Array Enhancement at the IceCube\nNeutrino Observatory has been taking data in its final design since 2023. This\nstation is part of the planned extension within the footprint of the existing\nsurface array, IceTop. One station consists of 8 scintillator detectors, 3\nradio antennas, and a central DAQ. The final upgrade of the scintillation\ndetectors and their firmware at the prototype station has extended the dynamic\nrange and increased the data-taking up-time, thereby expanding the observation\nwindow for air showers. This contribution will discuss the performance of the\nupgraded prototype station after commissioning and its angular resolution\ncapabilities when observing air showers with the scintillation detectors and in\ncoincidence with IceTop. Furthermore, the integration of additional stations\nduring the most recent deployment will be discussed."
                },
                "authors": [
                    {
                        "name": "S. Shefali for the IceCube Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "S. Shefali for the IceCube Collaboration"
                },
                "author": "S. Shefali for the IceCube Collaboration",
                "arxiv_comment": "Presented at the 39th International Cosmic Ray Conference (ICRC2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14635v1",
                "updated": "2025-10-16T12:49:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    49,
                    25,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:49:25Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    49,
                    25,
                    3,
                    289,
                    0
                ],
                "title": "ATGen: Adversarial Reinforcement Learning for Test Case Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ATGen: Adversarial Reinforcement Learning for Test Case Generation"
                },
                "summary": "Large Language Models (LLMs) excel at code generation, yet their outputs\noften contain subtle bugs, for which effective test cases are a critical\nbottleneck. Existing test generation methods, whether based on prompting or\nsupervised fine-tuning, rely on static datasets. This imposes a\n``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover\nnovel or more complex bugs beyond their training scope. To overcome this, we\nintroduce ATGen, a framework that trains a test case generator via adversarial\nreinforcement learning. ATGen pits a test generator against an adversarial code\ngenerator that continuously crafts harder bugs to evade the current policy.\nThis dynamic loop creates a curriculum of increasing difficulty challenging\ncurrent policy. The test generator is optimized via Reinforcement Learning (RL)\nto jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to\nlearn a progressively stronger policy that breaks the fixed-difficulty ceiling\nof static training. Extensive experiments demonstrate that ATGen significantly\noutperforms state-of-the-art baselines. We further validate its practical\nutility, showing it serves as both a more effective filter for Best-of-N\ninference and a higher-quality reward source for training code generation\nmodels. Our work establishes a new, dynamic paradigm for improving the\nreliability of LLM-generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at code generation, yet their outputs\noften contain subtle bugs, for which effective test cases are a critical\nbottleneck. Existing test generation methods, whether based on prompting or\nsupervised fine-tuning, rely on static datasets. This imposes a\n``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover\nnovel or more complex bugs beyond their training scope. To overcome this, we\nintroduce ATGen, a framework that trains a test case generator via adversarial\nreinforcement learning. ATGen pits a test generator against an adversarial code\ngenerator that continuously crafts harder bugs to evade the current policy.\nThis dynamic loop creates a curriculum of increasing difficulty challenging\ncurrent policy. The test generator is optimized via Reinforcement Learning (RL)\nto jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to\nlearn a progressively stronger policy that breaks the fixed-difficulty ceiling\nof static training. Extensive experiments demonstrate that ATGen significantly\noutperforms state-of-the-art baselines. We further validate its practical\nutility, showing it serves as both a more effective filter for Best-of-N\ninference and a higher-quality reward source for training code generation\nmodels. Our work establishes a new, dynamic paradigm for improving the\nreliability of LLM-generated code."
                },
                "authors": [
                    {
                        "name": "Qingyao Li"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14629v1",
                "updated": "2025-10-16T12:40:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    40,
                    48,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:40:48Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    40,
                    48,
                    3,
                    289,
                    0
                ],
                "title": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation\n  Assistant with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation\n  Assistant with LLMs"
                },
                "summary": "The application of Large Language Models (LLMs) in recommender systems faces\nkey challenges in delivering deep personalization and intelligent reasoning,\nespecially for interactive scenarios. Current methods are often constrained by\nlimited context windows and single-turn reasoning, hindering their ability to\ncapture dynamic user preferences and proactively reason over recommendation\ncontexts. To address these limitations, we propose MR.Rec, a novel framework\nthat synergizes memory and reasoning for LLM-based recommendations. To achieve\npersonalization, we develop a comprehensive Retrieval-Augmented Generation\n(RAG) system that efficiently indexes and retrieves relevant external memory to\nenhance LLM personalization capabilities. Furthermore, to enable the synergy\nbetween memory and reasoning, our RAG system goes beyond conventional\nquery-based retrieval by integrating reasoning enhanced memory retrieval.\nFinally, we design a reinforcement learning framework that trains the LLM to\nautonomously learn effective strategies for both memory utilization and\nreasoning refinement. By combining dynamic memory retrieval with adaptive\nreasoning, this approach ensures more accurate, context-aware, and highly\npersonalized recommendations. Extensive experiments demonstrate that MR.Rec\nsignificantly outperforms state-of-the-art baselines across multiple metrics,\nvalidating its efficacy in delivering intelligent and personalized\nrecommendations. We will release code and data upon paper notification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of Large Language Models (LLMs) in recommender systems faces\nkey challenges in delivering deep personalization and intelligent reasoning,\nespecially for interactive scenarios. Current methods are often constrained by\nlimited context windows and single-turn reasoning, hindering their ability to\ncapture dynamic user preferences and proactively reason over recommendation\ncontexts. To address these limitations, we propose MR.Rec, a novel framework\nthat synergizes memory and reasoning for LLM-based recommendations. To achieve\npersonalization, we develop a comprehensive Retrieval-Augmented Generation\n(RAG) system that efficiently indexes and retrieves relevant external memory to\nenhance LLM personalization capabilities. Furthermore, to enable the synergy\nbetween memory and reasoning, our RAG system goes beyond conventional\nquery-based retrieval by integrating reasoning enhanced memory retrieval.\nFinally, we design a reinforcement learning framework that trains the LLM to\nautonomously learn effective strategies for both memory utilization and\nreasoning refinement. By combining dynamic memory retrieval with adaptive\nreasoning, this approach ensures more accurate, context-aware, and highly\npersonalized recommendations. Extensive experiments demonstrate that MR.Rec\nsignificantly outperforms state-of-the-art baselines across multiple metrics,\nvalidating its efficacy in delivering intelligent and personalized\nrecommendations. We will release code and data upon paper notification."
                },
                "authors": [
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Xingchen Zou"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14628v1",
                "updated": "2025-10-16T12:40:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    40,
                    37,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:40:37Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    40,
                    37,
                    3,
                    289,
                    0
                ],
                "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF"
                },
                "summary": "Text-To-Speech synthesis has achieved near-human quality in neutral speech,\nbut emotional expressiveness remains a challenge. Existing methods often rely\non costly emotion annotations or optimize indirect objectives that fail to\ncapture the emotional expressiveness and perceptual naturalness of speech,\nleading to generated speech that is accurate but emotionally flat. To address\nthese challenges, we propose the RLAIF-SPA framework, incorporating a\nReinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic\nSpeech Recognition (ASR) and Large Language Model (LLM) techniques to\nrespectively judge semantic accuracy and prosodic-emotional label alignment as\na direct reward for emotional expressiveness and intelligibility optimization.\nSpecifically, it leverages Prosodic Label Alignment to enhance expressive\nquality by jointly considering semantic accuracy and prosodic-emotional\nalignment along four fine-grained dimensions: Structure, Emotion, Speed, and\nTone. In addition, it incorporates Semantic Accuracy Feedback to ensure the\ngeneration of clear and accurate speech. Experiments on the Libri Speech\ndataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in\nWER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-To-Speech synthesis has achieved near-human quality in neutral speech,\nbut emotional expressiveness remains a challenge. Existing methods often rely\non costly emotion annotations or optimize indirect objectives that fail to\ncapture the emotional expressiveness and perceptual naturalness of speech,\nleading to generated speech that is accurate but emotionally flat. To address\nthese challenges, we propose the RLAIF-SPA framework, incorporating a\nReinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic\nSpeech Recognition (ASR) and Large Language Model (LLM) techniques to\nrespectively judge semantic accuracy and prosodic-emotional label alignment as\na direct reward for emotional expressiveness and intelligibility optimization.\nSpecifically, it leverages Prosodic Label Alignment to enhance expressive\nquality by jointly considering semantic accuracy and prosodic-emotional\nalignment along four fine-grained dimensions: Structure, Emotion, Speed, and\nTone. In addition, it incorporates Semantic Accuracy Feedback to ensure the\ngeneration of clear and accurate speech. Experiments on the Libri Speech\ndataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in\nWER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation."
                },
                "authors": [
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Junxin Wang"
                    },
                    {
                        "name": "Yangfan Du"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Tong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xiao"
                },
                "author": "Tong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14624v1",
                "updated": "2025-10-16T12:34:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    34,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:34:38Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    34,
                    38,
                    3,
                    289,
                    0
                ],
                "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster\n  VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster\n  VLM Inference"
                },
                "summary": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have recently expanded from static image\nunderstanding to video reasoning, but their scalability is fundamentally\nlimited by the quadratic cost of processing dense frame sequences. Long videos\noften exceed the token budget of modern language models, leading to severe\ncontext limitations and latency issues. We introduce Efficient Video Sampling\n(EVS), a simple, plug-and-play method for reducing token redundancy in videos\nby identifying and pruning temporally static patches -- spatial regions that\nremain unchanged across consecutive frames. EVS preserves positional identity,\nrequires no architectural changes or retraining. We show that EVS substantially\nreduces token count while maintaining semantic fidelity, enabling faster\ninference and longer input sequences. Applied at inference time, EVS reduces\nlarge language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal\naccuracy loss. When combined with an uptraining phase using stochastic pruning\nrates, EVS yields models that are robust to varying compression levels and\nretain full performance under aggressive pruning. Extensive experiments\ndemonstrate that EVS consistently improves efficiency-accuracy trade-offs,\nunlocking scalable video-language understanding without sacrificing quality."
                },
                "authors": [
                    {
                        "name": "Natan Bagrov"
                    },
                    {
                        "name": "Eugene Khvedchenia"
                    },
                    {
                        "name": "Borys Tymchenko"
                    },
                    {
                        "name": "Shay Aharon"
                    },
                    {
                        "name": "Lior Kadoch"
                    },
                    {
                        "name": "Tomer Keren"
                    },
                    {
                        "name": "Ofri Masad"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Matthieu Le"
                    },
                    {
                        "name": "Andrew Tao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Tao"
                },
                "author": "Andrew Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12045v2",
                "updated": "2025-10-16T12:33:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    33,
                    13,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-16T13:40:44Z",
                "published_parsed": [
                    2025,
                    8,
                    16,
                    13,
                    40,
                    44,
                    5,
                    228,
                    0
                ],
                "title": "Large Language Models Enable Design of Personalized Nudges across\n  Cultures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Enable Design of Personalized Nudges across\n  Cultures"
                },
                "summary": "Nudge strategies are effective tools for influencing behaviour, but their\nimpact depends on individual preferences. Strategies that work for some\nindividuals may be counterproductive for others. We hypothesize that large\nlanguage models (LLMs) can facilitate the design of individual-specific nudges\nwithout the need for costly and time-intensive behavioural data collection and\nmodelling. To test this, we use LLMs to design personalized decoy-based nudges\ntailored to individual profiles and cultural contexts, aimed at encouraging air\ntravellers to voluntarily offset CO$_2$ emissions from flights. We evaluate\ntheir effectiveness through a large-scale survey experiment ($n=3495$)\nconducted across five countries. Results show that LLM-informed personalized\nnudges are more effective than uniform settings, raising offsetting rates by\n3-7$\\%$ in Germany, Singapore, and the US, though not in China or India. Our\nstudy highlights the potential of LLM as a low-cost testbed for piloting nudge\nstrategies. At the same time, cultural heterogeneity constrains their\ngeneralizability underscoring the need for combining LLM-based simulations with\ntargeted empirical validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nudge strategies are effective tools for influencing behaviour, but their\nimpact depends on individual preferences. Strategies that work for some\nindividuals may be counterproductive for others. We hypothesize that large\nlanguage models (LLMs) can facilitate the design of individual-specific nudges\nwithout the need for costly and time-intensive behavioural data collection and\nmodelling. To test this, we use LLMs to design personalized decoy-based nudges\ntailored to individual profiles and cultural contexts, aimed at encouraging air\ntravellers to voluntarily offset CO$_2$ emissions from flights. We evaluate\ntheir effectiveness through a large-scale survey experiment ($n=3495$)\nconducted across five countries. Results show that LLM-informed personalized\nnudges are more effective than uniform settings, raising offsetting rates by\n3-7$\\%$ in Germany, Singapore, and the US, though not in China or India. Our\nstudy highlights the potential of LLM as a low-cost testbed for piloting nudge\nstrategies. At the same time, cultural heterogeneity constrains their\ngeneralizability underscoring the need for combining LLM-based simulations with\ntargeted empirical validation."
                },
                "authors": [
                    {
                        "name": "Vladimir Maksimenko"
                    },
                    {
                        "name": "Qingyao Xin"
                    },
                    {
                        "name": "Prateek Gupta"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Prateek Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Bansal"
                },
                "author": "Prateek Bansal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15738v2",
                "updated": "2025-10-16T12:31:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    31,
                    18,
                    3,
                    289,
                    0
                ],
                "published": "2025-05-21T16:43:17Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    43,
                    17,
                    2,
                    141,
                    0
                ],
                "title": "Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt\n  Injection Defenses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt\n  Injection Defenses"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications ranging from chatbots to agentic systems, where they are expected\nto process untrusted data and follow trusted instructions. Failure to\ndistinguish between the two poses significant security risks, exploited by\nprompt injection attacks, which inject malicious instructions into the data to\ncontrol model outputs. Model-level defenses have been proposed to mitigate\nprompt injection attacks. These defenses fine-tune LLMs to ignore injected\ninstructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack\nagainst fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy\nCoordinate Gradient (GCG) attack by leveraging intermediate model checkpoints\nproduced during fine-tuning to initialize GCG, with each checkpoint acting as a\nstepping stone for the next one to continuously improve attacks. First, we\ninstantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art\ndefenses in an auditing setup, assuming both (a) full knowledge of the model\ninput and (b) access to intermediate model checkpoints. We show Checkpoint-GCG\nto achieve up to $96\\%$ attack success rate (ASR) against the strongest\ndefense. Second, we relax the first assumption by searching for a universal\nsuffix that would work on unseen inputs, and obtain up to $89.9\\%$ ASR against\nthe strongest defense. Finally, we relax both assumptions by searching for a\nuniversal suffix that would transfer to similar black-box models and defenses,\nachieving an ASR of $63.9\\%$ against a newly released defended model from Meta.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in real-world\napplications ranging from chatbots to agentic systems, where they are expected\nto process untrusted data and follow trusted instructions. Failure to\ndistinguish between the two poses significant security risks, exploited by\nprompt injection attacks, which inject malicious instructions into the data to\ncontrol model outputs. Model-level defenses have been proposed to mitigate\nprompt injection attacks. These defenses fine-tune LLMs to ignore injected\ninstructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack\nagainst fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy\nCoordinate Gradient (GCG) attack by leveraging intermediate model checkpoints\nproduced during fine-tuning to initialize GCG, with each checkpoint acting as a\nstepping stone for the next one to continuously improve attacks. First, we\ninstantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art\ndefenses in an auditing setup, assuming both (a) full knowledge of the model\ninput and (b) access to intermediate model checkpoints. We show Checkpoint-GCG\nto achieve up to $96\\%$ attack success rate (ASR) against the strongest\ndefense. Second, we relax the first assumption by searching for a universal\nsuffix that would work on unseen inputs, and obtain up to $89.9\\%$ ASR against\nthe strongest defense. Finally, we relax both assumptions by searching for a\nuniversal suffix that would transfer to similar black-box models and defenses,\nachieving an ASR of $63.9\\%$ against a newly released defended model from Meta."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Yang"
                    },
                    {
                        "name": "Bozhidar Stevanoski"
                    },
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14620v1",
                "updated": "2025-10-16T12:29:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    29,
                    40,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:29:40Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    29,
                    40,
                    3,
                    289,
                    0
                ],
                "title": "Code-driven Number Sequence Calculation: Enhancing the inductive\n  Reasoning Abilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-driven Number Sequence Calculation: Enhancing the inductive\n  Reasoning Abilities of Large Language Models"
                },
                "summary": "Large language models (LLMs) make remarkable progress in reasoning tasks.\nAmong different reasoning modes, inductive reasoning, due to its better\nalignment with human learning, attracts increasing interest. However, research\non inductive reasoning faces certain challenges. First, existing inductive data\nmostly focuses on superficial regularities while lacking more complex internal\npatterns. Second, current works merely prompt LLMs or finetune on simple\nprompt-response pairs, but do not provide precise thinking processes nor\nimplement difficulty control. Unlike previous work, we address these challenges\nby introducing \\textit{CodeSeq}, a synthetic post-training dataset built from\nnumber sequences. We package number sequences into algorithmic problems to\ndiscover their general terms, defining a general term generation (GTG) task\ncorrespondingly. Our pipeline generates supervised finetuning data by\nreflecting on failed test cases and incorporating iterative corrections,\nthereby teaching LLMs to learn autonomous case generation and self-checking.\nAdditionally, it leverages reinforcement learning with a novel Case-Synergy\nSolvability Scaling Reward based on both solvability, estimated from the\nproblem pass rate, and the success rate of self-directed case generation,\nenabling models to learn more effectively from both successes and failures.\nExperimental results show that the models trained with \\textit{CodeSeq} improve\non various reasoning tasks and can preserve the models' OOD performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) make remarkable progress in reasoning tasks.\nAmong different reasoning modes, inductive reasoning, due to its better\nalignment with human learning, attracts increasing interest. However, research\non inductive reasoning faces certain challenges. First, existing inductive data\nmostly focuses on superficial regularities while lacking more complex internal\npatterns. Second, current works merely prompt LLMs or finetune on simple\nprompt-response pairs, but do not provide precise thinking processes nor\nimplement difficulty control. Unlike previous work, we address these challenges\nby introducing \\textit{CodeSeq}, a synthetic post-training dataset built from\nnumber sequences. We package number sequences into algorithmic problems to\ndiscover their general terms, defining a general term generation (GTG) task\ncorrespondingly. Our pipeline generates supervised finetuning data by\nreflecting on failed test cases and incorporating iterative corrections,\nthereby teaching LLMs to learn autonomous case generation and self-checking.\nAdditionally, it leverages reinforcement learning with a novel Case-Synergy\nSolvability Scaling Reward based on both solvability, estimated from the\nproblem pass rate, and the success rate of self-directed case generation,\nenabling models to learn more effectively from both successes and failures.\nExperimental results show that the models trained with \\textit{CodeSeq} improve\non various reasoning tasks and can preserve the models' OOD performance."
                },
                "authors": [
                    {
                        "name": "Kedi Chen"
                    },
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Xuecheng Wu"
                    },
                    {
                        "name": "Siyuan Zeng"
                    },
                    {
                        "name": "Jianghao Yin"
                    },
                    {
                        "name": "Yinqi Zhang"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Liang He"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13430v2",
                "updated": "2025-10-16T12:22:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    22,
                    13,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T11:25:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    25,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps"
                },
                "summary": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development."
                },
                "authors": [
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Omar Alkaabi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hamza Alobeidli"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13524v2",
                "updated": "2025-10-16T12:21:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    21,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T13:17:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    17,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain"
                },
                "summary": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics"
                },
                "authors": [
                    {
                        "name": "William Flanagan"
                    },
                    {
                        "name": "Mukunda Das"
                    },
                    {
                        "name": "Rajitha Ramanayake"
                    },
                    {
                        "name": "Swanuja Maslekar"
                    },
                    {
                        "name": "Meghana Mangipudi"
                    },
                    {
                        "name": "Joong Ho Choi"
                    },
                    {
                        "name": "Shruti Nair"
                    },
                    {
                        "name": "Shambhavi Bhusan"
                    },
                    {
                        "name": "Sanjana Dulam"
                    },
                    {
                        "name": "Mouni Pendharkar"
                    },
                    {
                        "name": "Nidhi Singh"
                    },
                    {
                        "name": "Vashisth Doshi"
                    },
                    {
                        "name": "Sachi Shah Paresh"
                    }
                ],
                "author_detail": {
                    "name": "Sachi Shah Paresh"
                },
                "author": "Sachi Shah Paresh",
                "arxiv_comment": "NeurIPS 2025 GenAI in Finance Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10597v2",
                "updated": "2025-10-16T12:15:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    15,
                    42,
                    3,
                    289,
                    0
                ],
                "published": "2025-06-12T11:42:40Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    42,
                    40,
                    3,
                    163,
                    0
                ],
                "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Evaluating Jailbreak Guardrails for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety alignments. Guardrails--external defense\nmechanisms that monitor and control LLM interactions--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, provide insights into optimizing their defense mechanisms, and\nexplore their universality across attack types. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety alignments. Guardrails--external defense\nmechanisms that monitor and control LLM interactions--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, provide insights into optimizing their defense mechanisms, and\nexplore their universality across attack types. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails."
                },
                "authors": [
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Zhenlan Ji"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Accepted by IEEE S&P 2026 Cycle 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14599v1",
                "updated": "2025-10-16T12:04:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    4,
                    27,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T12:04:27Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    4,
                    27,
                    3,
                    289,
                    0
                ],
                "title": "JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job\n  Atomization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job\n  Atomization"
                },
                "summary": "The increasing complexity and temporal variability of workloads on\nMIG-enabled GPUs challenge the scalability of traditional centralized\nscheduling. Building upon the SJA concept, this paper introduces JASDA-a novel\nparadigm that extends SJA from a largely centralized scheduling model toward a\nfully decentralized negotiation process. In JASDA, jobs actively generate and\nscore feasible subjobs in response to scheduler-announced execution windows,\nwhile the scheduler performs policy-driven clearing that balances utilization,\nfairness, and temporal responsiveness. This bidirectional, iterative\ninteraction embeds feedback, calibration, and probabilistic safety directly\ninto the scheduling loop, enabling adaptive and transparent decision-making. By\ncoupling principles from auction theory and online optimization with the\ntemporal granularity of GPU workloads, JASDA provides a scalable foundation for\nmarket-aware and fairness-driven resource management-bridging theoretical\nscheduling models with practical deployment in modern MIG-enabled environments\nrelevant to Artificial Intelligence and Agriculture 4.0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity and temporal variability of workloads on\nMIG-enabled GPUs challenge the scalability of traditional centralized\nscheduling. Building upon the SJA concept, this paper introduces JASDA-a novel\nparadigm that extends SJA from a largely centralized scheduling model toward a\nfully decentralized negotiation process. In JASDA, jobs actively generate and\nscore feasible subjobs in response to scheduler-announced execution windows,\nwhile the scheduler performs policy-driven clearing that balances utilization,\nfairness, and temporal responsiveness. This bidirectional, iterative\ninteraction embeds feedback, calibration, and probabilistic safety directly\ninto the scheduling loop, enabling adaptive and transparent decision-making. By\ncoupling principles from auction theory and online optimization with the\ntemporal granularity of GPU workloads, JASDA provides a scalable foundation for\nmarket-aware and fairness-driven resource management-bridging theoretical\nscheduling models with practical deployment in modern MIG-enabled environments\nrelevant to Artificial Intelligence and Agriculture 4.0."
                },
                "authors": [
                    {
                        "name": "Michal Konopa"
                    },
                    {
                        "name": "Jan Fesl"
                    },
                    {
                        "name": "Ladislav Ber Ã¡nek"
                    }
                ],
                "author_detail": {
                    "name": "Ladislav Ber Ã¡nek"
                },
                "arxiv_affiliation": "Faculty of Agriculture and Technology, University of South Bohemia",
                "author": "Ladislav Ber Ã¡nek",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1; C.4; C.1.4; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22544v2",
                "updated": "2025-10-16T12:00:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    12,
                    0,
                    43,
                    3,
                    289,
                    0
                ],
                "published": "2025-09-26T16:20:06Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    16,
                    20,
                    6,
                    4,
                    269,
                    0
                ],
                "title": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection"
                },
                "summary": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video anomaly detection (VAD) is crucial for intelligent surveillance, but a\nsignificant challenge lies in identifying complex anomalies, which are events\ndefined by intricate relationships and temporal dependencies among multiple\nentities rather than by isolated actions. While self-supervised learning (SSL)\nmethods effectively model low-level spatiotemporal patterns, they often\nstruggle to grasp the semantic meaning of these interactions. Conversely, large\nlanguage models (LLMs) offer powerful contextual reasoning but are\ncomputationally expensive for frame-by-frame analysis and lack fine-grained\nspatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly\nDetection, a hybrid SSL-LLM model that combines a multi-task SSL temporal\nanalyzer with LLM validator. The SSL module is built upon an nnFormer backbone\nwhich is a transformer-based model for image segmentation. It is trained with\nmultiple proxy tasks, learns from video frames to identify those suspected of\nanomaly. The selected frames are then forwarded to the LLM, which enriches the\nanalysis with semantic context by applying structured, rule-based reasoning to\nvalidate the presence of anomalies. Experiments on the challenging ComplexVAD\ndataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming\nexisting baselines by 12.5% while reducing LLM computation. We release our\ninteraction anomaly taxonomy, adaptive thresholding protocol, and code to\nfacilitate future research in complex VAD scenarios."
                },
                "authors": [
                    {
                        "name": "Mohammad Mahdi Hemmatyar"
                    },
                    {
                        "name": "Mahdi Jafari"
                    },
                    {
                        "name": "Mohammad Amin Yousefi"
                    },
                    {
                        "name": "Mohammad Reza Nemati"
                    },
                    {
                        "name": "Mobin Azadani"
                    },
                    {
                        "name": "Hamid Reza Rastad"
                    },
                    {
                        "name": "Amirmohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Amirmohammad Akbari"
                },
                "author": "Amirmohammad Akbari",
                "arxiv_comment": "The submission was made prematurely. The authors plan to resubmit\n  under the supervision of the corresponding author",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04430v4",
                "updated": "2025-10-16T11:58:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    58,
                    47,
                    3,
                    289,
                    0
                ],
                "published": "2025-06-04T20:27:17Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    20,
                    27,
                    17,
                    2,
                    155,
                    0
                ],
                "title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized\n  Zero-Order"
                },
                "summary": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM"
                },
                "authors": [
                    {
                        "name": "Egor Petrov"
                    },
                    {
                        "name": "Grigoriy Evseev"
                    },
                    {
                        "name": "Aleksey Antonov"
                    },
                    {
                        "name": "Andrey Veprikov"
                    },
                    {
                        "name": "Nikolay Bushkov"
                    },
                    {
                        "name": "Stanislav Moiseev"
                    },
                    {
                        "name": "Aleksandr Beznosikov"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandr Beznosikov"
                },
                "author": "Aleksandr Beznosikov",
                "arxiv_comment": "26 pages, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07037v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07037v3",
                "updated": "2025-10-16T11:58:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    58,
                    33,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-08T14:04:14Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    14,
                    4,
                    14,
                    2,
                    281,
                    0
                ],
                "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era\n  of Large Language Models"
                },
                "summary": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multilingual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multilingual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/."
                },
                "authors": [
                    {
                        "name": "Rajvee Sheth"
                    },
                    {
                        "name": "Samridhi Raj Sinha"
                    },
                    {
                        "name": "Mahavir Patil"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Mayank Singh"
                    }
                ],
                "author_detail": {
                    "name": "Mayank Singh"
                },
                "author": "Mayank Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07037v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07037v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11401v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11401v3",
                "updated": "2025-10-16T11:55:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    55,
                    56,
                    3,
                    289,
                    0
                ],
                "published": "2025-02-17T03:36:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    3,
                    36,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following the Autoregressive Nature of LLM Embeddings via Compression\n  and Alignment"
                },
                "summary": "A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new trend uses LLMs as dense text encoders via contrastive learning.\nHowever, since LLM embeddings predict the probability distribution of the next\ntoken, they are inherently generative and distributive, conflicting with\ncontrastive learning, which requires embeddings to capture full-text semantics\nand align via cosine similarity. This discrepancy hinders the full utilization\nof LLMs' pre-training capabilities, resulting in inefficient learning. In\nresponse to this issue, we propose AutoRegEmbed, a new contrastive learning\nmethod built on embedding conditional probability distributions, which\nintegrates two core tasks: information compression and conditional distribution\nalignment. The information compression task encodes text into the embedding\nspace, ensuring that the embedding vectors capture global semantics. The\nconditional distribution alignment task focuses on aligning text embeddings\nwith positive samples embeddings by leveraging the conditional distribution of\nembeddings while simultaneously reducing the likelihood of generating negative\nsamples from text embeddings, thereby achieving embedding alignment and\nuniformity. Experimental results demonstrate that our method significantly\noutperforms traditional contrastive learning approaches and achieves\nperformance comparable to state-of-the-art models when using the same amount of\ndata."
                },
                "authors": [
                    {
                        "name": "Jingcheng Deng"
                    },
                    {
                        "name": "Zhongtao Jiang"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Liwei Chen"
                    },
                    {
                        "name": "Kun Xu"
                    },
                    {
                        "name": "Zihao Wei"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11401v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11401v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14591v1",
                "updated": "2025-10-16T11:53:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    53,
                    17,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-16T11:53:17Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    53,
                    17,
                    3,
                    289,
                    0
                ],
                "title": "Just-In-Time Objectives: A General Approach for Specialized AI\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just-In-Time Objectives: A General Approach for Specialized AI\n  Interactions"
                },
                "summary": "Large language models promise a broad set of functions, but when not given a\nspecific objective, they default to milquetoast results such as drafting emails\nlittered with cliches. We demonstrate that inferring the user's in-the-moment\nobjective, then rapidly optimizing for that singular objective, enables LLMs to\nproduce tools, interfaces, and responses that are more responsive and desired.\nWe contribute an architecture for automatically inducing just-in-time\nobjectives by passively observing user behavior, then steering downstream AI\nsystems through generation and evaluation against this objective. Inducing\njust-in-time objectives (e.g., \"Clarify the abstract's research contribution\")\nenables automatic generation of tools, e.g., those that critique a draft based\non relevant HCI methodologies, anticipate related researchers' reactions, or\nsurface ambiguous terminology. In a series of experiments (N=14, N=205) on\nparticipants' own tasks, JIT objectives enable LLM outputs that achieve 66-86%\nwin rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT\nobjectives produce specialized tools unique to each participant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models promise a broad set of functions, but when not given a\nspecific objective, they default to milquetoast results such as drafting emails\nlittered with cliches. We demonstrate that inferring the user's in-the-moment\nobjective, then rapidly optimizing for that singular objective, enables LLMs to\nproduce tools, interfaces, and responses that are more responsive and desired.\nWe contribute an architecture for automatically inducing just-in-time\nobjectives by passively observing user behavior, then steering downstream AI\nsystems through generation and evaluation against this objective. Inducing\njust-in-time objectives (e.g., \"Clarify the abstract's research contribution\")\nenables automatic generation of tools, e.g., those that critique a draft based\non relevant HCI methodologies, anticipate related researchers' reactions, or\nsurface ambiguous terminology. In a series of experiments (N=14, N=205) on\nparticipants' own tasks, JIT objectives enable LLM outputs that achieve 66-86%\nwin rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT\nobjectives produce specialized tools unique to each participant."
                },
                "authors": [
                    {
                        "name": "Michelle S. Lam"
                    },
                    {
                        "name": "Omar Shaikh"
                    },
                    {
                        "name": "Hallie Xu"
                    },
                    {
                        "name": "Alice Guo"
                    },
                    {
                        "name": "Diyi Yang"
                    },
                    {
                        "name": "Jeffrey Heer"
                    },
                    {
                        "name": "James A. Landay"
                    },
                    {
                        "name": "Michael S. Bernstein"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Bernstein"
                },
                "author": "Michael S. Bernstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13193v2",
                "updated": "2025-10-16T11:41:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    41,
                    38,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T06:31:29Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    6,
                    31,
                    29,
                    2,
                    288,
                    0
                ],
                "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient\n  RAG"
                },
                "summary": "Knowledge graphs (KGs), with their structured representation capabilities,\noffer promising avenue for enhancing Retrieval Augmented Generation (RAG)\nsystems, leading to the development of KG-RAG systems. Nevertheless, existing\nmethods often struggle to achieve effective synergy between system\neffectiveness and cost efficiency, leading to neither unsatisfying performance\nnor excessive LLM prompt tokens and inference time. To this end, this paper\nproposes REMINDRAG, which employs an LLM-guided graph traversal featuring node\nexploration, node exploitation, and, most notably, memory replay, to improve\nboth system effectiveness and cost efficiency. Specifically, REMINDRAG\nmemorizes traversal experience within KG edge embeddings, mirroring the way\nLLMs \"memorize\" world knowledge within their parameters, but in a train-free\nmanner. We theoretically and experimentally confirm the effectiveness of\nREMINDRAG, demonstrating its superiority over existing baselines across various\nbenchmark datasets and LLM backbones. Our code is available at\nhttps://github.com/kilgrims/ReMindRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs (KGs), with their structured representation capabilities,\noffer promising avenue for enhancing Retrieval Augmented Generation (RAG)\nsystems, leading to the development of KG-RAG systems. Nevertheless, existing\nmethods often struggle to achieve effective synergy between system\neffectiveness and cost efficiency, leading to neither unsatisfying performance\nnor excessive LLM prompt tokens and inference time. To this end, this paper\nproposes REMINDRAG, which employs an LLM-guided graph traversal featuring node\nexploration, node exploitation, and, most notably, memory replay, to improve\nboth system effectiveness and cost efficiency. Specifically, REMINDRAG\nmemorizes traversal experience within KG edge embeddings, mirroring the way\nLLMs \"memorize\" world knowledge within their parameters, but in a train-free\nmanner. We theoretically and experimentally confirm the effectiveness of\nREMINDRAG, demonstrating its superiority over existing baselines across various\nbenchmark datasets and LLM backbones. Our code is available at\nhttps://github.com/kilgrims/ReMindRAG."
                },
                "authors": [
                    {
                        "name": "Yikuan Hu"
                    },
                    {
                        "name": "Jifeng Zhu"
                    },
                    {
                        "name": "Lanrui Tang"
                    },
                    {
                        "name": "Chen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Huang"
                },
                "author": "Chen Huang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07141v2",
                "updated": "2025-10-16T11:40:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    11,
                    40,
                    29,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-08T15:42:49Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    15,
                    42,
                    49,
                    2,
                    281,
                    0
                ],
                "title": "Comparing Human and Language Models Sentence Processing Difficulties on\n  Complex Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Human and Language Models Sentence Processing Difficulties on\n  Complex Structures"
                },
                "summary": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs."
                },
                "authors": [
                    {
                        "name": "Samuel Joseph Amouyal"
                    },
                    {
                        "name": "Aya Meltzer-Asscher"
                    },
                    {
                        "name": "Jonathan Berant"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Berant"
                },
                "author": "Jonathan Berant",
                "arxiv_comment": "Data and code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]